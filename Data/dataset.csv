text,Title,Chapters
"  Chapter 1 Introduction: What Is the Matter Here? Pei Wang 1 and Ben Goertzel 2 1 Temple University, USA 2 Novamente LLC, USA pei.wang@temple.edu, ben@goertzel.org This chapter provides a general introduction to the volume, giving an overview of the AGI eld and the current need for exploration and clarication of its foundations, and briey summarizing the contents of the various chapters. 1.1 The Matter of Articial General Intelligence Articial General Intelligence (AGI), roughly speaking, refers to AI research and development in which intelligence is understood as a general-purpose capability, not restricted to any narrow collection of problems or domains, and including the ability to broadly generalize to fundamentally new areas [4]. The precise denition of AGI is part of the subject matter of the AGI eld, and different theoretical approaches to AGI may embody different slants on the very concept of AGI.",Theoretical Foundations of Artificial General Intelligence,chapter 1
"In practical terms, though, the various researchers in the eld share a strong common intuition regarding the core concerns of AGI  and how it differs from the narrow AI that currently dominates the AI eld. In the earliest days of AI research, in the middle of the last century, the objective of the eld was to build thinking machines with capacity comparable to that of the human mind [2,6,9]. In the decades following the founding of the AI eld, various attempts arose to attack the problem of human-level articial general intelligence, such as the General Problem Solver [7] and the Fifth Generation Computer Systems [3]. These early attempts failed to reach their original goals, and in the view of most AI researchers, failed to make dramatic conceptual or practical progress toward their goals.",Theoretical Foundations of Artificial General Intelligence,chapter 1
"Based on these experiences, 1      2 Theoretical Foundations of Articial General Intelligence the mainstream of the AI community became wary of overly ambitious research, and turned toward the study of domain-specic problems and individual cognitive functions. Some researchers view this shift as positive, arguing that it brought greater rigor to the AI eld  a typical comment being that it is now more common to build on existing theories than to propose brand new ones, to base claims on rigorous theorems or hard experimental evidence rather than on intuition, and to show relevance to real-world applications rather than toy examples. [8]. However, an alternate view would be that this greater focus on narrow problems and mathematical and experimental results has come at a great cost in terms of conceptual progress and practical achievement. The practical achievements of applied AI in the last decades should not be dismissed lightly, nor should be the progress made in various specialized AI algorithms.",Theoretical Foundations of Artificial General Intelligence,chapter 1
"Yet, ultimately, the mounting corpus of AI theorems and experimental results about narrow domains and specic cognitive processes has not led to any kind of clear progress toward the initial goals of the AI eld. AI as a whole does not show much progress toward its original goal of general-purpose systems, since the eld has become highly fragmented, and it is not easy, if possible at all, to put the parts together to get a coherent system with general intelligence [1]. Outside the mainstream of AI, a small but nontrivial set of researchers has continued to pursue the perspective that intelligence should be treated as a whole. To distinguish their work from the bulk of AI work focused on highly specic problems or cognitive processes (sometimes referred to as Narrow AI), the phrase Articial General Intelligence (AGI) has sometimes been used. There have also been related terms such as Human-Level AI [5].",Theoretical Foundations of Artificial General Intelligence,chapter 1
"The term AGI is meant to stress the general-purpose nature of intelligence  meaning that intelligence is a capacity that can be applied to various (though not necessarily all possible) environments to solve problems (though not necessarily being absolutely correct or optimal). Most AGI researchers believe that general-purpose intelligent systems cannot be obtained by simply bundling special-purpose intelligent systems together, but have to be designed and developed differently [11]. Though AGI projects share many problems and techniques with conventional AI projects, they are conceived, carried out, and evaluated differently. In recent years, the AGI community has signicantly grown, and now has its regular conferences and publications.       Introduction 3 1.2 The Matter of Theoretical Foundation Like all elds of science and technology, AGI relies on a subtle interplay of theory and experiment.",Theoretical Foundations of Artificial General Intelligence,chapter 1
"AGI has an engineering goal, the building of practical systems with a high level of general intelligence; and also a scientic goal, the rigorous understanding of the nature of general intelligence, and its relationship with an intelligent systems internal structures and processes, and the properties of its environment. This volume focuses on the theoretical aspect of AGI, though drawing connections between the theoretical and engineering aspects where this is useful to make the theory clearer. Even for those whose main interest is AGI engineering, AGI theory has multiple values: a good theory enables an engineer and empirical researcher to set objectives, to justify assumptions, to specify roadmaps and milestones, and to direct evaluation and comparison. Some AGI research is founded on theoretical notions in an immediate and transparent way. Other AGI research is centered on system-building, with theory taking a back burner to building things and making them work.",Theoretical Foundations of Artificial General Intelligence,chapter 1
"But every AGI project, no matter how pragmatic and empirical in nature, is ultimately based on some ideas about what intelligence is and how to realize it in artifacts. And it is valuable, as part of the process of shaping and growing an AGI project, that these ideas be claried, justied, and organized into a coherent theory. Many times the theory associated with an AGI project is partially presented in a formal and symbolic form, to reduce the ambiguity and fuzziness in natural languages; but this is not necessarily the case, and purely verbal and conceptual theories may have value also. Some of the theories used in AGI research are inherited from other elds (such as mathematics, psychology, and computer science), and some others are specially invented for AGI. In cases where AGI theories are inherited from other elds, careful adaptations to the context of AGI are often required.",Theoretical Foundations of Artificial General Intelligence,chapter 1
"At the current stage, there is no single widely accepted theory of AGI, which is why this book uses a plural foundations in its title. For any AGI project, the underlying (explicit or implicit) theoretical foundation plays a crucial role, since any limitation or error in the theory will eventually show up in the project, and it is rarely possible to correct a theoretical mistake by a technical remedy. Comparing and evaluating the various competing and complementary theoretical foundations existing in the eld is very important for AGI researchers, as well as for other interested individuals. The existing AGI literature contains many discussions of AGI theory; but these are often highly technical, and they are often wrapped up together with highly specic discussions of system architecture or engineering, or particular application problems.",Theoretical Foundations of Artificial General Intelligence,chapter 1
"We felt it       4 Theoretical Foundations of Articial General Intelligence would be valuable  especially for readers who are not intimately familiar with the AGI eld  to supplement this existing literature with a book providing a broad and relatively accessible perspective on the theoretical foundations of AGI. Rather than writing a volume on our own, and hence inevitably enforcing our own individual perspectives on the eld, we decided to invite a group of respected AGI researchers to write about what they considered as among the most important theoretical issues of the eld, in a language that is comprehensible to readers possessing at least modest scientic background, but not necessarily expertise in the AGI eld. To our delight we received many valuable contributions, which are organized in the following chapters. These chapters cover a wide spectrum of theoretical issues in AGI research.",Theoretical Foundations of Artificial General Intelligence,chapter 1
"In the following overview they are clustered into three groups: the nature of the AGI problem and the objective of AGI research, AGI design methodology and system architecture, and the crucial challenges facing AI research. 1.3 The Matter of Objective In the broadest sense, all works in AI and AGI aim at reproducing or exceeding the general intelligence displayed by the human mind in engineered systems. However, when describing this intelligence using more detailed and accurate words, different researchers effectively specify different objectives for their research [10]. Due to its stress on the general and holistic nature of intelligence, the AGI eld is much less fragmented than the mainstream of AI [1], with many overarching aims and recurring themes binding different AGI research programs together. But even so, substantial differences in various researchers concrete research objectives can still be recognized. The chapter by Nick Cassimatis provides a natural entry to the discussion.",Theoretical Foundations of Artificial General Intelligence,chapter 1
"One key goal of AGI research, in many though not all AGI research paradigms, is to build computer models of human intelligence; and thus, in many respects, AGI is not all that different from what is called cognitive modeling in cognitive science. Cassimatis shows the need for an intelligence science, as well as carefully selected challenge problems that must be solved by modeling the right data. The chapter by Selmer Bringsjord and John Licato addresses the question of how to dene and measure articial general intelligence, via proposing a psychometric paradigm in which AGI systems are evaluated using intelligence tests originally dened for humans. Since these tests have been dened to measure the g factor, which psychologists consider     Introduction 5 a measure of human general intelligence, in a sense this automatically places a focus on general rather than specialized intelligence.",Theoretical Foundations of Artificial General Intelligence,chapter 1
"Though human-level intelligence is a critical milestone in the development of AGI, it may be that the most feasible route to get there is via climbing a ladder of intelligence involving explicitly nonhuman varieties of intelligence, as suggested in the chapter by Sam Adams and Steve Burbeck. The authors describe some interesting capabilities of octopi, comparing them to those of human beings, and argues more broadly that each of the rungs of the ladder of intelligence should be reached before trying a higher level. The chapter by Marcus Hutter represents another alternative to the human-level AGI objective, though this time (crudely speaking) from above rather than below. Hutters Universal Articial Intelligence is a formalization of ideal rational behavior that leads to optimum results in a certain type of environment. This project attempt to capture the essence of intelligence, rather than to duplicate the messy details of the human mind. Even though such an ideal design cannot be directly implemented, it can be approximated in various ways. 1.",Theoretical Foundations of Artificial General Intelligence,chapter 1
"4 The Matter of Approach Just as important as having a clear objective for ones AGI research, is having a workable strategy and methodology for achieving ones goals. Here the difference between AGI and mainstream AI shows clearly: while conventional AI projects focus on domain-specic and problem-specic solutions (sometimes with the hope that they will be somehow eventually connected together to get a whole intelligence), an AGI project often starts with a blueprint of a whole system, attempting to capture intelligence as a whole. Such a blueprint is often called an architecture. The chapter by Itamar Arel proposes a very simple architecture, consisting of a perception module and an actuation module. After all, an AGI system should be able to take proper action in each perceived situation. Both modules use certain (different) types of learning algorithm, so that the system can learn to recognize patterns in various situations, as well as to acquire proper response to each situation.",Theoretical Foundations of Artificial General Intelligence,chapter 1
"Unlike in mainstream AI, here the perception module and the actuation module are designed together; and the two are designed to work together in a manner driven by reinforcement learning. Some other researchers feel the need to introduce more modules into their architectures, following results from psychology and other disciplines. The model introduced in    16 Theoretical Foundations of Articial General Intelligence the chapter by Usef Faghihi and Stan Franklin turns certain existing theories about human cognition into a coherent design for a computer system, which has a list of desired properties. This architecture is more complicated than Arels, which can be both an advantage and a disadvantage. The chapter by Ben Goertzel et al. provides an integrative architecture diagram that summarizes several related cognitive architectures, and a defense of this approach to architectural and paradigmatic integration. It is argued that various AGI architectures, that seem different on the surface, are actually fundamentally conceptually compatible, and differ most dramatically in which parts of cognition they emphasize.",Theoretical Foundations of Artificial General Intelligence,chapter 1
"Stress is laid on the hypothesis that the dynamics of an AGI system must possess cognitive synergy, that is, multiple processes interacting in such a way as to actively aid each other when solving problems. There are also researchers who do not want to design a xed architecture for the system, but stress the importance of letting an AGI system construct and modify its architecture by itself. The chapter by Kris Thrisson advocates a constructivist approach to AI, which does not depend on human designed architectures and programs, but on self-organizing architectures and self-generated code that grow from proper seeds provided by the designer. Just because a system has the ability for self-modication, does not necessarily mean that all the changes it makes will improve its performance. The chapter by Bas Steunebrink and Jrgen Schmidhuber introduces a formal model that reasons about its own programs, and only makes modications that can be proved to be benecial.",Theoretical Foundations of Artificial General Intelligence,chapter 1
"Specied accurately in a symbolic language, this model is theoretically optimal under certain assumptions. 1.5 Challenges at the Heart of the Matter Though AGI differs from mainstream AI in its holistic attitude toward intelligence, the design and development of an AGI system still needs to be carried out step by step, and some of the topics involved are considered to be more important and crucial than the others. Each chapter in this cluster addresses an issue that the author(s) takes to be one, though by no means the only one, major challenge in their research toward AGI. The chapter by Tsvi Achler considers recognition as the foundation of other processes that altogether form intelligence. To meet the general-purpose requirements of AGI, a more exible recognition mechanism is introduced.",Theoretical Foundations of Artificial General Intelligence,chapter 1
"While the majority of current recognition al   1Introduction 7 gorithms are based on a feedforward transformation from an input image to a recognized pattern, the mechanism Achler describes has a bidirectional feedforward-feedback structure, where the systems expectation plays an important role. Creativity is an aspect where computers are still far behind human intelligence. The chapter by Maricarmen Martinez et al. proposes analogy making and theory blending as ways to create new ideas. In this process, problem-solving theories are generalized from a source domain, then applied in a different target domain to solve novel problems. There is evidence showing that such processes indeed happen in human cognition, and are responsible for much of the creativity and generality of intelligence. Contrary to many peoples assumption that intelligence is cold and unemotional, Joscha Bach argues that a model of intelligence must cover emotion and affect, since these play important roles in motivational dynamics and other processes.",Theoretical Foundations of Artificial General Intelligence,chapter 1
"Emotion emerges via the systems appraisal of situations and objects, with respect to the systems needs and desires; and it in turn inuences the systems responses to those situations and objects, as well as its motivations and resource allocation. Consciousness is one of the most mysterious phenomena associated with the human mind. The chapter by Antonio Chella and Riccardo Manzotti concludes that consciousness is necessary for general intelligence, and provides a survey of the existing attempts at producing similar phenomena in computer and robot systems. This study attempts to give some philosophical notions (including consciousness, free will, and experience) functional and constructive interpretations. The difculty of the problem of consciousness partly comes from the fact that it is not only a technical issue, but also a conceptual one. The chapter by Richard Loosemore provides a conceptual analysis of the notion of consciousness, helping us to understand what kind of answer might qualify as a solution to the problem.",Theoretical Foundations of Artificial General Intelligence,chapter 1
"Such a meta-level reection is necessary because if we get the problem wrong, there is little chance to get the solution right. 1.6 Summary This book is not an attempt to settle all the fundamental problems of AGI, but merely to showcase and comprehensibly overview some of the key current theoretical explorations in the eld. Given its stress on the generality and holistic nature of intelligence, AGI arguably has a greater demand for coherent theoretical foundations than narrow AI; and yet, the task      8 Theoretical Foundations of Articial General Intelligence of formulating appropriate theories is harder for AGI than for narrow AI, due to the wider variety of interdependent factors involved. The last chapter by Pei Wang is an attempt to provide common criteria for the analysis, comparison, and evaluation of the competing AGI theories.",Theoretical Foundations of Artificial General Intelligence,chapter 1
"It is proposed that, due to the nature of the eld, a proper theory of intelligence for AGI should be correct according to our knowledge about human intelligence, concrete on how to build intelligent machines, and compact in its theoretical structure and content. Furthermore, these criteria should be balanced against each other. This collection of writings of representative, leading AGI researchers shows that there is still no eld-wide consensus on the accurate specication of the objective and methodology of AGI research. Instead, the eld is more or less held together by a shared attitude toward AI research, which treats the problem of AI as one problem, rather than as a group of loosely related problems, as in mainstream AI. Furthermore, AGI researchers believe that it is possible to directly attack the problem of general intelligence now, rather than to postpone it to a unspecied future time.",Theoretical Foundations of Artificial General Intelligence,chapter 1
"The problems discussed in this book are not the same as those addressed by the traditional AI literatures or in AIs various sibling disciplines. As we have argued previously [11], general-propose AI has its own set of problems, which is neither a subset, nor a superset, of the problems studied in mainstream AI (the latter being exemplied in [8], e.g.). Among the problems of AGI, many are theoretical in nature, and must be solved by theoretical analysis  which in turn, must often be inspired and informed by experimental and engineering work. We hope this book will attract more attention, from both inside and outside the AGI eld, toward the theoretical issues of the eld, so as to accelerate the progress of AGI research  a matter which has tremendous importance, both intellectually and practically, to present-day human beings and our human and articial successors. Bibliography [1] Brachman, R. J. (2006).",Theoretical Foundations of Artificial General Intelligence,chapter 1
"(AA)AI  more than the sum of its parts, 2005 AAAI Presidential Address, AI Magazine 27, 4, pp. 1934. [2] Feigenbaum, E. A. and Feldman, J. (1963). Computers and Thought (McGraw-Hill, New York). [3] Feigenbaum, E. A. and McCorduck, P. (1983). The Fifth Generation: Articial Intelligence and Japans Computer Challenge to the world (Addison-Wesley Publishing Company, Reading, Massachusetts). [4] Goertzel, B. and Pennachin, C. (eds.) (2007). Articial General Intelligence (Springer, New York). [5] McCarthy, J. (2007). From here to human-level AI, Articial Intelligence 171, pp. 11741182.      Bibliography 9 [6] McCarthy, J.",Theoretical Foundations of Artificial General Intelligence,chapter 1
"Minsky, M., Rochester, N. and Shannon, C. (1955). A Proposal for the Dartmouth Summer Research Project on Articial Intelligence, URL: http://www-formal.stanford.edu/jmc/history/dartmouth.html. [7] Newell, A. and Simon, H. A. (1963). GPS, a program that simulates human thought, in E. A. Feigenbaum and J. Feldman (eds.), Computers and Thought (McGraw-Hill, New York), pp. 279293. [8] Russell, S. and Norvig, P. (2010). Articial Intelligence: A Modern Approach, 3rd edn. (Prentice Hall, Upper Saddle River, New Jersey). [9] Turing, A. M. (1950). Computing machinery and intelligence, Mind LIX, pp. 433460. [10] Wang, P. (2008).",Theoretical Foundations of Artificial General Intelligence,chapter 1
"What do you mean by AI, in Proceedings of the First Conference on Articial General Intelligence, pp. 362373. [11] Wang, P. and Goertzel, B. (2007). Introduction: Aspects of articial general intelligence, in B. Goertzel and P. Wang (eds.), Advance of Articial General Intelligence (IOS Press, Amsterdam), pp. 116.   ",Theoretical Foundations of Artificial General Intelligence,chapter 1
"  Chapter 2 Articial Intelligence and Cognitive Modeling Have the Same Problem Nicholas L. Cassimatis Department of Cognitive Science, Rensselaer Polytechnic Institute, 108 Carnegie, 110 8th St. Troy, NY 12180 cassin@rpi.edu Cognitive modelers attempting to explain human intelligence share a puzzle with articial intelligence researchers aiming to create computers that exhibit human-level intelligence: how can a system composed of relatively unintelligent parts (such as neurons or transistors) behave intelligently? I argue that although cognitive science has made signicant progress towards many of its goals, that solving the puzzle of intelligence requires special standards and methods in addition to those already employed in cognitive science. To promote such research, I suggest creating a subeld within cognitive science called intelligence science and propose some guidelines for research addressing the intelligence puzzle. 2.",Theoretical Foundations of Artificial General Intelligence,chapter 2
"1 The Intelligence Problem Cognitive scientists attempting to fully understand human cognition share a puzzle with articial intelligence researchers aiming to create computers that exhibit human-level intelligence: how can a system composed of relatively unintelligent parts (say, neurons or transistors) behave intelligently? 2.1.1 Naming the problem I will call the problem of understanding how unintelligent components can combine to generate human-level intelligence the intelligence problem; the endeavor to understand how the human brain embodies a solution to this problem understanding human intelligence; and the project of making computers with human-level intelligence human-level articial intelligence. 11      12 Theoretical Foundations of Articial General Intelligence When I say that a system exhibits human-level intelligence, I mean that it can deal with the same set of situations that a human can with the same level of competence. For example, I will say a system is a human-level conversationalist to the extent that it can have the same kinds of conversations as a typical human.",Theoretical Foundations of Artificial General Intelligence,chapter 2
"A caveat to this is that articial intelligence systems may not be able to perform in some situations, not for reasons of their programming, but because of issues related to their physical manifestation. For example, it would be difcult for a machine without hand gestures and facial expressions to converse as well as a human in many situations because hand gestures and facial expressions are so important to many conversations. In the long term, it may be necessary therefore to sort out exactly which situations matter and which do not. However, the current abilities of articial systems are so far away from human-level that resolving this issue can generally be postponed for some time. One point that does follow from these reections, though, is the inadequacy of the Turing Test. Just as the invention of the airplane was an advance in articial ight without convincing a single person that it was a bird, it is often irrelevant whether a major step-step towards human-intelligence cons observers into believing a computer is a human.",Theoretical Foundations of Artificial General Intelligence,chapter 2
"2.1.2 Why the Intelligence Problem is Important Why is the human-level intelligence problem important to cognitive science? The theoretical interest is that human intelligence poses a problem for a naturalistic worldview insofar as our best theories about the laws governing the behavior of the physical world posit processes that do not include creative problems solving, purposeful behavior and other features of human-level cognition. Therefore, not understanding how the relatively simple and unintelligent mechanisms of atoms and molecules combine to create intelligent behavior is a major challenge for a naturalistic world view (upon which much cognitive science is based). Perhaps it is the last major challenge. Surmounting the human-level intelligence problem also has enormous technological benets which are obvious enough. 2.1.3 The State of the Science For these reasons, understanding how the human brain embodies a solution to the human-level intelligence problem is an important goal of cognitive science. At least at rst glance, we are quite far from achieving this goal.",Theoretical Foundations of Artificial General Intelligence,chapter 2
"There are no cognitive models that can, for example, fully understand language or solve problems that are simple for a young child. This paper evaluates the promise of applying existing methods and standards in      Articial Intelligence and Cognitive Modeling Have the Same Problem 13 cognitive science to solve this problem and ultimately proposes establishing a new subeld within cognitive science, called Intelligence Science1, and outlines some guiding principles for that eld. Before discussing how effective the methods and standards of cognitive science are in solving the intelligence problem, it is helpful to list some of the problems or questions intelligence science must answer. The elements of this list are not original (see (Cassimatis, 2010) and (Shanahan, 1997)) or exhaustive.",Theoretical Foundations of Artificial General Intelligence,chapter 2
"They are merely illustrative examples: Qualication problem How does the mind retrieve or infer in so short a time the exceptions to its knowledge? For example, a hill symbol on a map means there is a hill in the corresponding location in the real world except if: the mapmaker was deceptive, the hill was leveled during real estate development after the map was made, or the map is of shifting sand dunes. Even the exceptions have exceptions. The sand dunes could be part of a historical site and be carefully preserved or the map could be based on constantly updated satellite images. In these exceptions to the exceptions, a hill symbol does mean there is a hill there now. It is impossible to have foreseen or been taught all these exceptions in advance, yet we recognize them as exceptions almost instantly.",Theoretical Foundations of Artificial General Intelligence,chapter 2
"Relevance problem Of the enormous amount of knowledge people have, how do they manage to retrieve the relevant aspects of it, often in less than a second, to sort from many of the possible interpretations of a verbal utterance or perceived set of events? Integration problem How does the mind solve problems that require, say, probabilistic, memory-based and logical inferences when the best current models of each form of inference are based on such different computational methods? Is it merely a matter of time before cognitive science as it is currently practiced answers questions like these or will it require new methods and standards to achieve the intelligence problem? 2.2 Existing Methods and Standards are not Sufcient Historically, AI and cognitive science were driven in part by the goal of understanding and engineering human-level intelligence. There are many goals in cognitive science and, although momentous for several reasons, human-level intelligence is just one of them.",Theoretical Foundations of Artificial General Intelligence,chapter 2
"Some other goals are to generate models or theories that predict and explain empirical data, 11Obviously, for lack of a better name.      14 Theoretical Foundations of Articial General Intelligence to develop formal theories to predict human grammatically judgments and to associate certain kinds of cognitive processes with brain regions. Methods used today in cognitive science are very successful at achieving these goals and show every indication of continuing to do so. In this paper, I argue that these methods are not adequate to the task of understanding human-level intelligence. Put another way, it is possible to do good research by the current standards and goals of cognitive science and still not make much progress towards understanding human intelligence. Just to underline the point, the goal of this paper is not to argue that cognitive science is on the wrong track, but that despite great overall success on many of its goals, progress towards one of its goals, understanding human-level intelligence, requires methodological innovation. 2.2.",Theoretical Foundations of Artificial General Intelligence,chapter 2
"1 Formal linguistics The goal of many formal grammarians is to create a formal theory that predicts whether a given set of sentences is judged by people to be grammatical or not. Within this framework, whether elements of the theory correspond to a mechanism humans use to understand language is generally not a major issue. For example, at various times during the development of Chomsky and his students formal syntax, their grammar generated enormous numbers of syntactic trees and relied on grammatical principles to rule out ungrammatical trees. These researchers never considered it very relevant to criticize their framework by arguing that it was implausible to suppose that humans could generate and sort through this many trees in the second or two it takes them to understand most sentences. That was the province of what they call performance (the mechanisms the mind uses) not competence (what the mind, in some sense, knows, independent of how it uses this knowledge).",Theoretical Foundations of Artificial General Intelligence,chapter 2
"It is possible therefore to do great linguistics without addressing the computational problems (e.g. the relevance problem from the last section) involved in human-level language use. 2.2.2 Neuroscience The eld of neuroscience is so vast that it is difcult to even pretend to discuss it in total. I will conne my remarks to the two most relevant subelds of neuroscience. First, cognitive neuroscience is probably the subeld that most closely addresses mechanisms relevant to understanding human intelligence. What often counts as a result in this eld is a demonstration that certain regions of the brain are active during certain forms of cognition.      Articial Intelligence and Cognitive Modeling Have the Same Problem 15 A simplistic, but not wholly inaccurate way of describing how this methodology would apply to understanding intelligence would be to say that the eld is more concerned with what parts of the brain embody a solution to the intelligence problem, not how they actually solve the problem.",Theoretical Foundations of Artificial General Intelligence,chapter 2
"It is thus possible to be a highly successful cognitive neuroscientist without making progress towards solving the intelligence problem. Computational neuroscience is concerned with explaining complex computation in terms of the interaction of less complex parts (i.e., neurons) obviously relevant to this discussion. Much of what I say about cognitive modeling below also applies to computational neuroscience. 2.2.3 Articial intelligence An important aim of this paper is that cognitive sciences attempt to solve the intelligence problem is also an AI project and in later sections I will describe how this has and can still help cognitive science. There are, however, some ways AI practice can distract from that aim, too. Much AI research has been driven in part by at least one of these two goals. (1) A formal or empirical demonstration that an algorithm is consistent with, approximates, or converges on some normative standard.",Theoretical Foundations of Artificial General Intelligence,chapter 2
"Examples include proving that a Bayes network belief propagation algorithm converges on a probability distribution dictated by probability theory or proving that a theorem prover is sound and complete with respect to a semantics for some logic. Although there are many theoretical and practical reasons for seeking these results (I would like nuclear power plant software to be correct as much as anyone), they do not necessarily constitute progress towards solving the intelligence problem. For example, establishing that a Bayes Network belief propagation algorithm converges relatively quickly towards a normatively correct probability distribution given observed states of the world does not in any way indicate that solving such problems is part of human-level intelligence, nor is there any professional incentive or standard requiring researchers to argue for this. There is in fact extensive evidence that humans are not normatively correct reasoners. It may even be that some aws in human reasoning are a tradeoff required of any computational system that solves the problems humans do.",Theoretical Foundations of Artificial General Intelligence,chapter 2
"(2) Demonstrating with respect to some metric that an algorithm or system is faster, consumes fewer resources and/or is more accurate than some alternative(s). As with proving theorems, one can derive great professional mileage creating a more accurate part of speech      16 Theoretical Foundations of Articial General Intelligence tagger or faster STRIPS planner without needing to demonstrate in any way that their solution is consistent with or contributes to the goal of achieving human-level intelligence. 2.2.4 Experimental psychology Cognitive psychologists generally develop theories about how some cognitive process operates and run experiments to conrm these theories. There is nothing specically in this methodology that focuses the eld on solving the intelligence problem. The elds standards mainly regard the accuracy and precision of theories, not the level of intelligence they help explain.",Theoretical Foundations of Artificial General Intelligence,chapter 2
"A set of experiments discovering and explaining a surprising new phenomenon in (mammalian-level) place memory in humans will typically receive more plaudits than another humdrum experiment in high-level human reasoning. To the extent that the goal of the eld is solely to nd accurate theories of cognitive processes, this makes sense. But it also illustrates the lack of an impetuous towards understanding human-level intelligence. In addition to this point, many of Newells (Newell, 1973) themes apply to the project of understanding human-level intelligence with experimental psychology alone and will not be repeated here. A subeld of cognitive psychology, cognitive modeling, does, at its best, avoid many of the mistakes Newell cautions against and I believe understanding human cognition is ultimately a cognitive modeling problem. I will therefore address cognitive modeling extensively in the rest of this paper. 2.3 Cognitive Modeling: The Model Fit Imperative Cognitive modeling is indispensable to the project of understanding human-level intelligence.",Theoretical Foundations of Artificial General Intelligence,chapter 2
"Ultimately, you cannot say for sure that you have understood how the human brain embodies a solution to the intelligence problem unless you have (1) a computational model that behaves as intelligently as a human and (2) some way of knowing that the mechanisms of that model, or at least its behavior, reect what is going on in humans. Creating computer models to behave like humans and showing that the models mechanisms at some level correspond to mechanism underlying human cognition is a big part of what most cognitive modelers aim to do today. Understanding how the human brain embodies a solution to the intelligence problem is thus in part a cognitive modeling problem. This section describes why I think some of the practices and standards of the cognitive modeling community, while being well-suited for understanding many aspects of cognition,      Articial Intelligence and Cognitive Modeling Have the Same Problem 17 are not sufcient to, and sometimes even impede progress towards, understanding humanlevel intelligence.",Theoretical Foundations of Artificial General Intelligence,chapter 2
"The main approach to modeling today is to create a model of human cognition in a task that ts existing data regarding their behavior in that task and, ideally, predicts behavior in other versions of the task or other tasks altogether. When a single model with a few parameters predicts behavior in many variations of a task or in many different tasks, that is good evidence that the mechanisms posited by the model correspond, at least approximately, to actual mechanisms of human cognition. I will call the drive to do this kind of work the model t imperative. What this approach does not guarantee is that the mechanisms uncovered are important to understanding human-level intelligence. Nor does it do impel researchers to nd important problems or mechanisms that have not yet been addressed, but which are key to understanding human-level intelligence. An analogy with understanding and synthesizing ight will illustrate these points2.",Theoretical Foundations of Artificial General Intelligence,chapter 2
"Let us call the project of understanding birds aviary science; the project of creating computational models of birds aviary modeling and the project of making machines that y articial ight. We call the problem of how a system that is composed of parts that individually succumb to gravity can combine to defy gravity the ight problem; and we call the project of understanding how birds embody a solution to this problem understanding bird ight. You can clearly do great aviary science, i.e., work that advances the understanding of birds, without addressing the ight problem. You can create predictive models of bird mating patterns that can tell you something about how birds are constructed, but they will tell you nothing about how birds manage to y. You can create models that predict the apping rate of a birds wings and how that varies with the birds velocity, its mass, etc. While this work studies something related to bird ight, it does not give you any idea of how birds actually manage to y.",Theoretical Foundations of Artificial General Intelligence,chapter 2
"Thus, just because aviary science and aviary modeling are good at understanding many aspects of birds, it does not mean they are anywhere near understanding bird ight. If the only standard of their eld is to develop predictive models of bird behavior, they can operate with great success without ever understanding how birds solve the ight problem and manage to y. I suggest that the model t imperative in cognitive modeling alone is about as likely to lead to an understanding of human intelligence as it would be likely to drive aviary science towards understanding how birds y. It is possible to collect data about human cognition, 2I have been told that David Marr has also made an analogy between cognitive science and aeronautics, but I have been unable to nd the reference.      18 Theoretical Foundations of Articial General Intelligence build ne models that t the data and accurately predict new observations  it is possible to do all this without actually helping to understand human intelligence.",Theoretical Foundations of Artificial General Intelligence,chapter 2
"Two examples of what I consider the best cognitive modeling I know of illustrate this point. (Lewis & Vasishth, 2005) have developed a great model of some mechanisms involved in sentence understanding, but this and a dozen more ne pieces of cognitive modeling could be done and we would still not have a much better idea of how people actually mange to solve all of the inferential problems in having a conversation, how they sort from among all the various interpretations of a sentence, how they manage to ll in information not literally appearing in a sentence to understand the speakers intent. Likewise, Andersons (Anderson, 2005) work modeling brain activity during algebraic problem solving is a big advance in conrming that specic mechanisms in ACT-R models of cognition actually reect real, identiable, brain mechanisms.",Theoretical Foundations of Artificial General Intelligence,chapter 2
"But, as Anderson himself claimed3, these models only shed light on behavior where there is a preordained set of steps to take, not where people actually have to intelligently gure out a solution to the problem on their own. The point of these examples is not that they are failures. These projects are great successes. They actually achieved the goals of the researchers involved and the cognitive modeling community. That they did so without greatly advancing the project of understanding human intelligence is the point. The model t imperative is geared towards understanding cognition, but not specically towards making sure that human-level intelligence is part of the cognition we understand. To put the matter more concretely, there is nothing about the model t imperative that forces, say, someone making a cognitive model of memory to gure out how their model explains how humans solve the qualication and relevance problems.",Theoretical Foundations of Artificial General Intelligence,chapter 2
"When ones goal is to conrm that a model of a cognitive process actually reects how the mind implements that process, the model t imperative can be very useful. When one has the additional goal of explaining human-level intelligence, then some additional standard is necessary to show that this model is powerful enough to explain human-level performance. Further, I suggest that the model t imperative can actually impeded progress towards understanding human intelligence. Extending the analogy with the ight problem will help illustrate this point. Let us say the Wright Brothers decided for whatever reason to subject themselves to the standards of our hypothetical aviary modeling community. Their initial plane at Kitty Hawk was not based on detailed data on bird ight and made no predictions about it. Not only could their plane not predict bird wing apping frequencies, its wings 3In a talk at RPI.      Articial Intelligence and Cognitive Modeling Have the Same Problem 19 did not ap at all.",Theoretical Foundations of Artificial General Intelligence,chapter 2
"Thus, while perhaps a technological marvel, their plane was not much of an achievement by the aviary modeling communitys model t imperative. If they and the rest of that community had instead decided to measure bird wing apping rates and create a plane whose wings apped, they may have gone through a multi-decade diversion into understanding all the factors that contribute to wing apping rates (not to mention the engineering challenge of making plane whose wings aps) before they got back to the nub of the problem, to discover the aerodynamic principles and control structures that can enable ight and thereby solve the ight problem. The Wright Flyer demonstrated that these principles were enough to generate ight. Without it, we would not be condent that what we know about bird ight is enough to fully explain how they y. Thus, by adhering to the model t imperative, aviary science would have taken a lot longer to solve the ight problem in birds.",Theoretical Foundations of Artificial General Intelligence,chapter 2
"I suggest that, just as it would in aviary science, the model t imperative can retard progress towards understanding how the human brain embodies a solution to the intelligence problem. There are several reasons for this, which an example will illustrate. Imagine that someone has created a system that was able to have productive conversations about, say, managing ones schedule. The system incorporates new information and answer questions as good as a human assistant can. When it is uncertain about a statement or question it can engage in a dialog to correct the situation. Such a system would be a tremendous advance in solving the intelligence problem. The researchers who designed it would have had to nd a way, which has so far eluded cognitive science and AI researchers, to integrate multiple forms of information (acoustic, syntactic, semantic, social, etc.) within milliseconds to sort through the many ambiguous and incomplete utterance people make.",Theoretical Foundations of Artificial General Intelligence,chapter 2
"Of the millions of pieces of knowledge about this task, about the conversants and about whatever the conversants could refer to, the system must nd just the right knowledge, again, within a fraction of a second. No AI researchers have to this point been able to solve these problems. Cognitive scientists have not determined how people solve these problems in actual conversation. Thus, this work is very likely to contain some new, very powerful ideas that would help AI and cognitive science greatly.",Theoretical Foundations of Artificial General Intelligence,chapter 2
"Would we seriously tell these researchers that their work is not progress towards understanding the mind because their systems reaction times or error rates (for example) do not quite match up with those of people in such conversations? If so, and these researchers for some reason wanted our approval, what would it have meant for their research? Would they have for each component of their model run experiments to collect data about that      20 Theoretical Foundations of Articial General Intelligence component and calibrate the component to that data? What if their system had dozens of components, would they have had to spend years running these studies? If so, how would they have had the condence that the set of components they were studying was important to human-level conversation and that they were not leaving out components whose importance they did not initially anticipate? Thus, the data t model of research would either have forced these researchers to go down a long experimental path that they had little condence would address the right issues or they would have had to postpone",Theoretical Foundations of Artificial General Intelligence,chapter 2
"announcing, getting credit for and disseminating to the community the ideas underlying their system. For all these reasons, I conclude that the model t imperative in cognitive modeling does not adequately drive the eld towards achieving an understanding of human intelligence and that it can even potentially impede progress towards that goal. Does all this mean that cognitive science is somehow exceptional, that in every other part of science, the notion of creating a model, tting it to known data and accurately predicting new observations does not apply to understanding human-level intelligence? Not at all. There are different levels of detail and granularity in data. Most cognitive modeling involves tasks where there is more than one possible computer program known that can perform in that task. For example, the problem of solving algebraic equations can be achieved by many kinds of computer programs (e.g., Mathematica and production systems).",Theoretical Foundations of Artificial General Intelligence,chapter 2
"The task in that community is to see which program the brain uses and to select a program that exhibits the same reaction times and error rates as humans is a good way to go about this. However, in the case of human-level intelligence, there are no known programs that exhibit human-level intelligence. Thus, before we can get to the level of detail of traditional cognitive modeling, that is, before we can worry about tting data at the reaction time and error rate level of detail, we need to explain and predict the most fundamental datum: people are intelligent. Once we have a model that explains this, we can t the next level of detail and know that the mechanisms whose existence we are conrming are powerful enough to explain human intelligence. Creating models that predict that people are intelligent means writing computer programs that behave intelligently. This is also a goal of articial intelligence. Understanding human intelligence is therefore a kind of AI problem.",Theoretical Foundations of Artificial General Intelligence,chapter 2
"     Articial Intelligence and Cognitive Modeling Have the Same Problem 21 2.4 Articial Intelligence and Cognitive Modeling Can Help Each Other I have so far argued that existing standards and practices in the cognitive sciences do not adequately drive the eld towards understanding human intelligence. The main problems are that (1) each elds standards make it possible to reward work that is not highly relevant to understanding human intelligence; (2) there is nothing in these standards to encourage researchers to discover each elds gaps in its explanation of human intelligence; and (3) that these standards can actually make it difcult for signicant advances towards understanding human-intelligence to gain support and recognition. This section suggests some guidelines for cognitive science research into human intelligence. Understanding human-intelligence should be its own subeld Research towards understanding human intelligence needs to be its own subeld, intelligence science, within cognitive science. It needs its own scientic standards and funding mechanisms.",Theoretical Foundations of Artificial General Intelligence,chapter 2
"This is not to say that the other cognitive sciences are not important for understanding human intelligence; they are in fact indispensable. However, it will always be easier to prove theorems, t reaction time data, rene formal grammars or measure brain activity if solving the intelligence problem is not a major concern. Researchers in an environment where those are the principal standards will always be at a disadvantage professionally if they are also trying to solve the intelligence problem. Unless there is a eld that specically demands and rewards research that makes progress towards understanding how the brain solves the intelligence problem, it will normally be, at least from a professional point of view, more prudent to tackle another problem. Just as it is impossible to seriously propose a comprehensive grammatically theory without addressing verb use, we need a eld where it is impossible to propose a comprehensive theory of cognition or cognitive architecture without at least addressing the qualication, relevance, integration and other problems of human-level intelligence.",Theoretical Foundations of Artificial General Intelligence,chapter 2
"Model the right data I argued earlier and elsewhere (Cassimatis et al., 2008) that the most important datum for intelligence scientists to model is that humans are intelligent. With respect to the human-level intelligence problem, for example, to worry about whether, say, language learning follows a power or logarithmic law before actually discovering how the learning is even possible is akin to trying to model bird ap frequency before understanding how wings contribute to ight. The goal of building a model that behaves intelligently, instead of merely modeling mechanisms such as memory and attention implicated in intelligent cognition, assures that      22 Theoretical Foundations of Articial General Intelligence the eld addresses the hard problems involved in solving the intelligence problem. It is hard to avoid a hard problem or ignore an important mechanisms if, say, it is critical to humanlevel physical cognition and building a system that makes the same physical inferences that humans can is key to being published or getting a grant renewed.",Theoretical Foundations of Artificial General Intelligence,chapter 2
"A signicant part of motivating and evaluating a research project in intelligence science should be its relevance for (making progress towards) answering problems such as the qualication, relevance and integration problems. Take AI Seriously Since there are zero candidate cognitive models that exhibit humanlevel intelligence, researchers in intelligence science are in the same position as AI researchers aiming for human-level AI: they are both in need of and searching for computational mechanisms that exhibit a human-level of intelligence. Further, the history of AI conrms its relevance to cognitive science. Before AI many philosophers and psychologists did not trust themselves or their colleagues to posit internal mental representations without implicitly smuggling in some form of mysticism or homunculus. On a technical level, search, neural networks, Bayesian networks, production rules, etc. were all in part ideas developed by AI researchers but which play an important role in cognitive modeling today. Chess-playing programs are often used as examples of how AI can succeed with bruteforce methods that do not illuminate human intelligence.",Theoretical Foundations of Artificial General Intelligence,chapter 2
"Note, however, that chess programs are very narrow in their functionality. They only play chess. Humans can play many forms of games and can learn to play these rather quickly. Humans can draw on skills in playing one game to play another. If the next goal after making computer programs chess masters was not to make them grandmasters, but to make them learn, play new games and transfer their knowledge to other games, brute force methods would not have been sufcient and researchers would have had to develop new ideas, many of which would probably bear on human-level intelligence. Have a success Many AI researchers have retreated from trying to achieve human-level AI. The lesson many have taken from this is that one should work on more tractable problems or more practical applications. This attitude is tantamount to surrendering the goal of solving the human intelligence problem in our lifetimes. The eld needs a success to show that real progress is capable soon.",Theoretical Foundations of Artificial General Intelligence,chapter 2
"One obstacle to such a success is that the bar, especially in AI, has been raised so high that anything short of an outright demonstration of full human-level AI is considered by many to be hype. For a merely very important advance      Articial Intelligence and Cognitive Modeling Have the Same Problem 23 towards human-level intelligence that has no immediate application, there is no good way to undeniably conrm that importance. We thus need metrics that push the state of the art but are at the same time realistic. Develop realistic metrics Developing realistic methods for measuring a systems intelligence would make it possible to conrm that the ideas underlying it are an important part of solving the intelligence problem. Such metrics would also increase condence in the prospects of intelligence science enabling quicker demonstrations of progress. My work on a model of physical cognition has illustrated the value of such metrics (Cassimatis, in press).",Theoretical Foundations of Artificial General Intelligence,chapter 2
"I have so far tested this model by presenting it with sequences of partially occluded physical events that I have partly borrowed from the developmental psychology literature and have partly crafted myself. My strategy has been to continually nd new classes of scenarios that require different forms of reasoning (e.g., probabilistic, logical, defeasible, etc.) and update my model so that it could reason about each class of scenarios. Using supercially simple physical reasoning problems in this way has had several properties that illustrate the value of the right metric. Difculty Challenge problems should be difcult enough so that a solution to them requires a signicant advance in the level of intelligence it is possible to model. Human-level intelligence in the physical cognition domain requires advances towards understanding the frame problem, defeasible reasoning and how to integrate perpetual and cognitive models based on very different algorithms and data structures.",Theoretical Foundations of Artificial General Intelligence,chapter 2
"Ease While being difcult enough to require a real advance, challenge problem should be as simple as possible so that real progress is made while avoiding extraneous issues and tasks. One benet of the physical cognition domain over, for example, Middle East politics is the smaller amount of knowledge required for a system to have before it can actually demonstrate intelligent reasoning. Incremental It should be possible to demonstrate advances towards the goal short of actually achieving it. For example, it is possible to show progress in the physical cognition domain without actually providing a complete solution by showing that an addition to the model enables and explains reasoning in a signicantly wider, but still not complete, set of scenarios. General The extent to which a challenge problem involves issues that underlie cognition in many domains makes progress towards solving that problem more important.",Theoretical Foundations of Artificial General Intelligence,chapter 2
"For exam     24 Theoretical Foundations of Articial General Intelligence ple, I have shown (Cassimatis, 2004) how syntactic parsing can be mapped onto a physical reasoning problem. Thus, progress towards understanding physical cognition amounts to progress in two domains. 2.5 Conclusions I have argued that cognitive scientists attempting to understand human intelligence can be impeded by the standards of the cognitive sciences, that understanding human intelligence will require its own subeld, intelligence science, and that much of the work in this subeld will assume many of the characteristics of good human-level AI research. I have outlined some principles for guiding intelligence science that I suggest would support and motivate work towards solving the intelligence problem and understanding how the human brain embodies a solution to the intelligence problem. In only half a century we have made great progress towards understanding intelligence within elds that, with occasional exceptions, have not been specically and wholly directed towards solving the intelligence problem.",Theoretical Foundations of Artificial General Intelligence,chapter 2
"We have yet to see the progress that can happen when large numbers of individuals and institutions make this their overriding goal. Bibliography [1] Anderson, J.R. (2005). Human symbol manipulation within an integrated cognitive architecture. Cognitive Science, 313341. [2] Cassimatis, N.L. (2004). Grammatical Processing Using the Mechanisms of Physical Inferences. Paper presented at the Twentieth-Sixth Annual Conference of the Cognitive Science Society. [3] Cassimatis, N.L., & Bignoli, P. (in press). Testing Common Sense Reasoning Abilities. Journal of Theoretical and Experimental Articial Intelligence. [4] Cassimatis, N.L., Bello, P., & Langley, P. (2008). Ability, Parsimony and Breadth in Models of Higher-Order Cognition. Cognitive Science, 33 (8), 13041322. [5] Cassimatis, N.",Theoretical Foundations of Artificial General Intelligence,chapter 2
"Bignoli, P., Bugajska, M., Dugas, S., Kurup, U., Murugesan, A., & Bello, P. (2010). An Architecture for Adaptive Algorithmic Hybrids. IEEE Transactions on Systems, Man, and Cybernetics, Part B, 4 (3), 903914. [6] Lewis, R.L., & Vasishth, S. (2005). An activation-based model of sentence processing as skilled memory retrieval. Cognitive Science, 29, 375-419. [7] Newell, A. (1973). You cant play 20 questions with nature and win. In W.G. Chase (Ed.), Visual Information Processing, Academic Press. [8] Shanahan, M. (1997). Solving the frame problem: a mathematical investigation of the common sense law of inertia. MIT Press. Cambridge, MA.",Theoretical Foundations of Artificial General Intelligence,chapter 2
"  Chapter 3 Psychometric Articial General Intelligence: The Piaget-MacGuyver Room  Selmer Bringsjord and John Licato Department of Computer Science Department of Cognitive Science Lally School of Management & Technology Rensselaer Polytechnic Institute (RPI) Troy NY 12180 USA Psychometric AGI (PAGI) is the brand of AGI that anchors AGI science and engineering to explicit tests, by insisting that for an information-processing (i-p) artifact to be rationally judged generally intelligent, creative, wise, and so on, it must pass a suitable, well-dened test of such mental power(s). Under the tent of PAGI, and inspired by prior thinkers, we introduce the Piaget-MacGyver Room (PMR), which is such that, an i-p artifact can credibly be classied as general-intelligent if and only if it can succeed on any test constructed from the ingredients in this room.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"No advance notice is given to the engineers of the artifact in question, as to what the test is going to be; only the ingredients in the room are shared ahead of time. These ingredients are roughly equivalent to what would be fair game in the testing of neurobiologically normal Occidental students to see what stage within Piagets theory of cognitive development they are at. Our proposal and analysis puts special emphasis on a kind of cognition that marks Piagets Stage IV and beyond: viz., the intersection of hypothetico-deduction and analogical reasoning, which we call analogico-deduction. 3.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"1 Introduction Psychometric AGI (PAGI; pronounced pay guy), in a nutshell, is the brand of AGI that anchors AGI science and engineering to explicit tests, by insisting that for an information-processing1 (i-p) artifact to be rationally judged generally intelligent, creative, We are greatly indebted to not only the editors, but to two anonymous referees for invaluable feedback on earlier drafts of our paper. 1By using information-processing rather than computational we leave completely open the level of information-processing power  from that of a standard Turing machine, to so-called hypercomputers  the artifact in question has. Note that we also for the most part steer clear of the term agent, which is customary in 25      26 Theoretical Foundations of Articial General Intelligence wise, and so on, the artifact must be capable of passing a suitable, well-dened test of such mental power(s), even when it hasnt seen the test before.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"(PAGI is built upon PAI, psychometric AI; see Bringsjord and Schimanski, 2003).) For example, someone might claim that IBMs i-p artifact Deep Blue is really and truly intelligent, in light of the fact that if you test it by seeing whether it can prevail against the best human chessplayers, you will nd that it can. And someone might claim that natural-language-processing artifact Watson, another i-p artifact from IBM (Ferrucci et al., 2010), is really and truly intelligent because it can vanquish human opponents in the game of Jeopardy!. However, while both of these artifacts are intelligent simpliciter, they most certainly arent general-intelligent. Both Deep Blue and Watson were explicitly engineered to specically play chess and Jeopardy!, nothing more; and in both cases the artifacts knew what their nal tests would be.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"Inspired by PAGI, and by a line of three thinkers (Descartes, Newell, and esp. Piaget) who gave much thought to the hallmarks of general intelligence, we dene a room, the Piaget-MacGyver Room (PMR), which is such that, an i-p artifact can credibly be classied as general-intelligent if and only if it can succeed on any test constructed from the ingredients in this room. No advance notice is given to the engineers of the artifact in question as to what the test is going to be. This makes for rather a different situation than that seen in the case of both Deep Blue and Watson; for in both of these cases, again, the AI engineering that produced these i-p artifacts was guided by a thorough understanding and analysis, ahead of time, of the tests in question.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"In fact, in both cases, again, all along, the engineering was guided by repeatedly issuing pre-tests to both artifacts, and measuring their performance with an eye to making incremental improvements. This is particularly clear in the case of Watson; see (Ferrucci et al., 2010). Of course, we happily concede that both Deep Blue and Watson are intelligent; we just dont believe that either is generalintelligent.2 As we say, only the ingredients in PMR are shared ahead of time with the relevant engineers. These ingredients are equivalent to what would be fair game in the testing, by Piaget, of a neurobiologically normal Occidental student who has reached at least Piagets Stage III of cognitive development. If you will, Piaget is in control of the ingredients in the AI. We do so because agent is usually taken to imply a function that is Turing-computable or easier; e.g.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"see the use of agent in (Russell and Norvig, 2002). 2Our attitude is anticipated e.g. by Penrose, who for instance pointed out that Deep Blue would be paralyzed if challenged on the spot to play variants of chess; see (Penrose, 1994). In the case of Watson, questions based on neologisms would paralyze the system. E.g., Supposing that bloogering! is to take a prime and blooger it (add it to itself), and then blooger thrice more times, what is bloogering! 7?      The Piaget-MacGuyver Room 27 room, and, with a general understanding of the MacGyver television series,3 and of course with an understanding of his own account of cognitive development, assembles from the ingredients in the room a test of an articial agent that is purportedly general-intelligent.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"For example, Piaget might be armed with the ingredients shown in Figure 3.1.                   Fig. 3.1 A Possible Set of Ingredients From Which Piaget Can Work (weights, marbles (and its playing eld), human confederates, familiar shapes, magnets, etc.) If the artifact passes what Piaget assembles, we can safely say that its indeed generalintelligent; if it fails, we declare that it isnt. We shall allow a range of responses that fall into these two categories, since some with the general intelligence of a Feynman, after being given the test, might well be able to nd an abundance of solutions.4 As will be seen below, our proposal and analysis puts special emphasis on cognition that marks (Piagetian) Stage IV and beyond: viz., the intersection of hypothetico-deduction and analogical reasoning (which we call analogico-deduction).",Theoretical Foundations of Artificial General Intelligence,chapter 3
"In hypothetico-deduction one creates hy3For a description of the series, see: http://en.wikipedia.org/wiki/MacGyver. The hero and protagonist, MacGyver, is stunningly resourceful, and hence hard to trap, seriously injure, or kill; he always manages to carry out some physical manipulation that solves the problem at hand. In the harder of Piagets tests, a high degree of resourcefulness is a sine qua non; and the tests invariably call  la MacGyver for physical manipulation that conrms the resourcefulness. 4See http://blogs.msdn.com/b/ericlippert/archive/2011/02/14/what-would-feynman-do.aspx).      28 Theoretical Foundations of Articial General Intelligence potheses h1,h2,...",Theoretical Foundations of Artificial General Intelligence,chapter 3
",hn, conditionals of the form hi  ri, and then tests to see whether the results ri do indeed obtain, following upon an instantiation of hi. If ri doesnt obtain, modus tollens immediately implies that hi is to be rejected. When analogical reasoning undergirds either the generation of the hypotheses or the conditionals, or the negation of a result ri, the overall process falls under analogico-deduction. In order to focus matters we shall restrict our attention to not only such reasoning, but to such reasoning applied to a representative test fashioned by Piaget: his ingenious magnet test. The plan of the chapter is as follows. We rst ( 3.2) explain in a bit more detail what PAGI is, and why the roots of this brand of AGI are to be explicitly found in the thinking of Newell, and before him, in two tests described by Descartes. We then ( 3.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"3) look at these two tests in a bit more detail. Next, in section 3.4, we give a barbarically quick overview of Piagets view of thinking, and present his magnet challenge. We then ( 3.5) briey describe the LISA system for modeling analogical reasoning. Following this, we model and simulate, using the linked information-processing system Slate+LISA (Slate is an argument-engineering environment that can be used in the purely deductive mode for proof engineering), human problem-solving cognition in the magnet challenge ( 3.6). A brief pointer to the next research steps in the PAGI research program in connection with PMR (which, ultimately, we have every intention of building and furnishing), wraps up the chapter. 3.2 More on Psychometric AGI Rather long ago, Newell (1973) wrote a prophetic paper: You Cant Play 20 Questions with Nature and Win.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"This paper helped catalyze both modern-day computational cognitive modeling through cognitive architectures (such as ACT-R, NARS, Soar, Polyscheme, etc.), and AIs  now realized, of course  attempt to build a chess-playing machine better at the game than any human. However, not many know that in this paper Newell suggested a third avenue for achieving general machine intelligence, one closely aligned with psychometrics, and one  as we shall see  closely aligned as well with the way Piaget uncovered the nature of human intelligence. In the early days of AI, at least one thinker started decisively down this road for a time (Evans 1968); but now the approach, it may be fair to say, is not all that prominent in AI. We refer to this approach as Psychometric AGI, or just PAGI (rhymes with pay guy).      The Piaget-MacGuyver Room 29 3.2.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"1 Newell & the Neglected Route Toward General Machine Intelligence In the 20 Questions paper, Newell bemoans the fact that, at a symposium gathering together many of the greatest psychologists at the time, there is nothing whatsoever to indicate that any of their work is an organized, integrated program aimed seriously at uncovering the nature of intelligence as information processing. Instead, Newell perceives a situation in which everybody is carrying out work (of the highest quality, he cheerfully admits) on his or her own specic little part of human cognition. In short, there is nothing that, to use Newells phrase, pulls it all together. He says: We never seem in the experimental literature to put the results of all the experiments together.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"(1973: 298) After making clear that he presupposes that man is an information processor, and that therefore from his perspective the attempt to understand, simulate, and replicate human intelligence is by definition to grapple with the challenge of creating machine intelligence, Newell offers three possibilities for addressing the fragmentary nature of the study of mind as computer. The rst possibility Newell calls Complete Processing Models. He cites his own work (with others; e.g., Simon; the two, of course, were to be a dynamic duo in AI for many decades to come) based on production systems, but makes it clear that the productionsystem approach isnt the only way to go. Of course todays cognitive architectures [e.g.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"NARS (Wang, 2006); SOAR (Rosenbloom, Laird and Newell, 1993); ACT-R (Anderson, 1993; Anderson and Lebiere, 1998; Anderson and Lebiere, 2003); Clarion (Sun, 2001); and Polyscheme (Cassimatis, 2002; Cassimatis, Trafton, Schultz and Bugajska, 2004)] can be traced back to this rst possibility. The second possibility is to Analyze a Complex Task. Newell sums this possibility up as follows. A second experimental strategy, or paradigm, to help overcome the difculties enumerated earlier is to accept a single complex task and do all of it ... the aim being to demonstrate that one has a signicant theory of a genuine slab of human behavior. ...",Theoretical Foundations of Artificial General Intelligence,chapter 3
"A nal example [of such an approach] would be to take chess as the target super-task (Newell 1973: 303304). This second possibility is one most people in computational cognitive science and AI are familiar with. Though Deep Blues reliance upon standard search techniques having little cognitive plausibility perhaps signaled the death of the second avenue, there is no question that, at least for a period of time, many researchers were going down it. The third possibility, One Program for Many Tasks, is the one many people seem to have either largely forgotten or ignored. Newell described it this way: The third alternative paradigm I have in mind is to stay with the diverse collection of small experimental tasks, as now, but to construct a single system to perform them all. This single      30 Theoretical Foundations of Articial General Intelligence system (this model of the human information processor) would have to take the instructions for each, as well as carry out the task.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"For it must truly be a single system in order to provide the integration we seek (Newell 1973: 305). For those favorably inclined toward the test-based approach to AI or AGI, its the specic mold within Newells third possibility that is of acute interest. We read: A ... mold for such a task is to construct a single program that would take a standard intelligence test, say the WAIS or the Stanford-Binet. (Newell 1973: 305) We view this remark as a pointer to PAGI, and to a brief explication of this brand of AGI we now turn. 3.2.2 So, What is Psychometric AGI? What is AI? Wed be willing to wager that many of you have been asked this question  by colleagues, reporters, friends and family, and others.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"Even if by some uke youve dodged the question, perhaps youve asked it yourself, maybe even perhaps (in secret moments, if youre a practitioner) to yourself, without an immediate answer coming to mind. At any rate, AI itself repeatedly asks the question  as the rst chapter of many AI textbooks reveals. Unfortunately, many of the answers standardly given dont ensure that AI tackles head on the problem of general intelligence (whether human or machine). For instance, Russell and Norvig (2002) characterize AI in a way (via functions from percepts to actions; they call these functions intelligent agents) that, despite its many virtues, doesnt logically entail any notion of generality whatsoever: An agent consisting solely in the factorial function qualies as an intelligent agent on the R-N scheme. Our answer, however, is one in line with Newells third possibility, and one in line with a perfectly straightforward response to the What is AI? question.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"To move toward our answer, note rst that presumably the A part of AGI isnt the challenge: We seem to have a fairly good handle on what it means to say that something is an artifact, or articial. Its the G and the I parts that seem to throw us for a bit of a loop. First, whats intelligence? This is the rst of the two big, and hard, questions. Innumerable answers have been given, but many outside the test-based approach to AI seem to forget that there is a particularly clear and straightforward answer available, courtesy of the eld that has long sought to operationalize the concept in question; that eld is psychometrics. Psychometrics is devoted to systematically measuring psychological properties, usually via tests. These properties include the ones most important in the present context: both intelligence, and general intelligence.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"In a nutshell, the initial version of a psychometrics     The Piaget-MacGuyver Room 31 oriented account of general intelligence (and this denition marks an answer to the second big question: Whats general intelligence?) is simply this: Some i-p artifact is intelligent if and only if it can excel at all established, validated tests of neurobiologically normal cognition, even when these tests are new for the artifact. Of course, psychometrics is by denition devoted to specically dening and measuring human intelligence; the SAT, part of the very fabric of education in the United States (with its counterparts in widespread use in other technologized countries), is for example administered to humans, not machines. Some hold that its neither possible nor necessary for AI to be identical to human intelligence. After all, it seems possible for an AI to have a vision system that covers a different frequency range of light, compared to of a normal human.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"Consequently, such a system may fail some tests that require color recognition. In this context, someone might object: What makes the two of you think, then, that intelligence tests can be sensibly applied to i-p artifacts? Full analysis and rebuttal of this worry would occupy more space than we have; we must rest content with a brief response, via three points: (1) Judgments regarding whether i-p artifacts are intelligent are already informally, but rmly, rooted in the application of tests from the human sphere. We know that Kasparov is quite an intelligent chap; and we learned that Deep Blue, accordingly, is intelligent; a parallel moral emerged from the victory of Watson. We are simply, at bottom, extending and rigorizing this already-established human-centric way of gauging machine intelligence. (2) The eld of psychometrics is in reality constrained by the need for construct validity, but in PAGI this constraint is cheerfully defenestrated.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"Tests that are construct-valid are such that, when successfully taken, they ensure that the relevant underlying structures and processes have been active inside the agent in question. But in PAGI, the bottom line is getting the job done, and in fact we assume that i-p machines will, under the hood, depart from human techniques. (3) The third point in our answer ows from the second, and is simply a reminder that while in the human sphere the scoring of tests of mental ability is indeed constrained by comparison to other human test-takers (an IQ score, after all, is meaningless without relative comparison to other humans who take the relevant test), PAGI is founded upon a much more coarse-grained view of intelligence tests  a view according to which, for instance, a perfect score on the part of an i-p artifact indicates that its intelligent simpliciter, not that its intelligent within some humancentric continuum.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"This general point applies directly to PMR: For example, prowess in PMR specically requires sensorimotor prowess, but not human sensorimotor adroitness. We assume only that one side of general intelligence, as that concept covers both human and i-p machine, is perceiving and moving, in planful ways, physical objects. We anticipate that some will insist that while intelligence tests are sensibly applicable to i-p artifacts in principle, the fact remains that even broad intelligence tests are still just too narrow, when put in the context of the full array of cognitive capacities seen in homo sapiens. But one can understand general intelligence, from the standpoint of psychometrics, to      32 Theoretical Foundations of Articial General Intelligence include many varied, indeed for that matter all, tests of intellectual ability. Accordingly, one can work on the basis of a less nave denition of PAGI, which follows.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"5 Psychometric AGI is the eld devoted to building i-p artifacts capable of at least solid performance on all established, validated tests of intelligence and mental ability, without having seen these tests beforehand at all; the class of tests in play here includes not just the rather restrictive IQ tests, but also tests of the many different forms of intelligence seen in the human sphere.6 This denition, when referring to tests of mental ability, is pointing to much more than IQ tests. For example, following Sternberg (1988), someone with much musical aptitude would count as brilliant even if their scores on tests of academic aptitude (e.g., on the SAT, GRE, LSAT, etc.) were low. Nonetheless, even if, hypothetically, one were to restrict attention in PAGI to intelligence tests, a large part of cognition would be targeted. Along this line, in choosing the WAIS, Newell knew what he was doing.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"To see this, we begin by going back to the early days of AI, specically to a time when Psychometric AI was at least implicitly entertained. For example, in the mid 1960s, the largest Lisp program on earth was Evans (1968) ANALOGY program, which could solve problems like those shown in Figure 3.2. Evans himself predicted that systems able to solve such problems would be of great practical importance in the near future, and he pointed out that performance on such tests is often regarded to be the touchstone of human intelligence. However, ANALOGY simply hasnt turned out to be the rst system in a longstanding, comprehensive research program (Newellian or otherwise). Why is this? Given our approach and emphasis, this question is a penetrating one. After all, we focus on analogical reasoning, and ANALOGY certainly must be capable of such reasoning.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"(There is no deduction required by a program able to solve problems in the class in question, but if the artifact was asked to rigorously justify its selection, deduction would unstoppably enter the picture.) So again: Given that Evans was by our own herein-advertised lights on the right track, why the derailment? We think the main reason is summed up in this quote from Fischler & Firschein (1987): If one were offered a machine purported to be intelligent, what would be an appropriate method of evaluating this claim? The most obvious approach might be to give the machine an IQ test. ... However, [good performance on tasks seen in IQ tests would not] be completely satisfactory because the machine would have to be specially prepared for any specic task that it was asked to perform.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"The task could not be described to the machine in a normal conversation (verbal or written) if the specic nature of the task was not already 5For more on PAI, which of course forms the foundation for PAGI, readers can consult a recent issue of the Journal of Experimental and Theoretical Articial Intelligence devoted to the topic: 23.3. 6The notion that intelligence includes more than academic intelligence is unpacked and defended by numerous psychologists. E.g., see (Sternberg, 1988).      The Piaget-MacGuyver Room 33 Fig. 3.2 Sample Problem Solved by Evans (1968) ANALOGY Program. Given sample geometric congurations in blocks A, B, and C, choose one of the remaining ve possible congurations that completes the relationship: A is to B as C is to ...?. Subjects asked to prove that their answers are correct must resort to analogico-deduction.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"programmed into the machine. Such considerations led many people to believe that the ability to communicate freely using some form of natural language is an essential attribute of an intelligent entity (Fischler & Firschein 1987, p. 12; emphasis ours). 3.2.3 Springboard to the Rest of the Present Paper Our response to this response is three-fold. One, there is nothing here that tells against the suspicion that the marriage of analogical and deductive reasoning, which is specically called for by problems of the sort that the ANALOGY system solved, is at the heart of general intelligence, whether that intelligence is embodied in the mind of a person or machine. Two, a test-based approach to AI can, despite what F&F say, take full account of the requirement that a truly intelligent computing machine must not simply be pre-programmed. Indeed, this is one of the chief points of the PMR.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"And nally, three, a test-based approach      34 Theoretical Foundations of Articial General Intelligence to uncovering the nature of human intelligence, when broadened in the manner of Piaget, provides a suitable guide to engineering aimed at producing articial general intelligence. At this point the reader has sufcient understanding of PAGI to permit us to move on.7 3.3 Descartes Two Tests Descartes was quite convinced that animals are mechanical machines. He felt rather differently about persons, however: He held that persons, whether of the divine variety (e.g., God, the existence of whom he famously held to be easily provable) or the human, were more than mere machines. Someone might complain that Descartes, coming before the likes of Turing, Church, Post, and Gdel, could not have had a genuine understanding of the concept of a computing machine, and therefore couldnt have claimed the human persons are more than such machines.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"There are two reasons why this complaint falls at. One, while we must admit that Descartes didnt exactly have in the mind the concept of a computing machine in the manner of, say, of a universal Turing machine, or a register machine, and so on, what he did have in mind would subsume such modern logico-mathematical devices. For Descartes, a machine was overtly mechanical; but there is a good reason why recursion theory has been described as revolving around what is mechanically solvable. A Turing machine, and ditto for its equivalents (e.g., register machines) are themselves overtly mechanical. Descartes suggested two tests to use in order to separate mere machines from human persons. The rst of these directly anticipates the so-called Turing Test. The second test is the one that anticipates the Piaget-MacGyver Room.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"To see this, consider: If there were machines which bore a resemblance to our body and imitated our actions as far as it was morally possible to do so, we should always have two very certain tests by which to recognize that, for all that, they were not real men. The rst is, that they could never use speech or other signs as we do when placing our thoughts on record for the benet of others. For we can easily understand a machines being constituted so that it can utter words, and even emit some responses to action on it of a corporeal kind, which brings about a change in its organs; for instance, if it is touched in a particular part it may ask what we wish to say to it; if in another part it may exclaim that it is being hurt, and so on.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"But it never happens that it arranges its speech in various ways, in order to reply appropriately to everything that may be said in its presence, as even the lowest type of man can do. And the second difference is, that although machines can perform certain things as well as or perhaps better than any of us can do, they infallibly fall short in others, by which means we may discover that they did not act from knowledge, but only for the disposition of their organs. For while reason is a universal instrument which can serve for all contingencies, 7For more on PAI, the foundation for PAGI, readers can consult a recent issue of the Journal of Experimental and Theoretical Articial Intelligence devoted to the topic: 23.3.      The Piaget-MacGuyver Room 35 these organs have need of some special adaptation for every particular action.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"From this it follows that it is morally impossible that there should be sufcient diversity in any machine to allow it to act in all the events of life in the same way as our reason causes us to act (Descartes 1911, p. 116). We now know all too well that machines can perform certain things as well or perhaps better than any of us (witness Deep Blue and Watson, and perhaps, soon enough, say, auto-driving cars that likewise beat the pants off of human counterparts); but we also know that these machines are engineered for specic purposes that are known inside and out ahead of time. PMR is designed specically to test for the level of prociency in using what Descartes here refers to as a universal instrument. This is so because PMR inherits Piagets focus on general-purpose reasoning. We turn now to a brief discussion of Piaget and this focus. 3.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"4 Piagets View of Thinking & The Magnet Test Many people, including many outside psychology and cognitive science, know that Piaget seminally  and by Bringsjords lights, correctly  articulated and defended the view that mature human reasoning and decision-making consists in processes operating for the most part on formulas in the language of classical extensional logic (e.g., see Inhelder and Piaget, 1958b).8 You may yourself have this knowledge. You may also know that Piaget posited a sequence of cognitive stages through which humans, to varying degrees, pass; we have already referred above to Stages III and IV. How many stages are there, according to Piaget? The received answer is: four; in the fourth and nal stage, formal operations, neurobiologically normal humans can reason accurately and quickly over formulas expressed in the logical system known as rst-order logic, LI.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"This logic allows for use of relations, functions, the universal and existential quantiers, the familiar truth-functional connectives from the propositional calculus, and includes a so-called proof theory, that is, a mechanical method for deriving some formulas from others.9 One cornerstone of every classical proof theory, as the reader will likely well know, is modus ponens, according to which the formula  can be derived from the formulas  and    (read: if  then ). 8Many readers will know that Piagets position long ago came under direct attack, by such thinkers as Wason and Johnson-Laird (Wason, 1966; Wason and Johnson-Laird, 1972). In fact, unfortunately, for the most part academics believe that this attack succeeded. Bringsjord doesnt agree in the least, but this isnt the place to visit the debate in question.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"Interested readers can consult (Bringsjord, Bringsjord and Noel, 1998; Rinella, Bringsjord and Yang, 2001). Piaget himself retracted any claims of universal use of formal logic: (Piaget, 1972). 9A full overview of logic, LI included, in order to model and simulate large parts of cognition, can be found in (Bringsjord, 2008).      36 Theoretical Foundations of Articial General Intelligence Fig. 3.3 Piagets famous rigged rotating board to test for the development of Stage-III-or-better reasoning in children. The board, A, is divided into sectors of different colors and equal surfaces; opposite sectors match in color.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"B is a rotating disk with a metal rod spanning its diameterbut the catch is that the star cards have magnets buried under them (hidden inside wax), so the alignment after spinning is invariably as shown here, no matter how the shapes are repositioned in the sectors (with matching shapes directly across from each other). This phenomenon is what subjects struggle to explain. Details can be found in (Inhelder and Piaget, 1958b). Judging by the cognition taken by Piaget to be stage-III or stage-IV (e.g., see Figure 3.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"3, which shows one of the many problems presented to subjects in (Inhelder and Piaget, 1958b), the basic scheme is that an agent A receives a problem P (expressed as a visual scene accompanied by explanatory natural language), represents P in a formal language that is a superset of the language of I, producing [P], and then reasons over this representation (along with background knowledge, which we can assume to be a set  of formal declarative statements) using at least a combination of some of the proof theory of 1 and psychological operators.10 This reasoning allows the agent to obtain the solution [S].",Theoretical Foundations of Artificial General Intelligence,chapter 3
"To ease exposition, we shall ignore the heterodox operations that Piaget posits (see note 10) in favor of just standard proof theory, and we will moreover view [P] as a triple (,C,Q), where  is a (possibly complicated) formula in the language of LI, C is further information that provides context for the problem, and consists of a set of rst-order 10 The psychological operators in question cannot always be found in standard proof theories. For example, Piaget held that the quartet I N R C of transformations were crucial to thought at the formal level. Each member of the quartet transforms formulas in certain ways. E.g., N is inversion, so that N(pq) = pq; this seems to correspond to DeMorgans Law. But R is reciprocity, so R(pq) = pq, and of course this isnt a valid inference in the proof theory for the propositional calculus or LI.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"     The Piaget-MacGuyver Room 37 formulas, and Q is a query asking for a proof of  from C . So: [P] = (,C,Q = C   ?) At this point a reader might be puzzled about the fact that what we have so far described is exclusively deductive, given that we have said that our focus is reasoning that includes not just deduction, but also analogical reasoning; the key term, introduced above, is analogicodeduction. To answer this, and to begin to give a sense of how remarkably far-reaching Piagets magnet challenge is, rst consider how this deduction-oriented scheme can be instantiated. To begin, note that in the invisible magnetization problem shown in Figure 3.3, which requires stage-III reasoning in order to be solved, the idea is to explain how it is that , that is, that the rotation invariably stops with the two stars selected by the rod.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"Since Piaget is assuming the hypothetico-deductivemethod of explanation made famous by Popper (1959), to provide an explanation is to rule out hypotheses until one arrives deductively at . In experiments involving child subjects, a number of incorrect (and sometimes silly) hypotheses are entertainedthat the stars are heavier than the other shaped objects, that the colors of the sections make a difference, and so on. Piagets analysis of those who discard mistaken hypotheses in favor of  is that they expect consequences of a given hypothesis to occur, note that these consequences fail to obtain, and then reason backwards by modus tollens to the falsity of the hypotheses. For example, it is key in the magnet experiments of Figure 3.3 that for some spins of the disk, the rod will come to rest upon shapes other than the stars is an expectation. When expectations fail, disjunctive syllogism allows  to be concluded.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"However, the reasoning patterns so far described are only those at the top level, and even at that level exclude the generation of hypotheses. Beneath the top level, many non-deductive forms of reasoning are perfectly compatible with Piagets framework, and one thing that is crystal clear on a reading of his many experiments is that subjects draw from past experience to by analogy rule our hypotheses, and to generate hypotheses in the rst place. Hence the magnet challenge, like other famous challenges invented and presented by Piaget, is a portal to a remarkably wide landscape of the makings of general intelligence.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"This is conrmed not just by taking account of the magnet challenge in the context of Piagets framework, and more generally in the context of deliberative reasoning and decisionmaking; its also conrmed by placing the magnet challenge (and counterparts that can be fashioned from the raw materials for PMR) in the context of broad characterizations of intelligence offered even by AI researchers more narrowly oriented than AGI researchers. For      38 Theoretical Foundations of Articial General Intelligence example, the magnet challenge taps many elements in the expansive, more-than-deduction view of rational intelligence laid out by Pollock (1989), and likewise taps much of the functionality imparted to the more sophisticated kinds of agents that are pseudo-coded in Russell and Norvig (2009). As will soon be seen, our modeling and simulation of the magnet challenge reects its requiring much more than straight deduction.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"But before moving in earnest to that modeling and simulation, we provide a rapid overview of the system we use for analogical reasoning: LISA. 3.5 The LISA model LISA (Learning and Inference with Schemas and Analogies) is the formidable fruit of an attempt to create a neurally-plausible model of analogical reasoning by using a hybrid connectionist and symbolic architecture (Hummel and Holyoak, 2003a; Hummel and Holyoak, 2003b). We here provide only a very brief summary of some relevant features of LISA; for a more detailed description the reader is directed to (Hummel & Holyoak, 2003) and (Hummel & Landy, 2009). LISA allows for explicit representation of propositional knowledge, the arguments of which can be either token objects or other propositions.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"11 Propositional knowledge is organized into analogs, which contain the proposition nodes, along with other related units: the sub-propositional units which help to bind relational roles within propositions to their arguments, nodes representing the objects (one object unit corresponds to a token object across all propositions within an analog), predicate units which represent the individual roles within a proposition, and higher-level groupings of propositions (Hummel and Landy, 2009). Semantic units, which are outside of and shared by all of the analogs, connect to the object and predicate units. In self-supervised learning, LISA performs analogical inference by ring the propositional units in a preset order, which propagates down to the semantic units. This allows for units in different analogs to be temporarily mapped to each other if they re in synchrony, and for new units to be recruited (or inferred) if necessary.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"Of course, many details are left out here in the interests of space; for more, see (Hummel & Holyoak 2003). 11E.g., knows(Tom,loves(Sally,Jim)).      The Piaget-MacGuyver Room 39 3.6 Analogico-Deductive Reasoning in the Magnet Test The ways in which analogical and deductive reasoning interact in a typical human reasoner are, we concede, complex to the point of greatly exceeding any reasoning needed to excel in the narrower-than-real-life PMR; and, in addition, these ways no doubt vary considerably from person to person. A model such as the one we present here can thus only hope to be a simulation of a possible way that a reasoner might solve a problem on the order of the magnet challenge and its relatives.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"This said, and turning now to the Piagetian task on which we focus, we rst note again that analogical reasoning can often be useful in generation of hypotheses and theories to explain unfamiliar phenomena. For example, Holyoak et al. (2001) explain that the wave theory of sound, as it became better understood, was the basis for creating an analogy that described the wave theory of light. Such an analogical mapping would presumably be responsible for inferring the existence of a medium through which light would travel, just as sound needs air or something like it (indeed, the luminiferous aether was of course proposed to be this very medium). In contrast, Newtons particle theory of light would provide an analogical mapping that would not require a medium. Thus, we have two different analogical mappings; and each then suggests slightly different groups of hypotheses, members of which, in both cases, could in turn be tested with a combination of experimentation and deductive reasoning.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"Now lets get more specic. Analogico-deductive reasoning in the Piagetian hiddenmagnet experiment can be modeled using LISA and Slate together; specically, a dialogue between an experimenter and a subject referred as Gou provides an interesting basis for doing so (Inhelder and Piaget, 1958a). Gou, who is developmentally in Piagets concrete operations stage (Stage III), after being presented with the hidden-magnets challenge, does from the start suspect that magnets are responsible  but quickly abandons this hypothesis in favor of the one claiming that the weight of the objects is what leads the needle to repeatedly stop on the stars. The experimenter then asks Gou what he would have to do in order to prove that it isnt the weight, to which Gou responds by carrying out a series of small experiments designed to prove that weight isnt responsible for the bars stopping.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"One of these experiments involves removing the star and diamond boxes, and checking to see if the bar still stops on the heaviest of the remaining boxes. Predictably (given our understanding of the backgrounds mechanisms), it does not; this provides Gou with empirical evidence that weight is not causally responsible for the bars stopping as it invariably does (although      40 Theoretical Foundations of Articial General Intelligence he continues to subsequently perform small experiments to further verify that weight is not responsible). In our overall model of Gous reasoning as being of the analogico-deductive variety, we of course must make use of both deduction and analogical reasoning, woven together. The overall reasoning abides by the deductive framework known as proof by cases, which is straightforward and bound to be familiar to all our readers. The core idea is that if one knows that a disjunction 1 2 ...",Theoretical Foundations of Artificial General Intelligence,chapter 3
"n holds, and knows as well that one or more of the disjuncts i fail to hold, then one can infer a new disjunction lacking the false disjuncts. In the case at hand, in light not only of Gous thinking, but that of many other subjects, there are four hypotheses in play, as follows (with underlying S-expressions in FOL given in each case).12 H1 Weight accounts for the invariance.  (Initially boardWeighted) H2 Color accounts for the invariance.  (Initially boardColored) H3 Magnets account for the invariance.  (Initially boardMagnetized) H4 Order accounts for the invariance.  (Initially boardOrdered) The overall proof, which makes use of LISA to carry out analogical reasoning to rule out the hypothesis that weight is the causally responsible element in the test, is shown in Figure 3.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"4, which we encourage the reader to take a few minutes to assimilate. We can model Gous reasoning process, by rst assuming that he already understands that there is some force or property Pstop that causes the bar to stop. We can model this by invoking a predicate More_P(x,y), which is true iff a pair of boxes x is more likely to stop the rotating bar than another pair of boxes y. Gou does know that some boxes are heavier than others, which can be represented by predicates of the form Heavier(x,y). We will assume that Gou has some knowledge of the transitivity of weight. Finally, the causal relationship suggested to Gou by the experimenter  that weight is causally 12Modeling and simulating the generation of the full quartet of hypotheses is outside our scope, and we thus commence our analysis in earnest essentially at the point when this quartet is being entertained.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"     The Piaget-MacGuyver Room 41 Fig. 3.4 The Top-Level Reasoning Strategy.      42 Theoretical Foundations of Articial General Intelligence                                Fig. 3.5 Propositional knowledge used in LISA for the Gou example. Domain D is inferred from D using LISAs analogical inferencing capability. Propositions representing semantic connections are not pictured here. responsible for the bars stopping  is represented using the special-group proposition Causes(Heavier(x,y),More_P(x,y)).13 In the rst stage of this simulation, the set D, consisting of both propositional knowledge held by Gou and semantic knowledge about the objects in D, is represented in Slates memory. Semantic knowledge is represented using a special predicate Semantic_Prop, which simply connects an object to its relevant semantic unit. For example, Semantic_Prop(bill,tall) and Semantic_Prop(jim,tall) connect the objects bill and jim to the semantic unit tall.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"D is then subjected to reasoning by analogical inference. To do this, D must rst be divided into two subsets: Dsource and Dtarget. Note that these two subsets need not be mutually exclusive or collectively exhaustive  they only need to each be subsets of D. Choosing which propositions to include in Dsource and Dtarget may be an iterative process, the details of which we do not provide at this time. For now, we can assume that in a relatively simple problem such as this, a useful division such as the one we will describe shortly will occur. 13Proposition groupings are treated differently in LISA than regular propositions (Hummel and Holyoak, 2003a).      The Piaget-MacGuyver Room 43                                 Fig. 3.6 Structured knowledge used in LISA for the Gou example. The units in dotted lines are generated using analogical inference.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"Dsource and Dtarget are then sent to LISA, where they are each used to create an analog. Analogical inference then produces the structure seen in Figure 3.6. Note that the semantic connections from the inferred predicate are mapped to the relevant semantic values as a result of the analogical inference process (Hummel and Holyoak, 2003a). The inferred predicates and semantic connections are then collected as the set D (Figure 3.5), which is returned to Slate, where it is then subjected to further deductive reasoning. This reasoning over DD may ideally derive one of two things: a testable hypothesis, which a reasoner would then empirically verify or refute; or a contradiction. A failure to derive either can result in either a repeat of the analogical process with different subsets chosen for Dsource and Dtarget, or a general failure condition. In the present example, Figure 3.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"5 shows that D contains the proposition More_P(circle,square). Gous experiment, however, shows More_P(circle,square). This leads to a contradiction exploited in Slate, and hence the possibility that weight is causally responsible for whatever force is stopping the metal bar is rejected.      44 Theoretical Foundations of Articial General Intelligence One might ask at this point whether analogical reasoning is necessary to carry out this process. The answer is clearly No. But the question is wrong-headed. After all, every elementary logic textbook covering not just deduction, but also induction, abduction, and analogical reasoning,14 presents the alert reader with formal facts that allow her to see that, in principle, deduction can be used to arrive at whatever conclusion is produced in heterogeneous fashion  if additional premises are added. (This is actually an easily proved theorem, given that the commonality to all forms of reasoning is that the content reasoned over is relational and declarative.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"Accordingly, while we are not arguing that the precise procedure we have chronicled exactly models the thought processes all reasoners go through, it seems that analogical reasoning produces a plausible explanation. In fact, consider the following. After Gou is asked to investigate what force or property Pstop was responsible for stopping the bars, he might then perform some experiments on the assumption that Pstop is transitive. For example, he might think that if the star boxes are heavier than the square boxes, and a set of boxes b existed that were heavier than the star boxes, then the b boxes should be more likely to stop the bar than the star boxes. However, it doesnt follow from deductive reasoning alone that Pstop is transitive. After all, it may be the case that stacking two boxes on top of each other would cancel out their relative contributions to Pstop, or that the boxes together would have no stronger effect on stopping the rotating bar than they would have alone.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"He may have suspected that Pstop behaved in a similar way to forces familiar to him; forces like gravity or magnetism. If so, analogical ability neatly explains how he would have mapped the properties of magnetism  for example, its ability to pull on some objects more than others  on to Pstop. This process suggests to us that he previously understood the transitivity of weight, analogically inferred that Pstop was similarly transitive, and formed an easily testable hypothesis. Note that in the previous paragraph we say suggests to us. Although we have stated that complete psychological plausibility is not a primary goal of our simulation (it focuses more on possible ways in which analogical and deductive reasoning can interact), we should note here that Piaget himself was suspicious of the existence of analogical reasoning in children who have not yet reached Stage III.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"A series of experiments he carried out with Montangero and Billeter seemed to suggest that young children are not capable of consistently performing stable analogical reasoning, and that they instead tend to reason using surface similarity in analogical problems (Piaget, Montangero and Billeter, 2001). Goswami and Brown (1990) recreated a similar experiment with items and relations more 14E.g., Copi, Cohen and MacMahon (2011)      The Piaget-MacGuyver Room 45 likely to be familiar to small children; she demonstrated that they indeed had more analogical ability than Piaget suspected. Further experimentation by other researchers showed analogical ability in pre-linguistic children as well (Goswami, 2001). In any case, these results point to the complexity and ever-changing nature of the ways in which analogical and deductive reasoning mix.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"Recent work by Christie & Gentner (2010) suggests that at least in the case of young children, analogical reasoning is not likely to be used in generating hypotheses  unless the relevant stimuli are presented simultaneously, in a manner that invites side-by-side comparison and higher-level relational abstraction. Instead, the magnet experiments format would encourage hypotheses based on surface similarity, which presumably would lack the depth to provide a satisfactory set of testable hypotheses. (We see this with most of Piagets younger subjects: after a while, they simply give up (Inhelder and Piaget, 1958a).) The example we presented here does not have the child initially using analogy to generate a theory about weight. Instead, the mapping from weight is triggered by a suggestion from the experimenter himself. Analogico-deductive reasoning is then used to elaborate on this suggestion, and ultimately refute its validity. 3.7 Next Steps Fig. 3.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"7 Rendering of the Scale in Piagets Famous Balance Challenge.      46 Theoretical Foundations of Articial General Intelligence Alert readers will have observed that under the assumption that Piaget can draw from the ingredients in Figure 3.1 to construct a PAGI challenge in PMR, rather more than the magnet challenge is possible. Our next foray into PAGI via analogico-deduction, now underway, involves another of Piagets challenges: the balance problem. In this challenge, subjects are presented with a scale like that shown in Figure 3.7. To crack this puzzle, the subject must reason to the general rule r that balance is achieved under weight differentials when distance from the vertical post for hanging weights is proportional to the amount of weight in question.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"Victorious problem-solvers here, like MacGyver, manage in relatively short order to gure out that weights of different sizes can nonetheless by hung so that balance is achieved, as long as r is apprehended, and followed in the physical manipulation. In our work-in-progress, r is represented by a formula in FOL, and is arrived at via  no surprise here  analogico-deduction. Use of such reasoning is supported by what is seen in the subjects; for example in the case of a child who nds the secret to the balance puzzle in the game of marbles, which, if you look carefully, you will indeed see listed in Figure 3.1 as raw material for PMR. Bibliography Anderson, J. and Lebiere, C. (2003). The Newell test for a theory of cognition, Behavioral and Brain Sciences 26: 587640. Anderson, J. R. (1993).",Theoretical Foundations of Artificial General Intelligence,chapter 3
"Rules of Mind, Lawrence Erlbaum, Hillsdale, NJ. Anderson, J. R. and Lebiere, C. (1998). The Atomic Components of Thought, Lawrence Erlbaum, Mahwah, NJ. Bringsjord, S. (2008). Declarative/Logic-Based Cognitive Modeling, in R. Sun (ed.), The Handbook of Computational Psychology, Cambridge University Press, Cambridge, UK, pp. 127169. Bringsjord, S. and Schimanski, B. (2003). What is articial intelligence? Psychometric AI as an answer, Proceedings of the 18th International Joint Conference on Articial Intelligence (IJCAI 03), Morgan Kaufmann, San Francisco, CA, pp. 887893. Bringsjord, S., Bringsjord, E. and Noel, R. (1998).",Theoretical Foundations of Artificial General Intelligence,chapter 3
"In Defense of Logical Minds, Proceedings of the 20th Annual Conference of the Cognitive Science Society, Lawrence Erlbaum, Mahwah, NJ, pp. 173178. Cassimatis, N. (2002). Polyscheme: A Cognitive Architecture for Integrating Multiple Representation and Inference Schemes, PhD thesis, Massachusetts Institute of Technology (MIT). Cassimatis, N., Trafton, J., Schultz, A. and Bugajska, M. (2004). Integrating cognition, perception and action through mental simulation in robots, in C. Schlenoff and M. Uschold (eds), Proceedings of the 2004 AAAI Spring Symposium on Knowledge Representation and Ontology for Autonomous Systems, AAAI, Menlo Park, pp. 18. Christie, S. and Gentner, D. (2010).",Theoretical Foundations of Artificial General Intelligence,chapter 3
"Where hypotheses come from: Learning new relations by structural alignment, Journal of Cognition and Development 11(3): 356373. Copi, I., Cohen, C. and MacMahon, K. (2011). Introduction to Logic, Prentice-Hall, Upper Saddle River, NJ. This is the 14th (!) edition of the book.      Bibliography 47 Descartes, R. (1911). The Philosophical Works of Descartes, Volume 1. Translated by Elizabeth S. Haldane and G.R.T. Ross, Cambridge University Press, Cambridge, UK. Evans, G. (1968). A program for the solution of a class of geometric-analogy intelligence-test questions, in M. Minsky (ed.), Semantic Information Processing, MIT Press, Cambridge, MA, pp. 271353. Ferrucci, D., Brown, E., Chu-Carroll, J., Fan, J.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"Gondek, D., Kalyanpur, A., Lally, A., Murdock, W., Nyberg, E., Prager, J., Schlaefer, N. and Welty, C. (2010). Building Watson: An Overview of the DeepQA Project, AI Magazine pp. 5979. Fischler, M. and Firschein, O. (1987). Intelligence: The Eye, the Brain, and the Computer, AddisonWesley, Reading, MA. Goswami, U. (2001). Analogical reasoning in children, in D. Gentner, K. J. Holyoak and B. N. Kokinov (eds), The Analogical Mind: Perspectives from Cognitive Science, The MIT Press. Goswami, U. and Brown, A. L. (1990).",Theoretical Foundations of Artificial General Intelligence,chapter 3
"Melting chocolate and melting snowmen: Analogical reasoning and causal relations, Cognition 35(1): 6995. Holyoak, K. J., Gentner, D. and Kokinov, B. N. (2001). Introduction: The place of analogy in cognition, in D. Gentner, K. J. Holyoak and B. N. Kokinov (eds), The Analogical Mind: Perspectives from Cognitive Science, The MIT Press, chapter 1. Hummel, J. E. and Holyoak, K. J. (2003a). Relational reasoning in a neurally-plausible cognitive architecture: An overview of the lisa project, Cognitive Studies: Bulletin of the Japanese Cognitive Science Society 10: 5875. Hummel, J. E. and Holyoak, K. J. (2003b).",Theoretical Foundations of Artificial General Intelligence,chapter 3
"A symbolic-connectionist theory of relational inference and generalization, Psychological Review 110: 220264. Hummel, J. E. and Landy, D. H. (2009). From analogy to explanation: Relaxing the 1:1 mapping constraint...very carefully, in B. Kokinov, K. J. Holyoak and D. Gentner (eds), New Frontiers in Analogy Research: Proceedings of the Second International Conference on Analogy, Soa, Bulgaria. Inhelder, B. and Piaget, J. (1958a). The Growth of Logical Thinking: From Childhood to Adolescence, Basic Books, Inc. Inhelder, B. and Piaget, J. (1958b). The Growth of Logical Thinking from Childhood to Adolescence, Basic Books, New York, NY. Newell, A. (1973).",Theoretical Foundations of Artificial General Intelligence,chapter 3
"You cant play 20 questions with nature and win: Projective comments on the papers of this symposium, in W. Chase (ed.), Visual Information Processing, New York: Academic Press, pp. 283308. Penrose, R. (1994). Shadows of the Mind, Oxford, Oxford, UK. Piaget, J. (1972). Intellectual evolution from adolescence to adulthood, Human Development 15: 1 12. Piaget, J., Montangero, J. and Billeter, J. (2001). The formation of analogies, in R. Campbell (ed.), Studies in Reecting Abstraction, Psychology Press. Pollock, J. (1989). How to Build a Person: A Prolegomenon, MIT Press, Cambridge, MA. Popper, K. (1959). The Logic of Scientic Discovery, Hutchinson, London, UK. Rinella, K., Bringsjord, S.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"and Yang, Y. (2001). Efcacious Logic Instruction: People are not Irremediably Poor Deductive Reasoners, in J. D. Moore and K. Stenning (eds), Proceedings of the Twenty-Third Annual Conference of the Cognitive Science Society, Lawrence Erlbaum Associates, Mahwah, NJ, pp. 851856. Rosenbloom, P., Laird, J. and Newell, A. (eds) (1993). The Soar Papers: Research on Integrated Intelligence, MIT Press, Cambridge, MA. Russell, S. and Norvig, P. (2002). Articial Intelligence: A Modern Approach, Prentice Hall, Upper Saddle River, NJ. Russell, S. and Norvig, P. (2009). Articial Intelligence: A Modern Approach, Prentice Hall, Upper Saddle River, NJ. Third edition.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"     48 Theoretical Foundations of Articial General Intelligence Sternberg, R. (1988). The Triarchic Mind: A New Theory of Human Intelligence, Viking, New York, NY. Sun, R. (2001). Duality of the Mind, Lawrence Erlbaum Associates, Mahwah, NJ. Wang, P. (2006). The logic of intelligence, in B. Goertzel and C. Pennachin (eds), Articial General Intelligence, Springer, New York, NY, pp. 3162. Wason, P. (1966). Reasoning, New Horizons in Psychology, Penguin, Hammondsworth, UK. Wason, P. and Johnson-Laird, P. (1972). Psychology of Reasoning: Structure and Content, Harvard University Press, Cambridge, MA.",Theoretical Foundations of Artificial General Intelligence,chapter 3
"  Chapter 4 Beyond the Octopus: From General Intelligence toward a Human-like Mind Sam S. Adams 1 and Steve Burbeck 2 1 IBM Research 2 evolutionofcomputing.org E-mail: ssadams@us.ibm.com, sburbeck@mindspring.com General intelligence varies with species and environment. Octopuses are highly intelligent, sensing and rapidly learning the complex properties of their world. But as asocial creatures, all their learned knowledge dies with them. Humans, on the other hand, are exceedingly social, gathering much more complex information and sharing it with others in their family, community and wider culture. In between those extremes there are several distinct types, or levels, of reasoning and information sharing that we characterize as a metaphorical ladder of intelligence. Simple social species occupy a rung above octopuses. Their young passively learn the ways of their species from parents and siblings in their early lives.",Theoretical Foundations of Artificial General Intelligence,chapter 4
"On the next rung, cultural social animals such as primates, corvids, cetaceans, and elephants actively teach a complex culture to their young over much longer juvenile learning periods. Human-level intelligence relies on all of those lower rungs and adds three more: information sharing via oral language, then literacy, and nally civilization-wide sharing. The human mind, human behavior, and the very ontology with which we structure and reason about our world relies upon the integration of all these rungs. AGI researchers will need to recapitulate the entire ladder to produce a human-like mind. 4.1 Introduction Multi-strategy problem solving, spatial reasoning, rich sensory perception in multiple modalities, complex motor control, tool usage, theory-of-mind and even possibly consciousness  these and other capabilities form the target for digital systems designed to exhibit Articial General Intelligence (hereafter AGI) across a broad range of environments and domains.",Theoretical Foundations of Artificial General Intelligence,chapter 4
"49      50 Theoretical Foundations of Articial General Intelligence The ultimate goal of most AGI research is to create a system that can perform as well as humans in many scenarios and perhaps surpass human performance in some. And yet most of the capabilities listed above are already exhibited by the octopus, a solitary asocial creature that does not interact with its own kind save for a brief mating period at the end of its short life. The octopus learns very quickly and solves problems in idiosyncratic and creative ways. Most AGI researchers, let alone businesses and governments, would be thrilled to have systems that function with as much learning ability, creativity, and general intelligence as an adult octopus, and yet no system today comes even close. This chapter examines the lessons AGI researchers can learn from the capabilities of the octopus and the more social animals up to and including humans.",Theoretical Foundations of Artificial General Intelligence,chapter 4
"We do not attempt to characterize the intelligence of animals and humans, but rather focus on what sort of information they have to reason with, dependencies between different sorts of information, and the degree to which they learn from or pass information to others of their species. 4.2 Octopus Intelligence The octopus is an asocial genius that survives by its wits. The common octopus (Octopus vulgaris) lives from 12 to 18 months. A mature female mates, lays tens of thousands of eggs [1], tends them until they hatch, and dies soon thereafter. The tiny octopus hatchlings disperse quickly and seldom encounter others of their species until they eventually mate. The hatchlings spend 45 to 60 days oating in ocean currents and feeding in the plankton layer where most of them perish, becoming food for other predators.",Theoretical Foundations of Artificial General Intelligence,chapter 4
"The small proportion that survive this stage grow rapidly, parachute to the sea oor, and begin a bottom dwelling life in an environment that is quite different from the plankton environment [2]. When they land on the bottom, typically far away from where they hatched, octopuses must learn very quickly and be very lucky to survive. The typical adult octopus has a relatively large brain, estimated at 300 million neurons [3]. The ratio of octopus brain to body mass is much higher than that of most sh and amphibians, a ratio more similar to that of birds and mammals. The complex lobes of the octopus brain support an acute and sensitive vision system, good spatial memory, decision-making, and camouage behavior. Sensory and motor function is neatly separated into a series of well-dened lobes... There are two parallel learning systems, one for touch and one for vision, and a clear hierarchy of motor control [4].",Theoretical Foundations of Artificial General Intelligence,chapter 4
"Each arm has smaller, mostly independent neural systems (about 50 million neurons each) that deal with      Beyond the Octopus: From General Intelligence toward a Human-like Mind 51 chemical sensors, delicate touch sensors, force sensors, and control of the muscles in that arm. All this processing power supports general intelligence, but at a cost. Neurons use more energy than other cells. Just the photoreceptors in the eyes of a y consume 8% of the ys resting energy [5]. The metabolic costs of an octopus large brain must be justied by its contribution to rapid learning of more effective foraging and more effective defenses against predators. Adult octopuses are quite clever, adaptable, and rapid learners. Experts speculate that most octopus behaviors are learned independently rather than being based on instinct. At least one researcher [6] posits that cephalopods may even have a primitive form of consciousness.",Theoretical Foundations of Artificial General Intelligence,chapter 4
"The following anecdotes illustrate some of their most notable learning and creative talents: Opening a screw top jar A ve-month-old female octopus in a Munich zoo learned to open screw-top jars containing shrimp by pressing her body on the lid, grasping the sides with her eight tentacles and repeatedly twisting her body. She apparently learned this trick by watching human hands do the same task. Using coconut halves as portable shelters An octopus in Indonesia was observed (and lmed [7]) excavating a half of a coconut husk buried in sand, carrying it to the location of another similar half many meters away, crawling into one half and pulling the other over itself to hide from predators. Shooting out the lights An aquarium in Coburg, Germany was experiencing late-night blackouts. Upon investigation it turned out that their octopus had learned to ... swing onto the edge of his tank and shoot out the 2000 Watt spot light above him with a carefully directed jet of water [8].",Theoretical Foundations of Artificial General Intelligence,chapter 4
Spatial learning Studies show that octopuses learn maps of the territory in which they hunt. Researchers have ... traced young Octopus vulgaris in Bermuda on many of these hunting excursions and returns [typically returning by routes different from their outward path]. The octopuses      52 Theoretical Foundations of Articial General Intelligence seemed to cover different parts of their home range one after another on subsequent hunts and days [9]. In controlled laboratory experiments octopuses learn to navigate mazes and optimize their paths. They nd short cuts as if they could reason about the maze in its entirety from an internal map they have constructed for themselves. Observational learning Formal experiments show that captive octopuses can learn to choose the correct colored ball from a pair placed in their tank by observing other octopuses trained to do the task [10].,Theoretical Foundations of Artificial General Intelligence,chapter 4
"More noteworthy is that it required between 16 and 22 trials to train the demonstrator octopuses via formal conditioning (food reward for correct choices and electric shock punishment for wrong choices), yet the observer octopuses learned in as few as ve trials. Camouage and behavioral mimicry All cephalopods can dramatically alter their appearance by changing the color, patterning, and texture of their skin [11]. A few species of octopus also disguise themselves by mimicking the shape and movements of other animals in their environment. One Caribbean octopus that inhabits at sandy bottoms disguises itself by imitating the coloring, shape and swimming behavior of a kind of ounder (a bottom dwelling atsh) [12]. An Indonesian octopus (Thaumoctopus mimicus) learns to mimic the shape, coloring, and movement of various poisonous or dangerous sh that the octopus potential predators avoid [13].",Theoretical Foundations of Artificial General Intelligence,chapter 4
"It impersonates several species and may shift between impersonations as it crosses the ocean oor. Individual octopuses apparently learn these tricks on their own. Researchers point out that ... all animals were well separated (50m100m apart) and all displays were observed in the absence of conspecics [14]. Using its siphon to squirt an offending spotlight and using coconut halves to build a shelter against predators have been asserted to qualify as a sort of tool use. Whether or not that assertion is fully justied, the behaviors are quite creative. Furthermore, octopus mimicry suggests an intelligent response, even a possible meta-cognitive theory of predator behavior that is used to avoid unwanted interaction with predators. Less tangible evidence of octopus general intelligence comes from the assertion by many professional aquarists that octopuses have distinct personalities [15]. Octopuses seem to be so clever, learn so fast, and are so creative that one might wonder why 99.",Theoretical Foundations of Artificial General Intelligence,chapter 4
"99% of them fail to survive long enough to reproduce. However, we must be      Beyond the Octopus: From General Intelligence toward a Human-like Mind 53 cautious about drawing conclusions from the behavior of the rare octopus that reaches adulthood. Octopuses learn so rapidly in such complex environments that many of the associations they learn can best be thought of as ineffective or even counterproductive superstitions that may be fatal the next time they are invoked. AGI systems that jump to conclusions too quickly may face a similar fate. 4.3 A Ladder of Intelligence Unlike the octopus, humans can rely upon a large legacy of knowledge learned from, and actively taught by, parents, peers, and the culture at large. Social animals also make use of legacies of information that they are able to effectively transfer from one individual to the next and one generation to the next.",Theoretical Foundations of Artificial General Intelligence,chapter 4
"More effective knowledge legacies go handin-hand with more intelligent reasoning although the correlation is far from perfect, as the octopus demonstrates. Here we discuss a metaphorical ladder of cognitive abilities, each successive rung of which is characterized by larger and more complex legacies of knowledge. Reasoning at each rung of the ladder subsumes the capabilities of the lower rungs and accumulates additional sorts of information required to reason about and interact with more complex aspects of the world. Animals on the lowest rung, the octopus being perhaps the most intelligent, are asocial. They sense and act within their own bodies and their immediate environment, learning by trial-and-error with no cooperative interactions with others. What they learn dies with them. Less solitary animals participate in increasingly complex social interactions that communicate information learned by their ancestors and peers.",Theoretical Foundations of Artificial General Intelligence,chapter 4
"Humans can act more intelligently than animals in part because we are able to share more information more effectively via oral language, music, art, crafts, customs, and rituals. Based upon oral language, humans have developed written language that supports logic, science, formal government, and ultimately civilization-wide sharing of knowledge. Human intelligence, the eventual target for AGI, depends upon the combined capabilities of all of the rungs as described below. Asocial reasoning, the lowest rung, does not require cooperation or communication with other animals. Asocial animals reproduce via eggs, often large numbers of eggs, and the young fend for themselves from birth without any parental guidance or protection. These animals learn nothing from others of their species, do not cooperate with other creatures, and pass nothing they learn on to the next generation.",Theoretical Foundations of Artificial General Intelligence,chapter 4
"Asocial animals learn      54 Theoretical Foundations of Articial General Intelligence regularities in the world on their own by interacting with an environment that is at best indifferent and at worst predatory or dangerous. Typically, only a small percentage of them survive to adulthood. Social reasoning arises in animals where the young are tended by parents and interact with siblings and perhaps others of their species. As a result, they learn in an environment largely shaped by the parents. One generation thereby passes some knowledge to the next: what is edible and where to nd it in the environment, how to hunt, or what predators to avoid and how to do so. Animal cultural reasoning, found in species such as primates, elephants, corvids, dolphins, wolves, and parrots, requires considerably more transfer of information from one generation to the next. Parents and others of such species actively teach the young over relatively long childhoods. Communication in these species includes non-linguistic but nonetheless complex vocalizations and gestures.",Theoretical Foundations of Artificial General Intelligence,chapter 4
"Parents must teach those communication skills in addition to accumulated culture. Oral linguistic reasoning is unique to humans despite proto-linguistic behavior at the animal cultural rung. Language not only supports a much richer transfer of intergenerational information, but also a much richer sort of reasoning. Oral language is evanescent, however, not lingering in the minds of either speaker or listener for long unless deliberately memorized. Thus oral cultures are rich in social practices that aid memory such as ritual, storytelling, and master-apprentice relationships. Literate reasoning depends upon oral language, but is qualitatively different from oral linguistic reasoning. For its rst thousand years, writing merely preserved oral utterances. Reading required speaking out-loud until the ninth century [16] and even today many readers silently verbalize internally as they read. In the western hemisphere, literate skills were conned to a small subculture of priests and scribes for hundreds of years until literacy began to spread rapidly in the Renaissance.",Theoretical Foundations of Artificial General Intelligence,chapter 4
"Language committed to writing has several advantages over speech. Writing can immortalize long complex structures of words in the form of books and libraries of books. The preserved words can be reread and reinterpreted over time and thereby enable much longer and more complex chains of reasoning that can be shared by a larger group of thinkers. The collaboration enabled by written language gave birth to science, history, government, literature and formal reasoning that could not be supported by oral communication alone.      Beyond the Octopus: From General Intelligence toward a Human-like Mind 55 Finally, Civilization-scale reasoning applies to the ideas and behavior of large populations of humans, even entire civilizations. Such ideas impact what every individual human says or does. Long-lasting ideas (memes) evolve, spread and recombine in huge, slow, interconnected human systems exemplied by philosophies, religions, empires, technologies and commerce over long timescales and large geographies.",Theoretical Foundations of Artificial General Intelligence,chapter 4
"Recent technologies such as the Internet and worldwide live satellite television have accelerated the spread and evolution of memes across these temporal and geographic scales. Nonetheless, long-standing differences in language, religion, philosophy, and culture still balkanize civilizations. Within a human mind, all rungs are active at all times, in parallel, with different agendas that compete for shared resources such as where to direct the eyes, which auditory inputs to attend to (the cocktail party effect [17]), what direction to move (e.g., ght or ight decisions), or what the next utterance will be. For example, direction of gaze is a social signal for humans and many animals precisely because it provides information about which of many internal agendas has priority. 4.4 Linguistic Grounding Linguistic communication depends upon understanding the meaning of words (the familiar Symbol Grounding Problem [18]) as well as the meaning of longer utterances.",Theoretical Foundations of Artificial General Intelligence,chapter 4
"In social circumstances, the meaning of an utterance may include context from a prior utterance in the current conversation or at some time in the past. Or the meaning may simply be that it was uttered at all in a particular social context [19]. Each rung of the ladder surfaces in human language. The meaning of individual words and multi-word utterances often are grounded far lower than the verbal rung and often involve memes at more than one level. For example, stumble, crawl, fall, hungry, startle, pain, and sleep are grounded on basic facts of human bodies. We also use such words metaphorically e.g., stumble upon some problem or situation or trip over an awkward fact. We also hunger for love and slow to a crawl. Words such as regret and remorse are grounded in the subtleties of human emotion and memory.",Theoretical Foundations of Artificial General Intelligence,chapter 4
"An AGI cannot be expected to understand such words based only on dictionary denitions, or a semantic net, without having some exposure to the underlying phenomena to which they refer. Consider the rungs at which the root meanings of the following English words are grounded:      56 Theoretical Foundations of Articial General Intelligence  Hide, forage, hunt, kill, ee, eat, what and where are most deeply grounded on the asocial rung. They typically signal object parsing (what), spatial reasoning (where) and other survival issues crucial even to an asocial individual. Yet these same words may take on other meanings in a human social arena when they involve group behavior. To properly interpret such words either literally or metaphorically requires some gutlevel grounding on the asocial rung.  Nurture, protect, feed, bond, give, and share are grounded within the social rung. They refer to issues fundamental to the social relations within groups of animals, including humans.",Theoretical Foundations of Artificial General Intelligence,chapter 4
"Who is especially crucial because humans and other social animals often must recognize individuals of their species to determine if they are likely to be friendly, neutral, dangerous, or a competitor. Individuals are distinguishable from one another by subtle visual, olfactory, auditory, or movement cues that do not translate readily into language.  Follow, cooperate, play, lead, warn, trick, steal (as opposed to simply take), and teach (in the sense of interacting in a way designed to maximize its teaching value) are grounded within the non-linguistic animal cultural rung where more instinctive social behaviors extend into intentional meta-cognition (e.g., theory-of-mind). These behaviors occur in groups of elephants, corvids, cetaceans, parrots and primates, among others. They depend not only upon accurate classication of relationships and recognition of individuals and their relative roles, but also on memories of the history of each relationship.",Theoretical Foundations of Artificial General Intelligence,chapter 4
"Promise, apologize, oath, agree, covenant, name (as in a persons name or the name of an object or place), faith, god, game, gamble, plan, lie (or deceive), ritual, style, status, soul, judge, sing, clothing (hence nakedness), and above all why, are grounded in the human oral linguistic rung and depend on shared culture, customs and language  but are not grounded in literacy.  Library, contract, ction, technology, essay, spelling, acronym, document, and book are grounded in literate culture that has accumulated collective wisdom in written documents. New but similar conventions have already grown around video and audio recordings. Not only can such documents be read, debated, and reasoned about over wide geographies and time periods, but they also support more complex interrelated arguments across multiple documents.",Theoretical Foundations of Artificial General Intelligence,chapter 4
"Above the level of human current affairs, there are more abstract concepts such as democracy, empire, philosophy, science, mathematics, culture, economy, nation, liter     Beyond the Octopus: From General Intelligence toward a Human-like Mind 57 ature and many others that are about vast collections of memes evolving over decades or centuries within the minds of large numbers of people. Individual humans have some local sense of the meaning of such concepts even though their understanding may be little better than a shs awareness of water. An AGI that could not convincingly use or understand most of the above words and thousands more like them will not be able to engage even in at, monotone, prosaic human conversations. In our view, such an AGI would simply not be at the human-level no matter how well it can do nonverbal human tasks. AGI systems will need to be evaluated more like human toddlers [20] instead of adult typists in a Turing Test. 4.",Theoretical Foundations of Artificial General Intelligence,chapter 4
"5 Implications of the Ladder for AGI The ladder metaphor highlights the accumulation of knowledge from generation to generation and the communication of that knowledge to others of the species. Each rung of the ladder places unique requirements on knowledge representation and the ontologies required for reasoning at that level. The term ontology is used differently, albeit in related ways, in philosophy, anthropology, computer science, and in the Semantic Web [21]. One denition of ontology commonly used in computer science is: a formal representation of a set of concepts within a domain and the relationships between those concepts. In philosophical metaphysics, ontology is concerned with what entities exist or can be said to exist, how such entities can be grouped or placed in some hierarchy, or grouped according to similarities and differences. Within recent anthropological debates, it has been argued that ontology is just another word for culture [22]. None of the above denitions quite do the trick in our context.",Theoretical Foundations of Artificial General Intelligence,chapter 4
"For the purposes of the following discussion, the term ontology is used to describe the organization of the internal artifacts of mental existence in an intelligent system or subsystem, including an AGI system. It is not our goal here to dene a specic ontology for an AGI. In fact we argue that goal is pointless, if not impossible, because an ontology appropriate for a solitary asocial octopus has little in common with one appropriate for a herd herbivore such as a bison, a very long lived highly social animal such as an elephant, or a linguistically competent human. Instead, we seek to explore the implications of the different design choices faced by researchers seeking to develop AGI systems [23].",Theoretical Foundations of Artificial General Intelligence,chapter 4
"Since we are concerned here with human-level AGI, we will discuss the ontological issues related to the human version of      58 Theoretical Foundations of Articial General Intelligence the rungs of the ladder: asocial, social, animal cultural, linguistic, literate, and civilizationlevel. Let us examine each in turn. Asocial ontologies  Humans share with many asocial animals the ability to process and act upon visual 2D data and other spatial maps. Octopuses, insects, crabs and even some jellysh [24] use visual information for spatial navigation and object identication. The octopus is exceptionally intelligent, with complex behavior betting its large brain and visual system. Its ontology has no need for social interaction and may encompass no more than a few hundred categories or concepts representing the various predators and prey it deals with, perhaps landmarks in their territories, maps of recent foraging trips and tricks of camouage.",Theoretical Foundations of Artificial General Intelligence,chapter 4
"It presumably does not distinguish one instance of a class from another, for example, one particular damselsh from another. Octopus ontology also presumably supports the temporal sequences that underlie the ability of the octopus to make and execute multi-step plans such as shooting out the lights, opening shrimp jars, or building coconut shell shelters, although one can posit other mechanisms. Humans also have equivalents of other asocial processing abilities such as the ability to sense and process information about temporal ordering, proprioception, audition, and the chemical environment (smell, taste). What can AGI researchers learn from such parallels? In humans, the rungs are not as separable as AGI researchers might wish. Human infants are hardwired to orient to faces, yet that hardwired behavior soon grows into a social behavior. Infants cry asocially at rst, without consideration of impact or implications on others, but they soon learn to use crying socially.",Theoretical Foundations of Artificial General Intelligence,chapter 4
"The same can be said for smiling and eating, rst applied asocially, and then adapted to social purposes. In summary, many of our asocial behaviors and their supporting ontology develop over infancy into social behaviors. The social versions of asocial behaviors seem to be elaborations, or layers, that obscure but do not completely extinguish the initial asocial behavior, e.g., unceremoniously wolng down food when very hungry, or crying uncontrollably when tragedy strikes. Many behaviors apparent in infants are asocial simply because they are grounded in bodies and brains. Yet we learn to become consciously aware of many of our asocial behaviors, which then become associated with social concepts and become social aspects of our ontology. Because humans learn them over many years in the midst of other simultaneously operating rungs, the ontological categories inevitably become intertwined in ways difcult to disentangle.",Theoretical Foundations of Artificial General Intelligence,chapter 4
"Learning to understand the issues characteristic of the asocial rung by building an asocial octopus-level AGI would therefore be a good strategy for separation of concerns.      Beyond the Octopus: From General Intelligence toward a Human-like Mind 59 Social ontologies  Social interaction provides a richer learning experience than does hatching into an asocial existence. It ensures that learned ontologies about the environment, foods, and early experiences are biased by the parents and siblings. By sharing a nest or other group-dened environment the experiences of the young are much more like one another, teaching them what they need to know socially. What they learn may be communicated via posture, body language, herd behavior, behavioral imprinting (e.g., ducklings imprinting on their mother), pheromones, and many other means. Indirect interaction also may occur via persistent signals deposited on inanimate features of the environment, a phenomenon known as stigmergy [25].",Theoretical Foundations of Artificial General Intelligence,chapter 4
"Stigmergy is best understood in social insects where signals deposited on physical structures, e.g., termite mounds, honeycombs, or ant trails, affect and partially organize the behavior of the insects. Any modication of the environment by an individual that can inuence the behavior of others of its kind can also produce stigmergy. Nearly all higher animals, including humans, make extensive use of such indirect communication channels. Humans create especially rich stigmergy structures: clothes, jewelry, pottery, tools, dwellings, roads, cities, art, writing, video and audio recordings, and most recently the Internet. Social animals necessarily have larger and more complex ontologies than asocial animals because, in addition to what a comparable asocial animal must learn, social animals must learn how to communicate with others of their species.",Theoretical Foundations of Artificial General Intelligence,chapter 4
"That requires concepts and ontological categories for the communicative signals themselves (body postures, chemical signals such as urine as a territorial marker, sounds, touch, facial expressions and many others) as well as the ability to associate them to specic behaviors that the learner can interpret or mimic. The human version of such primitive social behavior includes social dominance and submission signals, group membership awareness, predator awareness and warnings. These cognitive skills are crucial to successful cooperation and perhaps to forming primitive morals, such as those that minimize fratricide or incest. Cultural ontologies  The most intelligent social species actively teach their young. Examples include elephants, primates, corvids (ravens, crows, magpies, etc.), parrots, dolphins, whales and wolves. Many of these explicitly pass on information via structured and emotive utterances that lack the syntactic structure necessary to qualify as a language  call them proto-languages. Elephant groups in the wild communicate with each other using ...",Theoretical Foundations of Artificial General Intelligence,chapter 4
"more than 70 kinds of vocal sounds and 160 different visual and tactile signals, expressions, and ges     60 Theoretical Foundations of Articial General Intelligence tures in their day-to-day interactions [26]. Wild elephants exhibit behaviors associated with grief, allomothering (non-maternal infant care), mimicry, a sense of humor, altruism, use of tools, compassion, and self recognition in a mirror [27]. Parrots develop and use individual unique names for each other that also encode their family and close knit clan relationships [28]. Crows even recognize and remember individual human faces and warn each other about humans that have been observed mistreating crows. These warnings spread quickly throughout the ock [29].",Theoretical Foundations of Artificial General Intelligence,chapter 4
"Aided by theory-of-mind insights into the juvenile learners forming mind, these species purposefully teach about what foods are safe, what predators to ee, the meaning of group signals such as postures and vocalization, the best places to hunt or nd water, cooperative behaviors when hunting, and whos who in the social structure of the group. Ontologies needed to support animal social cultures must be rich enough to allow for learning the vocal and non-vocal signals used to organize cooperative behavior such as group hunting or defense. The ontology must also support recognition of individuals, placement of those individuals in family and clan relationships, and meta-cognitive (Theory of Mind) models of individuals. Human versions of the animal cultural rung are quite similar to the animal version when the knowledge to be transferred is not well suited to verbal description. Non-verbal examples might include playing musical instruments, dancing, shing, athletic activities such as skiing, and perhaps the art of cooking.",Theoretical Foundations of Artificial General Intelligence,chapter 4
"We are, however, so skilled at developing vocabulary to teach verbally that completely non-verbal human teaching is relatively uncommon. Building a non-verbal cultural AGI beginning with a primitive social AGI may be a difcult step because it will require non-verbal theory-of-mind. It may be strategically important for AGI researchers to rst learn to build a primitive social AGI before attempting to address human culture because it is very difcult to determine whether a given human juvenile behavior has been learned by passive proximity (i.e., social mimicry) rather than by active teaching by adults or siblings (i.e., culture). That distinction is nonetheless important because humans tend to be less conscious of behavior learned by passive mimicry than behavior actively taught. And some behaviors, such as empathy, are very difcult, if not impossible, to teach.",Theoretical Foundations of Artificial General Intelligence,chapter 4
"A common example is epitomized by a parent saying, I just dont know how Mary learned that, when everyone else recognizes that Mary learned it by mimicking the parent. Moreover, an AGI that did not learn important primitive human social skills, skipping instead to the cultural learning rung may      Beyond the Octopus: From General Intelligence toward a Human-like Mind 61 turn out to be an AGI sociopath: a completely asocial, completely selsh predator with an overlaid set of behavioral rules taught without the necessary social grounding. Oral linguistic ontologies  Human language (including fully syntactic sign language) facilitates an explosion of concepts which at this level are more appropriately called memes [30]. Meme, as we use the term here, approximates the notion of a concept, often but not necessarily expressible in words. Language both supports and reects a richer and more complex meme structure than is possible without language.",Theoretical Foundations of Artificial General Intelligence,chapter 4
"Oral cultures are not dumbed-down literate cultures (a common misconception). Primary oral cultures  those that have never had writing  are qualitatively different from literate cultures [31]. Language in primary oral societies uses a more complex and idiosyncratic syntax than written language, with rules and customs for combining prexes, sufxes, and compound words that are more exible than those used in writing. The rules of oral language have to do with the sound, intonation and tempo of the language as spoken throughout prehistory and in the many non-literate societies that still exist. Such rules dene allowable vowel and consonant harmonies, or restrict allowable phoneme usage such as two phonemes that may not occur in the same word [32]. Oral cultures use constructs such as rhyme and rhythm, alliteration and other oratorical patterns to aid in the memorability of utterances without the aid of a written record.",Theoretical Foundations of Artificial General Intelligence,chapter 4
"Oral cultures also aid memorability via rituals, chanting, poetry, singing, storytelling, and much repetition. And they employ physical tokens and other stigmergy structures such as ritual masks and costumes, notched sticks for recording counts, decorations on pottery or cave walls, clothing and decorations symbolic of status, astrology (used for anticipating and marking seasons, e.g., Stonehenge) and the like. AGI researchers will need to be exceedingly careful to properly develop an orallanguage human-level AGI because academics are so thoroughly steeped in literate intelligence that they may nd it difcult to put that experience aside. For example, literate people nd it very difcult to grasp that the notion of a word is not necessarily well dened and hence it is not necessarily the atomic base of language nor the fundamental ontological concept for a primary oral language (or sign language) [33].",Theoretical Foundations of Artificial General Intelligence,chapter 4
"Literate ontologies  Writing emerged from spoken language in complicated ways that vary with the specic language and culture [34]. Writing and other forms of long-term recorded thought allow generations and populations far removed from each other, temporally or physically, to share verbal knowledge, reasoning, experience of events (history), literature, styles of thought, philosophy, religion and technologies. Caravans and ships that once transmitted verbal gossip, tall tales, and rumors, began also to carry scrolls and letters      62 Theoretical Foundations of Articial General Intelligence which communicated more authoritative information. Literacy exposes people to a wider variety of memes than does an oral culture. As literacy became more common, the memes transmitted via writing grew in number and importance. The specicity and permanence of a written work also allows more formal and longerrange relationships between the parts of the written argument than can be accomplished solely with real-time verbal communication.",Theoretical Foundations of Artificial General Intelligence,chapter 4
"The development in the last century of recorded audio and video has similarly and dramatically changed the way we learn and reason. And now that these kinds of media are globally available over the Internet, we can expect further changes in human reasoning and ontology. Since AGI researchers are familiar with the relationship between language and intelligence, little more need be said here. But that familiarity does not necessarily tell us what must be added to the ontology or the reasoning skills of an oral-language AGI to support literacy. It took a millennium after the invention of writing for humans to widely adopt literacy. We suspect that there are some mysteries hidden in the transition that will surface only when AGI researchers attempt to add literacy to an oral-only system. Understanding written material and writing original material are not simple modications of conversation. Writing does not provide for immediate feedback between the writer and the reader to signal understanding or disagreement, nor a clear context in which the information exchange is embedded.",Theoretical Foundations of Artificial General Intelligence,chapter 4
"The reader cannot interrupt to ask what the writer really means, or why the author even bothered to write the material in the rst place. What would AGI researchers need to add to an orally competent conversational AGI for it to pick up a book on its own and read it? Or sequester itself in some upper room to write a book, or even a prosaic email, or a tweet? We simply dont know. Civilization-scale ontologies  Over the longer term and wider geography, literacy and other persistent forms of human knowledge can affect large numbers of people who combine and recombine ideas (memes) in new ways to form new memes. These memes spread throughout and across cultures to be further combined and recombined and accepted or forgotten by large portions of the population. Ideas like money, capitalism, democracy, orchestral music, science, agriculture, or Articial Intelligence can gain or lose momentum as they travel from mind to mind across generations, centuries, and cultures.",Theoretical Foundations of Artificial General Intelligence,chapter 4
"Cultural memes of this sort are not phenomena that operate at the level of individuals, or even small groups of individuals. They do not run their course in only a few months or years. For example, the idea of humans landing on the moon played out over a century (from Jules Verne, a French author writing in the early 1860s, by way of German rocket scientists in      Beyond the Octopus: From General Intelligence toward a Human-like Mind 63 the 1940s, to the American Apollo 11 landing in 1969). At no time did any human mind encompass more than a tiny portion of the knowledge required for Neil Armstrong to make his one giant leap for mankind. Humanity collectively reasons about the world in slow, subtle and unobservable ways, and not necessarily with the help of literacy. Some ancient civilizations appear not to have relied on writing, e.g., the Indus Valley civilization [35].",Theoretical Foundations of Artificial General Intelligence,chapter 4
"Modern civilizations are continually being changed by audio and video shared over radio, television, mobile phones, and the Internet. Live television broadcasts allowed people worldwide to share the experience of the Apollo-11 moon landing, the 2001 destruction of the World Trade Towers, the recent Arab-spring events, and disasters such as oods, earthquakes and tsunamis. Widely shared events or manifestations of ideas directly affect civilization-level reasoning that is seldom observable in any individual human, yet is increasingly easy to observe in the Internet [36]. The ontological requirements for supporting intelligence at this level are largely unexplored. The civilization-level may turn out to be where researchers rst succeed in building an AGI that can surpass some of the abilities of humans on the basis of orders of magnitude more memory and search speed. IBMs WATSON [37] seemed to do so, but WATSON isnt even an AGI, let alone a human-level AGI. Time will tell.",Theoretical Foundations of Artificial General Intelligence,chapter 4
"First, the AGI needs to be competent in all the other rungs simply to make any sense of what it reads, sees, and hears in the libraries of civilization, or their digital equivalents on the Internet. 4.6 Conclusion The octopus is clearly quite clever. Building an AGI with intelligence roughly equivalent to that of an octopus would be quite a challenge, and perhaps an unwise one if it were allowed to act autonomously. A human-level AGI is far more challenging and, we believe, quite hopeless if one attempts to start at the higher rungs of the intelligence ladder and somehow nesse the lower rungs or ll them in later. From the beginning of ancient philosophical discourse through the recent decades of AI and now AGI research, mankinds quest to understand and eventually emulate the human mind in a machine has borne fruit in the ever-increasing understanding of our own intelligence and behavior as well as the sometimes daunting limitations of our machines.",Theoretical Foundations of Artificial General Intelligence,chapter 4
"The human mind does not exist in splendid isolation. It depends on other minds in other times and places interacting in multiple ways that we characterize in terms of a metaphorical lad     64 Theoretical Foundations of Articial General Intelligence der. Mapping out the journey ahead and acknowledging the challenges before us, we must begin at the base of the ladder and climb one rung at a time. Bibliography [1] V. Hernndez-Garca, J.L. Hernndez-Lpez, and J.J. Castro-Hdez, On the reproduction of Octopus vulgaris off the coast of the Canary Islands, Fisheries Research, Vol. 57, No. 2, August 2002, pp. 197203. [2] M. Nixon and K.",Theoretical Foundations of Artificial General Intelligence,chapter 4
"Mangold, The early life of Octopus vulgaris (Cephalopoda: Octopodidae) in the plankton and at settlement: a change in lifestyle, Journal of Zoology, Vol. 239, Issue 2, pp. 301327, June (1996). [3] J.Z. Young, The anatomy of the nervous system of Octopus vulgaris, Claredon Press Oxford (1971). [4] M. Wells, Review of The Anatomy of the Nervous System of Octopus vulgaris. By J.Z. Young, J. Anat. 1972 May; 112 (Pt 1): 144. [5] J.E. Niven, J.C. Anderson, and S.B. Laughlin, Fly Photoreceptors Demonstrate EnergyInformation Trade-Offs in Neural Coding PLoSBiol 5 (4): e116 (2007). [6] J.",Theoretical Foundations of Artificial General Intelligence,chapter 4
"Mather, Consciousness in Cephalopods?, J. Cosmology, vol. 14 (2011). [7] M. Kaplan, Bizarre Octopuses Carry Coconuts as Instant Shelters, National Geographic News, December 15, 2009. [8] A. Seabrook, The Story Of An Octopus Named Otto, All Things Considered, National Public Radio, aired November 2, 2008. [9] J. Mather, Journal of Comparative Physiology A 168, 491497 (1991). [10] G. Fiorito and P. Scotto, Observational Learning in Octopus vulgaris, Science, vol. 256, 24 April 1992, pp. 545547. [11] J.B. Messenger, Cephalopod chromatophores; neurobiology and natural history, Biological Reviews, Vol. 76, No. 4, pp.",Theoretical Foundations of Artificial General Intelligence,chapter 4
"473528. (2001). [12] R.T. Hanlon, A.C. Watson, and A Barbosa, A Mimic Octopus in the Atlantic: Flatsh Mimicry and Camouage by Macrotritopus delippi, Bio. Bull. 218: 1524 February 2010. [13] M.D. Norman and F.G. Hochberg, The Mimic Octopus (Thaumoctopus mimicus), a new octopus from the tropical Indo-West Pacic, Molluscan Research, Vol. 25: 5770 (2006). [14] M.D. Norman, J. Finn and T. Tregenza,Dynamic mimicry in an Indo-Malayan octopus, Proc. R. Soc. Lond. B, 268, 17551758 (2001). [15] J.A.",Theoretical Foundations of Artificial General Intelligence,chapter 4
"Mather, To boldly go where no mollusk has gone before: Personality, play, thinking, and consciousness in cephalopods, American Malacological Bulletin, 24 (1/2): 5158. (2008). [16] A. Manguel, A History of Reading, New York; Viking Press, (1996). [17] E.C. Cherry, Some Experiments on the Recognition of Speech, with One and with Two Ears, J. Acoust. Soc. Amer., Vol. 25, Issue 5, pp. 975979 (1953). [18] S. Harnad, The Symbol Grounding Problem, Physica D, 42: 335346. (1990). [19] C. Fleisher Feldman, Oral Metalanguage, in Literacy and Orality, Olson, D.R., & Torrance, N. (eds.) Cambridge Univ. Press, (1991).",Theoretical Foundations of Artificial General Intelligence,chapter 4
"[20] N. Alvarado, S.S. Adams, S. Burbeck, and C. Latta, Beyond the Turing Test: Performance metrics for evaluating a computer simulation of the human mind, Proceedings of The 2nd International Conference on Development and Learning (ICDL02), Cambridge, MA, IEEE Computer Society (2002). [21] T. Berners-Lee, J. Hendler, and O. Lassila, The Semantic Web, Scientic American Magazine, May 2001.      Bibliography 65 [22] S. Venkatesan, Ontology Is Just Another Word for Culture: Motion Tabled at the 2008 Meeting of the Group for Debates in Anthropological Theory, University of Manchester, Anthropology Today, Vol. 24, No. 3, p. 28 (2008). [23] A.",Theoretical Foundations of Artificial General Intelligence,chapter 4
"Sloman, Evolved Cognition and Articial Cognition: Some Genetic/Epigenetic Trade-offs for Organisms and Robots, CiteSeerX, doi: 10.1.1.193.7193 (2011). [24] A. Garm, M. Oskarsson, and D. Nilsson,Box Jellysh Use Terrestrial Visual Cues for Navigation, Current Biology, Vol. 21, Issue 9, pp. 798803 (2011). [25] http://evolutionofcomputing.org/Multicellular/Stigmergy.html [26] D.L. Parsell, In Africa, Decoding the Language of Elephants, National Geographic News, February 21, 2003. [27] J.M. Plotnik, F.B.M. de Waal, and D.",Theoretical Foundations of Artificial General Intelligence,chapter 4
"Reiss, Self-recognition in an Asian elephant, Proc Natl Acad Sci USA, October 30, 2006. 103 (45). [28] V. Morell, Parrotlet Chicks Learn Their Calls From Mom and Dad, Science NOW, July 12, 2011. [29] J.M. Marzluff, J. Walls, H.N. Cornell, J.C. Withey, and D.P. Craig, Lasting recognition of threatening people by wild American crows, Animal Behaviour, Vol. 79, No. 3, March 2010, pp. 699707. [30] S. Blackmore, Imitation and the denition of a meme, Journal of Memetics Evolutionary Models of Information Transmission, Vol. 2 (1998). [31] W.J. Ong, Orality and Literacy: The Technologizing of the Word (second edition; orig. 1982).",Theoretical Foundations of Artificial General Intelligence,chapter 4
"Routledge, London and New York, (2002). [32] R. Applegate, in Papers on the Chumash, San Luis Obispo County Archaeological Society occasional paper number nine (1975). [33] D.R. Olson, and N. Torrance (eds.), Literacy and Orality, Cambridge Univ. Press, (1991). [34] Ibid. [35] S. Farmer, R. Sproat, and M. Witzel. The Collapse of the Indus-Script Thesis: The Myth of a Literate Harappan Civilization, Electronic Journal of Vedic Studies (EJVS), Vol. 11. No. 2 (2004) pp. 1957. [36] J. Michel, Y.K. Shen, A. Presser Aiden, A. Veres, M.K. Gray, W. Brockman, The Google Books Team, J.P. Pickett, D.",Theoretical Foundations of Artificial General Intelligence,chapter 4
"Hoiberg, D. Clancy, P. Norvig, J. Orwant, S. Pinker, M.A. Nowak, and E. Lieberman Aiden. Quantitative Analysis of Culture Using Millions of Digitized Books, Science, December 16, 2010. [37] D. Ferrucci, E. Brown, J. Chu-Carroll, J. Fan, D. Gondek, A.A. Kalyanpur, A. Lally, J.W. Murdock, E. Nyberg, J. Prager, N. Schlaefer, and C. Welty,Building Watson: An Overview of the DeepQA Project, AI Magazine, Association for the Advancement of Articial Intelligence, (2010).",Theoretical Foundations of Artificial General Intelligence,chapter 4
"  Chapter 5 One Decade of Universal Articial Intelligence Marcus Hutter RSCS@ANU and SML@NICTA Canberra, ACT, 0200, Australia & Department of Computer Science ETH Zrich, Switzerland The rst decade of this century has seen the nascency of the rst mathematical theory of general articial intelligence. This theory of Universal Articial Intelligence (UAI) has made signicant contributions to many theoretical, philosophical, and practical AI questions. In a series of papers culminating in book [24] an exciting sound and complete mathematical model for a super intelligent agent (AIXI) has been developed and rigorously analyzed. While nowadays most AI researchers avoid discussing intelligence, the award-winning PhD thesis [38] provided the philosophical embedding and investigated the UAI-based universal measure of rational intelligence, which is formal, objective and non-anthropocentric.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"Recently, effective approximations of AIXI have been derived and experimentally investigated in JAIR paper [79] This practical breakthrough has resulted in some impressive applications, nally muting earlier critique that UAI is only a theory. For the rst time, without providing any domain knowledge, the same agent is able to selfadapt to a diverse range of interactive environments. For instance, AIXI is able to learn from scratch to play TicTacToe, Pacman, Kuhn Poker, and other games by trial and error, without even providing the rules of the games. These achievements give new hope that the grand goal of Articial General Intelligence is not elusive. This chapter provides an informal overview of UAI in context. It attempts to gently introduce a very theoretical, formal, and mathematical subject, and discusses philosophical and technical ingredients, traits of intelligence, some social questions, and the past and future of UAI.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"The formulation of a problem is often more essential than its solution, which may be merely a matter of mathematical or experimental skill. To raise new questions, new possibilities, to regard old problems from a new angle, requires creative imagination and marks real advance in science.  Albert Einstein (18791955) 67      68 Theoretical Foundations of Articial General Intelligence 5.1 Introduction The dream. The human mind is one of the great mysteries in the Universe, and arguably the most interesting phenomenon to study. After all, it is connected to consciousness and identity which dene who we are. Indeed, a healthy mind (and body) is our most precious possession. Intelligence is the most distinct characteristic of the human mind, and one we are particularly proud of. It enables us to understand, explore, and considerably shape our world, including ourselves.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"The eld of Articial Intelligence (AI) is concerned with the study and construction of artifacts that exhibit intelligent behavior, commonly by means of computer algorithms. The grand goal of AI is to develop systems that exhibit general intelligence on a human-level or beyond. If achieved, this would have a far greater impact on human society than all previous inventions together, likely resulting in a post-human civilization that only faintly resembles current humanity [31,36]. The dream of creating such articial devices that reach or outperform our own intelligence is an old one with a persistent great divide between optimists and pessimists.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"Apart from the overpowering technical challenges, research on machine intelligence also involves many fundamental philosophical questions with possibly inconvenient answers: What is intelligence? Can a machine be intelligent? Can a machine have free will? Does a human have free will? Is intelligence just an emergent phenomenon of a simple dynamical system or is it something intrinsically complex? What will our Mind Children be like? How does mortality affect decisions and actions? to name just a few. What was wrong with last centurys AI. Some claim that AI has not progressed much in the last 50 years. It denitely has progressed much slower than the fathers of AI expected and/or promised. There are also some philosophical arguments that the grand goal of creating super-human AI may even be elusive in principle. Both reasons have lead to a decreased interest in funding and research on the foundations of Articial General Intelligence (AGI).",Theoretical Foundations of Artificial General Intelligence,chapter 5
"The real problem in my opinion is that early on, AI has focussed on the wrong paradigm, namely deductive logical; and being unable to get the foundations right in this framework, AI soon concentrated on practical but limited algorithms. Some prominent early researchers such as Ray Solomonoff, who actually participated in the 1956 Dartmouth workshop, generally regarded as the birth of AI, and later Peter Cheeseman and others, advocated a probabilistic inductive approach but couldnt compete with the soon dominating gures such as Marvin Minsky, Nils Nilsson, and others who advocated a sym     One Decade of Universal Articial Intelligence 69 bolic/logic approach as the foundations of AI. (of course this paragraph is only a caricature of AI history). Indeed it has even become an acceptable attitude that general intelligence is in principle unamenable to a formal denition.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"In my opinion, claiming something to be impossible without strong evidence sounds close to an unscientic position; and there are no convincing arguments against the feasibility of AGI [6,38]. Also, the failure of once-thought-promising AI-paradigms at best shows that they were not the right approach or maybe they only lacked sufcient computing power at the time. Indeed, after early optimism mid-last century followed by an AI depression, there is renewed, justied, optimism [56, Sec. 1.3.10], as is evident by the new conference series on Articial General Intelligence, the Blue Brain project, the Singularity movement, and this book prove. AI research has come in waves and paradigms (computation, logic, expert systems, neural nets, soft approaches, learning, probability). Finally, with the free access to unlimited amounts of data on the internet, information-centered AI research has blossomed. New foundations of A(G)I.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"Universal Articial Intelligence (UAI) is such a modern information-theoretic inductive approach to AGI, in which logical reasoning plays no direct role. UAI is a new paradigm to AGI via a path from universal induction to prediction to decision to action. It has been investigated in great technical depth [24] and has already spawned promising formal denitions of rational intelligence, the optimal rational agent AIXI and practical approximations thereof, and put AI on solid mathematical foundations. It seems that we could, for the rst time, have a general mathematical theory of (rational) intelligence that is sound and complete in the sense of well-dening the general AI problem as detailed below. The theory allows a rigorous mathematical investigation of many interesting philosophical questions surrounding (articial) intelligence. Since the theory is complete, denite answers can be obtained for a large variety of intelligence-related questions, as foreshadowed by the award winning PhD thesis of [38]. Contents.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"Section 5.2 provides the context and background for UAI. It will summarize various last centurys paradigms for and approaches to understanding and building articial intelligences, highlighting their problems and how UAI is similar or different to them. Section 5.3 then informally describes the ingredients of UAI. It mentions the UAI-based intelligence measure only in passing to go directly to the core AIXI denition. In which sense AIXI is the most intelligent agent and a theoretical solution of the AI problem is explained. Section 5.4 explains how the complex phenomenon of intelligence with all its      70 Theoretical Foundations of Articial General Intelligence facets can emerge from the simple AIXI equation. Section 5.5 considers an embodied version of AIXI embedded into our society. I go through some important social questions and hint at how AIXI might behave, but this is essentially unexplored terrain.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"The technical state-of-the-art/development of UAI is summarized in Section 5.6: theoretical results for AIXI and universal Solomonoff induction; practical approximations, implementations, and applications of AIXI; UAI-based intelligence measures, tests, and denitions; and the human knowledge compression contest. Section 5.7 concludes this chapter with a summary and outlook how UAI helps in formalizing and answering deep philosophical questions around AGI and last but not least how to build super intelligent agents. 5.2 The AGI Problem The term AI means different things to different people. I will rst discuss why this is so, and will argue that this due to a lack of solid and generally agreed-upon foundations of AI. The eld of AI soon abandoned its efforts of rectifying this state of affairs, and pessimists even created a defense mechanism denying the possibility or usefulness of a (simple) formal theory of general intelligence.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"While human intelligence might indeed be messy and unintelligible, I will argue that a simple formal denition of machine intelligence is possible and useful. I will discuss how this denition ts into the various important dimensions of research on (articial) intelligence including humanrational, thinkingacting, topdownbottom-up, the agent framework, traits of intelligence, deductioninduction, and learningplanning. The problem. I dene the AI problem to mean the problem of building systems that possess general, rather than specic, intelligence in the sense of being able to solve a wide range of problems generally regarded to require human-level intelligence. Optimists believe that the AI problem can be solved within a couple of decades [36]. Pessimists deny its principle feasibility on religious, philosophical, mathematical, or technical grounds (see [56, Chp. 26] for a list of arguments). Optimists have refuted/rebutted all those arguments (see [6, Chp.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"9] and [38]), but havent produced super-human AI either, so the issue remains unsettled. One problem in AI, and I will argue key problem, is that there is no general agreement on what intelligence is. This has lead to endless circular and often fruitless arguments, and has held up progress. Generally, the lack of a generally-accepted solid foundation makes high card houses fold easily. Compare this with Russells paradox which shattered the      One Decade of Universal Articial Intelligence 71 foundations of mathematics, and which was nally resolved by the completely formal and generally agreed-upon ZF(C) theory of sets.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"On the other hand, it is an anomaly that nowadays most AI researchers avoid discussing or formalizing intelligence, which is caused by several factors: It is a difcult old subject, it is politically charged, it is not necessary for narrow AI which focusses on specic applications, AI research is done primarily by computer scientists who mainly care about algorithms rather than philosophical foundations, and the popular belief that general intelligence is principally unamenable to a mathematical denition. These reasons explain but only partially justify the limited effort in trying to formalize general intelligence. There is no convincing argument that this is impossible. Assume we had a formal, objective, non-anthropocentric, and direct denition, measure, and/or test of intelligence, or at least a very general intelligence-resembling formalism that could serve as an adequate substitute. This would bring the higher goals of the eld into tight focus and allow us to objectively and rigorously compare different approaches and judge the overall progress.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"Formalizing and rigorously dening a previously vague concept usually constitutes a quantum leap forward in the eld: Cf. the history of sets, numbers, logic, uxions/innitesimals, energy, innity, temperature, space, time, observer, etc. Is a simple formal denition of intelligence possible? Isnt intelligence a too complex and anthropocentric phenomenon to allow formalization? Likely not: There are very simple models of chaotic phenomena such as turbulence. Think about the simple iterative map z  z2 + c that produces the amazingly rich, fractal landscape, sophisticated versions of it used to produce images of virtual ecosystems as in the movie Avatar. Or the complexity of (bio)chemistry emerges out of the elegant mathematical theory Quantum Electro Dynamics. Modeling human intelligence is probably going to be messy, but ideal rational behavior seems to capture the essence of intelligence, and, as I claim, can indeed be completely formalized.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"Even if there is no unique denition capturing all aspects we want to include in a denition of intelligence, or if some aspects are forever beyond formalization (maybe consciousness and qualia), pushing the frontier and studying the best available formal proxy is of utmost importance for understanding articial and natural minds. Context. There are many elds that try to understand the phenomenon of intelligence and whose insights help in creating intelligent systems: cognitive psychology [67] and behaviorism [64], philosophy of mind [7, 61], neuroscience [18], linguistics [8, 17], an     72 Theoretical Foundations of Articial General Intelligence thropology [51], machine learning [5,74], logic [44,77], computer science [56], biological evolution [33,75], economics [46], and others.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"Cognitive science studies how humans think, Behaviorism and the Turing test how humans act, the laws of thought dene rational thinking, while AI research increasingly focusses on systems that act rationally. What is AI? Thinking Acting humanly Cognitive Turing test, Science Behaviorism rationally Laws of Doing the Thought Right Thing In computer science, most AI research is bottom-up; extending and improving existing or developing new algorithms and increasing their range of applicability; an interplay between experimentation on toy problems and theory, with occasional real-world applications. A top-down approach would start from a general principle and derive effective approximations (like heuristic approximations to minimax tree search). Maybe when the top-down and bottom-up approaches meet in the middle, we will have arrived at practical truly intelligent machines. The science of articial intelligence may be dened as the construction of intelligent systems (articial agents) and their analysis.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"A natural denition of a system is anything that has an input and an output stream, or equivalently an agent that acts and observes. This agent perspective of AI [56] brings some order and unication into the large variety of problems the elds wants to address, but it is only a framework rather than providing a complete theory of intelligence. In the absence of a perfect (stochastic) model of the environment the agent interacts with, machine learning techniques are needed and employed to learn from experience. There is no general theory for learning agents (apart from UAI). This has resulted in an ever increasing number of limited models and algorithms in the past.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"What distinguishes an intelligent system from a non-intelligent one? Intelligence can have many faces like reasoning, creativity, association, generalization, pattern recognition, problem solving, memorization, planning, achieving goals, learning, optimization, self-preservation, vision, language processing, classication, induction, deduction, and knowledge acquisition and processing. A formal denition incorporating every aspect of intelligence, however, seems difcult. There is no lack of attempts to characterize or dene intelligence trying to capture all traits informally [39]. One of the more successful characterizations is: Intelligence measures an agents ability to perform well in a large range of environments [40]. Most traits of intelligence are implicit in and emergent from this denition as these capacities      One Decade of Universal Articial Intelligence 73 enable an agent to succeed [38]. Convincing formal denitions other than the ones spawned by UAI are essentially lacking.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"Another important dichotomy is whether an approach focusses (more) on deduction or induction. Traditional AI concentrates mostly on the logical deductive reasoning aspect, while machine learning focusses on the inductive inference aspect. Learning and hence induction are indispensable traits of any AGI. Regrettably, induction is peripheral to traditional AI, and the machine learning community in large is not interested in A(G)I. It is the eld of reinforcement learning at the intersection of AI and machine learning that has AGI ambitions and takes learning seriously. UAI in perspective. The theory of Universal Articial Intelligence developed in the last decade is a modern information-theoretic, inductive, reinforcement learning approach to AGI that has been investigated in great technical depth [24].",Theoretical Foundations of Artificial General Intelligence,chapter 5
"Like traditional AI, UAI is concerned with agents doing the right thing, but is otherwise quite different: It is a top-down approach in the sense that it starts with a single completely formal general denition of intelligence from which an essentially unique agent that seems to possess all traits of rational intelligence is derived. It is not just another framework with some gaps to be lled in later, since the agent is completely dened. It also takes induction very seriously: Universal learning is one of the agents two key elements (the other is stochastic planning). Indeed, logic and deduction play no fundamental role in UAI (but are emergent). This also naturally dissolves Lucas and Penrose [52] argument against AGI that Goedels incompleteness result shows that the human mind is not a computer. The fallacy is to assume that the mind (human and machine alike) are infallible deductive machines. The status of UAI might be compared to Super String theory in physics.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"Both are currently the most promising candidates for a grand unication (of AI and physics, respectively), although there are also marked differences. Like the unication hierarchy of physical theories allows relating and regarding the myriad of limited models as effective approximations, UAI allows us to regard existing approaches to AI as effective approximations. Understanding AI in this way gives researchers a much more coherent view of the eld. Indeed, UAI seems to be the rst sound and complete mathematical theory of (rational) intelligence. The next section presents a very brief introduction to UAI from [29], together with an informal explanation of what the previous sentence actually means. See [24] for formal denitions and results.      74 Theoretical Foundations of Articial General Intelligence 5.3 Universal Articial Intelligence This section describes the theory of Universal Articial Intelligence (UAI), a modern information-theoretic approach to AI, which differs essentially from mainstream A(G)I research described in the previous sections.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"The connection of UAI to other research elds and the philosophical and technical ingredients of UAI (Ockham, Epicurus, Turing, Bayes, Solomonoff, Kolmogorov, Bellman) are briey discussed. The UAI-based universal intelligence measure and order relation in turn dene the (w.r.t. this measure) most intelligent agent AIXI, which seems to be the rst sound and complete theory of a universal optimal rational agent embedded in an arbitrary computable but unknown environment with reinforcement feedback. The nal paragraph claries what this actually means. Dening Intelligence. Philosophers, AI researchers, psychologists, and others have suggested many informal=verbal denitions of intelligence [39], but there is not too much work on formal denitions that are broad, objective, and non-anthropocentric. See [40] for a comprehensive collection, discussion and comparison of intelligence denitions, tests, and measures with all relevant references.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"It is beyond the scope of this article to discuss them. Intelligence is graded, since agents can be more or less intelligent. Therefore it is more natural to consider measures of intelligence, rather than binary denitions which would classify agents as intelligent or not based on an (arbitrary) threshold. This is exactly what UAI provides: A formal, broad, objective, universal measure of intelligence [40], which formalizes the verbal characterization stated in the previous section. Agents can be more or less intelligent w.r.t. this measure and hence can be sorted w.r.t. their intelligence [24, Sec. 5.1.4]. One can show that there is an agent, coined AIXI, that maximizes this measure, which could therefore be called the most intelligent agent.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"I will not present the UAI-based intelligence measure [40] and order relation [24] here, but, after listing the conceptual ingredients to UAI and AIXI, directly proceed to dening and discussing AIXI. UAI and AIXI ingredients [29]. The theory of UAI has interconnections with (draws from and contributes to) many research elds, encompassing computer science (articial intelligence, machine learning, computation), engineering (information theory, adaptive control), economics (rational agents, game theory), mathematics (statistics, probability), psychology (behaviorism, motivation, incentives), and philosophy (inductive inference, theory of knowledge). The concrete ingredients in AIXI are as follows: Intelligent actions are based      One Decade of Universal Articial Intelligence 75 on informed decisions. Attaining good decisions requires predictions which are typically based on models of the environments. Models are constructed or learned from past observations via induction.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"Fortunately, based on the deep philosophical insights and powerful mathematical developments, all these problems have been overcome, at least in theory: So what do we need (from a mathematical point of view) to construct a universal optimal learning agent interacting with an arbitrary unknown environment? The theory, coined UAI, developed in the last decade and explained in [24] says: All you need is Ockham, Epicurus, Turing, Bayes, Solomonoff [65], Kolmogorov [35], and Bellman [1]: Sequential decision theory [4] (Bellmans equation) formally solves the problem of rational agents in uncertain worlds if the true environmental probability distribution is known. If the environment is unknown, Bayesians [2] replace the true distribution by a weighted mixture of distributions from some (hypothesis) class.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"Using the large class of all (semi)measures that are (semi)computable on a Turing machine bears in mind Epicurus, who teaches not to discard any (consistent) hypothesis. In order not to ignore Ockham, who would select the simplest hypothesis, Solomonoff dened a universal prior that assigns high/low prior weight to simple/complex environments [54], where Kolmogorov quanties complexity [43]. Their unication constitutes the theory of UAI and resulted in the universal intelligence measure and order relation and the following model/agent AIXI. The AIXI Model in one line [29]. It is possible to write down the AIXI model explicitly in one line, although one should not expect to be able to grasp the full meaning and power from this compact and somewhat simplied representation. r1|o1 r2|o2 r3|o3 r4|o4 r5|o5 r6|o6 ...",Theoretical Foundations of Artificial General Intelligence,chapter 5
"a1 a2 a3 a4 a5 a6 ... work Agent tape ... work Environment tape ...    HHH H Y  1 PPPPP q .AIXI is an agent that interacts with an environment in cycles k = 1,2,...,m. In cycle k, AIXI takes action ak (e.g. a limb movement) based on past perceptions o1r1..ok1rk1 as dened below. Thereafter, the environment provides a (regular) observation ok (e.g. a camera image) to AIXI and a real-valued reward rk. The reward can be very scarce, e.g. just +1 (-1) for winning (losing) a chess game, and 0 at all other times. Then the next cycle k + 1 starts. This agent-environment interaction protocol can be depicted as on the right.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"Given the interaction protocol above, the simplest version of AIXI is dened by AIXI ak := argmax ak  okrk ...max am  omrm [rk + ...+ rm]  q:U(q,a1..am)=o1r1..omrm 2(q)      76 Theoretical Foundations of Articial General Intelligence The expression shows that AIXI tries to maximize its total future reward rk + ... + rm. If the environment is modeled by a deterministic program q, then the future perceptions ...okrk..omrm = U(q,a1..am) can be computed, where U is a universal (monotone Turing) machine executing q given a1..am. Since q is unknown, AIXI has to maximize its expected reward, i.e. average rk +...+rm over all possible future perceptions created by all possible environments q that are consistent with past perceptions.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"The simpler an environment, the higher is its a-priori contribution 2(q), where simplicity is measured by the length  of program q. AIXI effectively learns by eliminating Turing machines q once they become inconsistent with the progressing history. Since noisy environments are just mixtures of deterministic environments, they are automatically included [54, Sec. 7.2], [82]. The sums in the formula constitute the averaging process. Averaging and maximization have to be performed in chronological order, hence the interleaving of max and  (similarly to minimax for games). One can x any nite action and perception space, any reasonable U, and any large nite lifetime m. This completely and uniquely denes AIXIs actions ak, which are limitcomputable via the expression above (all quantities are known). Discussion.",Theoretical Foundations of Artificial General Intelligence,chapter 5
The AIXI model seems to be the rst sound and complete theory of a universal optimal rational agent embedded in an arbitrary computable but unknown environment with reinforcement feedback. AIXI is universal in the sense that it is designed to be able to interact with any (deterministic or stochastic) computable environment; the universal Turing machines on which it is based is crucially responsible for this. AIXI is complete in the sense that it is not an incomplete framework or partial specication (like Bayesian statistics which leaves open the choice of the prior or the rational agent framework or the subjective expected utility principle) but is completely and essentially uniquely dened. AIXI is sound in the sense of being (by construction) free of any internal contradictions (unlike e.g. in knowledge-based deductive reasoning systems where avoiding inconsistencies can be very challenging).,Theoretical Foundations of Artificial General Intelligence,chapter 5
"AIXI is optimal in the senses that: no other agent can perform uniformly better or equal in all environments, it is a unication of two optimal theories themselves, a variant is self-optimizing; and it is likely also optimal in other/stronger senses. AIXI is rational in the sense of trying to maximize its future long-term reward. For the reasons above I have argued that AIXI is a mathematical solution of the AI problem: AIXI would be able to learn any learnable task and likely better so than any other unbiased agent, but AIXI is more a theory or formal denition rather than an algorithm, since it is only limit-computable.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"How can an equation that ts into a single line capture the diver     One Decade of Universal Articial Intelligence 77 sity, complexity, and essence of (rational) intelligence? We know that complex appearing phenomena such as chaos and fractals can have simple descriptions such as iterative maps and the complexity of chemistry emerges from simple physical laws. There is no a-priori reason why ideal rational intelligent behavior should not also have a simple description, with most traits of intelligence being emergent. Indeed, even an axiomatic characterization seems possible [72,73]. 5.4 Facets of Intelligence Intelligence can have many faces. I will argue in this section that the AIXI model possesses all or at least most properties an intelligent rational agent should possess. Some facets have already been formalized, some are essentially built-in, but the majority have to be emergent. Some of the claims have been proven in [24] but the majority has yet to be addressed.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"Generalization is essentially inductive inference [54]. Induction is the process of inferring general laws or models from observations or data by nding regularities in past/other data. This trait is a fundamental cornerstone of intelligence. Prediction is concerned with forecasting future observations (often based on models of the world learned) from past observations. Solomonoffs theory of prediction [65,66] is a universally optimal solution of the prediction problem [26,54]. Since it is a key ingredient in the AIXI model, it is natural to expect that AIXI is an optimal predictor if rewarded for correct predictions. Curiously only weak and limited rigorous results could be proven so far [24, Sec. 6.2]. Pattern recognition, abstractly speaking, is concerned with classifying data (patterns). This requires a similarity measure between patterns.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"Supervised classication can essentially be reduced to a sequence prediction problem, hence formally pattern recognition reduces to the previous item, although interesting questions specic to classication emerge [24, Chp. 3]. Association. Two stimuli or observations are associated if there exists some (cor)relation between them. A set of observations can often be clustered into different categories of similar=associated items. For AGI, a universal similarity measure is required. Kolmogorov complexity via the universal similarity metric [9] can provide such a measure, but many fundamental questions have yet to be explored: How does association function in AIXI?      78 Theoretical Foundations of Articial General Intelligence How can Kolmogorov complexity well-dene the (inherently? so far?) ill-dened clustering problem? Reasoning is arguably the most prominent trait of human intelligence. Interestingly deductive reasoning and logic are not part of the AIXI architecture.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"The fundamental assumption is that there is no sure knowledge of the world, all inference is tentative and inductive, and that logic and deduction constitute an idealized limit applicable in situations where uncertainties are extremely small, i.e. probabilities are extremely close to 1 or 0. What would be very interesting to show is that logic is an emergent phenomenon, i.e. that AIXI learns to reason logically if/since this helps collect reward. Problem solving might be dened as goal-oriented reasoning, and hence reduces to the previous item, since AIXI is designed to achieve goals (which is reward maximization in the special case of a terminal reward when the goal is achieved). Problems can be of very different nature, and some of the other traits of intelligence can be regarded as instances of problem solving, e.g. planning. Planning ability is directly incorporated in AIXI via the alternating maximization and summation in the denition.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"Algorithmically AIXI plans through its entire life via a deep expectimax tree search up to its death, based on its belief about the world. In known constrained domains this search corresponds to classical exact planning strategies as e.g. exemplied in [24, Chp. 6]. Creativity is the ability to generate innovative ideas and to manifest these into reality. Creative people are often more successful than unimaginative ones. Since AIXI is the ultimate success-driven agent, AIXI should be highly creative, but this has yet to be formalized and proven, or at least exemplied. Knowledge. AIXI stores the entire interaction history and has perfect memory. Additionally, models of the experienced world are constructed (learned) from this information in form of short(est) programs. These models guide AIXIs behavior, so constitute knowledge for AIXI. Any ontology is implicit in these programs. How short-term, long-term, relational, hierarchical, etc.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"memory emerges out of this compression-based approach has not yet been explored. Actions inuence the environment which reacts back to the agent. Decisions can have long-term consequences, which the expectimax planner of AIXI should properly take into account. Particular issues of concern are the interplay of learning and planning (the in     One Decade of Universal Articial Intelligence 79 famous explorationexploitation tradeoff [37]). Additional complications that arise from embodied agents will be considered in the next section. Learning. There are many different forms of learning: supervised, unsupervised, semisupervised, reinforcement, transfer, associative, transductive, prequential, and many others. By design, AIXI is a reinforcement learner, but one can show that it will also listen to an informative teacher, i.e. it learns to learn supervised [24, Sec. 6.5]. It is plausible that AIXI can also acquire the other learning techniques.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"Self-awareness allows one to (meta)reason about ones own thoughts, which is an important trait of higher intelligence, in particularly when interacting with other forms of intelligence. Technically all what might be needed is that an agent has and exploits not only a model of the world but also a model of itself including aspects of its own algorithm, and this recursively. Is AIXI self-aware in this technical sense? Consciousness is possibly the most mysterious trait of the human mind. Whether anything rigorous can ever be said about the consciousness of AIXI or AIs in general is not clear and in any case beyond my expertise. I leave this to philosophers of the mind [7] like the world-renowned expert on (the hard problem of) consciousness, David Chalmers [6]. 5.5 Social Questions Consider now a sophisticated physical humanoid robot like Hondas ASIMO but equipped with an AIXI brain.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"The observations ok consist of camera image, microphone signal, and other sensory input. The actions ak consist of controlling mainly a loud speaker and motors for limbs, but possibly other internal functions it has direct control over. The reward rk should be some combination of its own well-being (e.g. proportional to its battery level and condition of its body parts) and external reward/punishment from some teacher(s). Imagine now what happens if this AIXI-robot is let loose in our society. Many questions deserving attention arise, and some are imperative to be rigorously investigated before risking this experiment. Children of higher animals require extensive nurturing in a safe environment because they lack sufcient innate skills for survival in the real world, but are compensated for their ability to learn to perform well in a large range of environments.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"AIXI is at the extreme of being born with essentially no knowledge about our world, but a universal brain for      80 Theoretical Foundations of Articial General Intelligence learning and planning in any environment where this is possible. As such, it also requires a guiding teacher initially. Otherwise it would simply run out of battery. AIXI has to learn vision, language, and motor skills from scratch, similarly to higher animals and machine learning algorithms, but more extreme/general. Indeed, Solomonoff [65] already showed how his system can learn grammar from positive instances only, but much remains to be done. Appropriate training sequences and reward shaping in this early childhood phase of AIXI are important. AIXI can learn from rather crude teachers as long as the reward is biased in the right direction.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"The answers to many of the following questions likely depend on the upbringing of AIXI:  Schooling: Will a pure reward maximizer such as AIXI listen to and trust a teacher and learn to learn supervised (=faster)? Yes [24, Sec. 6.5].  Take Drugs (hacking the reward system): Likely no, since long-term reward would be small (death), but see [55].  Replication or procreation: Likely yes, if AIXI believes that clones or descendants are useful for its own goals.  Suicide: Likely yes (no), if AIXI is raised to believe to go to heaven (hell) i.e. maximal (minimal) reward forever.  Self-Improvement: Likely yes, since this helps to increase reward.  Manipulation: Manipulate or threaten teacher to give more reward.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"Attitude: Are pure reward maximizers egoists, psychopaths, and/or killers or will they be friendly (altruism as extended ego(t)ism)?  Curiosity killed the cat and maybe AIXI, or is extra reward for curiosity necessary [48,60]?  Immortality can cause laziness [24, Sec. 5.7]!  Can self-preservation be learned or need (parts of) it be innate.  Socializing: How will AIXI interact with another AIXI [29, Sec. 5j], [53]? A partial discussion of some of these questions can be found in [24] but many are essentially unexplored. Point is that since AIXI is completely formal, it permits to formalize these questions and to mathematically analyze them. That is, UAI has the potential to arrive at denite answers to various questions regarding the social behavior of superintelligences.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"Some formalizations and semi-formal answers have recently appeared in the award-winning papers [49,55].      One Decade of Universal Articial Intelligence 81 5.6 State of the Art This section describes the technical achievements of UAI to date. Some remarkable and surprising results have already been obtained. Various theoretical consistency and optimality results for AIXI have been proven, although stronger results would be desirable. On the other hand, the special case of universal induction and prediction in non-reactive environments is essentially closed. From the practical side, various computable approximations of AIXI have been developed, with the latest MC-AIXI-CTW incarnation exhibiting impressive performance. Practical approximations of the universal intelligence measure have also been used to test and consistently order systems of limited intelligence. Some other related work such as the compression contest is also briey mentioned, and references to some more practical but less general work such as feature reinforcement learning are given. Theory of UAI.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"Forceful theoretical arguments that AIXI is the most intelligent generalpurpose agent incorporating all aspects of rational intelligence have been put forward, supported by partial proofs. For this, results of many elds had to be pulled together or developed in the rst place: Kolmogorov complexity [43], information theory [10], sequential decision theory [4], reinforcement learning [74], articial!intelligence [56], Bayesian statistics [3], universal induction [54], and rational agents [62]. Various notions of optimality have been considered. The difculty is coming up with sufciently strong but still satisable notions. Some are weaker than desirable, others are too strong for any agent to achieve.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"What has been shown thus far is that AIXI learns the correct predictive model [24], is Pareto optimal in the sense that no other agent can perform uniformly better or equal in all environments, and a variant is self-optimizing in the sense that asymptotically the accumulated reward is as high as possible, i.e. the same as the maximal reward achievable by a completely informed agent [23]. AIXI is likely also optimal in other/stronger senses. An axiomatic characterization has also been developed [72,73]. The induction problem. The induction problem is a fundamental problem in philosophy [12,54] and science [15,32,80], and a key sub-component of UAI.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"Classical open problems around induction are the zero prior problem and the conrmation of (universal) hypotheses in general and the Black ravens paradox in particular, reparametrization invariance, the old-evidence problem and ad-hoc hypotheses, and the updating problem [12]. In a series of papers (see [26] for references) it has been shown that Solomonoffs theory of universal induction essentially solves or circumvents all these problems [54]. It is also predictively optimal and has minimal regret for arbitrary loss functions.      82 Theoretical Foundations of Articial General Intelligence It is fair to say that Solomonoffs theory serves as an adequate mathematical/theoretical foundation of induction [54], machine learning [30], and component of UAI [24]. Computable approximations of AIXI. An early critique of UAI was that AIXI is incomputable. The down-scaled still provably optimal AIXItl model [24, Chp.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"7] based on universal search algorithms [16,22,42] was still computationally intractable. The Optimal Ordered Problem Solver [59] was the rst practical implementation of universal search and has been able to solve open learning tasks such as Towers-of-Hanoi for arbitrary number of disks, robotic behavior, and others. For repeated 2  2 matrix games such as the Prisoners dilemma, a direct brute-force approximation of AIXI is computationally tractable. Despite these domains being tiny, they raise notoriously difcult questions [62]. The experimental results conrmed the theoretical optimality claims of AIXI [53], as far as limited experiments are able to do so. A Monte-Carlo approximation of AIXI has been proposed in [50] that samples programs according to their algorithmic probability as a way of approximating Solomonoffs universal a-priori probability, similar to sampling from the speed prior [58]. 0 0.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"2 0.4 0.6 0.8 1 100 1000 10000 100000 1000000 Normalised Average Reward per Cycle  Experience Optimal Cheese Maze Tiger 4x4 Grid TicTacToe Biased RPS Kuhn Poker Pacman The most powerful systematic approximation, implementation, and application of AIXI so far is the MC-AIXI-CTW algorithm [78]. It combines award-winning ideas from universal Bayesian data compression [81] and the recent highly successful (in computer Go) upper condence bound algorithm for expectimax tree search [34]. For the rst time, without any domain knowledge, the same agent is able to self-adapt to a diverse range of environments. For instance, AIXI, is able to learn from scratch how to play TicTacToe, Pacman, Kuhn Poker, and other games by trial and error without even providing the rules of the games [79].",Theoretical Foundations of Artificial General Intelligence,chapter 5
"Measures/tests/denitions of intelligence. The history of informal denitions and measures of intelligence [39] and anthropocentric tests of intelligence [76] is long and old. In the last decade various formal denitions, measures and tests have been suggested: Solomonoff induction and Kolmogorov complexity inspired the universal Ctest [19, 21], while AIXI inspired an extremely general, objective, fundamental, and      One Decade of Universal Articial Intelligence 83 formal intelligence order relation [24] and a universal intelligence measure [38, 40], which have already attracted the popular scientic press [14] and received the SIAI award. Practical instantiations thereof [20, 41] also received quite some media attention (http://users.dsic.upv.es/proy/anynt/). Less related/general work.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"There is of course other less related, less general work, similar in spirit to or with similar aims as UAI/AIXI, e.g. UTree [45], URL [13], PORL [69, 70], FOMDP [57], FacMDP [68], PSR [63], POMDP [11], and others. The feature reinforcement learning approach also belongs to this category [27,28,47,71]. Compression contest. The ongoing Human Knowledge Compression Contest [25] is another outgrowth of UAI. The contest is motivated by the fact that being able to compress well is closely related to being able to predict well and ultimately to act intelligently, thus reducing the slippery concept of intelligence to hard le size numbers. Technically it is a community project to approximate Kolmogorov complexity on real-world textual data.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"In order to compress data, one has to nd regularities in them, which is intrinsically difcult (many researchers live from analyzing data and nding compact models). So compressors better than the current dumb compressors need to be smart(er). Since the prize wants to stimulate the development of universally smart compressors, a universal corpus of data has been chosen. Arguably the online encyclopedia Wikipedia is a good snapshot of the Human World Knowledge. So the ultimate compressor of it should understand all human knowledge, i.e. be really smart. The contest is meant to be a cost-effective way of motivating researchers to spend time towards achieving AGI via the promising and quantitative path of compression. The competition raised considerable attention when launched, but to retain attention the prize money should be increased (sponsors are welcome), and the setup needs some adaptation. 5.7 Discussion Formalizing and answering deep philosophical questions.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"UAI deepens our understanding of articial (and to a limited extent human) intelligence; in particular which and how facets of intelligence can be understood as emergent phenomena of goalor reward-driven actions in unknown environments. UAI allows a more quantitative and rigorous discussion of various philosophical questions around intelligence, and ultimately settling these questions. This can and partly has been done by formalizing the philosophical concepts related to intelligence under consideration, and by studying them mathematically. Formal      84 Theoretical Foundations of Articial General Intelligence denitions may not perfectly or not one-to-one or not uniquely correspond to their intuitive counterparts, but in this case alternative formalizations allow comparison and selection. In this way it might even be possible to rigorously answer various social and ethical questions: whether a super rational intelligence such as AIXI will be benign to humans and/or its ilk, or behave psychopathically and kill or enslave humans, or be insane and e.g. commit suicide.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"Building more intelligent agents. From a practical point of building intelligent agents, since AIXI is incomputable or more precisely only limit-computable, it has to be approximated in practice. The results achieved with the MC-AIXI-CTW approximation are only the beginning. As outlined in [79], many variations and extensions are possible, in particular to incorporate long-term memory and smarter planning heuristics. The same single MC-AIXI-CTW agent is already able to learn to play TicTacToe, Kuhn Poker, and most impressively Pacman [79] from scratch. Besides Pacman, there are hundreds of other arcade games from the 1980s, and it would be sensational if a single algorithm could learn them all solely by trial and error, which seems feasible for (a variant of) MC-AIXI-CTW.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"While these are just recreational games, they do contain many prototypical elements of the real world, such as food, enemies, friends, space, obstacles, objects, and weapons. Next could be a test in modern virtual worlds (e.g. bots for VR/role games or intelligent software agents for the internet) that require intelligent agents, and nally some selected real-world problems. Epilogue. It is virtually impossible to predict the future rate of progress but past progress on UAI makes me condent that UAI as a whole will continually progress. By providing rigorous foundations to AI, I believe that UAI will also speed up progress in the eld of A(G)I in general. In any case, UAI is a very useful educational tool with AIXI being a gold standard for intelligent agents which other practical general purpose AI programs should aim for. Bibliography [1] R. E. Bellman. Dynamic Programming.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"Princeton University Press, Princeton, NJ, 1957. [2] J. Berger. Statistical Decision Theory and Bayesian Analysis. Springer, Berlin, 3rd edition, 1993. [3] J. Berger. The case for objective Bayesian analysis. Bayesian Analysis, 1(3):385402, 2006. [4] D. P. Bertsekas. Dynamic Programming and Optimal Control, volume I and II. Athena Scientic, Belmont, MA, 3rd edition, 2006. Volumes 1 and 2. [5] C. M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006.      Bibliography 85 [6] D. J. Chalmers. The Conscious Mind. Oxford University Press, USA, 1996. [7] D. J. Chalmers, editor. Philosophy of Mind: Classical and Contemporary Readings.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"Oxford University Press, USA, 2002. [8] N. Chomsky. Language and Mind. Cambridge University Press, 3rd edition, 2006. [9] R. Cilibrasi and P. M. B. Vitnyi. Clustering by compression. IEEE Trans. Information Theory, 51(4):15231545, 2005. [10] T. M. Cover and J. A. Thomas. Elements of Information Theory. Wiley-Intersience, 2nd edition, 2006. [11] F. Doshi-Velez. The innite partially observable markov decision process. In Proc. 22nd Conf. on Neural Information Processing Systems 22 (NIPS09), 2009. [12] J. Earman. Bayes or Bust? A Critical Examination of Bayesian Conrmation Theory. MIT Press, Cambridge, MA, 1993. [13] V.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"Farias, C. C. Moallemi, B. V. Roy, and T. Weissman. Universal reinforcement learning. IEEE Transactions on Information Theory, 56(5):24412454, 2010. [14] C. Fivet. Mesurer lintelligence dune machine. In Le Monde de lintelligence, volume 1, pages 4245, Paris, November 2005. Mondeo publishing. [15] D. M. Gabbay, S. Hartmann, and J. Woods, editors. Handbook of Inductive Logic. North Holland, 2011. [16] M. Gaglio. Universal search. Scholarpedia, 2(11):2575, 2007. [17] R. Hausser. Foundations of Computational Linguistics: Human-Computer Communication in Natural Language. Springer, 2nd edition, 2001. [18] J. Hawkins and S. Blakeslee.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"On Intelligence. Times Books, 2004. [19] J. Hernndez-Orallo. On the computational measurement of intelligence factors. In Performance Metrics for Intelligent Systems Workshop, pages 18, Gaithersburg, MD, USA, 2000. [20] J. Hernandez-Orallo and D. L. Dowe. Measuring universal intelligence: Towards an anytime intelligence test. Articial Intelligence, 174(18):15081539, 2010. [21] J. Hernndez-Orallo and N. Minaya-Collado. A formal denition of intelligence based on an intensional variant of kolmogorov complexity. In International Symposium of Engineering of Intelligent Systems, pages 146163, 1998. [22] M. Hutter. The fastest and shortest algorithm for all well-dened problems. International Journal of Foundations of Computer Science, 13(3):431443, 2002.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"[23] M. Hutter. Self-optimizing and Pareto-optimal policies in general environments based on Bayes-mixtures. In Proc. 15th Annual Conf. on Computational Learning Theory (COLT02), volume 2375 of LNAI, pages 364379, Sydney, 2002. Springer, Berlin. [24] M. Hutter. Universal Articial Intelligence: Sequential Decisions based on Algorithmic Probability. Springer, Berlin, 2005. [25] M. Hutter. Human knowledge compression prize, 2006. open ended, http://prize.hutter1.net/. [26] M. Hutter. On universal prediction and Bayesian conrmation. Theoretical Computer Science, 384(1):3348, 2007. [27] M. Hutter. Feature dynamic Bayesian networks. In Proc. 2nd Conf.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"on Articial General Intelligence (AGI09), volume 8, pages 6773. Atlantis Press, 2009. [28] M. Hutter. Feature reinforcement learning: Part I: Unstructured MDPs. Journal of Articial General Intelligence, 1:324, 2009. [29] M. Hutter. Open problems in universal induction & intelligence. Algorithms, 3(2):879906, 2009. [30] M. Hutter. Universal learning theory. In C. Sammut and G. Webb, editors, Encyclopedia of Machine Learning, pages 10011008. Springer, 2011. [31] M. Hutter. Can intelligence explode? Journal of Consciousness Studies, 19(1-2), 143166, 2012.      86 Theoretical Foundations of Articial General Intelligence [32] E. T. Jaynes. Probability Theory: The Logic of Science.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"Cambridge University Press, Cambridge, MA, 2003. [33] K. V. Kardong. An Introduction to Biological Evolution. McGraw-Hill Science / Engineering / Math, 2nd edition, 2007. [34] L. Kocsis and C. Szepesvri. Bandit based Monte-Carlo planning. In Proc. 17th European Conf. on Machine Learning (ECML06), pages 282293, 2006. [35] A. N. Kolmogorov. Three approaches to the quantitative denition of information. Problems of Information and Transmission, 1(1):17, 1965. [36] R. Kurzweil. The Singularity Is Near. Viking, 2005. [37] T. Lattimore and M. Hutter. Asymptotically optimal agents. In Proc. 22nd International Conf.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"on Algorithmic Learning Theory (ALT11), volume 6925 of LNAI, pages 368382, Espoo, Finland, 2011. Springer, Berlin. [38] S. Legg. Machine Super Intelligence. PhD thesis, IDSIA, Lugano, Switzerland, 2008. Recipient of the $10000,Singularity Prize/Award. [39] S. Legg and M. Hutter. A collection of denitions of intelligence. In Advances in Articial General Intelligence: Concepts, Architectures and Algorithms, volume 157 of Frontiers in Articial Intelligence and Applications, pages 1724, Amsterdam, NL, 2007. IOS Press. [40] S. Legg and M. Hutter. Universal intelligence: A denition of machine intelligence. Minds & Machines, 17(4):391444, 2007. [41] S. Legg and J. Veness.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"An approximation of the universal intelligence measure. In Proc. Solomonoff 85th Memorial Conference, LNAI, Melbourne, Australia, 2011. Springer. [42] L. A. Levin. Universal sequential search problems. Problems of Information Transmission, 9:265266, 1973. [43] M. Li and P. M. B. Vitnyi. An Introduction to Kolmogorov Complexity and its Applications. Springer, Berlin, 3rd edition, 2008. [44] J. W. Lloyd. Foundations of Logic Programming. Springer, 2nd edition, 1987. [45] A. K. McCallum. Reinforcement Learning with Selective Perception and Hidden State. PhD thesis, Department of Computer Science, University of Rochester, 1996. [46] R. B. McKenzie. Predictably Rational? In Search of Defenses for Rational Behavior in Economics. Springer, 2009.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"[47] P. Nguyen, P. Sunehag, and M. Hutter. Feature reinforcement learning in practice. In Proc. 9th European Workshop on Reinforcement Learning (EWRL-9), volume 7188 of LNAI, Springer, 2011. to appear. [48] L. Orseau. Optimality issues of universal greedy agents with static priors. In Proc. 21st International Conf. on Algorithmic Learning Theory (ALT10), volume 6331 of LNAI, pages 345359, Canberra, 2010. Springer, Berlin. [49] L. Orseau and M. Ring. Self-modication and mortality in articial agents. In Proc. 4th Conf. on Articial General Intelligence (AGI11), volume 6830 of LNAI, pages 110. Springer, Berlin, 2011. [50] S. Pankov.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"A computational approximation to the AIXI model. In Proc. 1st Conference on Articial General Intelligence, volume 171, pages 256267, 2008. [51] M. A. Park. Introducing Anthropology: An Integrated Approach. McGraw-Hill, 4th edition, 2007. [52] R. Penrose. Shadows of the Mind, A Search for the Missing Science of Consciousness. Oxford University Press, 1994. [53] J. Poland and M. Hutter. Universal learning of repeated matrix games. In Proc. 15th Annual Machine Learning Conf. of Belgium and The Netherlands (Benelearn06), pages 714, Ghent, 2006. [54] S. Rathmanner and M. Hutter. A philosophical treatise of universal induction. Entropy, 13(6):10761136, 2011.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"     Bibliography 87 [55] M. Ring and L. Orseau. Delusion, survival, and intelligent agents. In Proc. 4th Conf. on Articial General Intelligence (AGI11), volume 6830 of LNAI, pages 1120. Springer, Berlin, 2011. [56] S. J. Russell and P. Norvig. Articial Intelligence. A Modern Approach. Prentice-Hall, Englewood Cliffs, NJ, 3rd edition, 2010. [57] S. Sanner and C. Boutilier. Practical solution techniques for rst-order MDPs. Articial Intelligence, 173(56):748788, 2009. [58] J. Schmidhuber. The speed prior: A new simplicity measure yielding near-optimal computable predictions. In Proc. 15th Conf.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"on Computational Learning Theory (COLT02), volume 2375 of LNAI, pages 216228, Sydney, 2002. Springer, Berlin. [59] J. Schmidhuber. Optimal ordered problem solver. Machine Learning, 54(3):211254, 2004. [60] J. Schmidhuber. Simple algorithmic principles of discovery, subjective beauty, selective attention, curiosity & creativity. In Proc. 10th Intl. Conf. on Discovery Science (DS07), volume LNAI 4755, pages 2638, Senday, 2007. Springer. [61] J. R. Searle. Mind: A Brief Introduction. Oxford University Press, USA, 2005. [62] Y. Shoham and K. Leyton-Brown. Multiagent Systems: Algorithmic, Game-Theoretic, and Logical Foundations. Cambridge University Press, 2009.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"[63] S. Singh, M. Littman, N. Jong, D. Pardoe, and P. Stone. Learning predictive state representations. In Proc. 20th International Conference on Machine Learning (ICML03), pages 712719, 2003. [64] B. F. Skinner. About Behaviorism. Random House, 1974. [65] R. J. Solomonoff. A formal theory of inductive inference: Parts 1 and 2. Information and Control, 7:122 and 224254, 1964. [66] R. J. Solomonoff. Complexity-based induction systems: Comparisons and convergence theorems. IEEE Transactions on Information Theory, IT-24:422432, 1978. [67] R. L. Solso, O. H. MacLin, and M. K. MacLin. Cognitive Psychology.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"Allyn & Bacon, 8th edition, 2007. [68] A. L. Strehl, C. Diuk, and M. L. Littman. Efcient structure learning in factored-state MDPs. In Proc. 27th AAAI Conference on Articial Intelligence, pages 645650, Vancouver, BC, 2007. AAAI Press. [69] N. Suematsu and A. Hayashi. A reinforcement learning algorithm in partially observable environments using short-term memory. In Advances in Neural Information Processing Systems 12 (NIPS09), pages 10591065, 1999. [70] N. Suematsu, A. Hayashi, and S. Li. A Bayesian approach to model learning in non-Markovian environments. In Proc. 14th Intl. Conf. on Machine Learning (ICML97), pages 349357, 1997. [71] P.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"Sunehag and M. Hutter. Consistency of feature Markov processes. In Proc. 21st International Conf. on Algorithmic Learning Theory (ALT10), volume 6331 of LNAI, pages 360374, Canberra, 2010. Springer, Berlin. [72] P. Sunehag and M. Hutter. Axioms for rational reinforcement learning. In Proc. 22nd International Conf. on Algorithmic Learning Theory (ALT11), volume 6925 of LNAI, pages 338352, Espoo, Finland, 2011. Springer, Berlin. [73] P. Sunehag and M. Hutter. Principles of Solomonoff induction and AIXI. In Proc. Solomonoff 85th Memorial Conference, LNAI, Melbourne, Australia, 2011. Springer. [74] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"MIT Press, Cambridge, MA, 1998. [75] A. Tettamanzi, M. Tomassini, and J. Janssen. Soft Computing: Integrating Evolutionary, Neural, and Fuzzy Systems. Springer, 2001. [76] A. M. Turing. Computing machinery and intelligence. Mind, 1950. [77] R. Turner. Logics for Articial Intelligence. Ellis Horwood Series in Articial Intelligence, 1984.      88 Theoretical Foundations of Articial General Intelligence [78] J. Veness, K. S. Ng, M. Hutter, and D. Silver. Reinforcement learning via AIXI approximation. In Proc. 24th AAAI Conference on Articial Intelligence (AAAI10), pages 605611, Atlanta, 2010. AAAI Press. [79] J. Veness, K. S. Ng, M. Hutter, W.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"Uther, and D. Silver. A Monte Carlo AIXI approximation. Journal of Articial Intelligence Research, 40:95142, 2011. [80] C. S. Wallace. Statistical and Inductive Inference by Minimum Message Length. Springer, Berlin, 2005. [81] F. M. J. Willems, Y. M. Shtarkov, and T. J. Tjalkens. The context tree weighting method: Basic properties. IEEE Transactions on Information Theory, 41:653664, 1995. [82] I. Wood, P. Sunehag, and M. Hutter. (Non-)equivalence of universal priors. In Proc. Solomonoff 85th Memorial Conference, LNAI, Melbourne, Australia, 2011. Springer.",Theoretical Foundations of Artificial General Intelligence,chapter 5
"  Chapter 6 Deep Reinforcement Learning as Foundation for Articial General Intelligence Itamar Arel Machine Intelligence Lab, Department of Electrical Engineering and Computer Science, University of Tennessee E-mail: itamar@ieee.org Deep machine learning and reinforcement learning are two complementing elds within the study of intelligent systems. When combined, it is argued that they offer a promising path for achieving articial general intelligence (AGI). This chapter outlines the concepts facilitating such merger of technologies and motivates a framework for building scalable intelligent machines. The prospect of utilizing custom neuromorphic devices to realize large-scale deep learning architectures is discussed, paving the way for achieving humanlevel AGI. 6.1 Introduction: Decomposing the AGI Problem A fundamental distinction between Articial General Intelligence (AGI) and conventional Articial Intelligence (AI) is that AGI focuses on the study of systems that can perform tasks successfully across different problem domains, while AI typically pertains to domain-specic expert systems.",Theoretical Foundations of Artificial General Intelligence,chapter 6
"General problem-solving ability is one that humans naturally exhibit. A related capability is generalization, which allows mammals to effectively associate causes perceived in their environment with regularities observed in the past. Another critical human skill involves decision making under uncertainty, tightly coupled with generalization since the latter facilitates broad situation inference. Following this line of thought, it can be argued that at a coarse level, intelligence involves two complementing sub-systems: perception and actuation. Perception can be interpreted as mapping sequences of observations, possibly received from multiple modalities, to an inferred state of the world with which the intelligence agent interacts. Actuation is 89      90 Theoretical Foundations of Articial General Intelligence often framed as a control problem, centering on the goal of selecting actions to be taken at any given time so as to maximize some utility function.",Theoretical Foundations of Artificial General Intelligence,chapter 6
"In other words, actuation is a direct byproduct of a decision making process, whereby inferred states are mapped to selected actions, thereby impacting the environment in some desirable way. This high-level view is depicted in Figure 6.1. Fig. 6.1 Bipartite AGI architecture comprising of a perception and control/actuation subsystem. The role of the perception subsystem is viewed as state inference while the control subsystem maps inferred states to desired actions. Actuation impacts subsequent perceptive stimuli, as denoted by the feedback signal. Two relatively new thrusts within machine learning contribute, respectively, to the core AGI components mentioned above. Deep machine learning (DML) is a niche that focuses on scalable information representation architectures that loosely mimic the manner by which the mammalian cortex interprets and represents observations. As a direct consequence, deep learning architectures [5] can serve as scalable state inference engines, driving perception subsystems.",Theoretical Foundations of Artificial General Intelligence,chapter 6
"Complementing DML is Reinforcement learning (RL) [11]  a fairly mature eld of study, concerning algorithms that attempt to approximately solve an optimal control problem, whereby action selection is guided by the desire to maximize an agents expected reward prospect. RL is inspired by many studies of recent years, supporting the notion that much of the learning that goes on in mammalian brain is driven by rewards and their expectations, both positive and negative. This chapter hypothesizes that the merger of these two technologies, in the context of the bipartite system architecture outlined above, may pave the way for a breakthrough in      Deep Reinforcement Learning as Foundation for Articial General Intelligence 91 our ability to build systems that will eventually exhibit human-level intelligence. Such merger is coined deep reinforcement learning (DRL). Moreover, recent advances in VLSI technology, particularly neuromorphic circuitry, suggest that the means to fabricate largescale DRL systems are within our reach.",Theoretical Foundations of Artificial General Intelligence,chapter 6
"The rest of the chapter is structured as follows. In Section 6.2 we review deep learning architectures and motivate their role in designing perception engines. Section 6.3 reviews reinforcement learning and outlines how it can be merged with deep learning architectures. Section 6.4 discusses the scalability implications of designing AGI systems using emerging neuromorphic technology, while in Section 6.5 conclusions are drawn and future outlook is discussed. 6.2 Deep Learning Architectures 6.2.1 Overcoming the Curse of Dimensionality Mimicking the efciency and robustness with which the human brain represents information has been a core challenge in articial intelligence research for decades. Humans are exposed to myriad of sensory data received every second of the day and are somehow able to capture critical aspects of this data in a way that allows for its future recollection.",Theoretical Foundations of Artificial General Intelligence,chapter 6
"Over 50 years ago, Richard Bellman, who introduced dynamic programming theory and pioneered the eld of optimal control, asserted that high dimensionality of data is a fundamental hurdle in many science and engineering applications. The main difculty that arises, particularly in the context of real-world observations such as large visual elds, is that the learning complexity grows exponentially with linear increase in the dimensionality of the data. He coined this phenomenon the curse of dimensionality [1]. The mainstream approach of overcoming the Curse of Dimensionality has been to preprocess the data in a manner that would reduce its dimensionality to that which can be effectively processed, for example by a classication engine. This dimensionality reduction scheme is often referred to as feature extraction. As a result, it can be argued that the intelligence behind many pattern recognition systems has shifted to the human-engineeredfeature extraction process, which at times can be challenging and highly application-dependent[2].",Theoretical Foundations of Artificial General Intelligence,chapter 6
"Moreover, if incomplete or erroneous features are extracted, the classication process is inherently limited in performance. Recent neuroscience ndings have provided insight into the principles governing information representation in the mammal brain, leading to new ideas for designing systems that      92 Theoretical Foundations of Articial General Intelligence represent information. Some researchers claim that that the neocortex, which is associated with many cognitive abilities, does not explicitly pre-process sensory signals, but rather allows them to propagate through a complex hierarchy [3] of modules that, over time, learn to represent observations based on the regularities they exhibit [4]. This discovery motivated the emergence of the subeld of deep machine learning [5, 6], which focuses on computational models for information representation that exhibit similar characteristics to that of the neocortex. In addition to the spatial dimensionality of real-life data, the temporal component also plays a key role.",Theoretical Foundations of Artificial General Intelligence,chapter 6
"A sequence of patterns that we observe often conveys a meaning to us, whereby independent fragments of this sequence would be hard to decipher in isolation. We often infer meaning from events or observations that are received close in time [7, 8]. To that end, modeling the temporal component of the observations plays a critical role in effective information representation. Capturing spatiotemporal dependencies, based on regularities in the observations, is therefore viewed as a fundamental goal for deep learning systems. Recent literature treats pure multi-layer perceptron (MLP) neural networks with more than two hidden layers as deep learning architectures. Although one can argue that technically that is a correct assertion, the mere fact that a learning system hosts multiple layers is insufcient to be considered as a deep learning architecture. The latter should also encompass the idea of a hierarchy of abstraction, whereby as one ascends the hierarchy more abstract notions are formed.",Theoretical Foundations of Artificial General Intelligence,chapter 6
"This is not directly attainable in a simple MLP consisting of a large number of layers. 6.2.2 Spatiotemporal State Inference A particular family of DML systems is compositional deep learning architectures. The latter are characterized by hosting multiple instantiations of a basic cortical circuit (or node) which populate all layers of the architecture. Each node is tasked with learning to represent the sequences of patterns that are presented to it by nodes in the layer that precede it. At the very lowest layer of the hierarchy nodes receive as input raw data (e.g. pixels of the image) and continuously construct a belief state that attempts to compactly characterize the sequences of patterns observed. The second layer, and all those above it, receive as input the belief states of nodes at their corresponding lower layers, and attempt to construct their own belief states that capture regularities in their inputs. Figure 6.2 illustrates a compositional deep learning architecture.",Theoretical Foundations of Artificial General Intelligence,chapter 6
"     Deep Reinforcement Learning as Foundation for Articial General Intelligence 93 Deep-layer  Inference Network Fig. 6.2 Compositional deep machine learning architecture, comprising of multiple instantiations of a common cortical circuit, illustrated in the context of visual information processing. Information ows both bottom up and top down. Bottom up processing essentially constitutes a feature extraction process, in which each layer aggregates data from the layer below it. Top down signaling helps lower layer nodes improve their representation accuracy by assisting in correctly disambiguating distorted observations. An AGI system should be able to adequately cope in a world where partial observability is assumed. Partial observability means that any given observation (regardless of the modalities from which it originates) does not provide full information needed to accurately infer the true state of the world. As such, an AGI system should map sequences of observations to an internal state construct that is consistent for regular causes. This implies that a dynamic (i.e.",Theoretical Foundations of Artificial General Intelligence,chapter 6
"memory-based) learning process should be exercised by each cortical circuit. For example, if a person looks at a car in a parking lot he/she would recognize it as such since there is consistent signaling being invoked in their brain whenever car patterns are observed. In fact, it is sufcient to hear a car (without viewing it) to invoke similar signaling in the brain. While every person may have different signaling for common causes in the world, such signaling remains consistent for each person. This consistency property allows      94 Theoretical Foundations of Articial General Intelligence a complementing control subsystem to map the (inferred) states to actions that impact the environment in some desirable way. If a deep learning architecture is to form an accurate state representation, it should include both spatial and temporal information. As a result, each belief state should capture spatiotemporal regularities in the observations, rather than just spatial saliencies.",Theoretical Foundations of Artificial General Intelligence,chapter 6
"The learning process at each node is unsupervised, guided by exposure to a large set of observations and allowing the salient attributes of these observations to be captured across the layers. In the context of an AGI system, signals originating from upper-layer nodes can be extracted to serve as inferred state representations. This extracted information should exhibit invariance to common distortions and variations in the observations, leading to representational robustness. In the context of visual data, robustness refers to the ability to exhibit invariance to a diverse range of transformations, including mild rotation, scale, different lighting conditions and noise. It should be noted that although deep architectures may appear to completely solve or overcome the curse of dimensionality, in reality they do so by hiding the key assumption of locality. The latter means that the dependencies that may exist between two signals (e.g. pixels) that are spatially close are captured with relative detail, where as relationships between signals that are distant (e.g.",Theoretical Foundations of Artificial General Intelligence,chapter 6
"pixels on opposite sides of a visual eld) are represented with very little detail. This is a direct result of the nature of the architecture depicted in Figure 6.2, in which fusion of information from inputs that are distant to the hierarchy occurs at the higher layers. It is also important to emphasize that deep learning architectures are not limited by any means to visual data. In fact, these architectures are modality agnostic, and attempt to discover underlying structure in data of any form. Moreover, fusion of information originating from different modalities is natural in deep learning and a pivotal requirement of AGI. If one imagines the architecture shown in Figure 6.1 to receive input at its lowest layer from multiple modalities, as one ascends the hierarchy, fusion of such information takes place by capturing regularities across the modalities. 6.3 Scaling Decision Making under Uncertainty 6.3.",Theoretical Foundations of Artificial General Intelligence,chapter 6
"1 Deep Reinforcement Learning While the role of the perception subsystem may be viewed as that of complex state inference, an AGI system must be able to take actions that impact its environment. In      Deep Reinforcement Learning as Foundation for Articial General Intelligence 95 other words, an AGI system must involve a controller that attempts to optimize some cost function. This controller is charged with mapping the inferred states to an action. In realworld scenarios, there is always some uncertainty. However, state signaling should exhibit the Markov property in the sense that it compactly represents the history that has led to the current state-of-affairs. This is a colossal assumption, and one that is unlikely to accurately hold. However, it is argued that while the Markov property does practically not hold, assuming that it does paves the way for obtaining good enough, albeit not optimal, AGI systems [21].",Theoretical Foundations of Artificial General Intelligence,chapter 6
"Reinforcement learning (RL) corresponds to a broad class of machine learning techniques that allow a system to learn how to behave in an environment that provides reward signals. A key related concept is that the agent learns by itself, based on acquired experience, rather than by being externally instructed or supervised. RL inherently facilitates autonomous learning as well as addresses many of the essential goals of AGI: it emphasizes the close interaction of an agent with the environment, it focuses on perception-to-action cycles and complete behaviors rather than separate functions and function modules, it relies on bottom-up intelligent learning paradigms, and it is not based on symbolic representations. It should be emphasized that it is inferred states, rather than pure perceptive stimuli, that is mapped to actions by the system. On that note, although it is argued that RL-based AGI is not achieved via explicit symbolic representations, the latter do form implicitly in the architecture, primarily in the form of attractors in both the perception and control subsystems.",Theoretical Foundations of Artificial General Intelligence,chapter 6
"As an example, regularities in the observations that offer semantic value to the agent, as reected by sequences of rewards, will receive an internal representation, relative to other notions acquired, thereby resembling classical symbolic knowledge maps. The ability to generalize is acknowledged as an inherent attribute of intelligent systems. Consequently, it may be claimed that no system can learn without employing some degree of approximation. The latter is particularly true when we consider large-scale, complex real-world scenarios, such as those implied by true AGI. Deep learning architectures can serve this exact purpose: they can provide a scalable state inference engine that a reinforcement learning based controller can map to actions.",Theoretical Foundations of Artificial General Intelligence,chapter 6
"A recent and very inuential development in RL is the actor-critic approach to modelfree learning, which is based on the notion that two distinct core functions accomplish learning: the rst (the actor) produces actions derived from an internal model and the second (the critic) renes the action selection policy based on prediction of long-term      96 Theoretical Foundations of Articial General Intelligence reward signals. As the actor gains prociency, it is required to learn an effective mapping from inferred states of the environment to actions. In parallel to the growing support of RL theories in modern cognitive science, recent work in neurophysiology provides some evidence arguing that the actor-critic RL theme is widely exploited in the human brain. The dominant mathematical methods supporting learning approximately solve the Hamilton-Jacobi-Bellman (HJB) equation of dynamic programming (DP) to iteratively adjust the parameters and structure of an agent as a means of encouraging desired behaviors [5].",Theoretical Foundations of Artificial General Intelligence,chapter 6
"A discounted future reward is typically used; however, researchers are aware of the importance of multiple time scales and the likelihood that training efciency will depend upon explicit consideration of multiple time horizons. Consequently, the trade-offs between short and long term memory should be considered. Cognitive science research supports these observations, nding similar structures and mechanisms in mammalian brains [9]. The HJB equation of DP requires estimation of expected future rewards, and a suitable dynamic model of the environment that maps the current observations and actions (along with inferred state information) to future observations. Such model-free reinforcement learning assumes no initial knowledge of the environment, and instead postulates a generic structure, such as a deep learning architecture, that can be trained to model environmental responses to actions and exogenous sensory inputs.",Theoretical Foundations of Artificial General Intelligence,chapter 6
"Contrary to existing function approximation technologies, such as standard multi-layer perceptron networks, current neurophysiology research reveals that the structure of the human brain is dynamic, with explosive growth of neurons and neural connections during fetal development followed by pruning. Spatial placement also plays critical roles, and it is probable that the spatial distribution of chemical reward signals selectively inuences neural adaptation to enhance learning [10]. It is suspected that the combination of multi-time horizon learning and memory processes with the dynamic topology of a spatially embedded deep architecture will dramatically enhance adaptability and effectiveness of articial cognitive agents. This is expected to yield novel AGI frameworks that can overcome the limitations of existing AI systems. RL can thus be viewed as a biologically-inspired decision making under uncertainty framework that is centered on the notion of learning from experience, through interaction with an environment, rather than by being explicitly guided by a teacher.",Theoretical Foundations of Artificial General Intelligence,chapter 6
"What sets RL apart from other machine learning methods is that it aims to solve the credit assignment problem, in which an agent is charged with evaluating the long-term impact of actions it      Deep Reinforcement Learning as Foundation for Articial General Intelligence 97 takes. In doing so, the agent attempts to choose actions that maximize its estimated value function, based only on state information and nonspecic reward signals. In a real-world setting, the agent constructs an estimated value function that expresses the prospect of rewards the agent expects to experience by taking a specic action at a given state. Temporal difference (TD) learning [11] is a central idea in reinforcement learning, and is primarily applied to model-free learning problems. The TD paradigm draws from both dynamic programming [1] and Monte Carlo methods [11]. Similar to dynamic programming, TD learning bootstraps in that it updates value estimates based on other value estimates, as such not having to complete an episode before updating its value function representation.",Theoretical Foundations of Artificial General Intelligence,chapter 6
"Like Monte Carlo methods, TD is heuristic in that it uses experience, obtained by following a given policy (i.e. mapping of states to actions), to predict subsequent value estimates. TD updates are performed as a single step look-ahead that typically takes the form of V(t + 1) = V(t)+   (target V(t)). (6.1) where target is derived from the Bellman equation [11] and depends on how rewards are evaluated over time, Vt denotes the value estimate of a given state at time t, and  is a small positive constant. In real-world settings, particularly those relevant to AGI, only partial information regarding the true state of the world is available to the agent. The agent is thus required to form a belief state from observations it receives of the environment. Assuming the Markov property holds, but state information is inaccurate or incomplete, we say that the problem is partially observable.",Theoretical Foundations of Artificial General Intelligence,chapter 6
"Deep learning architectures help overcome partial observability by utilizing their internal state constructs (across the different layers) to capture temporal dependencies. The latter disambiguate observations that are partial, noisy or otherwise insufcient to infer the state of the environment. 6.3.2 Actor-Critic Reinforcement Learning Themes in Cognitive Science The fundamental notion of learning on the basis of rewards is shared among several inuential branches of psychology, including behaviorism and cognitive psychology. The actor-critic architecture reects recent trends in cognitive neuroscience and cognitive psychology that highlight task decomposition and modular organization. For example, visual information-processing is served by two parallel pathways, one specialized to object location in space and the other to object identication or recognition over space and time [12, 13].",Theoretical Foundations of Artificial General Intelligence,chapter 6
"This approach exploits a divide-and-conquer processing strategy in which particular      98 Theoretical Foundations of Articial General Intelligence components of a complex task are computed in different cortical regions, and typically integrated, combined, or supervised by the prefrontal cortex. Computational models of this dual-route architecture suggest that it has numerous benets over conventional homogenous networks, including both learning speed and accuracy. More generally, the prefrontal cortex is implicated in a wide range of cognitive functions, including maintaining information in short-term or working memory, action planning or sequencing, behavioral inhibition, and anticipation of future states. These functions highlight the role of the prefrontal cortex as a key location that monitors information from various sources and provides top-down feedback and control to relevant motor areas (e.g., premotor cortex, frontal eye elds, etc.).",Theoretical Foundations of Artificial General Intelligence,chapter 6
"In addition to recent work in cognitive neuroscience, theoretical models of working memory in cognitive psychology also focus on the role of a central executive that actively stores and manipulates information that is relevant for solving ongoing tasks. A unique feature of the proposed AGI approach is a general-purpose cognitive structure for investigating both external and internal reward systems. Cognitive psychologists conceptualize these two forms of reward as extrinsic and intrinsic motivation [14]. Extrinsic motivation corresponds to changes in behavior as a function of external contingencies (e.g., rewards and punishments), and is a central element of Skinners theory of learning. Meanwhile, intrinsic motivation corresponds to changes in behavior that are mediated by internal states, drives, and experiences, and is manifested in a variety of forms including curiosity, surprise, and novelty. The concept of intrinsic motivation is ubiquitous in theories of learning and development, including the notions of (1) mastery motivation (i.e., a drive for prociency [15], (2) functional assimilation (i.",Theoretical Foundations of Artificial General Intelligence,chapter 6
"., the tendency to practice a new skill, e.g., Piaget, 1952), and (3) violation-of-expectation (i.e., the tendency to increase attention to unexpected or surprising events, e.g., [16]). It is interesting to note that while external rewards play a central role in RL, the use of intrinsic motivation has only recently begun to receive attention from the machine-learning community. This is an important trend, for a number of reasons. First, intrinsic motivation changes dynamically in humans, not only as a function of task context but also general experience. Implementing a similar approach in autonomous-agent design will enable the agent to exibly adapt or modify its objectives over time, deploying attention and computational resources to relevant goals and sub-goals as knowledge, skill, and task demands change.",Theoretical Foundations of Artificial General Intelligence,chapter 6
"Second, the integration of a dual-reward system that includes both external and      Deep Reinforcement Learning as Foundation for Articial General Intelligence 99 intrinsic motivation is not only biologically plausible, but also more accurately reects the continuum of inuences in both human and non-human learning systems. In parallel to the growing support of model-free Actor-Critic models in modern psychology, recent work in neurophysiologyprovides evidence suggesting that the Actor-Critic paradigm is widely exploited in the brain. In particular, it has been recently shown that the basal ganglia [17] can be coarsely modeled by an Actor-Critic version of temporal difference (TD) learning. The frontal dopaminergic input arises in a part of the basal ganglia called ventral tegmental area (VTA) and the substantia nigra (SN). The signal generated by dopaminergic (DA) neurons resembles the effective reinforcement signal of temporal difference (TD) learning algorithms.",Theoretical Foundations of Artificial General Intelligence,chapter 6
"Another important part of the basal ganglia is the striatum. This structure is comprised of two parts, the matriosome and the striosome. Both receive input from the cortex (mostly frontal) and from the DA neurons, but the striosome projects principally to DA neurons in VTA and SN. The striosome is hypothesized to act as a reward predictor, allowing the DA signal to compute the difference between the expected and received reward. The matriosome projects back to the frontal lobe (for example, to the motor cortex). Its hypothesized role is therefore in action selection. 6.4 Neuromorphic Devices Scaling AGI The computational complexity and storage requirements from deep reinforcement learning systems limit the scale at which they may be implemented using standard digital computers. An alternative would be to consider custom analog circuitry as means of overcoming the limitations of digital VLSI technology.",Theoretical Foundations of Artificial General Intelligence,chapter 6
"In order to achieve the largest possible learning system within any given constraints of cost or physical size, it is critical that the basic building blocks of the learning system be as dense as possible. Many operations can be realized in analog circuitry with a space saving of one to two orders of magnitude compared to a digital realization. Analog computation also frequently comes with a signicant reduction in power consumption, which will become critical as powerful learning systems are migrated to battery-operated platforms. This massive improvement in density is achieved by utilizing the natural physics of device operation to carry out computation. The benets in density and power come with certain disadvantages, such as offsets and inferior linearity compared to digital implementations. However, the weaknesses of analog circuits are not major limitations since the      100 Theoretical Foundations of Articial General Intelligence feedback inherent in the learning algorithms naturally compensates for errors/inaccuracies introduced by the analog circuits.",Theoretical Foundations of Artificial General Intelligence,chapter 6
"The argument made here is that the brain is far from being 64-bit accurate, so relaxing accuracy requirements of computational elements, for the purpose of aggressively optimized for area, is a valid tradeoff. The basic requirements of almost any machine learning algorithm include multiplication, addition, squashing functions (e.g. sigmoid), and distance/similarity calculation, all of which can be realized in a compact and power-efcient manner using analog circuitry. Summation is trivial in the current domain as it is accomplished by joining the wires with the currents to be summed. In the voltage domain, a feedback amplier with N + 1 resistors in the feedback path can compute the sum of N inputs. A Gilbert cell [18] provides four-quadrant multiplication while using only seven transistors. Table 6.1 contrasts the component count of digital and analog computational blocks.",Theoretical Foundations of Artificial General Intelligence,chapter 6
"As discussed above, the learning algorithms will be designed to be robust to analog circuit imperfections, allowing the use of very small transistors. Digital designs vary widely in transistor count, as area can frequently be traded for speed. For the comparison, we used designs that are optimized for area and appropriate for the application. For example, an Nbit shift-and-add multiplier uses a single adder by performing the multiplication over N clock cycles. A fast multiplier might require as many as N times more adders. Digital registers were chosen over SRAMs, despite their larger size because SRAMs require signicant peripheral circuitry (e.g. address decoders, sense ampliers) making them poorly suited to a system requiring many small memories. For the analog elements, we also counted any necessary resistors or capacitors. Table 6.1 Transistor Count Comparison of Analog and Digital Elements.",Theoretical Foundations of Artificial General Intelligence,chapter 6
"Operation Analog Digital (N bits) Notes Summation 12 24N Feedback voltage adder; Ripple-carry adder Multiplication 7 48N Shift & add multiplier Storage 15 12N Feedback oating-gate cell; Digital register A particularly suitable analog circuit, which is often used for computational purposes, is the oating gate transistor [19] (shown in Figure 6.3). Floating gates have proved themselves to be useful in many different applications; they make good programmable switches, allow for threshold matching in transistor circuits, and have been successfully used in the design of various adaptive systems.      Deep Reinforcement Learning as Foundation for Articial General Intelligence 101 Floating gates lack a DC path to ground, so any charge stored on the gate will stay there. Through the use of Fowler-Nordheim tunneling and hot electron injection, this trapped charge can be modied.",Theoretical Foundations of Artificial General Intelligence,chapter 6
"Floating-gate memories can provide a nely tuned voltage to match thresholds between multiple transistors, for example, to yield a circuit which has nearly perfect matching, improving accuracy relative to conventional techniques. In learning systems, a oating gate can be used to store a weight or learned parameter. Fig. 6.3 (A) Cutout showing parts of a typical oating gate transistor. (B) A oating gate in schematic. 6.5 Conclusions and Outlook This chapter has outlined a deep reinforcement learning based approach for achieving AGI. The merger between deep architectures as scalable state inference engines and reinforcement learning as a powerful control framework offers the potential for an AGI breakthrough. It was further argued that large-scale intelligence systems can be built using existing neuromorphic technology. Many issues remain in enhancing the introduced approach to address the various critical attributes of true AGI systems.",Theoretical Foundations of Artificial General Intelligence,chapter 6
"The capacity and role of intrinsic rewards, generated as a product on internal cognitive processes, needs to be better understood. The manner by which virtual goals are created in the brain merits further studying and modeling. The impact of traumatic experiences, for example, plays a key role in the human psyche, serving a critical purpose of imprinting vital information and/or experiences for long haul recollection and inference. While many such cognitive phenomena are poorly understood and are likely to pose modeling challenges, the paradigm proposed in this chapter is inherently generic and serves as solid basis for AGI research in the years to come. Pragmatically      102 Theoretical Foundations of Articial General Intelligence speaking, the technology needed to experiment with deep reinforcement learning based AGI exists today. Moreover, such technology is within reach for many research institutions, rendering AGI breakthrough a possible reality in the near future. Bibliography [1] R. Bellman, Dynamic Programming, Princeton University Press, (1957). [2] R.",Theoretical Foundations of Artificial General Intelligence,chapter 6
"Duda, P. Hart, and D. Stork, Pattern Recognition, Wiley-Interscience, 2nd Edition, (2000). [3] T. Lee and D. Mumford, Hierarchical Bayesian Inference in the Visual Cortex, Journal of the Optical Society of America, vol. 20, part 7, pp. 14341448, (2003). [4] T. Lee, D. Mumford, R. Romero, and V. Lamme, The role of the primary visual cortex in higher level vision, Vision Research, vol. 38, pp. 24292454, (1998). [5] I. Arel, D. Rose, and T. Karnowski, Deep Machine Learning A New Frontier in Articial Intelligence Research, IEEE Computational Intelligence Magazine, Vol. 14, pp. 1218, Nov., (2010). [6] Y.",Theoretical Foundations of Artificial General Intelligence,chapter 6
"Bengio, Learning Deep Architectures for AI, Foundations and Trends in Machine Learning, (2009). [7] G. Wallis and H. Blthoff, Learning to recognize objects, Trends in Cognitive Sciences, vol. 3, issue 1, pp. 2331, (1999). [8] G. Wallis and E. Rolls, Invariant Face and Object Recognition in the Visual System, Progress in Neurobiology, vol. 51, pp. 167194, (1997). [9] R.E. Suri and W. Schultz, A neural network model with dopamine-like reinforcement signal that learns a spatial delayed response task, Neuroscience, vol. 91, no. 3, pp. 871890, 1999. [10] W. Schultz, Predictive reward signal of dopamine neurons, The Journal of Neurophysiology, vol. 80, no. 1, pp. 127, 1998.",Theoretical Foundations of Artificial General Intelligence,chapter 6
"[11] R.S. Sutton and A.G. Barto, Reinforcement Learning: An Introduction, Cambridge MA, MIT Press, 1998. [12] A.D. Millner and M.A. Goodale, The Visual Brain in Action, Oxford University Press, 1996. [13] M. Mishkin, L. G. Ungerkeuder, and K. A. Macko, Object vision and spatial vision: two cortical pathways, Trends in Neuroscience, vol. 6, pp. 414417, 1983. [14] G.S. Ginsburg and P. Bronstein, Family factors related to childrens intrinsic/extrinsic motivational orientation and academic performance, Child Development, vol. 64, pp. 14611474, 1993. [15] S.A. Kelley, C.A. Brownell, and S.B.",Theoretical Foundations of Artificial General Intelligence,chapter 6
"Campbell, Mastery motivation and self-evaluative affect in toddlers: longitudinal relations with maternal behavior, Child Development, vol. 71, pp. 106171, 2000. [16] R. Baillargeon, Physical reasoning in young infants: Seeking explanations for impossible events, British Journal of Developmental Psychology, vol. 12, pp. 933, 1994. [17] D. Joel, Y. Niv, and E. Ruppin, Actor-critic models of the basal ganglia: New anatomical and computational perspectives, Neural Networks, vol. 15, pp. 535547, 2002. [18] P. Gray, P. Hurst, S. Lewis, and R. Meyer, Analysis and design of analog integrated circuits, John Wiley and Sons, 2001. [19] P. Hasler and J.",Theoretical Foundations of Artificial General Intelligence,chapter 6
"Dugger, An analog oating-gate node for supervised learning, IEEE Transactions on Circuits and Systems I, vol. 52, no. 5, pp. 834845, May 2005. [20] L.P. Kaelbling, M.L. Littman, and A.R. Cassandra, Planning and acting in partially observable stochastic domains, Articial Intelligence Journal, 101: 99134, 1998.   ",Theoretical Foundations of Artificial General Intelligence,chapter 6
"  Chapter 7 The LIDA Model as a Foundational Architecture for AGI Usef Faghihi and Stan Franklin Computer Science Department & Institute for Intelligent Systems The University of Memphis, Memphis, TN, 38152 USA E-mail: {ufaghihi, franklin}@memphis.edu Articial intelligence (AI) initially aimed at creating thinking machines, that is, computer systems having human level general intelligence. However, AI research has until recently focused on creating intelligent, but highly domain-specic, systems. Currently, researchers are again undertaking the original challenge of creating AI systems (agents) capable of human-level intelligence, or articial general intelligence (AGI). In this chapter, we will argue that Learning Intelligent Distribution Agent (LIDA), which implements Baars Global Workspace Theory (GWT), may be suitable as an underlying cognitive architecture on which others might build an AGI.",Theoretical Foundations of Artificial General Intelligence,chapter 7
"Our arguments rely mostly on an analysis of how LIDA satises Suns desiderata for cognitive architectures as well as Newells test for a theory of cognition. Finally, we measure LIDA against the architectural features listed in the BICA Table of Implemented Cognitive Architectures, as well as to the anticipated needs of AGI developers. 7.1 Introduction The eld of articial intelligence (AI) initially aimed at creating thinking machines, that is, creating computer systems having human level general intelligence. However, AI research has until recently mostly focused on creating intelligent, but highly domainspecic, computer systems. At present however, researchers are again undertaking the original challenge of creating AI systems (agents) capable of human-level intelligence, or articial general intelligence (AGI).",Theoretical Foundations of Artificial General Intelligence,chapter 7
"To do so it may well help to be guided by following question, how do minds work? Among different theories of cognition, we choose to work from the Global Workspace 103      104 Theoretical Foundations of Articial General Intelligence Theory (GWT) of Baars [1,2] the most widely accepted psychological and neurobiological theory of the role of consciousness in cognition [36]. GWT is a neuropsychological theory of the role of consciousness in cognition. It views the nervous system as a distributed parallel system incorporating many different specialized processes. Various coalitions of these specialized processes facilitate making sense of the sensory data currently coming in from the environment. Other coalitions sort through the results of this initial processing and pick out items requiring further attention. In the competition for attention a winner emerges, and occupies what Baars calls the global workspace, the winning contents of which are presumed to be at least functionally conscious [7].",Theoretical Foundations of Artificial General Intelligence,chapter 7
"The presence of a predator, enemy, or imminent danger should be expected, for example, to win the competition for attention. However, an unexpected loud noise might well usurp consciousness momentarily even in one of these situations. The contents of the global workspace are broadcast to processes throughout the nervous system in order to recruit an action or response to this salient aspect of the current situation. The contents of this global broadcast enable each of several modes of learning. We will argue that Learning Intelligent Distribution Agent (LIDA) [8], which implements Baars GWT, may be suitable as an underlying cognitive architecture on which to build an AGI. The LIDA architecture, a work in progress, is based on the earlier IDA, an intelligent, autonomous, conscious software agent that does personnel work for the US Navy [9]. IDA uses locally developed articial intelligence technology designed to model human cognition. IDAs task is to nd jobs for sailors whose current assignments are about to end.",Theoretical Foundations of Artificial General Intelligence,chapter 7
"She selects jobs to offer a sailor, taking into account the Navys policies, the jobs needs, the sailors preferences, and her own deliberation about feasible dates. Then she negotiates with the sailor, in English via a succession of emails, about job selection. We use the word conscious in the functional consciousness sense of Baars Global Workspace Theory [1,2], upon which our architecture is based (see also [7]) . 7.2 Why the LIDA model may be suitable for AGI The LIDA model of cognition is a fully integrated articial cognitive system capable of reaching across a broad spectrum of cognition, from low-level perception/action to highlevel reasoning. The LIDA model has two faces, its science side and its engineering side.",Theoretical Foundations of Artificial General Intelligence,chapter 7
"LIDAs science side eshes out a number of psychological and neuropsychological theories of human cognition including GWT [2], situated cognition [10], perceptual sym     The LIDA Model as a Foundational Architecture for AGI 105 bol systems [11], working memory [12], memory by affordances [13], long-term working memory [14], and the H-CogAff architecture [15]. The LIDA architecture engineering side explores architectural designs for software agents that promise more exible, more human-like intelligence within their domains. It employs several modules that are designed using computational mechanisms drawn from the new AI. These include variants of the Copycat Architecture [16, 17], Sparse Distributed Memory [18, 19], the Schema Mechanism [20, 21], the Behavior Net [22], and the Subsumption Architecture [23]. However, an AGI developer using LIDA need make no commitment to any of these mechanisms.",Theoretical Foundations of Artificial General Intelligence,chapter 7
"The computational framework of the LIDA architecture [24] allows free substitution of such mechanisms (see below). The required commitment is relatively modest, consisting primarily of a weak adherence to the LIDA cognitive cycle (see below). In addition, the LIDA architecture can accommodate the myriad features1 that will undoubtedly be required of an AGI (see below). Thus the LIDA architecture, empirically based on psychological and biological principles, offers the exibility to relatively easily experiment with different paths to an AGI. This makes us think of LIDA as eminently suitable for an underlying foundational architecture for AGI. 7.3 LIDA architecture Any AGI will have to deal with tremendous amounts of sensory inputs. It will therefore need attention to lter the incoming sensory data to recruit resources in order to respond, and to learn. Note that this greatly resembles the Global Workspace Theory broadcast. By denition, every AGI must be able to operate in a wide variety of domains.",Theoretical Foundations of Artificial General Intelligence,chapter 7
"It must therefore be capable of very exible decision making. Flexible motivation, resulting from and modulated by feelings and emotions are in turn crucial to this end. The LIDA framework is setup accordingly. LIDA can be thought of as a proof of concept model for GWT. Many of the tasks in this model are accomplished by codelets [16] implementing the processors in GWT. Codelets are small pieces of code, each running independently. A class of codelets called attention codelets serves, with the global workspace, to implement attention. An attention codelet attempts to bring the contents of its coalition to the consciousness spotlight. A broadcast then occurs, directed to all the processors in the system, to recruit resources with which to handle the current situation, and to learn. 1Here we distinguish between features of an architecture such as one of its main components (e.g., Sensory Processing) and features of, say, an object such as its colors.",Theoretical Foundations of Artificial General Intelligence,chapter 7
"     106 Theoretical Foundations of Articial General Intelligence The LIDA architecture is partly symbolic and partly connectionist with all perceptual symbols being grounded in the physical world in the sense of Brooks [25]. Thus the LIDA architecture is embodied. (For further information on situated or embodied cognition, please see [2629]. LIDA performs through its cognitive cycles (Figure 7.1), which occur ve to ten times a second [30, 31], and depend upon saliency determination by the agent. A cognitive cycle starts with a sensation and usually ends with an action. The cognitive cycle is conceived of as an active process that allows interactions between the different components of the architecture. Thus, cognitive cycles are always on-going. We now describe LIDAs primary mechanisms.",Theoretical Foundations of Artificial General Intelligence,chapter 7
"1) Perceptual Associative Memory (PAM): This corresponds neurally to the parts of different sensory cortices in humans (visual, auditory and somatosensory), plus parts of other areas (e.g. enterhinal cortex). PAM allows the agent to distinguish, classify and identify external and internal information. PAM is implemented in the LIDA architecture with a version of the slipnet [16]. There are connections between slipnet nodes. Segments of the slipnet are copied into the agents Workspace as parts of the percept [32]. In LIDA, perceptual learning is learning to recognize new objects, new categorizations, and new relationships. With the conscious broadcast (Figure 7.1), new objects, categories, and the relationships among them and between them and other elements of the agents ontology are learned by adding nodes (objects and categories) and links (relationships) to PAM. Existing nodes and links can have their base-level activations reinforced.",Theoretical Foundations of Artificial General Intelligence,chapter 7
"The conscious broadcast begins and updates the process of learning to recognize and to categorize, both employing perceptual memory [8]. 2) Workspace: This roughly corresponds to the human preconscious buffers of working memory [33]. This is the place that holds perceptual structures, which come from perception. It also includes previous percepts not yet decayed away, and local associations from episodic memories. These local associations are combined with the percepts to generate a Current Situational Model, the agents understanding of what is going on right now. Information written in the workspace may reappear in different cognitive cycles until it decays away. 3) Episodic memories: These are memories for events (what, where and when). When the consciousness mechanism broadcasts information, it is saved into transient episodic memory (TEM) and is later consolidated into LIDAs declarative memory (DM) [34].",Theoretical Foundations of Artificial General Intelligence,chapter 7
"In LIDA, episodic learning refers to the memory of events  the what, the where and the      The LIDA Model as a Foundational Architecture for AGI 107 when [12,35]. In the LIDA model such learned events are stored in transient episodic memory [34,36] and in the longer-term declarative memory [34]. Both are implemented using sparse distributed memory [18], which is both associative and content addressable, and has other characteristics that correspond to psychological properties of memory. In particular it knows when it doesnt know, and exhibits the tip of the tongue phenomenon. Episodic learning in the LIDA model is also a matter of generate and test, with such learning occurring at the conscious broadcast of each cognitive cycle. Episodic learning is initially directed only to transient episodic memory.",Theoretical Foundations of Artificial General Intelligence,chapter 7
"At a later time and ofine, the undecayed contents of transient episodic memory are consolidated [37] into declarative memory, where they still may decay away or may last a lifetime. 4) Attentional Memory (ATM): ATM is implemented as a collection of a particular kind of codelet called an attention codelet. All attention codelets are tasked with nding their own specic content in the Current Situational Model (CSM) of the Workspace. For example, one codelet may look for a node representing fear. When an attention codelet nds its content it creates a coalition containing this content and related content. The coalition is added to the Global Workspace to compete for consciousness.",Theoretical Foundations of Artificial General Intelligence,chapter 7
"Each attention codelet has the following attributes: 1) concern: that content, whose presence in the CSM, can trigger the codelet to act; 2) a base-level activation, a measure of the codelets usefulness in bringing information to consciousness, as well as its general importance; and 3) a current activation which measures the degree of intersection between its concern and the content of the current situational model. The total activation measures the current saliency of its concern. We use a sigmoid function to both reinforce and decay the base-level and the current activations. The ATM includes several kinds of attention codelets. The default attention codelet reacts to whatever content it nds in the Current Situational Model in the Workspace, trying to bring its most energetic content to the Global Workspace. Specic attention codelets are codelets that may have been learned. They bring particular Workspace content to the Global Workspace.",Theoretical Foundations of Artificial General Intelligence,chapter 7
"Expectation codelets, created during action selection, attempt to bring the result (or non-result) of a recently-executed action to consciousness. Intention codelets are attention codelets that bring any content that can help the agent reach a current goal to consciousness. That is, when the agent makes a volitional decision, an intention codelet is generated. There are attention codelets that react to the various dimensions of saliency, including novelty informativeness, importance, insistence, urgency and unexpectedness. Attentional learning is the learning of what to attend to [38, 39]. In the LIDA model attentional learning involves attention codelets, small processes whose job      108 Theoretical Foundations of Articial General Intelligence it is to focus the agents attention on some particular portion of its internal model of the current situation. Again, learning occurs with the conscious broadcast with new attention codelets being created and existing attention codelets being reinforced.",Theoretical Foundations of Artificial General Intelligence,chapter 7
"5) Procedural Memory, Action Selection and Sensory-motor Memory: LIDAs procedural memory deals with deciding what to do next. It is similar to Dreschers schema mechanism but with fewer parameters [20,40]. The scheme net is a directed graph in which each of the nodes has a context, an action, and results. As a result of the conscious broadcast, schemes from Procedural Memory are instantiated and put into the Action Selection mechanism. The Action Selection mechanism then chooses an action and Sensory-Motor Memory executes the action (Figure 7.1). LIDA uses Maes Behavior Network with some modications [41] as its Action Selection mechanism [22]. Thus, in LIDAs architecture, while Procedural Memory and the Action Selection mechanism are responsible for deciding what will be done next, Sensory-Motor memory is responsible for deciding how tasks will be performed. Thus, each of these memory systems requires distinct mechanisms.",Theoretical Foundations of Artificial General Intelligence,chapter 7
"In LIDA, procedural learning encodes procedures for possibly relevant behaviors into Procedural Memory (Figure 7.1). It is the learning of new actions and action sequences with which to accomplish new tasks. It is the learning of under what circumstances to perform new behaviors, or to improve knowledge of when to use existing behaviors. These procedural skills are shaped by reinforcement learning, operating by way of conscious processes over more than one cognitive cycle [8]. It must be noted that the LIDA model for the four aforementioned modes of learning, supports the instructionalist learning of new memory entities as well as the selectionist reinforcement of existing entities. 7.4 Cognitive architectures, features and the LIDA model An AGI has to be built on some cognitive architecture, and by its nature, should share many features with other cognitive architectures.",Theoretical Foundations of Artificial General Intelligence,chapter 7
"The most widely known cognitive architectures include Newells Soar architecture [4244], Andersons ACT-R architecture [4547], Suns CLARION architecture [48], and Franklins LIDA architecture [8]. The aforementioned cognitive architectures each have their strengths and weaknesses when it comes to dening a theory of mind [49, 50]. Some researchers have also tried to identify the most important features needed to construct biologically-inspired cognitive architectures (BICA). In particular, an AGI may well need the features listed by Sun (2004), and      The LIDA Model as a Foundational Architecture for AGI 109 by Newell [51], as well as features from the BICA table [52]. The BICA table is not shown in this paper because its size is beyond the size of this document (readers can refer to the online version for full details). The LIDA model seems to have room for all these features.",Theoretical Foundations of Artificial General Intelligence,chapter 7
"In the next sections, we describe Suns desiderata and Newells functional criteria for cognitive architectures and, in italics, the LIDA models features that correspond to each of these cognitive architecture criteria. An assessment of LIDA against the features from the BICA table follows. Fig. 7.1 LIDAs Architecture 7.4.1 Ron Suns Desiderata [53] In his article, Sun proposes a set of essential desiderata for developing cognitive architectures, and argues for the importance of taking into full consideration these desiderata in developing future architectures that are more cognitively and ecologically realistic. Though, AGI is not explicitly mentioned  the article predates the use of the term AGI  one could infer that Sun would consider them desiderata for an AGI as well.",Theoretical Foundations of Artificial General Intelligence,chapter 7
"Here are some      110 Theoretical Foundations of Articial General Intelligence of his desiderata interspersed with our assessment of how well the LIDA model does, or does not, achieve them. Ecological realism: Taking into account everyday activities of cognitive agents in their natural ecological environments. LIDA does allow ecological realism. It was designed for the everyday life of animals and/or articial agents. Animals typically respond to incoming stimuli, often in search of food, mates, safety from predators, etc. Doing so requires frequent sampling of the environment, interpreting stimuli, attending to the most salient, and responding appropriately. The cascading LIDA cognitive cycles provides exactly this processing. Bio-evolutionary realism supplements ecological realism. This feature refers to the idea that intelligences in species are on a continuum, and that cognitive models of human intelligence should not be limited to strictly human cognitive processes, but rather should be reducible to models of animal intelligence as well.",Theoretical Foundations of Artificial General Intelligence,chapter 7
"The LIDA model integrates diverse research on animal cognition. It has been used to provide both an ontology of concepts and their relations, and a working model of an animals cognitive processes [54,55]. In addition to helping to account for a broad range of cognitive processes, the LIDA model can help to comparatively assess the cognitive capabilities of different animal species. Cognitive realism. Cognition is highly variable within species. Cognitive realism refers to the idea that cognitive architectures should seek to replicate only essential characteristics of human cognition. The reader is referred to the Why the LIDA model may be suitable for AGI section above to see that this condition is satised. Eclecticism of methodologies and techniques. Adhering too closely to any methodology or paradigm will only prevent the creation of new or improved cognitive architectures. It is best to take a more broad-based approach. Again the reader is referred to the Why the LIDA model may be suitable for AGI section. Reactivity.",Theoretical Foundations of Artificial General Intelligence,chapter 7
"Reactivity refers to xed responses to given stimuli (as opposed to fulledged analyses of stimuli to select a response) that characterize many human behaviors [15].      The LIDA Model as a Foundational Architecture for AGI 111 The LIDA models reactive part is setup as a relatively direct connection between incoming sensory data and the outgoing actions of effectors. At the end of every cognitive cycle LIDA selects an appropriate behavior in response to the current situation. This consciously mediated, but unconscious, selection is always reactive. Sequentiality. Sequentiallity refers to the chronological nature of human everyday activities. In LIDA, cognitive cycles allow the agent to perform its activities in a sequential manner. The cycles can cascade. But, these cascading cognitive cycles must preserve the sequentiality of the LIDA agents stream of functional consciousness, as well as its selection of an action in each cycle [56]. Routineness.",Theoretical Foundations of Artificial General Intelligence,chapter 7
"Routineness refers to the fact that humans every day behaviors are made of routines, which are constantly and smoothly adapting to the changing environment. The LIDA model is equipped with both reactive and deliberative high-level cognitive processes. Such processes become routine when they are incorporated (learned) into LIDAs procedural memory as schemes representing behavior streams. Trial-and-error adaptation. Trial-and-error adaptation refers to the trial-and-error process through which humans learn and develop reactive routines. LIDA learns from experience, which may yield several lessons over several cognitive cycles. Such lessons include newly perceived objects and their relationship to already known objects and categories, relationships among objects and between objects and actions, effects of actions on sensation, and improved perception of sensory data. All of LIDAs learning be it, perceptual, episodic, or procedural, is very much trial and error (generate and test as AI researchers would say).",Theoretical Foundations of Artificial General Intelligence,chapter 7
"LIDA is proigate in its learning, with new entities and reinforcement of existing entities learned with every broadcast. Those that are sufciently reinforced (tested) remain. The others decay away as errors. 7.4.2 Newells functional criteria (adapted from Lebiere and Anderson 2003) Newell proposed multiple criteria that a human cognitive architecture should satisfy in order to be functional [57, 58]. Lebiere and Anderson [51] combined his two overlapping lists into the twelve criteria, phrased as questions, listed below. Each criterion described will be followed by an analysis of how LIDA does, or does not, satisfy it.      112 Theoretical Foundations of Articial General Intelligence Flexible behavior: Does the architecture behave as an (almost) arbitrary function of the environment? Is the architecture computationally universal with failure? This criterion demands exibility of action selection.",Theoretical Foundations of Artificial General Intelligence,chapter 7
"In LIDA, motivation for actions, learning and perceiving, come from feelings and emotions. These provide a much more exible kind of motivation for action selection than do drives, causations or rules, producing more exible action selection. In LIDA, various types of learning, including learning to recognize or perform procedures, also contribute to exible behavior. LIDAs sophisticated action selection itself allows such exibility as switching back and forth between various tasks. LIDA is exible in what to attend to, at any given time, increasing the exibility of action selection as well. We suspect that LIDA cannot do everything that can be done with a Turing machine; it is not computationally universal. We also suspect that this is not necessary for AGI.",Theoretical Foundations of Artificial General Intelligence,chapter 7
"Real-time operation: Does the architecture operate in real time? Given its timing assumptions, can it respond as fast as humans? LIDAs cognitive cycles individually take approximately 300 ms, and they sample the environment cascading at roughly ve to ten times per-second [59]. There is considerable empirical evidence from neuroscience suggestive of, and consistent with, such cognitive cycling in humans [6066]. An earlier software agent, IDA (see above), based on the LIDA architecture, found new billets for sailors in about the same time as it took a human detailer [59]. Rationality: Does the architecture exhibit rational, i.e., effective adaptive behavior? Does the system yield functional behavior in the real world? In the LIDA model, feelings and emotions play important role in decision making. The LIDA model can feature both the affective and rational human-inspired models of decision making [67].",Theoretical Foundations of Artificial General Intelligence,chapter 7
"LIDAs predecessor IDA, controlled by much the same architecture, was quite functional [68], promising the same for LIDA controlled agents. Knowledgeable in terms of size: Can it use vast amounts of knowledge about the environment? How does the size of the knowledge base affect performance? In the LIDA model, selective attention lters potentially large amounts of incoming sensory data. Selective attention also provides access to appropriate internal resources that allow the agent to select appropriate actions and to learn from vast amounts of data produced during interactions in a complex environment. The model has perceptual, episodic,      The LIDA Model as a Foundational Architecture for AGI 113 attentional and procedural memory for the long term storage of various kinds of information.",Theoretical Foundations of Artificial General Intelligence,chapter 7
"Knowledgeable in terms of variety: Does the agent integrate diverse knowledge? Is it capable of common examples of intellectual combination? A major function of LIDAs preconscious Workspace is precisely to integrate diverse knowledge in the process of updating the Current Situational Model from which the contents of consciousness is selected. Long-term sources of this knowledge include Perceptual Associative Memory and Declarative Memory. There are several sources of more immediate knowledge that come into play. LIDA is, in principle, capable of common examples of intellectual combination, though work has only begun on the rst implemented LIDA based agent promising such capability. Behaviorally robust: Does the agent behave robustly in the face of error, the unexpected, and the unknown? Can it produce cognitive agents that successfully inhabit dynamic environments? LIDAs predecessor, IDA, was developed for the US Navy to fulll tasks performed by human resource personnel called detailers.",Theoretical Foundations of Artificial General Intelligence,chapter 7
"At the end of each sailors tour of duty, he or she is assigned to a new billet. This assignment process is called distribution. The Navy employs almost 300 full time detailers to effect these new assignments. IDAs task is to facilitate this process, by automating the role of detailer. IDA was tested by former detailers and accepted by the Navy [68]. Linguistic: Does the agent use (natural) language? Is it ready to take a test of language prociency? IDA communicates with sailors by email in unstructured English. However, we think of this capability as pseudo-natural-language, since it is accomplished only due to the relatively narrow domain of discourse. Language comprehension and language production are high-level cognitive processes in humans. In the LIDA model, such higher-level processes are distinguished by requiring multiple cognitive cycles for their accomplishment.",Theoretical Foundations of Artificial General Intelligence,chapter 7
"In LIDA, higher-level cognitive processes can be implemented by one or more behavior streams; that is, streams of instantiated schemes and links from procedural memory. Thus LIDA should, in principle, be capable of natural language understanding and production. In practice, work on natural language has just begun. Self-awareness: Does the agent exhibit self-awareness and a sense of self? Can it produce functional accounts of phenomena that reect consciousness?      114 Theoretical Foundations of Articial General Intelligence Researchers in, philosophy, neuroscience and psychology postulate various forms of a self in humans and animals. All of these selves seem to have a basis in some form of consciousness. GWT suggests that a self-system can be thought of ... as the dominant context of experience and action. [2, Chapter 9].",Theoretical Foundations of Artificial General Intelligence,chapter 7
"Following others (see below) the various selves in an autonomous agent may be categorized into three major components, namely: 1) the Proto-Self; 2) the Minimal (Core) Self; and 3) the Extended Self. The LIDA model provides for the basic building blocks from which to implement the various parts of a multi-layered self-system as hypothesized by philosophers, psychologists and neuroscientists [69]. In the following, we discuss each component, and their sub-selves, very briey (for more detail see [69]). 1) The Proto-Self: Antonio Damasio conceived the Proto-self as a short-term collection of neural patterns of activity representing the current state of the organism [70].",Theoretical Foundations of Artificial General Intelligence,chapter 7
"In LIDA, the Proto-self is implemented as the set of global and relevant parameters in the various modules including the Action Selection and the memory systems, and the underlying computer systems memory and operating system; 2) the Minimal (Core) Self: The minimal or core self [71] is continually regenerated in a series of pulses, which blend together to give rise to a continuous stream of consciousness. The Minimal Self can be implemented as sets of entities in the LIDA ontology, that is, as computational collections of nodes in the slipnet of LIDAs perceptual associative memory; and 3) the Extended Self: The extended self consists of (a) the autobiographical self, (b) the self-concept, (c) the volitional or executive self, and (d) the narrative self. In human beings, the autobiographical self develops directly from episodic memory.",Theoretical Foundations of Artificial General Intelligence,chapter 7
"In the LIDA model, the autobiographical self can be described as the local associations from transient episodic memory and declarative memory which come to the workspace in every cognitive cycle. The self-concept consists of enduring self-beliefs and intentions, particularly those relating to personal identity and properties. In the LIDA model, the agents beliefs are in the semantic memory and each volitional goal has an intention codelet. The volitional self provides executive function. In the LIDA model, deliberate actions are implemented by behavior streams. Thus, LIDA has a volitional self. The narrative self is able to report actions, intentions, etc., sometimes equivocally, contradictorily or self-deceptively. In the LIDA model, feeling, motivation, and attention nodes play a very important role in the Narrative Self. That is, after understanding a self-report request, the LIDA model could, in principle, generate a report based on its understanding of such a request.",Theoretical Foundations of Artificial General Intelligence,chapter 7
"     The LIDA Model as a Foundational Architecture for AGI 115 Adaptive through learning: Does the agent learn from its environment? Can it produce the variety of human learning? LIDA is equipped with perceptual, episodic, procedural, attentional learning, all modulated by feelings and emotions. As humans do, LIDA learns continually and implicitly with each conscious broadcast in each cognitive cycle. Developmental: Does the agent acquire capabilities through development? Can it account for developmental phenomena? Since LIDA learns as humans do, we would expect a LIDA controlled agent to go through a developmental period of rapid learning as would a child. The work on replicating data from developmental experiments has just begun. Evolvable: Can the agent arise through evolution? Does the theory relate to evolutionary and comparative considerations? Since LIDA is attempted to model humans and other animals, presumably the model should be evolvable and comparative, at least in principle.",Theoretical Foundations of Artificial General Intelligence,chapter 7
"Be realizable within the brain: Do the components of the theory exhaustively map onto brain processes? Shanahan [6] devotes two chapters to a compelling argument that the brain is organized so as to support a conscious broadcast. LIDA is beginning to build on this insight, using the work of Freeman and colleagues [72], to create a non-linear dynamical systems bridge between LIDA and the underlying brain. Whether this bridge will lead to an exhaustive mapping is not at all clear as yet. 7.4.3 BICA table Any AGI is likely to be produced by a very diverse collection of cognitive modules and their processes. There is a computational framework for LIDA [24] that requires only a modest commitment to the underlying assumptions of the LIDA architecture. One can introduce into LIDAs framework a large variety of differently implemented modules and processes so that, many possible AGI architectures could be implemented from the LIDA framework.",Theoretical Foundations of Artificial General Intelligence,chapter 7
"One advantage of doing it in this way is that all of these AGI architectures implemented on the top of the LIDAs framework, would use a common ontology based on the LIDA model as presented to AGI 2011 [24].      116 Theoretical Foundations of Articial General Intelligence In the following, we will give an assessment of the LIDA model against the features of the BICA Table of Implemented Cognitive Architectures [52]. Column 1 of the BICA Table contains a list of features proposed by developers of cognitive architectures to be at least potentially useful, if not essential, for the support of an AGI. Subsequent columns are devoted to individual cognitive architectures with a cell describing how its column architecture addresses its row feature. The rest of this section is an expansion of the column devoted to LIDA in the BICA table. Note that all the Basic overview features listed in the BICAs rst column are detailed earlier in this chapter.",Theoretical Foundations of Artificial General Intelligence,chapter 7
"We will discuss the rest of the features in the following: Support for Common Components: The LIDA model supports all features mentioned in this part such as episodic and semantic memories. However, the auditory mechanism is not implemented in a LIDA-based agent as yet. Support for Common Learning Algorithms: The LIDA model supports different types of learning such as episodic, perceptual, procedural, and attentional learning. However, the Bayesian Update and Gradient Descent Methods (e.g., Backpropagation) are not implemented in a LIDA-based agent. Common General Paradigms Modeled: The LIDA model supports features listed in this part such as decision making and problem solving. However, perceptual illusions, meta-cognitive tasks, social psychology tasks, personality psychology tasks, motivational dynamics are not implemented in a LIDA-based agent.",Theoretical Foundations of Artificial General Intelligence,chapter 7
"Common Specic Paradigms Modeled columns: 1) Stroop; 2) Task Switching; 3) Tower of Hanoi/London; 4) Dual Task; 5) N-Back; 6) Visual perception with comprehension; 7) Spatial exploration; 8) Learning and navigation; 9) Object/feature search in an environment; 10) Learning from instructions; 11) Pretend-play. Although the Common Specic Paradigms Modeled features listed above are not implemented in LIDA, in principle LIDA is capable of implementing each of them. For instance, a LIDA-based agent is replicating some attentional tasks  la Van Bockstaeles and his colleagues [73]. Meta-Theoretical Questions: 1) Uses only local computations? Yes, throughout the architecture with the one exception of the conscious broadcast which is global; 2) Unsupervised learning? Yes.",Theoretical Foundations of Artificial General Intelligence,chapter 7
"The LIDA model supports four different modes of learning, perceptual, episodic, attentional and procedural;      The LIDA Model as a Foundational Architecture for AGI 117 3) Supervised learning? While in principle possible for a LIDA agent, supervised learning per se is not part of the architecture; 4) Can it learn in real time? Yes (see above); 5) Can it do fast stable learning; i.e., adaptive weights converge on each trial without forcing catastrophic forgetting? Yes. One shot learning in several modes occurs with the conscious broadcast during each cognitive cycle. With sufcient affective support and/or sufcient repeated attention, such learning can be quite stable; 6) Can it function autonomously? Yes. A LIDA-based agent can, in principle,operate machines and drive vehicles autonomously; 7) Is it general-purpose in its modality; i.e.",Theoretical Foundations of Artificial General Intelligence,chapter 7
"is it brittle? A LIDA-based agent can, in principle, be developed to be general purpose and robust in real world environments; 8) Can it learn from arbitrarily large databases; i.e., not toy problems? Yes, this question is already answered in the previous sections; 9) Can it learn about non-stationary databases; i.e., environmental rules change unpredictably? Yes, a LIDA-based agent is, in principle, capable of working properly in an unpredictable environment; 10) Can it pay attention to valued goals? Yes, already explained earlier in this chapter; 11) Can it exibly switch attention between unexpected challenges and valued goals? Yes.",Theoretical Foundations of Artificial General Intelligence,chapter 7
"A LIDA-based agent attends to what is most salient based on its situational awareness; 12) Can reinforcement learning and motivation modulate perceptual and cognitive decisionmaking? Yes; 13) Can it adaptively fuse information from multiple types of sensors and modalities? In principle, yes, but it has yet to be implemented in particular domains with multiple senses. 7.5 Discussion, Conclusions In this chapter, we argue that LIDA may be suitable as an underlying cognitive architecture on which others might build an AGI. Our arguments rely mostly on an analysis of how LIDA satises Suns desiderata for cognitive architectures as well as Newells test for a theory of cognition. We also measured LIDA against the architectural features listed in the BICA Table of Implemented Cognitive Architectures, as well as to the anticipated needs of AGI developers.      118 Theoretical Foundations of Articial General Intelligence As can be seen in Section 7.",Theoretical Foundations of Artificial General Intelligence,chapter 7
"4 above, the LIDA model seems to meet all of Suns ... essential desiderata for developing cognitive architectures, and Newells criteria that a human cognitive architecture should satisfy in order to be functional. In addition, the LIDA architecture seems to be able, at least in principle, of incorporating each of the features listed in the BICA Table of Implemented Cognitive Architectures. Thus the LIDA architecture would seem to offer the requisite breadth of features. The LIDA computational framework offers software support for the development of LIDA based software agents, as well as LIDA based control systems for autonomous robots [24]. As described in Section 7.2 above, developing an AGI based loosely on the LIDA architecture requires only a modest commitment. Higher-level cognitive processes such as reasoning, planning, deliberation, etc., must be implemented by behavior streams, that is, using cognitive cycles.",Theoretical Foundations of Artificial General Intelligence,chapter 7
"But this is not a strong commitment, since, using the LIDA computational framework, any individual module in LIDAs cognitive cycle can be modied at will, or even replaced by another designed by the AGI developer. Thus we are left with the contention that various AGI systems can effectively be developed based loosely on the LIDA architecture and its computational framework. Such systems would lend themselves to relatively easy incremental improvements by groups of developers and, due to their common foundation and ontology, would also allow relatively straightforward testing and comparison. Thus the LIDA architecture would seem to be an ideal starting point for the development of AGI systems. Bibliography [1] B.J. Baars. In the Theater of Consciousness: The Workspace of the Mind. Oxford: Oxford University Press (1997). [2] B.J. Baars. A cognitive theory of consciousness. Cambridge: Cambridge University Press (1988). [3] B.J. Baars.",Theoretical Foundations of Artificial General Intelligence,chapter 7
"The conscious access hypothesis: origins and recent evidence. Trends in Cognitive Science 4752 (2002). [4] S. Dehaene & L. Naccache. Towards a cognitive neuroscience of consciousness: basic evidence and a workspace framework. Cognition 79, 137 (2001). [5] N. Kanwisher. Neural events and perceptual awareness. Cognition 79:89, 89113 (2001). [6] M. Shanahan. Embodiment and the Inner Life. Oxford: Oxford University Press (2010). [7] S. Franklin. IDA: A Conscious Artifact? Journal of Consciousness Studies 10, 4766 (2003). [8] S. Franklin & F.G.J. Patterson. The LIDA Architecture: Adding New Modes of Learning to an Intelligent, Autonomous, Software Agent.",Theoretical Foundations of Artificial General Intelligence,chapter 7
"Integrated Design and Process Technology, IDPT2006, San Diego, CA, Society for Design and Process Science (2006). [9] S. Franklin, A. Kelemen & L. McCauley. IDA: A Cognitive Agent Architecture. IEEE Conf. on Systems, Man and Cybernetics, 26462651 (1998).      Bibliography 119 [10] F.J. Varela, E. Thompson & E. Rosch. The embodied mind: Cognitive Science and Human Experience. MIT Press, Cambridge, MA, USA (1991). [11] L.W. Barsalou. Perceptual Symbol Systems. Vol. 22 (MA: The MIT Press, 1999). [12] A.D. Baddeley. The episodic buffer: a new component of working memory? Trends in Cognitive Science 4, 417423 (2000). [13] A.M. Glenberg.",Theoretical Foundations of Artificial General Intelligence,chapter 7
"What memory is for. Behavioral and Brain Sciences, 119 (1997). [14] K.A. Ericsson & W. Kintsch. Long-term working memory. Psychological Review 102, 21245 (1995). [15] A. Sloman. What Sort of Architecture is Required for a Human-like Agent? In Foundations of Rational Agency, ed. M. Wooldridge, and A. Rao. Dordrecht, Netherlands: Kluwer Academic Publishers (1999). [16] D. Hofstadter, R & M. Mitchell. The Copycat Project: A model of mental uidity and analogymaking In Advances in Connectionist and Neural Computation theory, Vol. 2: Logical Connections, ed. K.J. Holyoak, and J.A. Barnden, N.J. Norwood: Ablex. (1994). [17] J. Marshall.",Theoretical Foundations of Artificial General Intelligence,chapter 7
"Metacat: A self-watching cognitive architecture for analogy-making. Proceedings of the 24th Annual Conference of the Cognitive Science Society 631636 (2002). [18] P. Kanerva. Sparse Distributed Memory. Cambridge MA: The MIT Press (1988). [19] R.P.N. Rao & O. Fuentes. Hierarchical Learning of Navigational Behaviors in an Autonomous Robot using a Predictive Sparse Distributed Memory. Machine Learning 31, 87113 (1998). [20] G.L. Drescher. Made-Up Minds: A Constructivist Approach to Articial Intelligence. Cambridge, MA: MIT Press (1991). [21] H.H. Chaput, B. Kuipers & R. Miikkulainen. Constructivist Learning: A Neural Implementation of the Schema Mechanism. Workshop for Self-Organizing Maps, Kitakyushu, Japan (2003). [22] P. Maes.",Theoretical Foundations of Artificial General Intelligence,chapter 7
"How to do the right thing. Connection Science 1, 291323 (1989). [23] R.A. Brooks. Intelligence without Representation. Articial intelligence. Elsevier (1991). [24] J. Snaider, R. McCall & S. Franklin. The LIDA Framework as a General Tool for AGI. Paper presented at the The Fourth Conference on Articial General Intelligence, Mountain View, California, USA (2011). [25] R.A. Brooks. A robust layered control system for a mobile robot. IEEE Journal of Robotics and Automation 2, pp. 1423 (1986). [26] M.L. Anderson. Embodied Cognition: A Field Guide. Articial Intelligence 149, 91130 (2003). [27] T. Ziemke, J. Zlatev & R. M. Frank. Body, Language and Mind: Volume 1: Embodiment.",Theoretical Foundations of Artificial General Intelligence,chapter 7
"(Mouton de Gruyter, 2007). [28] S. Harnad. The Symbol Grounding Problem. Physica D 42, 335346 (1990). [29] M. de Vega, A. Glenberg & A. Graesser. Symbols and Embodiment:Debates on meaning and cognition. Oxford: Oxford University Press (2008). [30] B.J. Baars & S. Franklin. How conscious experience and working memory interact. Trends in Cognitive Sciences 7 (2003). [31] S. Franklin, B.J. Baars, U. Ramamurthy & M. Ventura. The Role of Consciousness in Memory. Brains, Minds and Media 1, bmm150 (urn:nbn:de:0009-3-1505) (2005). [32] R. McCall, S. Franklin & D. Friedlander.",Theoretical Foundations of Artificial General Intelligence,chapter 7
"Grounded Event-Based and Modal Representations for Objects, Relations, Beliefs, Etc. Paper presented at the FLAIRS-23, Daytona Beach, FL (2010). [33] A.D. Baddeley. Working memory and conscious awareness. In Theories of memory, (eds. Alan Collins, S. Gathercole, M. A Conway, & P. Morris) 1128 (Erlbaum, 1993). [34] S. Franklin. Cognitive Robots: Perceptual associative memory and learning. In Proceedings of the 14th Annual International Workshop on Robot and Human Interactive Communication (2005).      120 Theoretical Foundations of Articial General Intelligence [35] E. Tulving. Elements of Episodic Memory. New York: Oxford University Press (1983). [36] M.A. Conway. Sensory-perceptual episodic memory and its context: autobiographical memory. In Episodic Memory, ed. A.",Theoretical Foundations of Artificial General Intelligence,chapter 7
"Baddeley, M. Conway, and J. Aggleton. Oxford: Oxford University Press (2002). [37] R. Stickgold & M.P. Walker. Memory consolidation and reconsolidation: what is the role of sleep? Trends Neurosci 28, 408415 (2005). [38] W.K. Estes. Classication and Cognition. Oxford: Oxford University Press (1993). [39] Z. Vidnynszky & W. Sohn. Attentional learning: learning to bias sensory competition. Journal of Vision 3 (2003). [40] G.L. Drescher. Learning from Experience Without Prior Knowledge in a Complicated World. Proceedings of the AAAI Symposium on Parallel Models. AAAI Press (1988). [41] A. Negatu & S. Franklin. An action selection mechanism for conscious software agents. Cognitive Science Quarterly 2, 363386 (2002).",Theoretical Foundations of Artificial General Intelligence,chapter 7
"[42] P. Rosenbloom, J. Laird & A. Newell. The Soar Papers: Research on Integrated Intelligence. Cambridge, Massachusetts: MIT Press (1993). [43] J.E. Laird, A. Newell & P.S. Rosenbloom. Soar: an architecture for general intelligence. Articial Intelligence 33, 164 (1987). [44] J.F. Lehman, J.E. Laird & P.S. Rosenbloom. A gentle introduction to Soar, an architecture for human cognition. In Invitation to Cognitive Science Methods, Models, and Conceptual Issues, Vol. 4 (eds. S. Sternberg & D. Scarborough) (MA: MIT Press, 1998). [45] J.R. Anderson. Rules of the mind. (Mahwah, NJ: Lawrence Erlbaum Associates, 1993). [46] J.R. Anderson. The Architecture of Cognition.",Theoretical Foundations of Artificial General Intelligence,chapter 7
"Cambridge, MA: Harvard University Press (1983). [47] J.R. Anderson, D. Bothell, M.D. Byrne, S. Douglass, C. Lebiere & Y. Qin. An integrated theory of the mind. Psychological Review 111, 10361060 (2004). [48] R. Sun. The CLARION cognitive architecture: Extending cognitive modeling to social simulation Cognition and Multi-Agent interaction. Cambridge University Press, New York (2006). [49] U. Faghihi. The use of emotions in the implementation of various types of learning in a cognitive agent. Ph.D thesis, University of Quebec at Montreal (UQAM), (2011). [50] D. Vernon, G. Metta & G. Sandini. A Survey of Articial Cognitive Systems: Implications for the Autonomous Development of Mental Capabilities in Computational Agents.",Theoretical Foundations of Artificial General Intelligence,chapter 7
"IEEE Transactions on Evolutionary Computation, Special Issue on Autonomous Mental Development 11, 151180 (2007). [51] J.R. Anderson & C. Lebiere. The Newell Test for a theory of cognition. Behavioral And Brain Sciences 26 (2003). [52] A.V. Samsonovich. Toward a Unied Catalog of Implemented Cognitive Architectures. Proceeding of the 2010 Conference on Biologically Inspired Cognitive Architectures, 195244 (2010). [53] R. Sun. Desiderata for cognitive architectures. Philosophical Psychology 17, 341373 (2004). [54] S. Franklin & M.H. Ferkin. An Ontology for Comparative Cognition: a Functional Approach. Comparative Cognition & Behavior Reviews 1, 3652 (2006). [55] S. DMello & S. Franklin. A cognitive models view of animal cognition. Cognitive models and animal cognition.",Theoretical Foundations of Artificial General Intelligence,chapter 7
"Current Zoology. (in press) (2011). [56] J. Snaider, R. McCall & S. Franklin. Time production and representation in a conceptual and computational cognitive model. Cognitive Systems Research. (in press). [57] A. Newell. Unied Theory of Cognition. Cambridge, MA: Harvard University Press (1990). [58] A. Newell. Precis of Unied theories of cognition. Behavioral and Brain Sciences (1992). [59] T. Madl, B.J. Baars & S. Franklin. The Timing of the Cognitive Cycle. PLoS ONE (2011). [60] S. Doesburg, J. Green, J. McDonald & L. Ward. Rhythms of consciousness: binocular rivalry reveals large-scale oscillatory network dynamics mediating visual perception. PLoS One. 4:      Bibliography 121 c6142 (2009).",Theoretical Foundations of Artificial General Intelligence,chapter 7
"[61] W. Freeman. The limbic action-perception cycle controlling goal-directed animal behavior. Neural Networks 3, 22492254 (2002). [62] J. Fuster. Upper processing stages of the perception-action cycle. Trends in Cognitive Sciences 8, 143145 (2004). [63] M. Massimini, F. Ferrarelli, R. Huber, S.K. Esser & H. Singh. Breakdown of Cortical Effective Connectivity During Sleep. Science 309 (2005). [64] M. Sigman & S. Dehaene. Dynamics of the Central Bottleneck: Dual-Task and Task Uncertainty. PLoS Biol. 4 (2006). [65] N. Uchida, A. Kepecs & Z. F. Mainen. Seeing at a glance, smelling in a whiff: rapid forms of perceptual decision making.",Theoretical Foundations of Artificial General Intelligence,chapter 7
"Nature Reviews Neuroscience 7, 485491 (2006). [66] J. Willis & A. Todorov. First Impressions: Making Up Your Mind After a 100-Ms Exposure to a Face. Psychological Science 17, 592599 (2006). [67] W. Wallach, S. Franklin & C. Allen. In Topics in Cognitive Science, special issue on Cognitive Based Theories of Moral Decision Making (eds. W. Wallach & S. Franklin) 454485 (Cognitive Science Society, 2010). [68] L. McCauley & S. Franklin. A Large-Scale Multi-Agent System for Navy Personnel Distribution. Connection Science 14, 371385 Comments: special issue on agent autonomy and groups. (2002). [69] U. Ramamurthy & S. Franklin. Self System in a model of Cognition.",Theoretical Foundations of Artificial General Intelligence,chapter 7
"Proceedings of Machine Consciousness Symposium at the Articial Intelligence and Simulation of Behavior Convention (AISB11), University of York, UK, 5154 (2011). [70] A.R. Damasio. The Feeling of What Happens: Body and Emotion in the Making of Consciousness. New York: Harcourt Inc (1999). [71] S. Gallagher. Philosophical conceptions of the self: implications for cognitive science. Trends in Cognitive Science 4, 1421 (2000). [72] C. Skarda & W.J. Freeman. How Brains Make Chaos in Order to Make Sense of the World. Behavioral and Brain Sciences 10, 161195 (1987). [73] B. Van Bockstaele, B. Verschuere, J.D. Houwer & G. Crombez. On the costs and benets of directing attention towards or away from threat-related stimuli: A classical conditioning experiment.",Theoretical Foundations of Artificial General Intelligence,chapter 7
"Behaviour Research and Therapy 48, 692697 (2010).   ",Theoretical Foundations of Artificial General Intelligence,chapter 7
"  Chapter 8 The Architecture of Human-Like General Intelligence Ben Goertzel 1, Matt Ikl 2 and Jared Wigmore 3 1 Novamente LLC, 1405 Bernerd Place, Rockville MD 2 Dept. of Mathematics and Computing, Adams State College, Alamosa CO 3 School of Design, Hong Kong Polytechnic University ben@goertzel.org By exploring the relationships between different AGI architectures, one can work toward a holistic cognitive model of human-level intelligence.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"In this vein, here an integrative architecture diagram for human-like general intelligence is proposed, via merging of lightly modied version of prior diagrams including Aaron Slomans high-level cognitive model, Stan Franklin and the LIDA groups model of working memory and the cognitive cycle, Joscha Bach and Dietrich Drners Psi model of motivated action and cognition, James Albuss three-hierarchy intelligent robotics model, and the authors prior work on cognitive synergy in deliberative thought and metacognition, along with ideas from deep learning and computational linguistics. The purpose is not to propose an actual merger of the various AGI systems considered, but rather to highlight the points of compatibility between the different approaches, as well as the differences of both focus and substance. The result is perhaps the most comprehensive architecture diagram of human-cognition yet produced, tying together all key aspects of human intelligence in a coherent way that is not tightly bound to any particular cognitive or AGI theory.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"Finally, the question of the dynamics associated with the architecture is considered, including the potential that human-level intelligence requires cognitive synergy between these various components is considered; and the possibility of a trickiness property causing the intelligence of the overall system to be badly suboptimal if any of the components are missing or insufciently cooperative. One idea emerging from these dynamic consideration is that implementing the whole integrative architecture diagram may be necessary for achieving anywhere near human-level, human-like general intelligence. 8.1 Introduction Cognitive science appears to have a problem with integrative understanding. Over the last few decades, cognitive science has discovered a great deal about the structures and 123      124 Theoretical Foundations of Articial General Intelligence dynamics underlying the human mind. However, as in many other branches of science, there has been more focus on detailed analysis of individual aspects, than on unied holistic understanding.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"As a result of this tendency, there are not many compelling examples of holistic cognitive architecture diagrams for human intelligence  diagrams systematically laying out all the pieces needed to generate human intelligence and how they interact with each other. Part of the reason why global human cognitive architecture diagrams are not so common is, of course, a lack of agreement in the eld regarding all the relevant issues. Since there are multiple opinions regarding nearly every aspect of human intelligence, it would be difcult to get two cognitive scientists to fully agree on every aspect of an overall human cognitive architecture diagram. Prior attempts to outline detailed mind architectures have tended to follow highly specic theories of intelligence, and hence have attracted only moderate interest from researchers not adhering to those theories. An example is Minskys work presented in The Emotion Machine [13], which arguably does constitute an architecture diagram for the human mind, but which is only loosely grounded in current empirical knowledge and stands more as a representation of Minskys own intuitive understanding.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"On the other hand, AGI appears to have a problem with the mutual comprehensibility and comparability of different research approaches. The AGI eld has in recent years seen a proliferation of cognitive architectures, each purporting to cover all aspects needed for the creation of human-level general intelligence. However, the differences of language and focus among the various approaches has often made it difcult for researchers to fully understand each others work, let alone collaborate effectively. This chapter describes a conceptual experiment aimed at addressing both of the above problems together. We aim to present a coherent, overall architecture diagram for human, and human-like, general intelligence, via combining the architecture diagrams associated with a number of contemporary AGI architectures. While the exercise is phrased in terms of diagrams, of course the underlying impetus is conceptual integration; and our hope is that the exercise described here will serve as a starting point for ongoing exploration of the relationships between multiple AGI architectures and cognitive theories.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"The architecture diagram we give here does not reect our own idiosyncratic understanding of human intelligence, as much as a combination of understandings previously presented by multiple researchers (including ourselves), arranged according to our own taste in a manner we nd conceptually coherent. With this in mind, we call it the integrative diagram (a longer, grander and more explanatory name would be The First Integra     The Architecture of Human-Like General Intelligence 125 tive Human-Like Cognitive Architecture Diagram). We have made an effort to ensure that as many pieces of the integrative diagram as possible are well grounded in psychological and even neuroscientic data, rather than mainly embodying speculative notions; however, given the current state of knowledge, this could not be done to a complete extent, and there is still some speculation involved here and there.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"While based largely on understandings of human intelligence, the integrative diagram is intended to serve as an architectural outline for human-like general intelligence more broadly. For example, the OpenCog AGI architecture which we have co-created is explicitly not intended as a precise emulation of human intelligence, and does many things quite differently than the human mind, yet can still fairly straightforwardly be mapped into the integrative diagram. Finally, having presented the integrative diagram which focuses on structure, we present some comments on the dynamics corresponding to that structure, focusing on the notion of cognitive synergy: the hypothesis that multiple subsystems of a generally intelligent system, focused on learning regarding different sorts of information, must interact in such a way as to actively aid each other in overcoming combinatorial explosions. This represents a fairly strong hypothesis regarding how the different components in the integrative diagram interact with each other.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"Further, it gives a way of addressing one of the more vexing issues in the AGI eld: the difculty of measuring partial progress toward human-level AGI. We will conjecture that this difculty is largely attributable to a trickiness property of cognitive synergy in the integrative diagram. One of the upshots of our discussion of dynamics is: We consider it likely that to achieve anything remotely like human-like general intelligence, it will be necessary to implement basically all the components in the integrative diagram, in a thorough and richly interconnected way. Implementing half the boxes in the diagram is not likely to get us to a system with half the general intelligence of a human. The architecture of human-like cognition is a richly interconnected whole. 8.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"2 Key Ingredients of the Integrative Human-Like Cognitive Architecture Diagram In assembling the integrative diagram, we have drawn on the work of researchers at the intersection of AGI and cognitive science  that is, researchers largely motivated by human cognitive science and neuroscience, but with aims of producing comprehensive architectures for human-like AGI. The main ingredients used in assembling the diagram are:      126 Theoretical Foundations of Articial General Intelligence  Aaron Slomans high-level architecture diagram of human intelligence [14], drawn from his CogAff archtiecture, which it strikes us as a particularly clear embodiment of modern common sense regarding the overall architecture of the human mind. We have added only a couple items to Slomans high-level diagram, which we felt deserved an explicit high-level role that he did not give them: emotion, language and reinforcement.  The LIDA architecture diagram presented by Stan Franklin and Bernard Baars [3].",Theoretical Foundations of Artificial General Intelligence,chapter 8
"We think LIDA is an excellent model of working memory and what Sloman calls reactive processes, with well-researched grounding in the psychology and neuroscience literature. We have adapted the LIDA diagram only very slightly for use here, changing some of the terminology on the arrows, and indicating where parts of the LIDA diagram indicate processes elaborated in more detail elsewhere in the integrative diagram.  The architecture diagram of the Psi model of motivated cognition, presented by Joscha Bach in [4] based on prior work by Dietrich Drner [5]. This diagram is presented without signicant modication; however it should be noted that Bach and Drner present this diagram in the context of larger and richer cognitive models, the other aspects of which are not all incorporated in the integrative diagram.  James Albuss three-hierarchy model of intelligence [1], involving coupled perception, action and reinforcement hierarchies.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"Albuss model, utilized in the creation of intelligent unmanned automated vehicles, is a crisp embodiment of many ideas emergent from the eld of intelligent control systems.  Deep learning networks as a model of perception (and action and reinforcement learning), as embodied for example in the work of Itamar Arel [2] and Jeff Hawkins [10]. The integrative diagram adopts this as the basic model of the perception and action subsystems of human intelligence. Language understanding and generation are also modeled according to this paradigm.  The OpenCog [7] integrative AGI architecture (in which we have played a key role), which places greatest focus on various types of long-term memory and their interrelationship, and is used mainly to guide the integrative architectures treatment of these matters. Most of these ingredients could be interpreted as holistic explanations of human-like intelligence on their own. However, each of them places a focus in a different place, more elaborated in some regards than others.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"So it is interesting to collage the architecture diagrams from the different approaches together, and see what results. The product of this      The Architecture of Human-Like General Intelligence 127 exercise does not accord precisely with any of the component AGI architectures, and is not proposed as an architecture diagram for an AGI. However, we believe it has value as an exercise in integrative cognitive science. It is a mind-architecture diagram, drawing preferentially on different cognitive-science-inspired AGI approaches in those aspects where they have been most thoroughly rened. One possible negative reaction to the integrative diagram might be to say that its a kind of Frankenstein monster, piecing together aspects of different theories in a way that violates the theoretical notions underlying all of them! For example, the integrative diagram takes LIDA as a model of working memory and reactive processing, but from the papers on LIDA its unclear whether the creators of LIDA construe it more broadly than that.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"The deep learning community tends to believe that the architecture of current deep learning networks, in itself, is close to sufcient for human-level general intelligence  whereas the integrative diagram appropriates the ideas from this community mainly for handling perception, action and language. Etc. On the other hand, in a more positive perspective, one could view the integrative diagram as consistent with LIDA, but merely providing much more detail on some of the boxes in the LIDA diagram (e.g. dealing with perception and long-term memory). And one could view the integrative diagram as consistent with the deep learning paradigm  via viewing it, not as a description of components to be explicitly implemented in an AGI system, but rather as a description of the key structures and processes that must emerge in deep learning network, based on its engagement with the world, in order for it to achieve human-like general intelligence.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"It seems to us that different communities of cognitive science and AGI researchers have focused on different aspects of intelligence, and have thus each created models that are more fully eshed out in some aspects than others. But these various models all link together fairly cleanly, which is not surprising as they are all grounded in the same data regarding human intelligence. Many judgment calls must be made in fusing multiple models in the way that the integrative diagram does, but we feel these can be made without violating the spirit of the component models. In assembling the integrative diagram, we have made these judgment calls as best we can, but were well aware that different judgments would also be feasible and defensible. Revisions are likely as time goes on, not only due to new data about human intelligence but also to evolution of understanding regarding the best approach to model integration.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"     128 Theoretical Foundations of Articial General Intelligence Another possible argument against the ideas presented here is that theres nothing new  all the ingredients presented have been given before elsewhere. To this our retort is to quote Pascal: Let no one say that I have said nothing new ... the arrangement of the subject is new. The various architecture diagrams incorporated into the integrative diagram are either extremely high level (Slomans diagram) or focus primarily on one aspect of intelligence, treating the others very concisely by summarizing large networks of distinction structures and processes in small boxes. The integrative diagram seeks to cover all aspects of humanlike intelligence at a roughly equal granularity  a different arrangement. 8.3 An Architecture Diagram for Human-Like General Intelligence The integrative diagram is presented here in a series of seven gures. Fig. 8.1 High-Level Architecture of a Human-Like Mind Figure 8.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"1 gives a high-level breakdown into components, based on Slomans highlevel cognitive-architecturalsketch [14]. This diagram represents, roughly speaking, modern common sense about how a human-like mind is architected. The separation between      The Architecture of Human-Like General Intelligence 129 structures and processes, embodied in having separate boxes for Working Memory vs. Reactive Processes, and for Long Term Memory vs. Deliberative Processes, could be viewed as somewhat articial, since in the human brain and most AGI architectures, memory and processing are closely integrated. However, the tradition in cognitive psychology is to separate out Working Memory and Long Term Memory from the cognitive processes acting thereupon, so we have adhered to that convention.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"The other changes from Slomans diagram are the explicit inclusion of language, representing the hypothesis that language processing is handled in a somewhat special way in the human brain; and the inclusion of a reinforcement component parallel to the perception and action hierarchies, as inspired by intelligent control systems theory (e.g. Albus as mentioned above) and deep learning theory. Of course Slomans high level diagram in its original form is intended as inclusive of language and reinforcement, but we felt it made sense to give them more emphasis. Fig. 8.2 Architecture of Working Memory and Reactive Processing, closely modeled on the LIDA architecture Figure 8.2, modeling working memory and reactive processing, is essentially the LIDA diagram as given in prior papers by Stan Franklin, Bernard Baars and colleagues [3].",Theoretical Foundations of Artificial General Intelligence,chapter 8
"The boxes in the upper left corner of the LIDA diagram pertain to sensory and motor processing, which LIDA does not handle in detail, and which are modeled more carefully by deep learning theory. The bottom left corner box refers to action selection, which in the integrative diagram is modeled in more detail by Psi. The top right corner box refers to      130 Theoretical Foundations of Articial General Intelligence Long-Term Memory, which the integrative diagram models in more detail as a synergetic multi-memory system (Figure 8.4). The original LIDA diagram refers to various codelets, a key concept in LIDA theory. We have replaced attention codelets here with attention ow, a more generic term. We suggest one can think of an attention codelet as a piece of information that its currently pertinent to pay attention to a certain collection of items together. Fig. 8.3 Architecture of Motivated Action Figure 8.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"3, modeling motivation and action selection, is a lightly modied version of the Psi diagram from Joscha Bachs book Principles of Synthetic Intelligence [4]. The main difference from Psi is that in the integrative diagram the Psi motivated action framework is embedded in a larger, more complex cognitive model. Psi comes with its own theory of working and long-term memory, which is related to but different from the one given in the      The Architecture of Human-Like General Intelligence 131 integrative diagram  it views the multiple memory types distinguished in the integrative diagram as emergent from a common memory substrate. Psi comes with its own theory of perception and action, which seems broadly consistent with the deep learning approach incorporated in the integrative diagram. Psis handling of working memory lacks the detailed, explicit workow of LIDA, though it seems broadly conceptually consistent with LIDA. In Figure 8.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"3, the box labeled Other parts of working memory is labeled Protocol and situation memory in the original diagram. The Perception, Action Execution and Action Selection boxes have fairly similar semantics to the similarly labeled boxes in the LIDA-like Figure 8.2, so that these diagrams may be viewed as overlapping. The LIDA model doesnt explain action selection and planning in as much detail as Psi, so the Psi-like Figure 8.3 could be viewed as an elaboration of the action-selection portion of the LIDAlike Figure 8.2. In Psi, reinforcement is considered as part of the learning process involved in action selection and planning; in Figure 8.3 an explicit reinforcement box has been added to the original Psi diagram, to emphasize this. Fig. 8.4 Architecture of Long-Term Memory and Deliberative and Metacognitive Thinking Figure 8.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"4, modeling long-term memory and deliberative processing, is derived from our own prior work studying the cognitive synergy between different cognitive processes associated with different types of memory. The division into types of memory is fairly standard. Declarative, procedural, episodic and sensorimotor memory are routinely distinguished; we like to distinguish attentional memory and intentional (goal) memory as well,      132 Theoretical Foundations of Articial General Intelligence and view these as the interface between long-term memory and the minds global control systems. One focus of our AGI design work has been on designing learning algorithms, corresponding to these various types of memory, that interact with each other in a synergetic way [7], helping each other to overcome their intrinsic combinatorial explosions. There is signicant evidence that these various types of long-term memory are differently implemented in the brain, but the degree of structure and dynamical commonality underlying these different implementations remains unclear.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"Each of these long-term memory types has its analogue in working memory as well. In some cognitive models, the working memory and long-term memory versions of a memory type and corresponding cognitive processes, are basically the same thing. OpenCog is mostly like this  it implements working memory as a subset of long-term memory consisting of items with particularly high importance values. The distinctive nature of working memory is enforced via using slightly different dynamical equations to update the importance values of items with importance above a certain threshold. On the other hand, many cognitive models treat working and long term memory as more distinct than this, and there is evidence for signicant functional and anatomical distinctness in the brain in some cases. So for the purpose of the integrative diagram, it seemed best to leave working and long-term memory subcomponents as parallel but distinguished. Figure 8.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"4 also encompasses metacognition, under the hypothesis that in human beings and human-like minds, metacognitive thinking is carried out using basically the same processes as plain ordinary deliberative thinking, perhaps with various tweaks optimizing them for thinking about thinking. If it turns out that humans have, say, a special kind of reasoning faculty exclusively for metacognition, then the diagram would need to be modied. Modeling of self and others is understood to occur via a combination of metacognition and deliberative thinking, as well as via implicit adaptation based on reactive processing. Figure 8.5 models perception, according to the basic ideas of deep learning theory. Vision and audition are modeled as deep learning hierarchies, with bottom-up and top-down dynamics. The lower layers in each hierarchy refer to more localized patterns recognized in, and abstracted from, sensory data.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"Output from these hierarchies to the rest of the mind is not just through the top layers, but via some sort of sampling from various layers, with a bias toward the top layers. The different hierarchies cross-connect, and are hence to an extent dynamically coupled together. It is also recognized that there are some sensory modalities that arent strongly hierarchical, e.g. touch and smell (the latter being better modeled as something like an asymmetric Hopeld net, prone to frequent chaotic dynam     The Architecture of Human-Like General Intelligence 133 Fig. 8.5 Architecture for Multimodal Perception ics [12])  these may also cross-connect with each other and with the more hierarchical perceptual subnetworks. Of course the suggested architecture could include any number of sensory modalities; the diagram is restricted to four just for simplicity.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"The self-organized patterns in the upper layers of perceptual hierarchies may become quite complex and may develop advanced cognitive capabilities like episodic memory, reasoning, language learning, etc. A pure deep learning approach to intelligence argues that all the aspects of intelligence emerge from this kind of dynamics (among perceptual, action and reinforcement hierarchies). Our own view is that the heterogeneity of human brain architecture argues against this perspective, and that deep learning systems are probably better as models of perception and action than of general cognition. However, the integrative diagram is not committed to our perspective on this  a deep-learning theorist could accept the integrative diagram, but argue that all the other portions besides the perceptual, action and reinforcement hierarchies should be viewed as descriptions of phenomena that emerge in these hierarchies due to their interaction. Figure 8.6 shows an action subsystem and a reinforcement subsystem, parallel to the perception subsystem.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"Two action hierarchies, one for an arm and one for a leg, are shown for concreteness, but of course the architecture is intended to be extended more broadly. In      134 Theoretical Foundations of Articial General Intelligence Fig. 8.6 Architecture for Action and Reinforcement the hierarchy corresponding to an arm, for example, the lowest level would contain control patterns corresponding to individual joints, the next level up to groupings of joints (like ngers), the next level up to larger parts of the arm (hand, elbow). The different hierarchies corresponding to different body parts cross-link, enabling coordination among body parts; and they also connect at multiple levels to perception hierarchies, enabling sensorimotor coordination. Finally there is a module for motor planning, which links tightly with all the motor hierarchies, and also overlaps with the more cognitive, inferential planning activities of the mind, in a manner that is modeled different ways by different theorists.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"Albus [1] has elaborated this kind of hierarchy quite elaborately.      The Architecture of Human-Like General Intelligence 135 The reward hierarchy in Figure 8.6 provides reinforcement to actions at various levels on the hierarchy, and includes dynamics for propagating information about reinforcement up and down the hierarchy. Fig. 8.7 Architecture for Language Processing Figure 8.7 deals with language, treating it as a special case of coupled perception and action. The traditional architecture of a computational language comprehension system is a pipeline [9,11], which is equivalent to a hierarchy with the lowest-level linguistic features (e.g. sounds, words) at the bottom, and the highest level features (semantic abstractions) at the top, and syntactic features in the middle. Feedback connections enable semantic and cognitive modulation of lower-level linguistic processing.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"Similarly, language generation is commonly modeled hierarchically, with the top levels being the ideas needing verbalization, and the bottom level corresponding to the actual sentence produced. In generation the primary ow is top-down, with bottom-up ow providing modulation of abstract concepts by linguistic surface forms. So, thats it  an integrative architecture diagram for human-like general intelligence, split among 7 different pictures, formed by judiciously merging together architecture diagrams produced via a number of cognitive theorists with different, overlapping foci and research paradigms. Is anything critical left out of the diagram? A quick perusal of the table of contents of cognitive psychology textbooks suggests to me that if anything major is left out, its also unknown to current cognitive psychology. However, one could certainly make an argument for explicit inclusion of certain other aspects of intelligence, that in the integrative      136 Theoretical Foundations of Articial General Intelligence diagram are left as implicit emergent phenomena.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"For instance, creativity is obviously very important to intelligence, but, there is no creativity box in any of these diagrams  because in our view, and the view of the cognitive theorists whose work weve directly drawn on here, creativity is best viewed as a process emergent from other processes that are explicitly included in the diagrams. 8.4 Interpretation and Application of the Integrative Diagram A tongue-partly-in-cheek denition of a biological pathway is a subnetwork of a biological network, that ts on a single journal page. Cognitive architecture diagrams have a similar property  they are crude abstractions of complex structures and dynamics, sculpted in accordance with the size of the printed page, and the tolerance of the human eye for absorbing diagrams, and the tolerance of the human author for making diagrams. However, sometimes constraints  even arbitrary ones  are useful for guiding creative efforts, due to the fact that they force choices.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"Creating an architecture for human-like general intelligence that ts in a few (okay, 7) fairly compact diagrams, requires one to make many choices about what features and relationships are most essential. In constructing the integrative diagram, we have sought to make these choices, not purely according to our own tastes in cognitive theory or AGI system design, but according to a sort of blend of the taste and judgment of a number of scientists whose views we respect, and who seem to have fairly compatible, complementary perspectives. What is the use of a cognitive architecture diagram like this? It can help to give newcomers to the eld a basic idea about what is known and suspected about the nature of human-like general intelligence. Also, it could potentially be used as a tool for crosscorrelating different AGI architectures.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"If everyone who authored an AGI architecture would explain how their architecture accounts for each of the structures and processes identied in the integrative diagram, this would give a means of relating the various AGI designs to each other. The integrative diagram could also be used to help connect AGI and cognitive psychology to neuroscience in a more systematic way. In the case of LIDA, a fairly careful correspondence has been drawn up between the LIDA diagram nodes and links and various neural structures and processes [6]. Similar knowledge exists for the rest of the integrative diagram, though not organized in such a systematic fashion. A systematic curation of links between the nodes and links in the integrative diagram and current neuroscience knowl     The Architecture of Human-Like General Intelligence 137 edge, would constitute an interesting rst approximation of the holistic cognitive behavior of the human brain. Finally (and harking forward to the next section), the big omission in the integrative diagram is dynamics.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"Structure alone will only get you so far, and you could build an AGI system with reasonable-looking things in each of the integrative diagrams boxes, interrelating according to the given arrows, and yet still fail to make a viable AGI system. Given the limitations the real world places on computing resources, its not enough to have adequate representations and algorithms in all the boxes, communicating together properly and capable doing the right things given sufcient resources. Rather, one needs to have all the boxes lled in properly with structures and processes that, when they act together using feasible computing resources, will yield appropriately intelligent behaviors via their cooperative activity. And this has to do with the complex interactive dynamics of all the processes in all the different boxes  which is something the integrative diagram doesnt touch at all. This brings us again to the network of ideas weve discussed under the name of cognitive synergy, to be discussed more extensively below.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"It might be possible to make something similar to the integrative diagram on the level of dynamics rather than structures, complementing the structural integrative diagram given here; but this would seem signicantly more challenging, because we lack a standard set of tools for depicting system dynamics. Most cognitive theorists and AGI architects describe their structural ideas using boxes-and-lines diagrams of some sort, but there is no standard method for depicting complex system dynamics. So to make a dynamical analogue to the integrative diagram, via a similar integrative methodology, one would rst need to create appropriate diagrammatic formalizations of the dynamics of the various cognitive theories being integrated  a fascinating but onerous task. When we rst set out to make an integrated cognitive architecture diagram, via combining the complementary insights of various cognitive science and AGI theorists, we werent sure how well it would work. But now we feel the experiment was generally a success  the resultant integrated architecture seems sensible and coherent, and reasonably complete.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"It doesnt come close to telling you everything you need to know to understand or implement a human-like mind  but it tells you the various processes and structures you need to deal with, and which of their interrelations are most critical. And, perhaps just as importantly, it gives a concrete way of understanding the insights of a specic but fairly diverse set of cognitive science and AGI theorists as complementary rather than contradictory.      138 Theoretical Foundations of Articial General Intelligence 8.5 Cognitive Synergy The architecture of the mind, ultimately, has no meaning without an associated dynamics. Architecture emerges from dynamics, and channels dynamics. The cognitive dynamics of AGI systems is a large topic which we wont attempt to thoroughly pursue here, but we will mention one dynamical principle that we feel is essential for properly interpreting the integrative diagram: cognitive synergy. Cognitive synergy has been proposed as a general principle of feasible general intelligence [8].",Theoretical Foundations of Artificial General Intelligence,chapter 8
"It is both a conceptual hypothesis about the structure of generally intelligent systems in certain classes of environments, and a design principle that one may used to guide the architecting of AGI systems. First we review how cognitive synergy has been previously developed in the context of multi-memory systems  i.e., in the context of the diagram given above for long-term memory and deliberative processing, Figure 8.4.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"In this context, the cognitive synergy hypothesis states that human-like, human-level intelligent systems possess a combination of environment, embodiment and motivational system that makes it important for them to possess memories that divide into partially but not wholly distinct components corresponding to the categories such as:  Declarative memory  Procedural memory (memory about how to do certain things)  Sensory and episodic memory  Attentional memory (knowledge about what to pay attention to in what contexts  Intentional memory (knowledge about the systems own goals and subgoals) The essential idea of cognitive synergy, in the context of multi-memory systems possessing the above memory types, may be expressed in terms of the following points: (1) Intelligence, relative to a certain set of environments, may be understood as the capability to achieve complex goals in these environments.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"(2) With respect to certain classes of goals and environments, an intelligent system requires a multi-memory architecture, meaning the possession of a number of specialized yet interconnected knowledge types, including: declarative, procedural, attentional, sensory, episodic and intentional (goal-related). These knowledge types may be viewed as different sorts of pattern that a system recognizes in itself and its environment.      The Architecture of Human-Like General Intelligence 139 (3) Such a system must possess knowledge creation (i.e. pattern recognition / formation) mechanisms corresponding to each of these memory types. These mechanisms are also called cognitive processes.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"(4) Each of these cognitive processes, to be effective, must have the capability to recognize when it lacks the information to perform effectively on its own; and in this case, to dynamically and interactively draw information from knowledge creation mechanisms dealing with other types of knowledge (5) This cross-mechanism interaction must have the result of enabling the knowledge creation mechanisms to perform much more effectively in combination than they would if operated non-interactively. This is cognitive synergy. Interactions as mentioned in Points 4 and 5 in the above list are the real conceptual meat of the cognitive synergy idea.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"One way to express the key idea here is that most AI algorithms suffer from combinatorial explosions: the number of possible elements to be combined in a synthesis or analysis is just too great, and the algorithms are unable to lter through all the possibilities, given the lack of intrinsic constraint that comes along with a general intelligence context (as opposed to a narrow-AI problem like chess-playing, where the context is constrained and hence restricts the scope of possible combinations that needs to be considered). In an AGI architecture based on cognitive synergy, the different learning mechanisms must be designed specically to interact in such a way as to palliate each others combinatorial explosions  so that, for instance, each learning mechanism dealing with a certain sort of knowledge, must synergize with learning mechanisms dealing with the other sorts of knowledge, in a way that decreases the severity of combinatorial explosion.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"One prerequisite for cognitive synergy to work is that each learning mechanism must recognize when it is stuck, meaning its in a situation where it has inadequate information to make a condent judgment about what steps to take next. Then, when it does recognize that its stuck, it may request help from other, complementary cognitive mechanisms. The key point we wish to make here regarding cognitive synergy is that this same principle, previously articulated mainly in the context of deliberative processes acting on long-term memory, seems intuitively to hold on the level of the integrative diagram. Most likely, cognitive synergy holds not only between the learning algorithms associated with different memory systems, but also between the dynamical processes associated with different large-scale components, such as are depicted in the different sub-diagrams of the integrative diagram depicted above.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"If this is so, then all the subdiagrams depend inti     140 Theoretical Foundations of Articial General Intelligence mately on each other in a dynamic sense, meaning that the processes within each of them must be attuned to the processes within each of the others, in order for the whole system to operate effectively. We do not have a proof of this hypothesis at present, so we present it as our intuitive judgment based on informal integration of a wide variety of evidence from cognitive science, neuroscience and articial intelligence. Of course, some pieces of the integrative diagram are bound to be more critical than others. Removing the language box might result in an AGI system with the level of intelligence of a great ape rather than a human, whereas removing signicant portion of the perception box might have direr consequences.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"Removing episodic memory might yield behavior similar to certain humans with brain lesions, whereas removing procedural memory would more likely yield an agent with a basic inability to act in the world. But the point of cognitive synergy is not just that all the boxes in the integrative diagram are needed for human-level intelligence, but rather that the dynamics inside all the boxes need to interact closely in order to achieve human-level intelligence. For instance, its not just removing the perception box that would harm the systems intelligence  forcing the perception box to operate dynamically in isolation from the processes inside the other boxes would have a similar effect. 8.6 Why Is It So Hard to Measure Partial Progress Toward Human-Level AGI? Why it is so hard to measure partial progress toward human-level AGI? The reason is not so hard to understand, if one thinking in terms of cognitive synergy and system integration.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"Supposing the integrative diagram is accurate  then why cant we get, say, 75% of the way to human level intelligence by implementing 75% of the boxes in the integrative diagram? The reason this doesnt work, we suggest, is that cognitive synergy possesses a frustrating but important property called trickiness. Trickiness has implications specically for the evaluation of partial progress toward human-level AGI. Its not entirely straightforward to create tests to measure the nal achievement of human-level AGI, but there are some fairly obvious candidates for evaluation methods. Theres the Turing Test (fooling judges into believing youre human, in a text chat) the video Turing Test, the Robot College Student test (passing university, via being judged exactly the same way a human student would), etc.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"Theres certainly no agree     The Architecture of Human-Like General Intelligence 141 ment on which is the most meaningful such goal to strive for, but theres broad agreement that a number of goals of this nature basically make sense. On the other hand, its much less clear how one should measure whether one is, say, 50 percent of the way to human-level AGI? Or, say, 75 or 25 percent? Its possible to pose many practical tests of incremental progress toward human-level AGI, with the property that IF a proto-AGI system passes the test using a certain sort of architecture and/or dynamics, then this implies a certain amount of progress toward human-level AGI based on particular theoretical assumptions about AGI.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"However, in each case of such a practical test, it seems intuitively likely to a signicant percentage of AGI researchers that there is some way to game the test via designing a system specically oriented toward passing that test, and which doesnt constitute dramatic progress toward AGI. Some examples of practical tests of this nature would be  The Wozniak coffee test: go into an average American house and gure out how to make coffee, including identifying the coffee machine, guring out what the buttons do, nding the coffee in the cabinet, etc.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"Story understanding  reading a story, or watching it on video, and then answering questions about what happened (including questions at various levels of abstraction)  Graduating (virtual-world or robotic) preschool  Passing the elementary school reading curriculum (which involves reading and answering questions about some picture books as well as purely textual ones)  Learning to play an arbitrary video game based on experience only, or based on experience plus reading instructions One interesting point about tests like this is that each of them seems to some AGI researchers to encapsulate the crux of the AGI problem, and be unsolvable by any system not far along the path to human-level AGI  yet seems to other AGI researchers, with different conceptual perspectives, to be something probably game-able by narrow-AI methods.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"And of course, given the current state of science, theres no way to tell which of these practical tests really can be solved via a narrow-AI approach, except by having a lot of people try really hard over a long period of time. A question raised by these observations is whether there is some fundamental reason why its hard to make an objective, theory-independent measure of intermediate progress toward advanced AGI. Is it just that we havent been smart enough to gure out the right test  or is there some conceptual reason why the very notion of such a test is problematic?      142 Theoretical Foundations of Articial General Intelligence We suggest that a partial answer is provided by the trickiness of cognitive synergy. Recall that, in its simplest form, the cognitive synergy hypothesis states that human-level AGI intrinsically depends on the synergetic interaction of multiple components.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"In this hypothesis, for instance, it might be that there are 10 critical components required for a human-level AGI system. Having all 10 of them in place results in human-level AGI, but having only 8 of them in place results in having a dramatically impaired system  and maybe having only 6 or 7 of them in place results in a system that can hardly do anything at all. Of course, the reality is almost surely not as strict as the simplied example in the above paragraph suggests. No AGI theorist has really posited a list of 10 crisply-dened subsystems and claimed them necessary and sufcient for AGI. We suspect there are many different routes to AGI, involving integration of different sorts of subsystems. However, if the cognitive synergy hypothesis is correct, then human-level AGI behaves roughly like the simplistic example in the prior paragraph suggests.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"Perhaps instead of using the 10 components, you could achieve human-level AGI with 7 components, but having only 5 of these 7 would yield drastically impaired functionality  etc. Or the point could be made without any decomposition into a nite set of components, using continuous probability distributions. To mathematically formalize the cognitive synergy hypothesis in its fully generality would become quite complex, but here were only aiming for a qualitative argument. So for illustrative purposes, well stick with the 10 components example, just for communicative simplicity. Next, lets suppose that for any given task, there are ways to achieve this task using a system that is much simpler than any subset of size 6 drawn from the set of 10 components needed for human-level AGI, but works much better for the task than this subset of 6 components (assuming the latter are used as a set of only 6 components, without the other 4 components).",Theoretical Foundations of Artificial General Intelligence,chapter 8
"Note that this supposition is a good bit stronger than mere cognitive synergy. For lack of a better name, well call it tricky cognitive synergy. The tricky cognitive synergy hypothesis would be true if, for example, the following possibilities were true:  creating components to serve as parts of a synergetic AGI is harder than creating components intended to serve as parts of simpler AI systems without synergetic dynamics  components capable of serving as parts of a synergetic AGI are necessarily more complicated than components intended to serve as parts of simpler AGI systems.      The Architecture of Human-Like General Intelligence 143 These certainly seem reasonable possibilities, since to serve as a component of a synergetic AGI system, a component must have the internal exibility to usefully handle interactions with a lot of other components as well as to solve the problems that come its way.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"In a CogPrime context, these possibilities ring true, in the sense that tailoring an AI process for tight integration with other AI processes within CogPrime, tends to require more work than preparing a conceptually similar AI process for use on its own or in a more task-specic narrow AI system. It seems fairly obvious that, if tricky cognitive synergy really holds up as a property of human-level general intelligence, the difculty of formulating tests for intermediate progress toward human-level AGI follows as a consequence. Because, according to the tricky cognitive synergy hypothesis, any test is going to be more easily solved by some simpler narrow AI process than by a partially complete human-level AGI system. 8.7 Conclusion We have presented an integrative diagram summarizing and merging multiple researchers views regarding the architecture of human-level general intelligence.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"We believe the results of our work demonstrate a strong degree of overlap and synergy between different contemporary perspectives on AGI, and illustrate that a substantial plurality of the AGI eld is moving toward consensus on the basic architecture of human-like general intelligence. Also, we suggest the integrative diagram may be useful from a purely cognitive science view, as a coherent high-level picture of all the parts of the human mind and how they work together. We have also presented an argument that, to achieve anything remotely similar to human-level general intelligence, it will be necessary to implement all of the integrative diagram, not just isolated bits and pieces. We believe the arguments given here regarding trickiness provide a plausible explanation for the empirical observation that positing tests for intermediate progress toward human-level AGI is a very difcult prospect.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"If the theoretical notions sketched here are correct, then this difculty is not due to incompetence or lack of imagination on the part of the AGI community, nor due to the primitive state of the AGI eld, but is rather intrinsic to the subject matter. And in that case, the practical implication for AGI development is, very simply, that one shouldnt worry a lot about producing intermediary results that are compelling to skeptical observers. Just at 2/3 of a human brain may not be much use, similarly, 2/3 of an AGI system may not be much use. Lack of      144 Theoretical Foundations of Articial General Intelligence impressive intermediary results may not imply one is on a wrong development path; and comparison with narrow AI systems on specic tasks may be badly misleading as a gauge of incremental progress toward human-level AGI. Thus our overall conclusion is both optimistic and pessimistic.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"If one implements a system instantiating the integrative diagram, and lls in each box with processes that cooperate synergetically with the processes in the other boxes to minimize combinatorial explosion  then one will get a human-level general intelligence. On the other hand, due to the trickiness of cognitive synergy, such a system may not display dramatic general intelligence until it is just about nished! Bibliography [1] J. S. Albus and A. M. Meystel. Engineering of Mind: An Introduction to the Science of Intelligent Systems. Wiley and Sons, 2001. [2] I. Arel, D. Rose, and R. Coop. Destin: A scalable deep learning architecture with application to high-dimensional robust pattern recognition. Proc. AAAI Workshop on Biologically Inspired Cognitive Architectures, 2009. [3] Bernard Baars and Stan Franklin. Consciousness is computational: The lida model of global workspace theory.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"International Journal of Machine Consciousness., 2009. [4] Joscha Bach. Principles of Synthetic Intelligence. Oxford University Press, 2009. [5] Dietrich Drner. Die Mechanik des Seelenwagens. Eine neuronale Theorie der Handlungsregulation. Verlag Hans Huber, 2002. ISBN 345683814X. [6] Stan Franklin and Bernard Baars. Possible neural correlates of cognitive processes and modules from the lida model of cognition. Cognitive Computing Research Group, University of Memphis, 2008. http://ccrg.cs.memphis.edu/tutorial/correlates.html. [7] Ben Goertzel. Opencog prime: A cognitive synergy based architecture for embodied articial general intelligence. In ICCI 2009, Hong Kong, 2009a. [8] Ben Goertzel. Cognitive synergy: A universal principle of feasible general intelligence? 2009b.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"[9] Ben Goertzel et al. A general intelligence oriented architecture for embodied natural language processing. In Proc. of the Third Conf. on Articial General Intelligence (AGI-10). Atlantis Press, 2010. [10] Jeff Hawkins and Sandra Blakeslee. On Intelligence. Brown Walker, 2006. [11] Daniel Jurafsky and James Martin. Speech and Language Processing. Pearson Prentice Hall, 2009. [12] Guang Li, Zhengguo Lou, Le Wang, Xu Li, and Walter J. Freeman. Application of chaotic neural model based on olfactory system on pattern recognition. ICNC, 1:378381, 2005. [13] Marvin Minsky. The Emotion Machine. 2007. [14] Aaron Sloman. Varieties of affect and the cogaff architecture schema.",Theoretical Foundations of Artificial General Intelligence,chapter 8
"In Proceedings of the Symposium on Emotion, Cognition, and Affective Computing, AISB-01, 2001.   ",Theoretical Foundations of Artificial General Intelligence,chapter 8
"  Chapter 9 A New Constructivist AI: From Manual Methods to Self-Constructive Systems Kristinn R. Thrisson Center for Analysis & Design of Intelligent Agents, Reykjavik University and Icelandic Institute for Intelligent Machines Menntavegur 1, IS-101 Reykjavik, Iceland thorisson@{ru.is, iiim.is} The development of articial intelligence (AI) systems has to date been largely one of manual labor. This constructionist approach to AI has resulted in systems with limited-domain application and severe performance brittleness. No AI architecture to date incorporates, in a single system, the many features that make natural intelligence general-purpose, including system-wide attention, analogy-making, system-wide learning, and various other complex transversal functions. Going beyond current AI systems will require signicantly more complex system architecture than attempted to date.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"The heavy reliance on direct human specication and intervention in constructionist AI brings severe theoretical and practical limitations to any system built that way. One way to address the challenge of articial general intelligence (AGI) is replacing a top-down architectural design approach with methods that allow the system to manage its own growth. This calls for a fundamental shift from hand-crafting to self-organizing architectures and self-generated code  what we call a constructivist AI approach, in reference to the self-constructive principles on which it must be based. Methodologies employed for constructivist AI will be very different from todays software development methods; instead of relying on direct design of mental functions and their implementation in a cognitive architecture, they must address the principles  the seeds  from which a cognitive architecture can automatically grow. In this paper I describe the argument in detail and examine some of the implications of this impending paradigm shift. 9.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"1 Introduction Articial intelligence researchers have traditionally been rather optimistic about the rate of progress in the eld. The origin of this optimism dates back numerous deacdes, possibly 145      146 Theoretical Foundations of Articial General Intelligence as far back as our understanding of the electrical properties of neurons and their role in controlling animal behavior, but at the very least to the invention of using electricity for automatic calculation and related tasks  tasks that only humans used to be capable of. These realizations seemed so laden with potential that even the most ardent skeptics couldnt help imagining near-future scenarios where electrical machines would be doing all sorts of tasks requiring intelligence, helping out in every area of human endeavor.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"In spite of measurable progress since the early discoveries, one big question still remains unanswered: How does the human and animal mind work? Bits and pieces of answers have been popping out of neuroscience, cognitive science, psychology, and AI research, but a holistic picture seems as far in the future as ever. What we would like to see  and this is a vision shared by many of those who started the eld of AI over 50 years ago  is an articial system that can learn numerous disparate tasks and facts, reliably perform them in a variety of circumstances and environments of real-world complexity; a system that can apply itself to learning a wide range of skills in a wide range of domains. Such a system would be considered by most as generally intelligent. A trivial example is an AI that can learn, over a period of say 3 months, to cook dinner, do the dishes, and x automobiles.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"An AI that can acquire the skills to invent new things, solve global warming, and negotiate peace treaties would be yet another step forward, but for all we know, this might not be too far-fetched if we can achieve the former. A functioning brain is a reasonably good place to start studying how thought works  after all natural intelligence is what gave us the idea to build articial minds in the rst place. Consider functions of natural minds such as global attention with introspective capabilities, the ability to discover, understand and abstract facts and causal chains, to make analogies and inferences, and to learn a large amount of vastly different skills, facts and tasks, including the control of ones own thoughts. These are features that seem simply too critical to leave out when attempting to build an intelligent system. It is in part the historical neglect of such key features  but especially their integration  that motivates the present discussion.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"The vast collection of atoms and molecules found in everything we see in this universe tells us little about familiar phenomena such as oceans, rainforests and Picasso paintings. In the same way, brain neurons are but one of the many building blocks behind the complex phenomenon we normally recognize as intelligence. Just like the atoms in the trunk of a tree or water molecules on the surface of the Earth, neurons participate in pattens of interaction that form complex structures of lower granularity, all of which are more complex than any      A New Constructivist AI 147 single neuron alone. Without the right superstructures, brain neurons are nothing more than fancy amoebas, and just as far from what we recognize as high-level intelligence. Studying only neurons for nding out how the mind works is akin to restricting oneself to atoms when studying weather patterns.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"Similarly, languages used today for programming computers are too restrictive  instead of helping us build complex animated systems they focus our attention on grains of sand, which are then used to implement bricks  software modules  which, no surprise, turn out to be great for building brick houses, but are too inexible for creating the mobile autonomous robot we were hoping for. In short, modern software techniques are too inexible for helping us realize the kinds of complex dynamic systems necessary to support general intelligence. What is called for are new methodologies that can bring more power to AI development teams, enabling them to study cognition in a much more holistic way than possible today, and focus on architecture  the operation of the system as a whole. To see why this is so we need to look more closely at what it is that makes natural intelligence special. 9.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"2 The Nature of (General) Intelligence Whether or not natural intelligence is at the center of our quest for thinking machines, it certainly gives us a benchmark; even simple animals are capable of an array of skills way beyond the most advanced man-made machines. Just to take an example, a fully grown human brain must contain millions of task-specic abilities. This fact in itself is impressive, but pales in comparison to the fact that these skills have been acquired largely autonomously by the system itself, through self-directed growth, development, and training, and are managed, selected, improved, updated, and replaced dynamically, while the system is in use. An integrated system with these capabilities is able to apply acquired skills in realtime in varied circumstances; it can determine  on the y  when and how to combine them to achieve its goals.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"And in spite of already having a vast amount of acquired skills, it must retain the ability to acquire new ones, some of which may have very little overlap with any prior existing knowledge. The eld of articial intelligence has so far produced numerous partial solutions to what we normally call intelligent behavior. These address isolated sub-topics such as playing board games, using data from a camera to perform a small set of predetermined operations, transcribing spoken words into written ones, and learning a limited set of actions from experience. In systems that learn, the learning targets (goals) are hand-picked by the      148 Theoretical Foundations of Articial General Intelligence programmer; in systems that can to some extent see their environment, the variations in operating contexts and lighting must be highly restricted; in systems that can play board games, the types of games are limited in numerous ways  in short, there are signicant limitations to the systems in each of these areas.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"No system has yet been built that can learn two or more of these domains by itself, given only the top-level goal of simply learning any task that comes your way. These systems are nonetheless labeled as articially intelligent by a majority of the research community. In contrast, what we mean by general intelligence is the ability of a system to learn many skills (e.g. games, reading, singing, playing racketball, building houses, etc.) and to learn to perform these in many different circumstances and environments. To some extent one could say that the ability to learn to learn may be an important characteristic of such a system. Wang (2004) puts this in the following way: If the existing domain-specic AI techniques are seen as tools, each of which is designed to solve a special problem, then to get a general-purpose intelligent system, it is not enough to put these tools into a toolbox. What we need here is a hand.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"To build an integrated system that is self-consistent, it is crucial to build the system around a general and exible core, as the hand that uses the tools [assuming] different forms and shapes. Many abilities of an average human mind, that are still largely missing from present AI systems, may be critical for higher-level intelligence, including: The ability to learn through experience via examples or through instruction; to separate important things from unimportant ones in light of the systems goals and generalize this to a diverse set of situations, goals and tasks; steering attention to important issues, objects and events; the ability to quickly judge how much time to spend on the various subtasks of a task in a complex sequence of actions; the ability to continuously improve attention steering; to make analogies between anything and everything; the ability to learn a wide range of related or unrelated skills without damaging what was learned before; and the ability to use introspection (thinking about ones own thinking) as a way to understand and",Theoretical Foundations of Artificial General Intelligence,chapter 9
"improve ones own behavior and mental processes, and ultimately ones performance and existence in the world. These are but a few examples of many pan-architectural abilities that involve large parts of the entire system, including the process of their development, training, and situated application. All of these abilities are desirable for a generally intelligent articial mind,      A New Constructivist AI 149 and many of them may be necessary. For example, a system working on the docks and helping to tie boats to the pier must be able to ignore rain and glaring sun, the shouts of others doing their own tasks, detect the edge of the pier, catch a glimpse of the rope in the air, prepare its manipulators to catch it, catch it, and tie it to the pier  all within the span of a few seconds. In a real-world scenario this task has so many parameters that the systems realtime attentional capabilities must be quite powerful.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"It is hard to see how a general-purpose intelligence can be implemented without an attention mechanism that can  in realtime  learn to shift focus of attention effectively from internal events (e.g. remembering the name of a colleague) to external events (e.g. apologizing to those present for not remembering her name), and at runtime  while operating  improve this skill based on the goals presented by social norms. Natural intelligences probably include many such attention mechanisms, at different levels of detail. Attention is just one of the seemingly complex transversal skills  pan-architectural skills that represent in some way fundamental operational characteristics of the system  without which it seems rather unlikely that we will ever see articial general intelligence, whether in the lab or on the street. The enormous gap in size and complexity between a single neuron and a functioning animal brain harbors a host of challenging questions; the size and nature of this challenge is completely unclear.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"What is clear, however, is that a fully formed brain is made up of a network of neurons forming substructure upon substructure of causal information connections, which in turn form architectures within architectures within architectures, nested at numerous levels of granularity, each having a complex relationship with the others both within and between layers of granularity [10, 34]. This relationship, in the form of data connections, determines how the mind deals with information captured by sensory organs, how it is manipulated, responded to, and learned from over time. The architectures implement highly coordinated storage systems, comparison mechanisms, reasoning systems, and control systems. For example, our sense of balance informs us how to apply forces to our arm when we reach for a glass of water while standing on a rocking boat. Past experience in similar situations tells us to move slowly.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"Our sense of where the glass is positioned in space informs our movement; the information from our eyes and inner ear combines to form a plan, and produce control signals for the muscles, instructing them how to get the hand moving in the direction of the glass without us falling down or tipping the glass over. A system that can do this in real-time is impressive; a system that can learn to do this, for a large set of variations thereof, along with a host of other tasks, must also have a highly exible architecture.      150 Theoretical Foundations of Articial General Intelligence Elsewhere I have discussed how the computational architecture, software implementation, and the cognitive architecture that it implements, need not be isomorphic [35].",Theoretical Foundations of Artificial General Intelligence,chapter 9
"Thus, the arguments made here do not rest on, and do not need to rest on, assumptions about the particular kind of architecture in the human and animal mind; in the drive for a new methodology we are primarily concerned with the operational characteristics that the system we aim to build  the architecture  must have. Past discussions about cognitive architecture, whether the mind is primarily symbol-based (cf. [20]), dynamical (cf. [9, 41]), massively modular (cf. [2]), or something else entirely, can thus be put aside. Instead we focus on the sheer size of the system and the extreme architectural plasticity, while exhibiting a tight integration calling for a high degree of interdependencies between an enormous set of functions.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"Just as an ecosystem cannot be understood by studying one lake and three inhabiting animal species, intelligence cannot be realized in machines by modeling only a few of its necessary features; by neglecting critical interdependencies of its relatively large number of heterogeneous mechanisms most dissections are prevented from providing more than a small fragment of the big picture. General intelligence is thus a system that implements numerous complex functions organized at multiple levels of organization. This is the kind of system we want to build, and it cannot be achieved with the present methodologies, as will become evident in the next section. To summarize, work towards articial general intelligence (AGI) cannot ignore necessary features of such systems, including:  Tight integration: A general-purpose system must tightly and nely coordinate a host of skills, including their acquisition, transitions between skills at runtime, how to combine two or more skills, and transfer of learning between them over time at many levels of temporal and topical detail.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"Transversal functions: Related to the last point, but a separate issue; lies at the heart of system exibility and generality: The system must have pan-architectural characteristics that enable it to operate consistently as a whole, to be highly adaptive (yet robust) in its own operation across the board, including meta-cognitive abilities. Some such functions have been listed already, namely attention, learning, analogy-making capabilities, and self-inspection, to name some.  Time: Ignoring (general) temporal constraints is not an option if we want AGI. Time is a semantic property, and the system must be able to understand  and be able to learn to understand  time as a real-world phenomenon in relation to its own skills and architectural operation.      A New Constructivist AI 151  Large architecture: An architecture that is considerably larger and more complex than systems being built in AI labs today is likely unavoidable, unless we are targeting toy systems.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"In a complex architecture the issue of concurrency of processes must be addressed, a problem that has not yet been sufciently resolved in present software and hardware. This scaling problem cannot be addressed by the usual well wait for Moores law to catch up [19] because the issue does not primarily revolve around speed of execution, but around the nature of the architectural principles of the system and their runtime operation. This list could be extended even further with various other desirable features found in natural intelligences such as predictable robustness and graceful degradation; the important point to understand here is that if some (or all) of these features must exist in the same system, then it is highly unlikely that we can create a system that addresses them unless we address them at the same time, for (at least) one obvious reason: For any partially operating complex architecture, retrotting one or more of these into it would be a practical  and possibly a theoretical  impossibility (cf. [11]).",Theoretical Foundations of Artificial General Intelligence,chapter 9
"As I myself and others have argued before [43], the conclusion can only be that the fundamental principles of an AGI must be addressed holistically. This is clearly a reason to think long and hard about our methodological approach to the work at hand. 9.3 Constructionist AI: A Critical Look In computer science, architecture refers to the layout of a large software system made up of many interacting parts, often organized as structures within structures, and an operation where the whole is greater than the sum of the parts. Although not perfect, the metaphorical reference to physical structures and urban layout is not too far off; in both cases system designs are the result of compromises that the designers had to make based on a large and often conicting set of constraints, which they resolved through their own insight and ingenuity.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"In both cases trafc and sequence of events is steered by how things are connected; in the case of software architecture, it is the trafc of information  who does what when and who sends and receives what information when.1 There is another parallel between urban planning and software development that is even more important; it has to do with the tools and methodologies used to design and imple1I use the term software architecture here somewhat more inclusively than the metaphorical sense might imply, to cover both the parts responsible for system operation, processing, and data manipulation, as well as the data items and data structures that they operate on.      152 Theoretical Foundations of Articial General Intelligence ment the target subject.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"When using computer simulations as an integral part of a research methodology, as is done increasingly in many elds including astrophysics, cognitive science, and biology, an important determinant of the speed of progress is how such software models are developed and implemented; the methodology used to write the software and the surrounding software framework in which they are developed and run, is in fact a key determinant of progress. Could present methodologies in computer science be used to address the challenges that articial general intelligence (AGI) presents? Since software systems are developed by human coders, the programming languages now used have been designed for human use and readability, and it is no surprise that they reect prototypical ways in which humans understand the world. For instance, the most popular method for organizing programming languages in recent years is based on object orientation, in which data structures and processes are organized into groups intended to give the human designer a better overview of the system being developed.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"Dependencies between the operation and information ow in such groups, that is, what particulars of what groups of processes are allowed to receive, process, and output the various types of data, is decided at design time. This also holds for the groups operational semantics, where each group is essentially a special-purpose processor that has special-purpose behavior, with a pre-dened role in the system as a whole, dened by the software coder at design time. In this constructionist approach variables, commands, and data, play the role of bricks; the software developer takes the role of the construction worker. The eld of articial intelligence too relies on a constructionist approach: Systems are built by hand, and both the gross and ne levels of architectures are laid down brick by brick.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"This is where we nd the most important similarity between architecture and software development, and the one with which we are primarily concerned here; in both cases everything  from the ne points to the overall layout of the nal large-scale processing structures  is dened, decided and placed in the system by human designers. As history unequivocally shows, all AI systems developed to date with constructionist methodologies have had the following drawbacks, when compared to natural intelligence:  They are extremely limited in what they can do, and certainly dont come anywhere close to addressing the issues discussed in the prior section, especially transversal functions such as global attention mechanisms and system-wide learning.  They are brittle, stripped of the ability to operate outside of the limited scope targeted by their designer.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"More often than not they also tend to be brittle when operating within their target domain, especially when operating for prolonged periods, which      A New Constructivist AI 153 reveals their sensitivity to small errors in their implementation and lack of graceful degradation in respect to partial failure of some components. To be sure, AI researchers often extend and augment typical software development methodologies in various ways, going beyond what can be done with standard approaches. The question then becomes, how far could constructionist methodology be taken? Over the last few decades only a handful of methodologies have been proposed for building large, integrated AI systems  Behavior-Oriented Design (BOD; [5]), the Subsumption Architecture [4], Belief, Desires, Intentions (BDI; cf. [28]), and the Constructionist Design Methodology (CDM) [37] are among them.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"CDM rests on solid present-day principles of software engineering, but is specically designed to help developers manage system expansion; its principles allow continuous architecture-preserving expansion of complex systems involving large numbers of executable modules, computers, developers, and research teams.2 The Cognitive Map architecture [22], implemented on the Honda ASIMO robot, enables it to play card games with children using reciprocal speech and gesture. Implementing and coordinating state of the art vision and speech recognition methods, novel spatio-temporal interpretation mechanisms and human-robot interaction, this system is fairly large, and it is among the relatively few that make it a specic goal to push the envelope on scale and breadth of integration. The Cognitive Map architecture has beneted greatly from the application of the CDM to its construction, as system development has involved many developers over several years. The architecture is implemented as multiple semi-independent modules running on multiple CPUs interacting over a network.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"Each module is responsible for various parts of the robots operations, including numerous perceptors for detecting and indexing perceived phenomena and various types of deciders, spatial, and semantic memory systems, action controllers, etc. Each of the modules is a (hand-crafted) piece of software, counting anywhere from a few dozen lines of code to tens of thousands. As most of the modules are at the smaller end of this spectrum, interaction and network trafc during system runtime is substantial.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"The components in the Cognitive Map architecture are coordinated via an infrastructure called Psyclone AIOS, a middleware that targets large AI systems based on multiple dynamically interacting modules, with powerful blackboard-based data sharing 2Results of the application of CDM have been collected for several types of systems, in many contexts, at three different research institutions, CADIA [15, 29, 38], Honda Research Labs (HRI-US) [21, 22] and the Computer Graphics and User Interfaces Lab at Columbia University [37].      154 Theoretical Foundations of Articial General Intelligence mechanisms [39]. Psyclone AIOS allows multiple programming languages to be used together, supports easy distribution of processes over a network, handles data streams that must be routed to various destinations at runtime (e.g. from video cameras), and offers various other features that help with AI architecture development.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"The system supports directly the principles of the CDM and is certainly among the most exible such systems involving the creation and management of large architectures.3 In light of progress within the eld of robotics and AI, the ASIMO Cognitive Map architecture is a strong contender: It integrates a large set of functionality in more complex ways than comparable systems did only a few years ago. It relies on a methodology specifically targeted to AI and robotic systems, where multiple diverse functions implemented via thousands of lines of code must be tightly integrated and coordinated in realtime. So how large is the advancement demonstrated by this system? The resulting system has all of the same crippling aws as the vast majority of such systems built before it: It is brittle, and complex to the degree that it is becoming exponentially more expensive to add features to it  we are already eyeing the limit.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"Worst of all, it has still not addressed  with the exception of a slight increase in breadth  a single one of the key features listed above as necessary for AGI systems. In spite of good intentions, neither the CDM  nor any of the other methodologies, for that matter  could help address the difcult questions of transversal functions and architectural construction which are orders of magnitude larger than attempted to date. Some of the systems I have been involved with developing have contained an unusually large number of modules, with sizes varying from very small (a few dozen lines of code) to very large (tens of thousands). We refer to them as granular architectures (cf. [15]). In these systems the algorithms coordinating the modules (i.e. the gross architecture) dynamically control which components are active at what time, which ones receive input from where, etc.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"Some of the components can change dynamically, such as the Indexers/Deciders in the Cognitive Map [21], and learn, such as the module complex for learning turntaking in the articial radio-show host [15]. Increased granularity certainly helps making the architecture more powerful. But yet again, the modules in these systems are typically black-box with prescribed dependencies, which precludes them from automatically changing their operation, expand their reach, or even modify their input and output proles in any signicant way, beyond what the coder could prescribe. Their inputs and outputs are directly dependent on the surrounding architecture, which is restricted by the 3Other solutions addressing similar needs include Elvin [33], the Open Agent Architecture [18] and NetP [13].      A New Constructivist AI 155 inability of components to change their operation. Many component technologies used in these architectures are built from different theoretical assumptions about their operating context, increasing this dependency problem.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"On the practical side, many problems get worse as the architectures get bigger, e.g. for lack of fault tolerance (such as code-level bugs). Some new problems are introduced, especially architecture-level problems and those we call interaction problems: Loosely-coupled modules often have complex (and infrequent) patterns of interaction that are difcult to understand for the developers in the runtime system; interaction problems grow exponentially as the system gets bigger. A constructionist approach does not help us unlock tight interdependencies between components, or remove the need for humans to oversee and directly interact with the system at the code level. Examples of other AI architectures with ambitions towards articial general intelligence include LIDA [8], AKIRA [25], NARS [45], SOAR [16], CLARION [32], ACTR [1], OSCAR [27], and Ikon Flux [23].",Theoretical Foundations of Artificial General Intelligence,chapter 9
"However, those that have been tested on non-trivial tasks, such as ACT-R, SOAR, and CLARION, are based on constructionist methodologies with clear limitations in scaling arising thereof; those that promise to go beyond current practices, e.g. Ikon Flux, NARS, LIDA, and OSCAR, suffer from having either been applied only to toy problems, or are so new that thorough evaluation is still pending. No matter how dynamic and granular the components of an architecture are made, or which expanded version of a constructionist methodology is being applied, a heavy reliance on manual construction has the following effects:  System components that are fairly static. Manual construction limits the complexity that can be built into each component.  The sheer number of components that can form a single architecture is limited by what a designer or team can handle.  The components and their interconnections in the architecture are managed by algorithms that are hand-crafted themselves, and thus also of limited exibility.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"Together these three problems remove hopes of autonomous architectural adaptation and system growth. Without system-wide adaptation, the systems cannot break free of targeted learning. Like most if not all other engineered systems of a comparable scale and level of integration, e.g. telephone networks, CPUs, and power grids, these systems are incapable of architecture-level evolution, precluding architecture-wide learning (what one might metaphorically think of as cognitive growth) and deep automatic adaptation, all of which precludes general-purpose systems capable of applying themselves autonomously to      156 Theoretical Foundations of Articial General Intelligence arbitrary problems and environments. That is precisely the kind of exibility we want a new methodology to enable us to imbue an AI architecture with.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"The solution  and most fruitful way for AI in the coming decades  rests on the development of a new style of programming, with greater attention given to the architectural makeup, structure, and nature of large complex systems, bringing with it the element of automated systems management, resting on the principles of transparent operational semantics. 9.4 The Call for a New Methodology Available evidence strongly indicates that the power of general intelligence, arising from a high degree of architectural plasticity, is of a complexity well beyond the maximum reach of traditional software methodologies. At least three shortcomings of constructionist AI need to be addressed in a new methodology: Scale, integration, and exibility. These are fundamental shortcomings of all software systems developed to date, yet, as we have seen above, they must all be overcome at the same time in the same system, if we wish to achieve articial general intelligence. Any approach that is successful in addressing these must therefore represent a fundamentally new type of methodology.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"Scale matters in at least two ways. First, we have reason to believe that a fairly large set of cognitive functions is necessary for even modestly complex real-world environments. A small system, one that was of a size that could be implemented by a handful of engineers in half a decade, is not likely to support the reliable running of supernumerary functions. And historical evidence certainly does not help refute this claim: In spite of decades of dealing with the various problems of scaling beyond toy contexts (context interpreted in a rather general form, as in the ocean versus the desert; indoors versus outdoors, etc.), standard component-based software methodology has theoretical limitations in the size of systems that it can allow to be built; as these systems are programmed by humans, their size and complexity is in fact restricted by the industriousness of a dedicated team of researchers in the same way that building a house is. This is the reason why we still have not seen systems that scale easily.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"What is needed is the equivalent of a highly automated factory. Second, a small system is not very likely to lend sufcient support to the kind of functions that characterize higher-level intelligences, such as system-wide analogy-making, abstraction, cross-domain knowledge and knowledge transfer, dynamic      A New Constructivist AI 157 and learnable attention, all of which require transversal functionality of some sort to be of general use.4 The issue of integration ultimately revolves around software architecture. Most architectures built to date are coarse-grained, built of relatively large modules, because this is the kind of architecture that traditional methodologies most naturally support. The size of components in constructionist systems built to date varies from a few to dozens, depending on which system you look at.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"To take some concrete examples, in our own single-mind (as opposed to multi-agent) systems we have had close to 100 modules, most of which are only a few pages of C++ code each, but often include two or three signicantly larger ones (thousands of lines or more) in the full system (see e.g. [38]). Each of these may have somewhere from 0 to, at most, 20 parameters that can be changed or tuned at runtime. Such a system simply does not permit the highly dynamic communication and behavior patterns required for these sophisticated functionalities. More importantly, the architecture is too inexible for sub-functions to be shared between modules at the gross-architecture level.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"In such an arrangement tight integration is precluded: For AGIs we need a tighter, deeper integration of cognitive functions; we need a methodology that allows us to design architectures composed of tens of thousands of components with ease  where the smallest component is peewee-size (the size of a medium-size C++ operation [40]) and where the combination of these into larger programs, and their management, is largely automatic. We are looking for more than a linear increase in the power of our systems to operate reliably, and in a variety of (unforeseen) circumstances; experience strongly suggests that a linear increase in present methods will not bring this about: Nothing in traditional methods shows even a hint of how more exible systems could be built using that approach. One of the biggest challenges in AI today is to move away from brittle, limited-domain systems.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"Hand-crafted software systems tend to break very easily, for example when taking inputs outside the scope of those anticipated by their designers or because of unexpected interaction effects amongst the systems components. What seems clear is that a new methodology must inevitably revolve around what could be thought of as meta-programs; programs that guide the creation of new programs that guide the systems interaction patterns with the world, and possibly test these in simulation mode beforehand, as a mental exercise, to predict how well they might work. The systems need to have a reliable way to judge whether prior knowledge exists to solve the problem, and whether the problem or situation 4Although system-wide learning and self-modication could be realized in isolation by a small system, these features are likely to be impossible to maintain in a large system when taking a constructionist approach, and we need the system to incorporate all of these features in a unied manner.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"     158 Theoretical Foundations of Articial General Intelligence falls within what the system has already encountered or whether it nds itself in completely new circumstances. So, we need a methodology for designing a system whose output, cognitive work, is a set of programs that are more or less new compared to what existed in the system before; programs that can be given a chance to guide the system in its interactions with new operating contexts (domains), and ways to assess the results of such hypothesis testing: The programs must be compared and evaluated on the grounds of the results they achieve. It may even be necessary for the evaluation itself to be learnable, for, after all, the system should be as self-organizing as possible. For a large system, doing all of this with present reinforcement learning and genetic algorithm techniques is likely to be too limited, too slow, or, most probably, impossible; the system must therefore be endowed with analogy making, reasoning, and inference capabilities to support such skills.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"If we can create systems that can adapt their operating characteristics from one context to another, and propose what would have to be fairly new techniques with a better chance of enabling the systems operation in the new envrionment, then we have created a system that can change its own architecture. And that is exactly what I believe we need: For any real-world domain (e.g. an indoor ofce environment) that is sufciently different from another real-world domain (e.g. a busy street), creating a system that can not only operate but learn to operate in both, as well as in new domains, must be a system that can change its own operation in fundamentally new ways  these systems would have self-organizing architectures that largely manage their own growth. 9.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"5 Towards a New Constructivist AI A system that can learn to change its own architecture in sensible ways is a constructivist AI system, as it is to some signicant extent self-constructive [36]. The name and inspiration comes partly from Piaget [26], who argued that during their youth humans develop cognitive faculties via a self-directed constructive process, emphasizing the active role that a learning mind itself has in any learning process. Piagets ideas later became the foundation for the work by Drescher [7], who brought this idea to articial intelligence, arguing that AI should study the way minds grow. The present work shares Dreschers aim for more powerful ways of learning, aligning with the general hypothesis behind his work that an (articial) mind requires sophisticated abilities to build representations and grow its knowledge about the world based on its own direct experiences.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"But in the present work we take this idea a step further by arguing for a fundamental change in methodolog     A New Constructivist AI 159 ical assumptions, emphasizing the need for new principles of automatic management of whole AI architectures  i.e. the mind itself: It is not only the systems learning but the control structures themselves that must be part of such cognitive development and constant (auto-)construction. The methodologies employed for such AI development  constructivist AI  are likely to be qualitatively different from todays software development methods. Here a drive for constructivist AI  that is, a system that can modify its own internal structures to improve its own operational characteristics, based on experience  arises thus from two fundamental assumptions, namely that (a) constructionist methodologies (i.e.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"by and large all traditional software development techniques) are not sufcient  both in principle and in practice  to realize systems with articial general intelligence; (b) the hypothesis that automation of not just parameter tuning or control of (coarse-grained) module operation but of architectural construction (as understood in computer science) and management is what is needed to address this shortcoming. This is in line with the ideas presented by the proponents of second-order cybernetics and cybernetic epistemology [42] [12], which studies the nature of self-regulation.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"In our approach we are by and large ruling out all methodologies that require some form of hand-coding of domain-level operational functionality (what Wang metaphorically referred to as tools [43]), as well as any and all approaches that require extensive handcoding of the nal static architecture for an articial general intelligence (metaphorically referred to as hand by [43], limiting initial (manual) construction to a form of seed  a kind of meta-program  which automates the management of all levels below. This is what we call constructivist AI. Pure constructivist systems do not exist yet in practice, but some theoretical work already shows promise in this direction. The following are topics that I consider likely to play a critical role in the impending paradigm shift towards constructivist AI. The topics are: Temporal grounding, feedback loops, pan-architectural pattern matching, small white-box components, and architecture meta-programming and integration.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"Other key factors probably exist that are not included here, so this list should be considered a necessary but insufcient set of topics to focus on in the coming decades as we turn our sights to building larger, more self-organizing systems. 9.5.1 Temporal Grounding As now seems widely accepted in the AI community, it is fairly useless to talk of an entity being intelligent without referencing the context in which the entity operates. In     160 Theoretical Foundations of Articial General Intelligence telligence must be judged by its behavioral effects on the world in particular circumstances which are not part of the entitys operation: We cannot rightfully show an entity to be smart unless we consider both what the entity does and what kinds of challenges the environment presents. This means that intelligent behavior requires grounding  a meaningful hook-up between an intelligent systems thoughts and the world in which it operates.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"This grounding must include a connection to both space and time: Ignoring either would cripple the entitys possibility of acting intelligently in its world, whether real or virtual. So, for one, the entity must have a means to be situated in this world  it must have a body, a (limited) collection of sensors to accept raw data through and some (limited) set of actuators  to affect its surroundings. By the same token, its body must be connected to the entitys internal thought/computational processes to transfer the results of its thinking to the body.5 The issue goes beyond situatedness, which is a necessary but not sufcient condition for grounding: via situated perception and action, a feedback loop is created allowing the system to adapt to its environment and to produce models of the world that enable it to plan in that world, using predicted results of sequences of actions (plans).",Theoretical Foundations of Artificial General Intelligence,chapter 9
"Results that do not match predictions become grounds for revision of its models of the world, and thus enable it to learn to exist in the given environment (cf. [44]). Therefore, to be grounded, an intelligent entity must be able to compute using processes that have a causal, predictable relationship with the external reality. This leads us to a discussion of time. As any student of computer science knows, computation can be discussed, scrutinized and reasoned about without regard for how long it actually takes in a particular implemented system. It may be argued that this simplication has to some extent helped advance the elds of computer science and engineering. However, lack of a stronger foundation for the semantics of the actual, realtime execution of computational operations has hampered progress in elds dealing with highly time-critical topics, such as embedded systems, user interfaces, distributed networks, and articial intelligence systems. As others have pointed out, timeliness is a semantic property [17] and must be treated as such.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"To be grounded, the computational operations of an intelligent entity must have a causal, temporally contextualized and predictable  and thus temporally meaningful  relationship with the external reality. To make an intelligent machine that does not understand time is a strange undertaking. No example of natural intelligence exists where time isnt integral in its operation: 5Froese (2007) gives a good overview of past research on these topics.      A New Constructivist AI 161 When it comes to doing intelligent things in the world, time is of the essence. Indeed, in a world without the arrow of time there would be little need for the kind of intelligence we see in nature. The lack of a strong connection between computational operations and the temporal dimension is preventing a necessary theoretical and practical understanding of the construction of large architectural solutions that operate predictably under external time constraints. We need to nd ways to build an operational knowledge of external time into AI architectures.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"To use its own mental powers wisely an intelligent machine must not only be able to understand the march of the real-world clock itself, it should preferably understand its own capabilities and limitations with regards to time, lest it cannot properly make plans to guide its own learning or evolution. One method is to link the execution of the software tightly with the CPUs operation and inputs from the systems operating environment to create a clear semantic relationship between logical operations and the passing of realtime (running of the CPU), in a way that allows the system to do the inferencing and modeling of this relation itself and use this in its own operation throughout the abstraction layers in the entire architecture, producing a temporally grounded system. This is not mere conjecture; in the Ikon Flux system [23] lambda terms were used to implement this idea in a system containing hundreds of thousands of such terms, showing that this is indeed possible on large architectural scales, even on present hardware.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"And as mentioned above, the full perception-action loop needs to be included in these operational semantics. There are thus at least three key aspects of temporal representation. The rst is the perception of external time. The system exists in some world; this world has a clock; any system that cannot reasonably and accurately sense time at a resolution relevant to its operation will not be able to take actions with regards to events that march along to this clock, and thus  by denition  is not intelligent. Second, an intelligent system must have a representation of mental time and be able to estimate how long its own mental operations take. Third, an AI architecture must understand how the rst two aspects relate, so that mental actions can be planned for based on externally or internally-imposed timelines and deadlines. The challenge is how to implement this in distributed, ne-grained architectures with parallel execution of subcomponents. 9.5.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"2 Feedback Loops Any generally intelligent system operating in the real world, or a world of equivalent complexity, will have vastly greater information available than the mind of such a system      162 Theoretical Foundations of Articial General Intelligence will have time to process. Thus, only a tiny fraction of the available data in the world is being processed by the system at any point in time. The actual data selected to be thought about must be selected based on the systems goals  of which there will be many, for any system of reasonable complexity. To be able to respond to unexpected events the system must further divide its processing capability between thinking about long-term things (those that have not happened yet and are either wanted, or to be avoided), and those that require immediate processing. Any mechanism that manages how the system chooses this division is generally called attention, or considered a part of an attentional mechanism.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"Attention is intricately linked with the perception-action loop, which deals with how the system monitors for changes in the world, and how quickly it is able to respond to these. For any intelligent system in a reasonably complex environment the nature and operation of these mechanisms are of utmost importance, as they are fundamental to the cognitive makeup of the system and put limits on all its other cognitive abilities. Focus on the perception-action loop in current AI curricula is minimal. A quick look at some of the more popular textbooks on the subject reveals hardly any mention of the subject. Given that this most important loop in intelligent systems is largely ignored in the mainstream AI literature, it is no surprise that the little discussion there is dwells on their trivial aspects. Yet the only means for intelligent systems to achieve stability far from (thermodynamic) equilibrium is through feedback loops. The growth of a system, and its adaptation to genuinely new contexts, must rely on feedback loops to stabilise the system and protect it from collapsing.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"Furthermore, any expansion or modication of existing skills or capabilities, whether it is to support more complex inferencing, making skills more general-purpose or improving the performance on a particular task, requires an evaluation feedback loop. For general-purpose intelligence such loops need to permeate the intelligence architecture. The way any entity achieves grounding is through feedback loops: repeated interactions which serve as experiments on the context in which the entity nds itself, and the abstractions which it has built of that context, of its own actions, and of its tasks. Feedback loops related to the complexities of tasks are key to a systems ability to learn particular tasks, and about the world. But the process must involve not only experience (feedback) of its actions on the context outside itself, it must also involve the context of its internal processes. So self-modeling is a necessary part of any intelligent being.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"The feedback loop between the systems thinking and its evaluation of its own cognitive actions is therefore just as important as that to the external world, because this determines the systems      A New Constructivist AI 163 ability to learn to learn. This, it can be argued, is a key aspect of general intelligence. Together these extremely important feedback loops provide a foundation for any increases in a systems intelligence. Self-organization requires feedback loops, and constructionist AI methodologies make no contributions in this respect. The science of self-organization is a young discipline that has made relatively slow progress (cf. [30, 46]). As a result, concrete results are hard to come by (cf. [14]). Perhaps one of the important contributions that this eld has to offer at present is to show how the principles behind self-organization call for a way of thinking that is very different from that employed in traditional software development methodologies. 9.5.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"3 Pan-Architectural Pattern Matching Complex, tightly-integrated intelligence architectures will not work without large-scale pattern matching, that is, pattern matching that involves large portions of the system itself, both in knowledge of domain (e.g. tasks, contexts, objects, etc.) and of architecture (e.g. perception and action structures, hypothesis generation methods, etc.). Such patternmatching functionality plays many roles; I will mention a few. Any creature living in a complex world must be able to classify and remember the salient features of a large number of contexts.6 Without knowing which features to remember, as is bound to happen regularly as various new contexts are encountered, it must store potential features  a much larger set than the (ultimately) relevant features  and subsequently hone these as it experiences an increasingly larger numbers of contexts over time. In a complex environment like the real-world, the number of potential states or taskrelevant contexts a being may nd itself in is virtually innite.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"Yet the beings processing power, unlike the number of contexts it may nd itself in, is nite. So, as already mentioned, it must have some sort of attention.7 At any point in time the attentional mechanism selects memories, mental processes, memories of having applied/used a mental process for a particular purpose, or all of the above, to determine which mental process to apply in the present, identify potential for improvement, or simply for the purpose of reminiscing about the past. The pan-architectural nature of such mechanisms crystallizes in the rather large amounts of recall required for prior patterns involving not only features of objects to 6One way to think of contextual change, and hence the differentiators between one context and another, is as the smallest amount of change in a particular environmental conguration that renders a priorly successful behavior for achieving a particular goal in that conguration unable to achieve that goal after the change.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"7Here attention refers to a broader set of actions than our typical introspective notion of attention, including the global control of parts of the mind that are active at any point in time, as well as what each one is doing.      164 Theoretical Foundations of Articial General Intelligence be recognized, or the contexts in which these objects (including the creature itself) may be at any point, but also involving the way in which the being controls its attention in these contexts with regards to its task (something which it must also be able to learn), the various analogies it has made for the purpose of choosing a course of action, and the mechanisms that made these analogies possible. To do all this in realtime in one and the same system is obviously a challenge. This will, however, be impossible without the ability to compare key portions of the systems knowledge and control structures through large-scale pattern matching.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"Yet another example of a process for which such transversal pattern matching is important is the growth of the system as it gets smarter with experience. To grow in a particular way, according to some specication,8 the architecture must have built-in ways to compare its own status between days, months, and years, and verify that this growth is according to the specication. This might involve pattern matching of large parts of the realtime mind, that is, the part of the mind that controls the creature from moment to moment at different points in time. For a large, heterogeneous architecture such architecture-scale pattern matching can get quite complicated. But it is unlikely that we will ever build highly intelligent articial systems without it. 9.5.4 Transparent Operational Semantics As already discussed, most integration in AI systems has involved relatively small numbers of the functions found in natural minds.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"A close look at these components  whether they are computer vision, speech recognition, navigation capabilities, planning, or other such specialized mechanisms  reveals internals with an intricate structure based on programming languages with syntax targeted for human programmers, involving mixtures of commercial and home-brewed algorithms. The syntax and semantics of black-box internals is difcult or impossible to discover from the outside, by observing only their inputs, outputs, and behaviors. This is a critical issue in self-organizing systems: The more opaque complex mechanisms encompassed by any single component in an architecture are, the harder it is to understand its operational semantics. In other words, the greater the complexity of atomic components, the greater the intelligence required to understand them. For this reason, to make architectures that construct themselves, we must move away from large black-box components.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"8Such a specication could be small or medium-sized, compared to the resulting system, and it could be evolved, as our DNA has been, or provided via new meta-programming methods.      A New Constructivist AI 165 But the grand goal of self-construction cuts even deeper than dictating the size of our building blocks: It calls for them to have a somewhat different nature. Without exception, present programming languages used in AI are designed for human interpretation. By requiring human-level intelligence to be understood, these programming languages have little chance of being interpreted by other software programs automatically; their operational semantics are well above a semantic threshold of complexity that can be understood by automatic methods presently available. Creating systems that can inspect their own code, for the purpose of improving their own operation, thus requires that we rst solve the problem we started out to solve, namely, the creation of an articial general intelligence.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"We need to move away from programming languages with complex syntax and semantics (all programming languages intended for humans), towards transparent programming languages with simple syntax and simple operational semantics. Such programming languages must have fewer basic atomic operations, and their combinatorics would be based on simpler principles than current programming languages. A foundational mechanism of such a programming language is likely to be small and large-scale pattern matching: Systems built with it would likely be fairly uniform in their semantics, from the small scale to the gross architecture level, as then the same small number of pattern matching operations could be used throughout the system  regardless of the level of granularity  to detect, compare, add, delete, and improve any function implemented at any level of detail, from code snippets to large architectural constructs. Small white-box (transparent) components, executed asynchronously, where each component implements one of only a few primitive functions, could help streamline the assembly of architectural components.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"As long as each component is based on a few fundamental principles, it can easily be inspected; assemblies of these will thus also be easily inspectable, and in turn enable the detection (identication, analysis, and modication) of functional patterns realized by even large parts of an architecture. This is a prerequisite for implementing automatic evaluation and learning operational semantics, which lies at the heart of constructivist AI. Incidentally, this is also what is called for to enable transversal functions implementing the kinds of introspection, system-wide learning and dynamic attention which I have already argued as being necessary for articial general intelligence. How small need the components be? Elsewhere we have pointed out the need to move towards what we call peewee-size granularity [40]  systems composed of hundreds of      166 Theoretical Foundations of Articial General Intelligence thousands of small modules, possibly millions.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"9 To see why the size of the smallest modiable entities must be small, we need only look at a hypothetical cognitive operation involving hierarchical summation of excitation signals at several levels of detail: at the lowest as well as the highest levels the system  e.g. for improving its operation  may need to change addition to subtraction in particular places. In this case the operation being modied is addition. In a large system we are likely to nd a vast number of such cases where, during its growth, a system needs to modify its operation at such low levels of functional detail. If each peewee-size module is no larger than a lambda term or small function written in e.g. C++ we have reached the code level, as normally meant by that term  this should thus be of a sufciently low-level of granularity for building highly exible systems: self-constructive on temporal and complexity scales that we consider useful, yet running on hardware that is already available.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"I am aware of only one architecture that has actually implemented such an approach, the Loki system, which was built using the Ikon Flux framework [23], a system based on Lambda terms. The system implemented a live virtual performer in the play Roma Amor which ran for a number of months at Cite des Sciences et de LIndustrie in Paris in 2005, proving beyond question that this approach is tractable. Whether the extremely small size of peewee granularity is required for self-construction or whether larger components can be used is an important question that we are unable to answer at the moment. But whatever their size, the components  architectural building blocks  must be expressible using simple syntax, as rich syntax begets rich semantics, and rich semantics call for smarter self-inspection mechanisms whose required smarts eventually rise above a threshold of complexity beyond which self-construction and self-organization can be bootstrapped, capsizing the whole attempt.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"The ner the granularity and the simpler the syntax the more likely it is to succeed in this regard; however, there may also be a lower bound, i.e. building blocks should not be too simple; if the operational semantics is kept at a level that is small enough but not smaller than what can support self-inspection, there is reason to believe a window opens up within which both practical and powerful components can be realized. 9These numbers can be thought of as a rough guide  the actual number for any such architecture will of course depend on a host of things that are hard to currently foresee, including processor speed, cost of memory transactions, architectural distributedness, and more.      A New Constructivist AI 167 9.5.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"5 Integration and Architecture Metaconstruction Architectural meta-programming is needed to handle larger and more complex systems [3, 31], scaling up to systems with architectural designs that are more complex than even the most complex systems yet engineered, such as microprocessors, the Terrestrial telephone network, or the largest known natural neural networks [24]. A cognitive architecture supporting many of the features seen in natural intelligence will be highly coordinated and highly integrated  more so than probably any man-made dynamic system today. All of the issues already discussed in this section are relevant to achieving archiectural metaprogramming and integration: General principles for learning a variety of contexts can equally well be applied to the architecture itself, which then becomes yet another context. Constructivist AI will certainly be easier if we nd a cognitive principle as hypothesized by [6], where the same small set of basic principles can be used throughout to construct every function of a cognitive system.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"Perhaps fractal architectures  exhibiting selfsimilarity at multiple levels of granularity  based on simple operational semantics is just that principle. But it is fairly unlikely that a single principle alone will open up the doors to articial general intelligence  I nd it more likely to be based around something like the numerous electro-spatio-temporal principles, rooted in physics and chemistry, that make a car engine run than, say, the principle of ight. Either way, there is no getting around focusing on methods that deal more efciently with large, distributed, semi-autonomously evolving architectures with heterogeneous functionality and a high degree of exibility at coarse-grain levels. New meta-programming languages, constructivist design methodologies, and powerful visualization systems must be developed for signicant progress to be made.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"Transversal functions, as described above, are what ultimately forces us to look at the whole architecture when thinking about our new methodology: Without taking the operation of the whole into account, pan-architectural features are precluded. The transition to architectures built via a constructivist methodology will be challenging. 9.6 Conclusions The hope for generally intelligent machines has not disappeared, yet functions critical to generally intelligent systems continue to be largely ignored, examples being the ability to learn to operate in new environments, introspection, and pan-architectural attention. Systems whose gross architecture is mostly designed from the top-down and programmed by hand, constructionist AI, has been the norm since the elds inception, more      168 Theoretical Foundations of Articial General Intelligence than 50 years ago.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"Few methodologies have been proposed specically for AI and researchers have relied on standard software methodologies (with small additions and modications), one of the more popular ones in recent times being object-oriented programming and component-based architectures. As far as large AI systems go these methodologies rely on fairly primitive tools for integration and have generally resulted in brittle systems with little or no adaptation ability and targeted domain application. Based on the weaknesses inherent in current practices, the conclusion argued for here is that the limitations of present AI software systems cannot be addressed through incremental improvement of current practices, even assuming continued exponential growth of computing power, because the approach has fundamental theoretical and practical limitations: AI systems built to date show that while some integration is possible using current software development methods and extensions thereof (cf. [37]), the kind of deep integration needed for developing general articial intelligence is unlikely to be attained this way.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"Standard software development methods do not scale; they assume that their results can be linearly combined, but this is unlikely to produce systemic features that seem key in general intelligence; too many difcult problems, including a freely roving attentional mechanism, equally capable of real-world inspection and introspection, system-wide learning, cognitive growth and improvement, to take some key examples, would be left by the wayside. To create generally intelligent systems we will need to build signicantly larger and more complex systems than can be built with present methods. To address the limitations of present methodologies a paradigm shift is needed  a shift towards constructivist AI, comprised of new methodologies that emphasize auto-generated code and self-organization. Constructivist AI calls for a very different approach than offered by traditional software methodologies, shifting the focus from manual brick-laying to creating the equivalent of self-constructing factories.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"Architectures would be formed through interactions between exible autonomous auto-construction principles, where a complex environment and initial seed code would interact to automatically create the kinds of architectures needed for general-purpose intelligence. In this paper I have outlined some of the key topics that need to be advanced in order for this paradigm shift to happen, including a stronger emphasis on feedback loops, temporal grounding, architecture metaprogramming and integration, pan-architectural pattern matching and transparent operational semantics with small white-box components. The list, while non-exhaustive, clearly illustrates the relatively large shift in focus that needs to happen, as most of these topics are not well understood today. To increase our      Bibliography 169 chances of progress towards articial general intelligence, future work in AI should focus on constructivist-based tools including new development environments, programming languages, and architectural metaconstruction principles. Acknowledgments This is an updated version of my BICA 2009 keynote paper [36].",Theoretical Foundations of Artificial General Intelligence,chapter 9
"I would like to thank Eric Nivel for brilliant insights and numerous discussions on these topics, as well as excellent suggestions for improving the paper. Big thanks to Pei Wang, Hannes H. Vilhjalmsson, Deon Garrett, Kevin McGee, Hrafn Th. Thorisson and Gudny R. Jonsdottir for valuable comments and suggestions for improvement, and to the anonymous reviewers for helpful comments. Thanks to the HUMANOBS team for helping lay the groundwork for an exciting future. This work was supported in part by the EU-funded project HUMANOBS: Humanoids That Learn Socio-Communicative Skills Through Observation, contract no. FP7STREP-231453 (www.humanobs.org), and by a Strategic Research Programme Centres of Excellence and Research Clusters 2009-2016 project grant (IIIM; www.iiim.",Theoretical Foundations of Artificial General Intelligence,chapter 9
") awarded by the Science and Technology Policy Board of Iceland and managed by the Icelandic Center for Research (Ranns; www.rannis.is). Bibliography [1] Anderson, J. R. (1996). Act: A simple theory of complex cognition, American Psychologist 51, pp. 355365. [2] Barrett, H. C. and Kurzban, R. (2006). Modularity in cognition: Framing the debate, Psychological Revivew 113(3), pp. 628647. [3] Baum, E. B. (2009). Project to build programs that understand, in Proceedings of the Second Conference on Articial General Intelligence, pp. 16. [4] Brooks, R. A. (1986). Robust layered control system for a mobile robot, IEEE Journal of Robotics and Automation 2(1), p. 1423. [5] Bryson, J. (2003).",Theoretical Foundations of Artificial General Intelligence,chapter 9
"The behavior-oriented design of modular agent intelligence, Lecture notes in computer science 2592, pp. 6176. [6] Cassimatis, N. (2006). A cognitive substrate for achieving human-level intelligence, A.I. Magazine 27(2), pp. 4556. [7] Drescher, G. L. (1991). Made-up minds: a constructivist approach to articial intelligence (M.I.T. Press, Boston, Massachusetts). [8] Franklin, S. (2011). Global workspace theory, shanahan, and LIDA, International Journal of Machine Consciousness 3(2). [9] Froese, T. (2007). On the role of AI in the ongoing paradigm shift within the cognitive sciences, 50 Years of Articial Intelligence Lecture Notes in Computer Science 4850, pp. 6375. [10] Garel, S. and Rubenstein, J. L. R.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"(2004). Patterning of the cerebral cortex, in M. S. Gazzaniga (ed.), The Cognitive Neurosciences III, pp. 6984.      170 Theoretical Foundations of Articial General Intelligence [11] Garlan, D., Allen, R. and Ockerbloom, J. (1995). Architectural mismatch or why its hard to build systems out of existing parts, in Proceedings of the Seventeenth International Conference on Software Engineering, pp. 179185. [12] Heylighen, F. and Joslyn, C. (2001). Cybernetics and second order cybernetics, in R. A. Mayers (ed.), Encyclopedia of Physical Science and Technology, Vol. 4 (Academic Press), pp. 155170. [13] Hsiao, K., Gorniak, P. and Roy, D. (2005).",Theoretical Foundations of Artificial General Intelligence,chapter 9
"Netp: A network API for building heterogeneous modular intelligent systems, in Proceedings of AAAI 2005 Workshop on modular construction of human-like intelligence. AAAI Technical Report WS-0508, pp. 2431. [14] Iizuka, H. and Paolo, E. A. D. (2007). Toward spinozist robotics: Exploring the minimal dynamics of behavioural preference, Adaptive Behavior 15(4), pp. 359376. [15] Jonsdottir, G. R., Thrisson, K. R. and Eric, N. (2008). Learning smooth, human-like turntaking in realtime dialogue, in IVA 08: Proceedings of the 8th international conference on Intelligent Virtual Agents (Springer-Verlag, Berlin, Heidelberg), ISBN 978-3-540-85482-1, pp. 162175, http://dx.doi.org/10.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"1007/978-3-540-85483-8_17. [16] Laird, J. (2008). Extending the soar cognitive architecture, in Proceedings of the 2008 conference on Articial General Intelligence, pp. 224235. [17] Lee, E. E. (2009). Computing needs time, Communications of the ACM 52(5), pp. 7079. [18] Martin, D., Cheyer, A. and Moran, D. (1999). The open agent architecture: A framework for building distributed software systems, Applied Articial Intelligence 13(1-2), pp. 91128. [19] Moore, G. E. (1965). Cramming more components onto integrated circuits, Electronics Review 31(8). [20] Newell, A. and Simon, H. A. (1976).",Theoretical Foundations of Artificial General Intelligence,chapter 9
"Computer science as empirical enquiry: Symbols and search, Communications of the Association for Computing Machinery 19(3), p. 113126. [21] Ng-Thow-Hing, V., List, T., Thrisson, K. R., Lim, J. and Wormer, J. (2007). Design and evaluation of communication middleware in a distributed humanoid robot architecture, in IROS 07 Workshop: Measures and Procedures for the Evaluation of Robot Architectures and Middleware. [22] Ng-Thow-Hing, V., Thrisson, K. R., Sarvadevabhatla, R. K., Wormer, J. and List, T. (2009). Cognitive map architecture: Faciliation of human-robot interaction in humanoid robots, IEEE Robotics & Automation 16(1), pp. 5566. [23] Nivel, E. (2007). Ikon ux 2.0, Tech. rep.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"Reykjavik University Department of Computer Science, technical Report RUTR-CS07006. [24] Oshio, K., Morita, S., Osana, Y. and Oka, K. (1998). C. elegans synaptic connectivity data, Technical Report of CCeP, Keio Future, No. 1, Keyo University . [25] Pezzulo, G. and Calvi, G. (2007). Designing modular architectures in the framework akira, Multiagent and Grid Systems , pp. 6586. [26] Piaget, J. (1950). The Psychology of Intelligence (Routledge and Kegan Paul, London, England). [27] Pollock, J. L. (2008). Oscar: An architecture for generally intelligent agents, Frontiers in Articial Intelligence and Applications 171, pp. 275286. [28] Rao, A. S.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"and Georgeff, M. P. (1991). Modeling rational agents within a BDI-architecture, in J. Allen, R. Fikes and E. Sandewall (eds.), Proceedings of the 2nd International Conference on Principles of Knowledge Representation and Reasoning (Morgan Kaufmann publishers Inc.: San Mateo, CA, USA), pp. 473484. [29] Saemundsson, R. J., Thrisson, K. R., Jonsdottir, G. R., Arinbjarnar, M., Finnsson, H., Gudnason, H., Hafsteinsson, V., Hannesson, G., Isleifsdottir, J., Jhannsson, ., Kristjansson, G. and Sigmundarson, S. (2006). Modular simulation of knowledge development in industry: A multi-level framework, in WEHIA Proc. of the First Intl. Conf.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"on Economic Science with Heterogeneous Interacting Agents (Bologna, Italy).      Bibliography 171 [30] Salthe, S. N. and Matsuno, K. (1995). Self-organization in hierarchical systems, Journal of Social and Evolurionary Systems 18(4), pp. 3273. [31] Sanz, R., Lpez, I., Rodrguez, M. and Hernandz, C. (2007). Principles for consciousness in integrated cognitive control, in Neural Networks, Vol. 20, pp. 938946. [32] Sun, R., Merrill, E. and Peterson, T. (2001). From implicit skills to explicit knowledge: A bottom-up model of skill learning, Cognitive Science 25, pp. 203244. [33] Sutton, P., Arkins, R. and Segall, B. (2001).",Theoretical Foundations of Artificial General Intelligence,chapter 9
"Supporting disconnectedness transparent information delivery for mobile and invisible computing, in CCGrid 2001 IEEE International Symposium on Cluster Computing and the Grid, pp. 277285. [34] Swanson, L. W. (2001). Interactive brain maps and atlases, in M. A. Arbib and J. S. Grethe (eds.), Computing the Brain (Academic Press), pp. 167177. [35] Thrisson, K. R. (2008). Modeling multimodal communication as a complex system. in I. Wachsmuth and G. Knoblich (eds.), ZiF Workshop, Lecture Notes in Computer Science, Vol. 4930 (Springer), ISBN 978-3-540-79036-5, pp. 143168. [36] Thrisson, K. R. (2009). From constructionist to constructivist A.I. in A. Samsonovich (ed.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"Keynote, AAAI Fall Symposium Series: Biologically Inspired Cognitive Architectures; AAAI Tech Report FS-09-01 (AAAI press), pp. 175183. [37] Thrisson, K. R., Benko, H., Arnold, A., Abramov, D., Maskey, S. and Vaseekaran, A. (2004). Constructionist design methodology for interactive intelligences, A.I. Magazine 25(4), pp. 77 90. [38] Thrisson, K. R. and Jonsdottir, G. R. (2008). A granular architecture for dynamic realtime dialogue, in Intelligent Virtual Agents, IVA08, pp. 13. [39] Thrisson, K. R., List, T., Pennock, C. and DiPirro, J. (2005). Whiteboards: Scheduling blackboards for semantic routing of messages & streams.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"in AAAI-05, AAAI Technical Report WS05-08, pp. 815. [40] Thrisson, K. R. and Nivel, E. (2009). Achieving articial general intelligence through peewee granularity, in Proceedings of the Second Conference on Articial General Intelligence, pp. 222223. [41] van Gelder, T. J. (1995). What might cognition be, if not computation? Journal of Philosophy 91, pp. 345381. [42] von Foerster, H. (1995). The Cybernetics of Cybernetics (2nd edition), 2nd edn. (FutureSystems Inc.). [43] Wang, P. (2004). Toward a unied articial intelligence, in In Papers from the 2004 AAAI Fall Symposium on Achieving Human-Level Intelligence through Integrated Research and Systems, pp. 8390.",Theoretical Foundations of Artificial General Intelligence,chapter 9
"[44] Wang, P. (2005). Experience-grounded semantics: A theory for intelligent systems, in Cognitive Systems Research (Springer-Verlag), pp. 282302. [45] Wang, P. (2006). Rigid Flexibility: The Logic of Intelligence (Springer). [46] Zitterbart, M. and De Meer, H. (eds.) (2011). International Workshop on self-organizing systems (IWSOS 2011) (Springer, New York, NY, USA).",Theoretical Foundations of Artificial General Intelligence,chapter 9
"  Chapter 10 Towards an Actual Gdel Machine Implementation: A Lesson in Self-Reective Systems Bas R. Steunebrink and Jrgen Schmidhuber IDSIA & University of Lugano, Switzerland {bas,juergen}@idsia.ch Recently, interest has been revived in self-reective systems in the context of Articial General Intelligence (AGI). An AGI system should be intelligent enough to be able to reason about its own program code, and make modications where it sees t, improving on the initial code written by human programmers. A pertinent example is the Gdel Machine, which employs a proof searcherin parallel to its regular problem solves dutiesto nd a self-rewrite of which it can prove that it will be benecial. Obviously there are technical challenges involved in attaining such a level of self-reection in an AGI system, but many of them are not widely known or properly appreciated.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"In this chapter we go back to the theoretical foundations of self-reection and examine the (often subtle) issues encountered when embarking on actually implementing a self-reective AGI system in general and a Gdel Machine in particular. 10.1 Introduction An Articial General Intelligence (AGI) system is likely to require the ability of selfreection; that is, to inspect and reason about its own program code and to perform comprehensive modications to it, while the system itself is running. This is because it seems unlikely that a human programmer can come up with a completely predetermined program that satises sufcient conditions for general intelligence, without requiring adaptation. Of course self-modications can take on different forms, ranging from simple adaptation of a few parameters through a machine learning technique, to the system having complete read and write access to its own currently running program code [1].",Theoretical Foundations of Artificial General Intelligence,chapter 10
"In this chapter we consider the technical implications of approaching the latter extreme, by building towards a pro173      174 Theoretical Foundations of Articial General Intelligence gramming language plus interpreter that allows for complete inspection and manipulation of its own internals in a safe and easily understandable way. The ability to inspect and manipulate ones own program code is not novel; in fact, it was standard practice in the old days when computer memory was very limited and expensive. In recent times, however, programmers are discouraged of using self-modifying code because it is very hard for human programmers to grasp all consequences (especially over longer timespans) of self-modications and thus this practice is considered error-prone. Consequently, many modern (high-level) programming languages severely restrict access to internals (such as the call stack) or hide them altogether. There are two reasons, however, why we should not outlaw writing of self-modifying code forever.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"First, it may yet be possible to come up with a programming language that allows for writing self-modifying code in a safe and easy-to-understand way. Second, now that automated reasoning systems are becoming more mature, it is worthwhile investigating the possibility of letting the machineinstead of human programmersdo all the self-modications based on automated reasoning about its own programming. As an example of a system embodying the second motivation above we will consider the Gdel Machine [25], in order to put our technical discussion of self-reection in context. The fully self-referential Gdel Machine is a universal articial intelligence that is theoretically optimal in a certain sense. It may interact with some initially unknown, partially observable environment to solve arbitrary user-dened computational tasks by maximizing expected cumulative future utility.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"Its initial algorithm is not hardwired; it can completely rewrite itself without essential limits apart from the limits of computability, provided a proof searcher embedded within the initial algorithm can rst prove that the rewrite is useful, according to its formalized utility function taking into account the limited computational resources. Self-rewrites due to this approach can be shown to be globally optimal with respect to the initial utility function (e.g., a Reinforcement Learners reward function), relative to Gdels well-known fundamental restrictions of provability [6]. In the next section we provide an outline of the specication of the Gdel Machine concept, which then provides the context for a subsequent discussion of self-reective systems. As alluded to above, a Gdel Machine implementation calls for a system with the ability to make arbitrary changes to its currently running program.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"But what does that mean? What is a change, how arbitrary can a change be, and what does a running program actually look like? We will see that these are nontrivial questions, involving several subtle but important      Towards an Actual Gdel Machine Implementation 175 issues. In this chapter we provide an overview of the issues involved and ways to overcome them. 10.2 The Gdel Machine Concept One can view a Gdel Machine as a program consisting of two parts. One part, which we will call the solver, can be any problem-solving program. For clarity of presentation, we will pretend the solver is a Reinforcement Learning (RL) [7] program interacting with some external environment. This will provide us with a convenient way of determining utility (using the RL programs reward function), which will be an important topic later on. But in general, no constraints are placed on the solver.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"The second part of the Gdel Machine, which we will call the searcher, is a program that tries to improve the entire Gdel Machine (including the searcher) in a provably optimal way. This searcher will be the main topic of this section. A Gdel Machines searcher is said to hold a function computing the machines expected cumulative future utility. This utility function is then used to construct a target theorem; that is, the Gdel Machine is only allowed to perform a self-modication if it is provably useful to do so. As we will see below, the target theorem species precisely when a rewrite is considered useful. Basically, the function for determining the expected cumulative future utility, shown as u below, sums all rewards for all future time steps. Here time steps actually means not clock ticks, but execution of elementary instructions.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"Indeed, each instruction takes time to execute, so if we can nd a way to explicitly represent the instructions that are going to be executed in the future, we automatically have a window into a future time. An obvious choice of such a representation is the continuation, which is a well-studied concept in light of -calculus-based programming languages (e.g., Lisp, Scheme) [8]. Intuitively, a continuation can be seen as the opposite of a call stack; instead of showing where we came from, a continuation explicitly shows what is going to happen next. Note that in all but the simplest cases, a continuation will only be partially expanded.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"For example, suppose the current continuation is { A(); if B() then C() else D() }; this continuation species that the next thing to be done is expanding A and executing its body, and then the conditional statement will be executed, which means that rst B will be expanded and depending on its result, either C or D will be expanded. Note that before executing B, it is      176 Theoretical Foundations of Articial General Intelligence not clear yet whether C or D will be executed in the future; so it makes no sense to expand either of them before we know the result of B. In what follows we consistently use subscripts to indicate where some element is encoded. u is a function of two parameters, us(s,c), which represents the expected cumulative future utility of running continuation c on state s. Here s represents the evaluating state (where u is encoded), whereas s is the evaluated state.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"The reason for this separation will become clear when considering the specication of u: us(s,c) = Es,Ms[u ] with u(env) = rs(s,env)+ Ec,Kc[us | env] (10.1) As indicated by subscripts, the representation M of the external environment is encoded inside s, because all knowledge a Gdel Machine has must be encoded in s. For clarity, let M be a set of bitstrings, each constituting a representation of the environment held possible by the Gdel Machine.  is a mapping from M to probabilities, also encoded in s. c encodes not only a (partially expanded) representation of the instructions that are going to be executed in the future, but also a set K of statecontinuation pairs representing which possible next states and continuations can result from executing the rst instruction in c, and a mapping  from K to probabilities.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"So  and  are (discrete) probability distributions on sample spaces M and K, respectively. rs(s,env) determines whether state s is rewarding given environment env. For example, in the case where solver (which is part of s) is an RL program, rs(s,env) will be nonzero only when s represents a state just after performing an input receiving instruction. Finally, the term Ec,Kc[us | env] recurses on u with the state and continuation following from executing the next instruction in continuation c. It is crucial to note that u and r are taken from the evaluating state s, not from the state s under evaluation. Doing the latter would break the global optimality [5] of the Gdel Machine, because then it would be capable of rewriting its utility function in arbitrary, even detrimental ways. Therefore, the original, unchanged utility function of s must be used to rst show that a rewrite is useful.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"In contrast,  and M are taken from s, because for any interesting Gdel Machine, c will both contain instructions that affect the external environment (e.g., output to actuators) and instructions that update the internal model of the environment (encoded in s) accordingly. To make use of these updated models, u must extract them from s. Likewise,  and K are taken from c, because after each instruction the continuation necessarily changes, resulting in new  and K. It is also important to notice that the function u is recursive. In formula (10.1), a Gdel Machines life ends when the continuation sample space Kc is empty. In that case, the term Ec,Kc[us | env] is zero and recursion on u stops. In the most common case, how     Towards an Actual Gdel Machine Implementation 177 ever, an instruction will be deterministic, e.g., Kc = {(s,c)}. Then formula (10.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"1) reduces to us(s,c) = Es,Ms[rs(s, )] + us(s,c). So for chains of deterministic state transitions, the expected cumulative future utility reduces to a summation of expected rewards in the successive states. But in e.g. RL settings, even those expected rewards are usually zero, because most instructions are not involved with receiving information from the environment (which in RL is usually the source of rewards). We are now in a position to introduce the target theorem, which is based on the utility function u. Let s be an expected future state before some time t. Furthermore, let c = {wait_until(t);switchprog()} denote the continuation that waits until time t (e.g., by continually calling a primitive clock function until the returned time tests as greater or equal to t) and then executes switchprog, which may inspect and modify any part of s.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"Now us(s,c) tells us what is the expected cumulative future utility of running switchprog on s at time t. If this utility compares positively against the expected cumulative future utility of not switching, then the Gdel Machine must perform the switch at time t. Thus, the searcher is to nd a state s, a time t, and a nullary procedure switchprog such that tts(s,t,switchprog) holds, where: tts(s,t,switchprog) =  us(s,{wait_untils(t);switchprog()}) > us(s,{schedulers()})  (10.2) where scheduler is the procedure that interleaves the Gdel Machines solver and searcher. Thus {schedulers()} denotes the continuation that proceeds as usual from s (including searching for new switchprogs). Note that, since switchprog may overwrite the scheduler, it must as its last instruction invoke a new scheduler (if needed), or simply invoke the old scheduler if it has been left intact.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"There are two ways in which this target theorem can be used to nd provably useful self-rewrites: (1) generate a candidate switchprog (e.g., a program performing optimization techniques also applied by state-of-the-art compilers), generate a future switch time t at least as far ahead as the time the evaluation of the utility of running switchprog is expected to take, and make s a copy of the current state s; then run both continuations specied by tt on s (this is safe because s acts like a sandbox) until the inequality can be shown to be true or false, or until the current time exceeds t; if the inequality is false or time has run out, generate a new switchprog and start over; else, swap the current state for s, wait until time t and execute switchprog.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"(2) Start with an axiomatic system with axiom and rules, and generate theorems (or generate programs that calculate theorems [5]), testing them for equivalence with tt. The rst method can be seen as a top-down approach to proving tt and      178 Theoretical Foundations of Articial General Intelligence the latter as bottom-up. Hybrid approaches are also possible, so this specication of the target theorem allows for a family of possible Gdel Machine implementations [9]. To make sure the Gdel Machine is at least asymptotically optimal even before the rst self-rewrite, it may be initialized with Hutters non-self-referential but asymptotically fastest algorithm for all well-dened problems HSEARCH [10], which uses a hardwired brute force proof searcher and ignores the costs of proof search.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"Given some problem, the Gdel Machine may decide to replace HSEARCH by a faster method suffering less from large constant overhead, but even if it does not, its performance will not be less than asymptotically optimal. Having explained an example of a system needing self-reection, we will now move on to explaining the technical aspects of attaining self-reection. 10.3 The Theoretical Foundations of Self-Reective Systems In the mid-eighties, there was a fashion for reective interpreters, a fad that gave rise to a remarkable term: reective towers. Just imagine a marsh shrouded in mist and a rising tower with its summit lost in gray and cloudy skiespure Rackham! (. . .",Theoretical Foundations of Artificial General Intelligence,chapter 10
") Well, who hasnt dreamed about inventing (or at least having available) a language where anything could be redened, where our imagination could gallop unbridled, where we could play around in complete programming liberty without trammel nor hindrance? [8] The reective tower that Queinnec is so poetically referring to, is a visualization of what happens when performing self-reection.1 A program running on the nth oor of the tower is the effect of an evaluator running on the (n  1)th oor. When a program running on the nth oor performs a reective instruction, this means it gains access to the state of the program running at the (n  1)th oor. But the program running on the nth oor can also invoke the evaluator function, which causes a program to be run on the (n+1)th oor. If an evaluator evaluates an evaluator evaluating an evaluator etc.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"etc., we get the image of a rising tower with its summit lost in gray and cloudy skies. If a program reects on a reection of a reection etc. etc., we get the image of the base of the tower shrouded in mist. What happens were we to stumble upon a ground oor? In the original vision of the reective tower, there is none, because it extends innitely in both directions (up and down). Of course in practice such innities will have to be relaxed, but the point is that it will be of great interest to see exactly when, where, and how problems will surface, which is precisely the point of this section. 1Reection can have different meanings in different contexts, but here we maintain the meaning dened in the introduction: the ability to inspect and modify ones own currently running program.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"     Towards an Actual Gdel Machine Implementation 179 According to Queinnec, there are two things that a reective interpreter must allow for. First: Reective interpreters should support introspection, so they must offer the programmer a means of grabbing the computational context at any time. By computational context, we mean the lexical environment and the continuation [8]. The lexical environment is the set of bindings of variables to values, for all variables in the lexical scope2 of the currently running program. Note that the lexical environment has to do only with the internal state of a program; it has nothing to do with the external environment with which a program may be interacting through actuators. The continuation is a representation of future computations, as we have seen in the previous section. And second: A reective interpreter must also provide means to modify itself (a real thrill, no doubt), so (. . . ) that functions implementing the interpreter are accessible to interpreted programs [8].",Theoretical Foundations of Artificial General Intelligence,chapter 10
"In this section we will explore what these two requirements for self-reection mean technically, and to what extend they are realizable theoretically and practically. 10.3.1 Basic -calculus Let us rst focus on representing a computational context; that is, a lexical environment plus a continuation. The most obvious and well-studied way of elucidating these is in calculus, which is a very simple (theoretical) programming language. The notation of expressions in -calculus is a bit different from usual mathematical notation; for example, parentheses in function application are written on the outside, so that we have (f x) instead of the usual f(x). Note also the rounded parentheses in ( f x); they are part of the syntax and always indicate function application, i.e., they are not allowed freely. Functions are made using lambda abstraction; for example, the identity function is written as x.x.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"So we write a  symbol, then a variable, then a period, and nally an expression that is the body of the function. Variables can be used to name expression; for example, applying the identity function to y can be written as g(y) where g(x) = x, which is written in -calculus as (g.(g y) x.x). Formally, the language (syntax) 1 of basic -calculus is specied using the following recursive grammar. 1 ::= v | v.1 | (1 1) (10.3) 2For readers unfamiliar with the different possible scoping methods, it sufces to know that lexical (also called static) scoping is the intuitive, normal method found in most modern programming languages.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"     180 Theoretical Foundations of Articial General Intelligence This expresses succinctly that (1) if x is a variable then x is an expression, (2) if x is a variable and M is an expression then x.M is an expression (lambda abstraction), and (3) if M and N are expressions then (M N) is an expression (application). In pure -calculus, the only operation that we can perform that is somewhat akin to evaluation, is -reduction. -reduction can be applied to a 1 (sub)expression if that (sub)expression is an application with a lambda abstraction in the operator position. Formally: (x.M N)  = M[x  N] (10.4) So the value of supplying an expression N to a lambda abstraction x.M is M with all occurrences of x replaced by N.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"Of course we can get into all sorts of subtle issues if several nested lambda abstractions use the same variable names, but lets not go into that here, and assume a unique variable name is used in each lambda abstraction. At this point one might wonder how to evaluate an arbitrary 1 expression. For example, a variable should evaluate to its binding, but how do we keep track of the bindings introduced by lambda abstractions? For this we need to introduce a lexical environment, which contains the bindings of all the variables in scope. A lexical environment is historically denoted using the letter  and is represented here as a function taking a variable and returning the value bound to it. We can then specify our rst evaluator E1 as a function taking an expression and a lexical environment and returning the value of the expression.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"In principle this evaluator can be specied in any formalism, but if we specify it in -calculus, we can easily build towards the innite reective tower, because then the computational context will have the same format for both the evaluator and evaluated expression. There are three syntactic cases in language 1 and E1 splits them using double bracket notation. Again, we have to be careful not to confuse the specication language and the language being specied. Here they are both -calculus in order to show the reective tower at work, but we should not confuse the different oors of the tower! So in the specication below, if the 1 expression between the double brackets is oor n, then the right-hand side of the equal sign is a 1 expression on oor n  1. E1 is then specied as follows. E1[[x]] = .( x) (10.5) E1[[x.M]] = ..",Theoretical Foundations of Artificial General Intelligence,chapter 10
"(E1[[M]] [x  ]) (10.6) E1[[(M N)]] = .((E1[[M]] ) (E1[[N]] )) (10.7) It should rst be noted that all expression on the right-hand side are lambda abstractions expecting a lexical environment , which is needed to look up the values of variables. To      Towards an Actual Gdel Machine Implementation 181 start evaluating an expression, an empty environment can be provided. According to the rst case, the value of a variable x is its binding in . According to the second case, the value of a lambda abstraction is itself a function, waiting for a value ; when received, M is evaluated in  extended with a binding of x to . According to the third case, the value of an application is the value of the operator M applied to the value of the operand N, assuming the value of M is indeed a function.",Theoretical Foundations of Artificial General Intelligence,chapter 10
". Both operator and operand are evaluated in the same lexical environment. For notational convenience, we will abbreviate nested applications and lambda abstractions from now on. So ((f x) y) will be written as ( f x y) and x.y.M as xy.M. We have now introduced an explicit lexical environment, but for a complete computational context, we also need a representation of the continuation. To that end, we rewrite the evaluator E1 in continuation-passing style (CPS). This means that a continuation, which is a function historically denoted using the letter , is extended whenever further evaluation is required, or invoked when a value has been obtained. That way,  always expresses the future of computationsalthough, as described before, it is usually only partially expanded. The new evaluator E  1 works explicitly with the full computational context by taking both the lexical environment  and the continuation  as arguments. Formally: E  1[[x]] = .",Theoretical Foundations of Artificial General Intelligence,chapter 10
"( ( x)) (10.8) E  1[[x.M]] = .( .(E  1[[M]] [x  ] )) (10.9) E  1[[(M N)]] = .(E  1[[M]]   f.(E  1[[N]]  x.( f x ))) (10.10) In the rst and second case, the continuation is immediately invoked, which means that the future of computations is reduced. In these cases this is appropriate, because there is nothing extra to be evaluated. In the third case, however, two things need to be done: the operator and the operand need to be evaluated. In other words, while the operator (M) is being evaluated, the evaluation of the operand (N) is a computation that lies in the future. Therefore the continuation, which represents this future, must be extended.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"This fact is now precisely represented by supplying to the evaluator of the operator E  1[[M]] a new continuation which is an extension of the supplied continuation : it is a function waiting for the value f of the operator. As soon as f has been received, this extended continuation invokes the evaluator for the operand E  1[[N]], but again with a new continuation: x.(f x ). This is because, while the operand N is being evaluated, there is still a computation lying in the future; namely, the actual invocation of the value ( f) of the operator on the value (x) of the operand. At the moment of this invocation, the future of computations is exactly the same as the future before evaluating both M and N; therefore, the continuation  of that moment      182 Theoretical Foundations of Articial General Intelligence must be passed on to the function invocation. Indeed, in (10.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"9) we see what will be done with this continuation: a lambda abstraction evaluates to a binary function, where  is the value of its operand and  is the continuation at the time when the function is actually invoked. This is then also the continuation that has to be supplied to the evaluator of the functions body (E  1[[M]]). For the reader previously unfamiliar with CPS it will be very instructive to carefully compare (10.5)(10.7) with (10.8)(10.10), especially the application (third) case. Lets return now to the whole point of explicitly representing the lexical environment and the continuation. Together they constitute the computational context of a program under evaluation; for this program to have self-reective capabilities, it must be able to grab both of them. This is easily achieved now, by adding two special constructs to our language, grab-r and grab-k, which evaluate to the current lexical environment and current continuation, respectively.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"E  1[[grab-r]] = .( ) (10.11) E  1[[grab-k]] = .( ) (10.12) These specications look deceptively simple, and indeed they are. Because although we now have a means to grab the computational context at any time, there is little we can do with it. Specically, there is no way of inspecting or modifying the lexical environment and continuation after obtaining them. So they are as good as black boxes. Unfortunately there are several more subtle issues with the evaluator E  1, which are easily overlooked. Suppose we have a program  in language 1 and we want to evaluate it. For this we would have to determine the value of (E  1[[]] 0 x.x), supplying the evaluator with an initial environment and an initial continuation. The initial continuation is easy: it is just the identity function.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"The initial environment 0 and the extension of an environment (used in (10.9) but not specied yet) can be specied as follows. 0 = x. (10.13) [x  ] = y.if (= x y)  ( y) (10.14) So the initial environment is a function failing on every binding lookup, whereas an extended environment is a function that tests for the new binding, returning either the bound value () when the variables match ((= x y)), or the binding according to the unextended environment (( y)). But here we see more problems of our limited language 1 surfacing: there are no conditional statements (if), no primitive functions like =, and no constants like , so we are not allowed to write the initial and extended lexical environment as above.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"     Towards an Actual Gdel Machine Implementation 183 Also the language does not contain numbers to do arithmetics (which has also caused our examples of 1 expressions to be rather abstract). Admittedly, conditionals, booleans, and numbers can be represented in pure -calculus, but that is more of an academic exercise than a practical approach. Here we are interested in building towards practically feasible self-reection, so lets see how far we can get by extending our specication language to look more like a real programming language. 10.3.2 Constants, Conditionals, Side-effects, and Quoting Lets extend our very austere language 1 and add constructs commonly found in programming languages, and in Scheme [11] in particular. Scheme is a dialect of Lisp, is very close to -calculus, and is often used to study reection in programming [8, 12, 13].",Theoretical Foundations of Artificial General Intelligence,chapter 10
"The Scheme-like language 2 is specied using the following recursive grammar. 2 ::= v | v.2 | (2 2) | c | if 2 2 2 | set! v 2 | quote 2 (10.15) where constants (c) include booleans, numbers, and, notably, primitive functions. These primitive functions are supposed to include operators for doing arithmetics and for inspecting and modifying data structures (including environments and continuations!), as well as IO interactions. The if construct introduces the familiar if-then-else expression, set! introduces assignment (and thereby side-effects), and quote introduces quoting (which means treating programs as data). In order to appropriately model side-effects, we need to introduce the storage, which is historically denoted using the letter . From now on the lexical environment () does not bind variables to values, but to addresses. The storage, then, binds addresses to values.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"This setup allows set! to change a binding by changing the storage but not the lexical environment. The new evaluator E2 is then specied as follows. E2[[x]] = .(  ( ( x))) (10.16) E2[[x.M]] = .(  .(E2[[M]] [x  ] [  ] )) (10.17) E2[[(M N)]] = .(E2[[M]]    f.(E2[[N]]   x.( f x  ))) (10.18) where  is a fresh address. Again, it will be very instructive to carefully compare the familiar (10.8)(10.10) with (10.16)(10.18). The main difference is the storage which is being passed around, invoked (10.16), and extended (10.17). The new cases are: E2[[c]] = .(  c) (10.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"19) E2[[if C T F]] = .(E2[[C]]   c.(if c E2[[T]] E2[[F]]   )) (10.20)      184 Theoretical Foundations of Articial General Intelligence E2[[set! x M]] = .(E2[[M]]   .( [( x)  ] )) (10.21) E2[[quote M]] = .(  M) (10.22) Constants simply evaluate to themselves. Similarly, quoting an expression M returns M unevaluated. Conditional statements force a choice between evaluating the then-body (E2[[T]]) and the else-body (E2[[F]]). Note that in (10.21) only the storage () is changed, such that variable x retains the address that is associated with it in the lexical environment, causing future look-ups (see (10.16)) to return the new value () for x.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"As an example of a primitive function, the binary addition operator can be specied as follows. + = x.(  y.(  (+ x y))) (10.23) Note the recursion; + is specied in terms of the + operator used one oor lower in the reective tower. The same holds for if in (10.20). Where does it bottom out? The marsh is still very much shrouded in mist. The reason that we cannot specify primitive procedures and constructs nonrecursively at this moment is because the specications so far say nothing about the data structures used to represent the language constructs. Theoretically this is irrelevant, because the ground oor of the reective tower is innitely far away. But the inability to inspect and modify data structures makes it hard to comply with Queinnecs second condition for self-reection, namely that everythingincluding the evaluatorshould be modiable.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"There are now (at least) two ways to proceed: (1) set up a recursive loop of evaluators with shared state to make the reective tower oat without a ground oor, or (2) let go of the circular language specication and retire to a reective bungalow where everything happens at the ground oor. Both options will be discussed in the next two sections, respectively. 10.4 Nested Meta-Circular Evaluators Using the well-studied technique of the meta-circular evaluator [12], it is possible to attain self-reectivity in any (Turing-complete) programming language. A meta-circular evaluator is basically an interpreter for the same programming language as the one in which the interpreter is written.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"Especially suitable for this technique are homoiconic languages such as Lisp and in particular its dialect Scheme [11], which is very close to -calculus and is often used to study meta-circular evaluators and self-reection in programming in general [8, 1219]. So a meta-circular Scheme evaluator is a program written in Scheme      Towards an Actual Gdel Machine Implementation 185 scheduler() runs  E [[scheduler()]] runs  E [[E [[scheduler()]]]] runs       Fig. 10.1 The self-inspection and self-modication required for a Gdel Machine implementation can be attained by having a double nesting of meta-circular evaluators run the Gdel Machines scheduler. Every instruction is grounded in some virtual machine running underwater, but the nested meta-circular evaluators can form a loop of self-reection without ever getting their feet wet. which can interpret programs written in Scheme.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"There is no problem with circularity here, because the program running the meta-circular Scheme evaluator itself can be written in any language. For clarity of presentation let us consider how a meta-circular Scheme evaluator can be used to obtain the self-reectivity needed for e.g. a Gdel Machine. In what follows, let E [[]] denote a call to an evaluator with as argument program . As before, the double brackets mean that program  is to be taken literally (i.e., unevaluated), for it is the task of the evaluator to determine the value of . Now for a meta-circular evaluator, E [[]] will give the same value as E [[E [[]]]] and E [[E [[E [[]]]]]] and so on. Note that E [[E [[]]]] can be viewed as the evaluator reading and interpreting its own source code and determining how the program constituting that source code evaluates .",Theoretical Foundations of Artificial General Intelligence,chapter 10
"A very clear account of a complete implementation (in Scheme) of a simple reective interpreter (for Scheme) is provided by Jefferson et al. [13], of which we shall highlight one property that is very interesting in light of our goal to obtain a self-reective system. Namely, in the implementation by Jefferson et al., no matter how deep one would nest the evaluator (as in E [[E [[ ]]]]), all levels will share the same global environment3 for retrieving and assigning procedures and data. This implies that self-reection becomes possible when running a program (in particular, a Gdel Machines scheduler) in a doubly nested metacircular interpreter with a shared global environment (see gure 10.1). Note that this setup can be attained regardless of the hardware architecture and without having to invent new techniques. Consider again the quote at the start of section 10.3.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"It is interesting to note that this paragraph is immediately followed by equally poetic words of caution: However, we pay 3A global environment can be seen as an extension of the lexical environment. When looking up the value of a variable, rst the lexical environment is searched; if no binding is found, the global environment is searched. Similarly for assignment. The global environment is the same in every lexical context. Again, all this has nothing to do with the external environment, which lies outside the system.      186 Theoretical Foundations of Articial General Intelligence for this dream with exasperatingly slow systems that are almost incompilable and plunge us into a world with few laws, hardly even any gravity. We can now see more clearly this world without gravity in gure 10.1: the nested meta-circular evaluators are oating above the water, seemingly without ever getting their feet wet. But this is of course an illusion.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"Consider again the third quote, stating that in a selfreective system the functions implementing the interpreter must be modiable. But what are those functions implementing the interpreter? For example, in the shared global environment, there might be a function called evaluate and helper functions like lookup and allocate. These are all modiable, as desired. But all these functions are composed of primitive functions (such as Schemes cons and cdr) and of syntactical compositions like function applications, conditionals, and lambda abstractions. Are those functions and constructs also supposed to be modiable? All the way down the functions implementing the interpreter can be described in terms of machine code, but we cannot change the instruction set of the processor. The regress in self-modiability has to end somewhere.4 10.5 A Functional Self-Reective System Taking the last sentence above to its logical conclusion, let us now investigate the consequences of ending the regress in self-modiability already at the functions implementing the interpreter.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"If the interpreter interprets a Turing-complete instruction set, then having the ability to inspect and modify the lexical environment and the continuation at any point is already enough to attain functionally complete reective control. The reason that Queinnec mentions modiability of the functions implementing the interpreter is probably not for functional reasons, but for timing and efciency reasons. As we have seen, the ability to grab the computational context is enough to attain functional reection, but it does not allow a program to speed up its own interpreter (if it would know how). In this section we will introduce a new interpreter for a self-reective system that takes a middle road. The functions implementing this interpreter are primitives (i.e., black boxes); however, they are also (1) very few in number, (2) very small, and (3) fast 4Unless we consider systems that are capable of changing their own hardware.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"But then still there are (probably?) physical limits to the modiability of hardware.      Towards an Actual Gdel Machine Implementation 187 in execution.5 We will show that a program being run by this interpreter can inspect and modify itself (including speed upgrades) in a sufciently powerful way to be self-reective. First of all, we need a programming language. Here we use a syntax that is even simpler than classical -calculus, specied by the following recursive grammar. 3 ::= c | n | (3 3) (10.24) where c are constants (symbols for booleans and primitive functions) and n are numbers (sequences of digits). There are no special forms (such as quote, lambda, set!, and if in Scheme), just function application, where all functions are unary. Primitive binary functions are invoked as, e.g., ((+ 1) 2).",Theoretical Foundations of Artificial General Intelligence,chapter 10
"Under the hood, the only compound data structure is the pair6 and an application (f x) is simply represented as a pair f:x. An instance of a number takes as much space as one pair (say, 64 bits); constants do not take any space. For representing what happens under the hood, we extensively use the notation a:d, meaning a pair with head a and tail d. The colon associates from right to left, so a:ad:dd is the same as (a:(ad:dd)).  is a constant used to represent false or an error or undened value; for example, the result of an integer division by zero is  (programs are never halted due to an error). The empty list (Scheme: ()) is represented by the constant . To look up a value stored in a storage  at location p, we use the notation [p  n] for numbers and [p  a:d] for pairs.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"Allocation is denoted as [q  n] for numbers and [q  a:d] for pairs, where q is a fresh location where the allocated number or pair can be found. N is used to denote the set of locations where numbers are stored; all other locations store pairs. The set of constants is denoted as P. We refer the reader to the appendix for more details on notation and storage whenever something appears unclear. The crux of the interpreter to be specied in this section is that not only programs are stored as pairs, but also the lexical environment and the continuation. Since programs can build (cons), inspect (car, cdr),7 and modify (set-car!, set-cdr!) pairs, they can then also build, inspect, and modify lexical environments and continuations using these same functions. That is, provided that they are accessible. The interpreter presented below will see to that.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"5The last point of course depends on the exact implementation, but since they are so small, it will be clear that they can be implemented very efciently. In our not-so-optimized reference implementation written in C++, one primitive step of the evaluator takes about twenty nanoseconds on a 2009-average PC. Since reection is possible after every such step, there are about fty million chances per second to interrupt evaluation, grab the computational context, and inspect and possibly modify it. 6Our reference implementation also supports vectors (xed-size arrays) though. 7In Scheme and 3, car and cdr are primitive unary functions that return the head and tail of a pair, respectively.      188 Theoretical Foundations of Articial General Intelligence As is evident from (10.24), the language lacks variables, as well as a construct for lambda abstraction. Still, we stay close to -calculus by using the technique of De Bruijn indices [20].",Theoretical Foundations of Artificial General Intelligence,chapter 10
"Instead of named variables, numbers are used to refer to bound values. For example, where in Scheme the -calculus expression x.y.x is written as (lambda (x y) x), with De Bruijn indices one would write (lambda (lambda 1)). That is, a number n evaluates to the value bound by the nth enclosing lambda operator (counting the rst as zero). A number has to be quoted to be used as a number (e.g., to do arithmetics). But as we will see shortly, neither lambda nor quote exists in 3; however, a different, more general mechanism is employed that achieves the same effect. By using De Bruijn indices the lexical environment can be represented as a list,8 with variable lookup being simply a matter of indexing the lexical environment.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"The continuation will also be represented as a list; specically, a list of functions, where each function species what has to be done as the next step in evaluating the program. So at the heart of the interpreter is a stepper function, which pops the next function from the continuation and invokes it on the current program state. This is performed by the primitive unary function step, which is specied as follows. (Note that every n-ary primitive function takes n + 2 arguments, the extra two being the continuation and the storage.) step()(,) =                        ()(,) if [  :] and   F1 step()(,[  :]) if [  :] and   F2 (,)(,) if [  (:):] and   F2 step()(,) if [  :]  otherwise (10.25) where F1 and F2 are the sets of all primitive unary and binary functions, respectively.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"In the rst case, the top of the continuation is a primitive unary function, and step supplies  to that function. In the second case, the top is a primitive binary function, but since we need a second argument to invoke that function, step makes a pair of the function and  as its rst argument. In the third case, where such a pair with a binary function is found at the top of the continuation,  is taken to be the second argument and the binary function is invoked. In the fourth case, the top of the continuation is found not to be a function (because the rst three cases are tried rst); this is an error, so the non-function is popped from the continuation and step continues with  to indicate the error. Finally, in the fth 8In Scheme and 3, a list is a tail-linked, -terminated sequence of pairs, where the elements of the list are held by the heads of the pairs.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"     Towards an Actual Gdel Machine Implementation 189 case, when the continuation is not a pair at all, step cannot continue with anything and  is returned as the nal value. It is crucial that all primitive functions invoke step, for it is the heart of the evaluator, processing the continuation. For example, the primitive unary function cdr, which takes the second element of a pair, is specied as follows. cdr(p)(,) =    step(d)(,) if [p  a:d] step()(,) otherwise (10.26) Henceforth we will not explicitly write the step()(,) otherwise part anymore. As a second example, consider addition, which is a primitive binary function using the underlying implementations addition operator. +(n,n)(,) = step(n)(,[n  (m 64+ m)]) if n,n  N, [n  m] and [n  m] (10.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"27) Notice that, like every expression, n and n are mere indices of the storage, so rst their values (m and m, respectively) have to be retrieved. Then a fresh index n is allocated and the sum of m and m is stored there. Here 64+ may be the 64-bit integer addition operator of the underlying implementations language. Now we turn to the actual evaluation function: eval. It is a unary function taking a closure, denoted here using the letter . A closure is a pair whose head is a lexical environment and whose tail is an (unevaluated) expression. Given these facts we can immediately answer the question of how to start the evaluation of a 3 program  stored in storage : initialize  = [  :][  eval:] and determine the value of step()(,). So we form a pair (closure) of the lexical environment (which is initially empty: ) and the program  to be evaluated in that environment.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"What step()(,) will do is go to the rst case in (10.25), namely to call the primitive unary function eval with  as argument. Then eval must distinguish three cases because a program can either be a primitive constant (i.e., , , or a function), a number, or a pair. Constants are self-evaluating, numbers are treated as indices of the lexical environment, and pairs are treated as applications. eval is then specied as follows. eval()(,) =          step()(,) if [  :] and   P step(  n)(,) if [  :] and   N and [  n] step(1)(,) if [  ::] (10.28) where  = [1  :][2  :][  eval:(next:2):]. In the rst case the constant is simply extracted from the closure ().",Theoretical Foundations of Artificial General Intelligence,chapter 10
"In the second case, the De Bruijn index inside the      190 Theoretical Foundations of Articial General Intelligence closure is used as an index of the lexical environment , also contained in . (Denitions of P, N, and  are provided in the appendix). In the third case, eval makes two closures: 1 is the operator, to be evaluated immediately; 2 is the operand, to be evaluated next. Both closures get the same lexical environment (). The new continuation  reects the order of evaluation of rst  then : eval itself is placed at the top of the continuation, followed by the primitive binary function next with its rst argument (2) already lled in. The second argument of next will become the value of , i.e., the operator of the application. next then simply has to save the operator on the continuation and evaluate the operand, which is its rst argument (a closure). This behavior is specied in the second case below.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"next(,)(,) =    step()(,[  :]) if [  quote2:] step()(,[  eval::]) otherwise (10.29) What does the rst case do? It handles a generalized form of quoting. The reason that quote, lambda, set!, and if are special forms in Scheme (as we have seen in 2) is because their arguments are not to be evaluated immediately. This common theme can be handled by just one mechanism. We introduce the binary primitive function quote2 and add a hook to next (the rst case above) to prevent quote2s second argument from being evaluated. Instead, the closure (, which represented the unevaluated second argument of quote2) is supplied to the rst argument of quote2 (which must therefore be a function).9 So where in Scheme one would write (quote (1 2)), here we have to write ((quote2 cdr) (1 2)).",Theoretical Foundations of Artificial General Intelligence,chapter 10
"Since quote2 supplies a closure (an environmentexpression pair) to its rst argument, cdr can be used to select the (unevaluated) expression from that closure. Also, we have to quote numbers explicitly, otherwise they are taken to be De Bruijn indices. For example, Schemes (+ 1 2) here becomes ((+ ((quote2 cdr) 1)) ((quote2 cdr) 2)). This may all seem cumbersome, but it should be kept in mind that the Scheme representation can easily be converted automatically, so a programmer does not have to notice any difference and can just continue programming using Scheme syntax. What about lambda abstractions? For that we have the primitive binary function lambda2, which also needs the help of quote2 in order to prevent its argument from being evaluated prematurely. For example, where in Scheme the calculus expression x.y.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"is written as (lambda (x y) x), here we have to write ((quote2 lambda2) ((quote2 lambda2) 1)). lambda2 is specied as follows 9quote2 may also be seen as a kind of fexpr builder.      Towards an Actual Gdel Machine Implementation 191 (cf. (10.17)). lambda2(,)(,) = step( )(,[   (:):][  eval:]) if [  :] (10.30) So lambda2 takes a closure and a value, extends the lexical environment captured by the closure with the provided value, and then signals that it wants the new closure to be evaluated by pushing eval onto the continuation. As we saw earlier, conditional expressions are ternary, taking a condition, a then-part, and an else-part.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"In the very simple system presented here, if is just a primitive unary function, which pops from the continuation both the then-part and the else-part, and sets up one of them for evaluation, depending on the value of the condition. if()(,) =    step()(,[  eval:]) if  =  and [  (next:)::] step()(,[  eval:]) if  =  and [  :(next:):] (10.31) So any value other than  is interpreted as true. Where in Scheme one would write (if c t f), here we simply write (((if c) t) f). An implementation of Schemes set! in terms of quote2 can be found in the appendix. Now we have seen that the lexical environment and the program currently under evaluation are easily obtained using quote2. What remains is reection on the continuation. Both inspection and modication of the continuation can be achieved with just one very simple primitive: swap-continuation.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"It simply swaps the current continuation with its argument. swap-continuation()(,) = step()(,) (10.32) The interested reader can nd an implementation of Schemes call-with-current-continuation in terms of swap-continuation in the appendix. This was the last of the core functions implementing the interpreter for 3. All that is missing is some more (trivial) primitive function like cons, pair?, *, eq?, and function(s) for communicating with the external environment. 10.6 Discussion So is 3 the language a self-reective Gdel Machine can be programmed in? It certainly is a suitable core for one. It is simple and small to the extreme (so it is easy to reason about), yet it allows for full functional self-reection.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"It is also important to note that we      192 Theoretical Foundations of Articial General Intelligence have left nothing unspecied; it is exactly known how programs, functions, and numbers are represented structurally and in memory. Even the number of memory allocations that each primitive function performs is known in advance and predictable. This is important information for self-reasoning systems such as the Gdel Machine. In that sense 3s evaluator solves all the issues involved in self-reection that we had uncovered while studying the pure -calculus-based tower of meta-circular evaluators. We are currently using a 3-like language for our ongoing actual implementation of a Gdel Machine.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"The most important extension that we have made is that, instead of maintaining one continuation, we keep a stack of continuations, reminiscent of the poetically beautiful reective tower with its summit lost in gray and cloudy skiesexcept this time with solid foundations and none of those misty marshes! Although space does not permit us to go into the details, this construction allows for an easy and efcient way to perform interleaved computations (including critical sections), as called for by the specication of the Gdel Machines scheduler.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"Appendix: Details of Notation Used After parsing and before evaluation, a 3 program has the following structure: #f,error =  ( ) = : #t,() =  lambda  = (quote2:lambda2): x = x for x  PF N quote  = (quote2:cdr): For historical reasons [8],  is a typical variable denoting a program or expression (same thing),  is a typical variable denoting an environment,  is a typical variable denoting a continuation (or call stack), and  is a typical variable denoting a storage (or working memory). A storage  = S,N consists of a xed-size array S of 64 bit chunks and a set N of indices indicating at which positions numbers are stored. All other positions store pairs, i.e., two 32 bit indices to other positions in S.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"To look up the value in a storage  at location p, we use the notation [p  n] or [p  a:d]. The former associates n with all 64 bits located at index p, whereas the latter associates a with the least signicant 32 bits and d with the most signicant 32 bits. We say that the value of p is the number n if [p  n] and p  N or the pair with head a and tail d if [p  a:d]. Whenever we write [p  a:d], we tacitly assume that p  N.      Towards an Actual Gdel Machine Implementation 193 A new pair is allocated in the storage using the notation [p  a:d], where a and d are locations already in use and p is a fresh variable pointing to a previously unused location in .",Theoretical Foundations of Artificial General Intelligence,chapter 10
"Similarly, [p  n] is used to allocate the number n in storage , after which it can be referred to using the fresh variable p. Note that we do not specify where in S new pairs and numbers are allocated, nor how and when unreachable ones are garbage collected. We only assume the existence of a function free() indicating how many free places are left (after garbage collection). If free() = 0, allocation returns the error value . Locating a primitive in a storage also returns . These last two facts are formalized respectively as follows: [  x] iff free() = 0 (10.33) [c  ] for all c  P (10.34) where the set of primitives P is dened as follows: P = BF, B = {,}, F = F1 F2, (10.35) F1 = {step,eval,if,car,cdr,pair?,number?,swap-continuation}, (10.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"36) F2 = {next,lambda2,quote2,cons,set-car!,set-cdr!,eq?,=,+,-,*,/,>} (10.37) These are the basic primitive functions. More can be added for speed reasons (for common operations) or introspection or other external data (such as amount of free memory in storage, current clock time, IO interaction, etc.). Note that quote2 is just a dummy binary function, i.e., quote2(,)(,) = step()(,). For convenience we often shorten the allocation and looking up of nested pairs: [p  a:(ad:dd)] def = [p  ad:dd][p  a:p] (10.38) [p  (aa:da):d] def = [p  aa:da][p  p:d] (10.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"39) [p  a:(ad:dd)] def = ([p  a:p] and [p  ad:dd]) (10.40) [p  (aa:da):d] def = ([p  p:d] and [p  aa:da]) (10.41) Note that the result of an allocation is always the modied storage, whereas the result of a lookup is true or false. Taking the nth element of list  stored in  is dened as follows.   n =           if n = 0 and [  :]   (n  1) if n > 0 and [  :]  otherwise (10.42)      194 Theoretical Foundations of Articial General Intelligence Schemes set! and call-with-current-continuation can be implemented in 3 as follows.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"They are written in Scheme syntax instead of 3 for clarity, but this is no problem because there is a simple procedure for automatically converting almost any Scheme program to the much simpler 3. 1 (define set! 2 (quote2 (lambda (env-n) 3 (lambda (x) 4 (set-car! (list-tail (car env-n) (cdr env-n)) x))))) 5 (define (call-with-current-continuation f) 6 (get-continuation 7 (lambda (k) 8 (set-continuation (cons f k) (set-continuation k))))) 9 (define (get-continuation f) 10 (swap-continuation (list f))) 11 (define (set-continuation k x) 12 (swap-continuation (cons (lambda (old_k) x) k))) where the functions cons, list, car, cdr, set-car!, and",Theoretical Foundations of Artificial General Intelligence,chapter 10
"list-tail work as in Scheme [11]. Bibliography [1] T. Schaul and J. Schmidhuber, Metalearning, Scholarpedia. 6(5), 4650, (2010). [2] J. Schmidhuber. Gdel machines: Self-referential universal problem solvers making provably optimal self-improvements. Technical Report IDSIA-19-03, arXiv:cs.LO/0309048 v2, IDSIA, (2003). [3] J. Schmidhuber. Gdel machines: Fully self-referential optimal universal self-improvers. In eds. B. Goertzel and C. Pennachin, Articial General Intelligence, pp. 199226. Springer Verlag, (2006). Variant available as arXiv:cs.LO/0309048. [4] J. Schmidhuber.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"Completely self-referential optimal reinforcement learners. In eds. W. Duch, J. Kacprzyk, E. Oja, and S. Zadrozny, Articial Neural Networks: Biological Inspirations ICANN 2005, LNCS 3697, pp. 223233. Springer-Verlag Berlin Heidelberg, (2005). Plenary talk. [5] J. Schmidhuber, Ultimate cognition  la Gdel, Cognitive Computation. 1(2), 177193, (2009). [6] K. Gdel, ber formal unentscheidbare Stze der Principia Mathematica und verwandter Systeme I, Monatshefte fr Mathematik und Physik. 38, 173198, (1931). [7] L. P. Kaelbling, M. L. Littman, and A. W.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"Moore, Reinforcement learning: a survey, Journal of AI research. 4, 237285, (1996). [8] C. Queinnec, Lisp in Small Pieces. (Cambridge University Press, 1996). [9] B. R. Steunebrink and J. Schmidhuber. A family of Gdel Machine implementations. In Proceedings of the 4th Conference on Articial General Intelligence (AGI-11). Springer, (2011). [10] M. Hutter, The fastest and shortest algorithm for all well-dened problems, International Journal of Foundations of Computer Science. 13(3), 431443, (2002).      Bibliography 195 [11] R. Kelsey, W. Clinger, J. Rees, and (eds.), Revised5 report on the algorithmic language Scheme, Higher-Order and Symbolic Computation.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"11(1) (August, 1998). [12] H. Abelson, G. J. Sussman, and J. Sussman, Structure and Interpretation of Computer Programs. (MIT Press, 1996), second edition. [13] S. Jefferson and D. P. Friedman, A simple reective interpreter, LISP and Symbolic Computation. 9(2-3), 181202, (1996). [14] B. C. Smith. Reection and semantics in LISP. In Principles of programming languages (POPL84), (1984). [15] J. des Rivires and B. C. Smith. The implementation of procedurally reective languages. In 1984 ACM Symposium on LISP and functional programming, (1984). [16] D. P. Friedman and M. Wand. Reication: Reection without metaphysics.",Theoretical Foundations of Artificial General Intelligence,chapter 10
"In Proceedings of ACM Symposium on Lisp and Functional Programming, (1984). [17] M. Wand and D. P. Friedman. The mystery of the tower revealed: A non-reective description of the reective tower. In Proceedings of ACM Symposium on Lisp and Functional Programming, (1986). [18] O. Danvy and K. Malmkjr. Intensions and extensions in a reective tower. In Lisp and Functional Programming (LFP88), (1988). [19] A. Bawden. Reication without evaluation. In Proceedings of the 1988 ACM conference on LISP and functional programming, (1988). [20] N. de Bruijn, Lambda calculus notation with nameless dummies, a tool for automatic formula manipulation, Indagationes Mathematicae. 34, 381392, (1972).",Theoretical Foundations of Artificial General Intelligence,chapter 10
"  Chapter 11 Articial General Intelligence Begins with Recognition: Evaluating the Flexibility of Recognition Tsvi Achler Los Alamos National Labs, Los Alamos, USA achler@gmail.com Many types of supervised recognition algorithms have been developed over the past halfcentury. However, it remains difcult to compare their exibility and ability to reason. Part of the difculty is the need of a good denition of exibility. This chapter is dedicated to dening and evaluating exibility in recognition. Articial Intelligence and even more so Articial General Intelligence are inseparable from the context of recognition. Recognition is an essential foundation on top of which virtually every function of intelligence is based e.g.: memory, logic, internal understanding, and reasoning. Many logic and reasoning questions can be directly answered by reasoning based on recognition information. Thus it is important to understand forms of exible recognition structures in order to know how to store and reason based on exible information.",Theoretical Foundations of Artificial General Intelligence,chapter 11
"The rst section describes various methods that perform recognition. The second section proposes tests and metrics to evaluate exibility, and the third provides an example of applying the tests to these methods. 11.1 Introduction Although many recognition algorithms have been developed over the past half century, arguably the most prevalent method of classication is based on learned feedforward weights W that solve the recognition relationship:  Y = W X or  Y = f(W, X) (11.1) Vector  Y represents the activity of a set of labeled nodes, called neurons or outputs in different literatures and individually written as  Y = (Y1,Y2,Y3,...,YH)T. They are considered supervised because the nodes can be labeled for example: Y1 represents dog,Y2 represents cat, and so on.",Theoretical Foundations of Artificial General Intelligence,chapter 11
"Vector  X represents sensory nodes that sample the environment, or input 197      198 Theoretical Foundations of Articial General Intelligence space to be recognized, and are composed of individual features  X = (X1,X2,X3,...,XN)T. The input features can be sensors that detect edges, lines, frequencies, kernel features, and so on. W represents a matrix of weights or parameters that associates inputs and outputs. Learning weights W may require error propagation and comparison of inputs and outputs, but once W is dened, recognition is a feedforward process. Thus the direction of information ow during recognition is feedforward: one-way from inputs to the outputs.",Theoretical Foundations of Artificial General Intelligence,chapter 11
"Variations on this theme can be found within different algorithm optimizations, for example: Perceptrons (Rosenblatt, 1958), Neural Networks (NN) with nonlinearities introduced into calculation of  Y (Rumelhart and McClelland, 1986), and Support Vector Machines (SVM) with nonlinearities introduced into the inputs through the kernel trick (Vapnik, 1995). Although these algorithms vary in specics such as nonlinearities determining the function f, they share the commonality in that recognition involves a feedforward transformation using W. Recognition  Output Output Output Output Feed Forward e.g. SVMs, NNs Input Input Input Lateral Connections e.g. WTA Generative Auto-Associative Input or Fig. 11.1 Comparison of possible architectures used during recognition. In feedforward methods connections ow from inputs to outputs (top).",Theoretical Foundations of Artificial General Intelligence,chapter 11
"After feedforward processing lateral connections may connect between outputs or back to inputs of a different node (middle). Generative or AutoAssociative connections are symmetrical and each node projects back to their own inputs (bottom). Some recognition models implement lateral connections for competition between output nodes  Y, such as: one-vs-all, winner-take-all, all-vs-all. However such competition methods rely on initially calculating  Y node activities based on the feedforward transformation W. A variation of feedforward algorithms are recurrent networks. Recurrent networks are feedforward networks in a hierarchy where a limited number of outputs are also used as inputs. This allows the processing of time. These networks can be unfolded into a recursive      Articial General Intelligence Begins with Recognition 199 feedforward network e.g. (Schmidhuber 1992; Williams & Zipser 1994; Boden 2006). Thus they also fall into a feedforward category. In auto-associative networks, e.g.",Theoretical Foundations of Artificial General Intelligence,chapter 11
"(Hopeld, 1982), all outputs feed back to their own inputs. When part of an input pattern is given, the network completes the whole pattern. Generative models are a variation of auto-associative networks. The difference between generative and auto-associative networks is that in generative models, the auto-associative patterns are compared to, or subtracted from, the inputs. I focus on supervised generative models because they can have the same xed points or solutions as feedforward models. Thus supervised generative and feedforward models can be directly compared. Mathematically, generative models can be described by taking the inverse of equation (11.1): W1 Y =  X (11.2) Lets dene M as the inverse or pseudoinverse of W. The relation becomes: M Y   X = 0 (11.",Theoretical Foundations of Artificial General Intelligence,chapter 11
"3) Using this equation can be called a generative process because it reconstructs the input based on what the network has previously learned. The term M Y is an internal prototype that best matches  X constructed using previously learned information. The values of  Y are the solution to the system. The xed-points or solutions of equations (11.3) and (11.1) are identical, so the same  Y values also match the feedforward equation  Y = W X. Equation (11.3) describes the solution but does not provide a way to project input information to the outputs. Thus dynamical networks are used that converge to equation (11.3). One method is based on Least Squares which minimizes the energy function: E = 1 2  X  M Y2. Taking the derivative relative to  Y the dynamic equation is: d Y dt = MT M Y   X  (11.",Theoretical Foundations of Artificial General Intelligence,chapter 11
"4) This equation can be iterated until d Y/dt = 0 resulting in the xed point solution that is equivalent to  Y = W X. M Y represents top-down feedback connections that convert  Y to  X domain. MT X represents feedforward connections that convert  X to  Y domains. Both feedforward and feedback connections are determined by M and together emulate feedforward weights W. Another way to converge to equation (11.3) is using Regulatory Feedback (RF). The equation can be written as: d Y dt =  Y  1 V MT   X M Y   1  where V = N  j=1 Mji (11.5)      200 Theoretical Foundations of Articial General Intelligence Alternatively, using expanded notation it can be written as: dYi dt = Yi N  j=1 Mji N  k=1 Mki     Xk H  h=1 MkhYh    Yi (11.",Theoretical Foundations of Artificial General Intelligence,chapter 11
"6) assuming MNH dimensions for M. Both generative-type models have the identical xed points (Achler & Bettencourt, 2011). Generative models, have roots in Independent Component Analysis (ICA), are commonly unsupervised and determine M so that sparseness is maximized e.g. (Olshausen & Field, 1996). The unsupervised paradigm does not have supervised labels, so ultimately a supervised feedforward method is still used for classication e.g. (Zieler et al., 2010). Since the goal is to compare alternate structures that perform supervised recognition and W is supervised, then M must be supervised and sparseness is not implemented. Some supervised generative models use a restricted Boltzmann machine to help training which is limited to binary activation e.g. (Hinton & Salakhutdinov, 2006). However in effect, the testing conguration remains feedforward.",Theoretical Foundations of Artificial General Intelligence,chapter 11
"The difference between the approach taken here and other approaches is that it is assumed that M is already learned. M remains xed throughout the calculations. M is easier to learn than W because it represents xedpoints (Achler 2012, in press). Thus supervised generative models using equations (11.5), (11.6) are evaluated using a xed M during testing. Equation (11.4) can be used as well but may require an additional parameter to converge. In-depth comparisons between supervised generative models, including differences between equations (11.4) and (11.5), are beyond the scope of this chapter and will be addressed in future work. 11.2 Evaluating Flexibility As with intelligence, evaluating robustness in recognition algorithms is not straight forward. It is also not clear what is the best metric. For example, an algorithm that is designed for a specic task and performs superbly on that task is not necessarily robust.",Theoretical Foundations of Artificial General Intelligence,chapter 11
"An algorithm that shows fair performance in multiple domains may be more robust. In order to move the eld forward, particularly with the introduction of completely new methods, it is necessary to objectively evaluate and compare algorithm performance. This requires creating a benchmark. Cognitive psychologists have struggled with such metrics to      Articial General Intelligence Begins with Recognition 201 evaluate human performance for almost a century and came up with IQ metrics. A similar philosophy has is suggested for AI, e.g. (Legg & Hutter, 2007). However testing for robustness in a eld such as AI poses a catch-22. Once a test is dened, it becomes a benchmark. Benchmarks commonly generate a competition for narrow algorithms that focus on the benchmark, as is the case with tests in the AI and recognition elds (e.g. Caltech-256 Object Category Dataset). Subsequently the algorithms that may perform well on benchmarks are not necessary the most exible.",Theoretical Foundations of Artificial General Intelligence,chapter 11
"The proposed workaround of the catch-22 is to focus on combinatorial problems combined with measurements of resources. The proposed tests for robustness should not reward algorithms that are over-trained for the test. Thus the majority of the tests within the battery are designed to present a combinatorial explosion for a brute-force method that tries to achieve good performance only by learning specic instances of the testing suite. However multiple over-trained algorithms may be implemented as one algorithm to overcome the multiple tests. This is why the evaluation of resources is essential. The measurement of parameters, training, and setup costs of algorithms serves as a measure of narrowness. The proposed evaluation compares performance on the test set with the total number of parameters and training required. This is an Occams razor-like metric rewarding the simplest solution with the most functionality. The test battery can be updated periodically (e.g. every several years).",Theoretical Foundations of Artificial General Intelligence,chapter 11
"The overall goals of evaluation are to: 1) Design tests where performance cannot be improved by learning instances from the test suite. 2) Provide a framework where performance can be compared across algorithms. 3) Re-evaluate algorithm performance and update the tests periodically to ensure exibility and promote progress. 11.2.1 The Testing Paradigm For testing the contending recognition algorithms are treated as a black box and given the same patterns. Lets dene supervised input-label patterns A  Z. Thus if  X A is presented to the network, then it is expected to recognize it and YA should go to one. Each network to be tested is given the same input to label associations for training (A,B,C,...,Z). The actual patterns can be random patterns. Testing is conducted by presenting patterns and pattern manipulations to the input layer  X and evaluating based on expectations the output layer  Y responses.",Theoretical Foundations of Artificial General Intelligence,chapter 11
"The performance is pooled across tests and the number of parameters counted in the evaluation.      202 Theoretical Foundations of Articial General Intelligence The majority of tests of classiers in the literature do not test beyond the single  X e.g. ( X test =  X A,  X B,..., X Z). Thus robustness in recognizing mixtures is not explicitly evaluated in the literature. In this work both single  X and the sensitivity to the manipulation of mixtures of  X are evaluated e.g. ( X test =  X A +  X B,  X A  X B ...). Three types of problems are drawn upon to produce combinatorial explosions: superposition catastrophe (problems with mixtures of patterns), the binding problem (problems of grouping components of patterns), and numerosity (estimating the number patterns without individually counting each one). Unless algorithms have a powerful method of generalizing what they have learned, the training required can increase combinatorially with network size.",Theoretical Foundations of Artificial General Intelligence,chapter 11
"11.2.2 Combinatorial Difculties of Superposition or Mixes A simple way to create a combinatorial difculty is with mixtures of patterns. For example if a network can process 5 000 patterns, there are about 12 million possible two pattern combinations of those patterns, 20 billion possible three pattern combinations, and so on. In methods that over-rely on learning, the mixtures must be learned. This can quickly overcome the number of available variables. We refer to this property as a combinatorial explosion regardless if it is technically exponential or another function. The mixture test evaluates algorithm performance when features of patterns are mixed or added together forming a superposition (von der Malsburg, 1999; Rosenblatt, 1962).",Theoretical Foundations of Artificial General Intelligence,chapter 11
"In the superposition test, the networks are tested on patterns formed from mixtures of input vectors, for example  X mix =  X A +  X B,  X mix =  X A +  X B +  XC and so on. Let k represent the number of patterns mixtures superimposed in  X mix. Let n represent the number of individual patterns the network knows, then the possible number of combinations increases exponentially and is given by: number_of_combinations = n k  = n! k!(n  k)! (11.7) During testing, for each network the top k y-values are selected and their identities are compared to the patterns in  X mix. If the top k nodes match the patterns that were used to compose  X mix, then a correct classication for that combination is recorded. This is repeated for all possible combinations. As the number of simultaneous patterns increases the number of possible combinations increases combinatorially.",Theoretical Foundations of Artificial General Intelligence,chapter 11
"When the networks are presented with 26 patterns, and one      Articial General Intelligence Begins with Recognition 203 is chosen, k = 1 (i.e.  X test =  X A,  X B,  XC ...), there are 26 possible patterns. When networks are presented with two patterns simultaneously k = 2, there are 325 possible combinations of non-repeating patterns (e.g.  X mix =  X A+ X B,  X A+ XC,  X A+ X D,...). When networks are presented with three pattern combinations, k = 3 (e.g.  X mix =  X A +  X B +  XC,  X A +  X B +  X D,  X A +  X B +  X E ...) there are 2,600 possible combinations. For eight simultaneous patterns (i.e. k = 8), there are 1,562,275 combinations.",Theoretical Foundations of Artificial General Intelligence,chapter 11
"If networks cannot generalize for superpositions, they must train for most of these combinations, i.e. most of the 1.5 million combinations. Learning k = 8 does not guarantee good performance with other k values. Thus potentially the networks must be trained on all ks. In this test the superpositions are combined noiselessly. If two patterns say  X A and  X B have the same feature X1 and each pattern has X1 = 1, then if both patterns are superimposed (pattern1+pattern2) the value of that feature is X1 = 2 in the superposition. Next we evaluate combinations with information loss. Train: Single Patterns Test: Pattern Mixtures Patterns Combined by:  X A  X B  XC  X D ... Union Summing X1 X2 X3 X...",Theoretical Foundations of Artificial General Intelligence,chapter 11
"XN          0 1 0 0 1                   1 1 1 0 0                   1 1 0 0 0                   0 1 0 0 1          =          1 1 1 0 1                   2 4 1 0 2          X1 X2 X3 X... XN Single Random Patterns  X mix =  X A  X B  XC  X D  X A +  X B +  XC +  X D Tests Generated for All Possible Combinations Fig. 11.2 Training with single patterns, testing with pattern mixtures. Test patterns are superpositions of single patterns combined by summing or union. Only 5 input features shown. 11.2.3 Occluding Superpositions Flexible algorithms must function in multiple scenarios, even if fundamental assumptions of the scenarios change.",Theoretical Foundations of Artificial General Intelligence,chapter 11
"For example, instead summing, overlapping features may block each other and their overlap may be better represented as a union or min of features.      204 Theoretical Foundations of Articial General Intelligence In the union or min superposition test, the networks are tested on vectors that are combined using unions ( X A   X B,  X A   X B   XC, etc.). Not only do the same combinatorial problems in training occur with this scenario, a method suitable for superposition using addition may not apply to union superposition. The symbol  will be used to represent a union. Note that if values within the matrixes are non-binary then the union can be represented by the minimal value of the intersection, a min function. In this test the superpositions are combined with information loss. Suppose two patterns say  X A and  X B have the same feature X1 and each pattern has X1 = 1.",Theoretical Foundations of Artificial General Intelligence,chapter 11
"If both patterns are superimposed (pattern1pattern2) the value of that feature is X1 = 1 in the superposition. Signicant information can be lost in a union. For example, repeats cannot be decoded:  X mix =  X A   X A, will be identical to  X A,  X A   X A   X A etc. However this is a useful test for the exibility of algorithms. Since the comparison is between algorithms, any systematic information loss will affect the tested algorithms equally. Thus un-decodable aspects of test cases may reduce the number correct but will not affect overall ranking of algorithms since the algorithms are compared against each other. An example of the Union Superposition test is found in (Achler, Omar and Amir, 2008). 11.2.4 Counting Tests The Superposition Catastrophe test has no more than one instance per pattern, without repeats. However, repeats (e.g.",Theoretical Foundations of Artificial General Intelligence,chapter 11
"X mix =  X A +  X A) also provide important test cases. The ability to generalize without training specically for the repeats is important for the ability to process or count simultaneous patterns without counting them one by one. Babies and animals have an inherent ability to estimate amounts or subitize, even while demonstrating an impoverished ability to individually count (Feigenson, Dehaene & Spelke, 2004). This ability may be important to quickly estimate the most abundant food resources and make decisions relevant to survival. The counting test is designed to evaluate the ability to estimate amounts without individually counting. A network that can automatically process a superposition mixture such as  X mix =  X A +  X K +  X J +  X E +  X J +  X K +  X K +  X G can evaluate whether there are more Ks than Js without individually counting the patterns. In the counting test, the amplitude value of ys are evaluated.",Theoretical Foundations of Artificial General Intelligence,chapter 11
"For the  X mix above it is expected that  Y will show YA = 1, YE = 1, YG = 1, YJ = 2, YK = 3, all other Ys= 0. In the count test, instead of selecting the top k Y-values and comparing their identity to patterns in  X mix, the amplitude value      Articial General Intelligence Begins with Recognition 205 of Ys are compared to the number of repeats of the corresponding patterns within  X mix. number_of_combinations = nk k! (11.8) The number of possibilities in the count test increase even more than the superposition test. An example of the superposition and counting test is found in (Achler, Vural & Amir, 2009). 11.2.5 Binding Tests The binding problem represents another scenario with a combinatorial explosion. However this often has different meanings in neuroscience, cognitive science, and philosophy.",Theoretical Foundations of Artificial General Intelligence,chapter 11
"Thus we briey review some denitions and usage. According to the neural binding hypothesis, neurons within different neuronal assemblies re in synchrony to bind different features of neuronal representations together (Gray et al., 1989; von der Malsburg, 1999). This denes binding as a mechanism of attention and represents the internal workings of a specic mechanism. This type of binding is not within the scope of this paper since algorithms are treated as black boxes. Fig. 11.3 Another denition of binding is a unity of perception. This idea is somewhat metaphysical since it is hard to objectively dene and measure unity from a humans report. However there is a rich literature on errors of perception. Binding problems occur in humans when image features can be interpreted through more than one representation. An intuitive way to describe this problem is through visual illusions. For example, a local feature can support different representations based on the overall interpretation of the picture.",Theoretical Foundations of Artificial General Intelligence,chapter 11
"In the old woman/young woman illusion of Fig 11.3, the young womans cheek is the old womans nose. Though the features are exactly the same, the interpretation is different. In humans, this gure forms an illusion because all features in the image can t into two representations. Classiers have similar difculties but with simpler patterns. If a pattern can be part of two representations then the networks must determine to which it belongs. Training is used to nd optimal interconnection weights for each possible scenario. However, this is not trivial for combinations of patterns and training can grow exponentially (Rosenblatt, 1962). We rene this denition of binding using simple tests and suggest it ts in a more general mathematical framework of set-cover.      206 Theoretical Foundations of Articial General Intelligence The most basic binding scenario is given in gure 11.4D.",Theoretical Foundations of Artificial General Intelligence,chapter 11
"Suppose a larger pattern completely overlaps with a smaller pattern: activation of node Y1 is determined by pattern  X 1 = (1,), where feature X1 = 1 and feature X2 is not relevant for node Y1. Activation of node Y2 is determined by pattern  X 2 = (1,1), where features X1 and X2 = 1. When presented with the larger pattern, (features X1 & X2 = 1) the representation of the smaller pattern should not predominate. The network should recognizeY2 (settle on Y1 = 0, Y2 = 1). A correct classication is Y1 when only X1 = 1, but Y2 when X1 & X2 = 1. This satises the set-cover problem. The classier should prefer the representation that covers the most inputs with the least amount of overlap in the inputs.",Theoretical Foundations of Artificial General Intelligence,chapter 11
"A more complex scenario occurs with the addition of Y3, see gure 11.4E. Node Y3 is determined by pattern  X 3 = (,1,1), where features X2 and X3 = 1 (and X1 is not relevant for Y3). Set-cover still holds. Since the same basic overlap exists between Y1 and Y2, the same interaction should remain given X1 and X2. However if X1 & X2 & X3 = 1, then activation of Y1 and Y3 simultaneously can completely cover these inputs. Any activation of Y2 would be redundant because X2 would be covered twice. Choosing Y2 given X1 & X2 & X3 = 1 is equivalent to choosing the irrelevant features for binding. For intuition lets give nodes Y1, Y2, and Y3, representations of wheels, barbell, and chassis respectively.",Theoretical Foundations of Artificial General Intelligence,chapter 11
"The inputs represent spatially invariant features where feature X1 represents circles, X3 represents the body shape and feature X2 represents a horizontal bar. Y1 represents wheels and thus when it is active, feature X1 is interpreted as wheels. Y2 represents a barbell composed of a bar adjacent to two round weights (features X1 and X2). Note: even though Y2 includes circles (feature X1), the circles do not represent wheels (Y1), they represent barbell weights. Thus if Y2 is active feature X1 is interpreted as part of the barbell. Y3 represents a car body without wheels (features X2 and X3), where feature X2 is interpreted as part of the chassis. Given an image of a car with all features simultaneously (X1, X2 and X3), choosing the barbell (Y2) is equivalent to a binding error within the wrong context in light of all of the inputs.",Theoretical Foundations of Artificial General Intelligence,chapter 11
"Most classiers if not trained otherwise are as likely to choose barbell or car chassis (Achler 2009). In that case the complete picture is not analyzed in terms of the best t given all of the information present. This training is not trivial and may represent a combinatorial problem. A solution based on set-cover automatically classies the components and subcomponents of this problem. If X1, X2, X3 = 1, then Y1 and Y3 represents the most efcient solution. If X1, X2 = 1, then Y2 represents the most efcient solution.",Theoretical Foundations of Artificial General Intelligence,chapter 11
"Set-cover also re     Articial General Intelligence Begins with Recognition 207 Inputs y1 x2 x3 y3 Outputs x1 y2 x2 x1 Wheels Barbell Car Chassis y1 y2 x2 x1 A & B  Combined x1,x2=1:  A: B: C: D: y1 x3 y3 y2 x2 x1 A, B & C  Combined x1,x2,x3=1:  E: Fig. 11.4 (AE): Modular combination of nodes Y1, Y2, Y3 (A, B & C) display binding in combined networks (D and E). Y1 & Y3 represent car with wheels, Y2 represents barbell. If X1, X2 = 1 then Y2 should predominate (not Y1), because it encompasses all active inputs.",Theoretical Foundations of Artificial General Intelligence,chapter 11
"If X1, X2, X3 = 1 then Y2 should not predominate because interpreting a barbell within the car is a binding error. solves basic recognition: if X1 = 1, then Y1 is the most efcient solution, if X2, X3 = 1, then Y3 is the most efcient solution. Analysis of larger scenarios with innite chains can be found in (Achler and Amir, 2008). 11.2.6 Binding and The Set-Cover Problem The notion of set-cover can guide the design more complex binding tests. Given a universe of possible Xs of input patterns and Ys that cover Xs. Set-cover asks what is the sub-set of Ys that cover any arbitrary set of X but which use the fewest Ys. Set-cover can explain the old-young woman illusion, where the most encompassing representations mutually predominate.",Theoretical Foundations of Artificial General Intelligence,chapter 11
"In that illusion there are two equal encompassing representations. Recognition settles on either the old woman or young woman, and no other combinations of features. All features are interpreted as part of either representation even if the interpretation of the individual feature can vary drastically (e.g. cheek of young woman vs. nose of old woman). Set-cover is not a trivial problem to compute. The evaluation of set covering is NPcomplete, but the optimization of set-cover is NP-hard. This means that training networks      208 Theoretical Foundations of Articial General Intelligence to account for every possible cover is not practical because it may require every possibility to be trained, exposing another combinatorial explosion. Thus networks that can look beyond what is learned should be able to resolve setcover. Simple examples that require set-cover can be introduced to a test set. Examples of binding tests can be found in (Achler & Amir, 2008) and (Achler, 2009). 11.2.",Theoretical Foundations of Artificial General Intelligence,chapter 11
"7 Noise Tests Random noise is often used as a metric for exibility. Random noise can be predicted and trained. Since training with noise does not necessarily result in a combinatorial explosion, this is less favored. However resistance to random noise is important and a commonly used measure. Thus it is added to the battery. Noisy stimuli can be generated using real-valued random exponential noise with mean amplitude , added to the prototypical stimulus. Noisy test vectors can be given to the classiers and their result is compared to its original labels. The percent of patterns correctly identied for that noise level can be recorded. Other systematic non-random noise scenarios can be tested as well. 11.2.8 Scoring the Tests The Intelligence Quotient (IQ) scoring system is borrowed from the eld of psychology because it provides a familiar reference. Here each algorithm tested is like an individual.",Theoretical Foundations of Artificial General Intelligence,chapter 11
"Performance on the battery of tests is pooled and the algorithms are ranked relative to the average of all algorithms. Performance is ranked on the Gaussian bell curve with a center value (average IQ) of 100, each standard deviation is 15 points. IQ value will be 100 for the average performing individuals (> 100 for better than average, < 100 for less than average), regardless of the actual score on a particular sub-test. The absolute value of performance an algorithm achieves in a single test is not relevant since comparisons are the ultimate evaluation. For example, suppose one algorithms performance on a particular sub-test in the battery is low, say 10% correct. Since IQ values only reect differences from average performance, if the average is 7% correct, that performance provides a better-than-average contribution to that algorithms IQ score for that test. Another advantage of this method is that tests can be simply combined to give an aggregate for the battery.",Theoretical Foundations of Artificial General Intelligence,chapter 11
"For example: Battery_IQ = Average(IQnumerosity + IQsuperposition + IQbinding + IQnoisy + ) (11.9)      Articial General Intelligence Begins with Recognition 209 The IQ ranking system, evaluates algorithm exibility while measuring and encouraging performance improvement. Additional tests are easy to include into the battery and scoring system. 11.2.9 Evaluating Algorithms Resources One advantage of AI testing over humans testing is the ability to evaluate the number of training trials and resources used by the algorithm. An algorithm with many parameters and extensive training may do slightly better than another algorithm that was not given as much training and does not have as many free parameters. However, a slightly better performance at the cost of more training and variables may actually be less exible. The algorithm that performs best with the least amount of training and the least amount of free parameters, is the most desirable.",Theoretical Foundations of Artificial General Intelligence,chapter 11
"Thus we have included a measure of resources to the test score that accounts for the number of training epochs and free variables. These factors would modify the nal test score to give a nal AI_IQ score. AI_IQ = abilities resources = Battery_Score training_epochs+ variables+ training (11.10) Flexible AI should require less degrees of freedom and apply to a greater number of scenarios (without extensive retraining for each scenario). The purpose of the metrics is to encourage such capabilities, limit degrees of freedom but maximize applicability. This philosophy of evaluation  rather than the exact details  is important for exibility evaluation. 11.3 Evaluation of Flexibility This section provides concrete examples of tests that compare the exibility of feedforward methods to generative and other methods. This analysis is not complete but provides in-depth examples. We begin by dening random patterns outlined in Section 11.2.",Theoretical Foundations of Artificial General Intelligence,chapter 11
"1: with 26 labels (H = 26) and 512 input features (N = 512). A SVM, Winner-take-all (WTA) algorithm, generative RF, and three different versions of NN algorithms are compared. The three versions are: nave NNs trained only on single patterns, NNs trained on single and 2 pattern mixes, and NNs trained with WEKA. The purpose of the publicly available Waikato Environment for Knowledge Analysis (WEKA) package (Witten & Frank, 2005) is to facilitate algorithm comparison and maintains up to date algorithms.      210 Theoretical Foundations of Articial General Intelligence We begin by training a NN. The NNs are trained via backpropagation with momentum. 100,000 training examples are randomly chosen with replacement from the prototypes. The number of hidden units used is increased until it was sufcient to learn the inputoutput mapping to a component-wise tolerance of 0.05.",Theoretical Foundations of Artificial General Intelligence,chapter 11
"Resulting NNs have on average 12 hidden units. The learning rate was .1, the momentum was 1 and there is a bias unit projecting to both hidden and output layers with value of 1. Lets evaluate the number of parameters for NN: 26 output nodes, 512 input nodes, 12 hidden units, bias unit, momentum, component-wise tolerance, and learning rate. There are 551 variables so far. However during training unsuccessful NNs were created with 1-11 hidden units which were rejected due to suboptimal performance. There were also 100,000 training episodes for NN. All of those resources should be included in the evaluation. That is more than 1,200,000 episodes of training. To simplify our analysis lets suppose 100 511 represents the number of variables and resources. After this, suppose performance on the mixtures is poor and to improve performance the network is trained on 2-pattern mixtures.",Theoretical Foundations of Artificial General Intelligence,chapter 11
"That represents 325 more training patterns and more epochs. Those should be included as well. There are 26 training episodes for RF. No training was done for combinations. There is one variable for tolerance determining when to end the simulation. This represents a total count of 565 for the number of resources: 26 outputs +512 inputs +1 tolerance variables +26 training episodes. The winner-take-all algorithm is trained the same way as the RF algorithm however each output node has inhibitory connections to all other nodes. Since all of the output nodes are uniformly inhibitory, only one variable is added. This is a total of 566 resources. The WTA algorithm is evaluated for multiple patterns by 1) nding the most active node, 2) determining its identity, 3) inhibiting it, 4) nding the next most active node. In the WEKA environment the number of variables and training episodes were not easily available.",Theoretical Foundations of Artificial General Intelligence,chapter 11
"Thus it is assumed WEKA implemented algorithms are the most efcient possible, despite this being a generous oversimplication. SVMs do not have hidden units so the number output and input nodes should be the same as RF. However SVMs have kernels increasing the input space size and those are also governed by variables. A similar situation holds for the number of hidden variables for the WEKA neural network algorithm. Again, since WEKA is treated as a black box, these are not counted and the number of training episodes is not counted as well.      Articial General Intelligence Begins with Recognition 211 11.3.1 Superposition Tests with Information Loss Superposition pattern mixtures were combined using a union (or min function). Some information is lost in the mixtures. Following (Achler, Omar, Amir, 2008) lets look at test performance.",Theoretical Foundations of Artificial General Intelligence,chapter 11
"Two more networks that were not present in the original paper are included here: SVM and winner-take-all WTA. See table 11.1. Table 11.1 (a) Occluding Superposition: evaluation of performance given mixtures with information loss. Raw scores of single patterns (left), 2-patern mixtures, and 4-pattern mixtures (right). number mixed: k = 1 k = 2 k = 4 combinations: 26 325 14 950 RF 100% 100% 90% NN nave 100% 52% 4% NN 2-pattern 100% 92% 32% NN WEKA 100% 91% 28% SVM WEKA 100% 91% 42% WTA 100% 85% 24% Table 11.1 (b) Total score for the 15301 occluding mixtures and analysis.",Theoretical Foundations of Artificial General Intelligence,chapter 11
"Raw overall score (top row), followed by IQ values based on those scores, then the number of parameters and training episodes for each algorithm, followed by the AI_IQ scores. AI_IQ scores (bottom) are sensitive to the number of parameters. * indicates estimated number of parameters. RF NN nave NN 2-pat NN WEKA SVM WEKA WTA Score (%) 90.2 5.2 33.4 29.5 43.1 25.4 IQ 128 83 98 96 103 93 Parameters 565 100,551 100,551 565* 565* 566 AI_IQ 126 86 86 99 105 97 RF performed well within all mixtures showing more exibility. The number of parameters in WEKA are grossly underestimated, because it is being treated as a black box here. This demonstrates that an accurate count of parameters is critical for evaluating exibility. 11.",Theoretical Foundations of Artificial General Intelligence,chapter 11
"3.2 Superpositions without loss Next superposition tests, where pattern mixtures are combined using an addition function without information loss, are evaluated following Achler (2009).      212 Theoretical Foundations of Articial General Intelligence Table 11.2 (a) Performance for superposition mixtures without information loss. 26 single patterns (left), 2-patern mixtures and 4-pattern mixtures (right) were tested. RF correctly recognized all patterns in all 325 k = 2 and 14950 k = 4 combinations. number mixed: k = 1 k = 2 k = 4 combinations: 26 325 14950 RF 100% 100% 100% NN WEKA 100% 91% 4% SVM WEKA 100% 94% 8% WTA 100% 91% 42% Table 11.",Theoretical Foundations of Artificial General Intelligence,chapter 11
"2 (b) Total score for the 15301 superposition mixtures (top row), followed by IQ values based on those scores, then the number of parameters and training episodes for each algorithm. AI_IQ scores (bottom) are sensitive to the number of parameters. * indicates estimated number of parameters. RF NN WEKA SVM WEKA WTA Score (%) 100 6 10 43.1 IQ 121 88 90 101 parameters 565 565* 565* 566 AI_IQ 131 89 90 105 Comparing this test to the occluding test, winner-take-all performed better while NN and SVM performed worse. See table 11.2. 11.3.3 Counting Tests This section is intended to evaluate counting tests where repeating patterns are combined in the mixtures using an addition function e.g. (Achler, Vural & Amir, 2009).",Theoretical Foundations of Artificial General Intelligence,chapter 11
"Unfortunately WEKA could not test more than 15 000 tests. It is not designed for such large number of tests. This is an indication that the literature is not focusing on manipulations during recognition: primarily testing single patterns. Hopefully future versions of WEKA will allow more testing. Winner-take-all by its nature is not well suited for this test because it is not clear how to select a node several times. The tests are based on the same 26 patterns used in the superposition. 17,576 possible combinations of 3 and 456,976 possible combinations of 4 pattern repeats were tested. RF scored well on all tests. Although this is an important part of the test battery, this test is not included in the overall score because of the limited comparisons available using WEKA.      Articial General Intelligence Begins with Recognition 213 11.3.4 Binding Scenarios The simplest and next simplest binding scenarios require new patterns.",Theoretical Foundations of Artificial General Intelligence,chapter 11
"In the simplest binding scenario there are two output nodes and two input nodes. Networks trained on the simplest scenario with two nodes perform correctly. NN WEKA, SVM WEKA, Winner-take-all, RF all score 100%. Again, the number of training epochs, hidden nodes and kernels parameters in WEKA are not evaluated since WEKA is considered a black box. Thus the performance of all methods are approximately equal. In the next simplest binding scenario there are three inputs and three output nodes. Not all algorithms perform this correctly. In the case of all inputs active, RF decides on the correct two patterns that best t the inputs. NN WEKA and SVM WEKA settle 50% on two representations that capture most of the inputs but do not cover all of the inputs in an efcient manner, a poor set-cover. The other input patterns were correctly matched.",Theoretical Foundations of Artificial General Intelligence,chapter 11
"Of the 4 possible of input patterns, RF achieved 4/4, SVM & NN WEKA and Winnertake-all achieved 3/4. Thus scores for this test are 100%, 75%, 75%, 75% respectively. IQ scores are: 123, 93, 93, 93 respectively. Resources are estimated as 3 inputs and 3 outputs for all methods but WTA which has an extra variable. AI_IQ scores based on those resource estimates are: 121, 96, 96, 86 respectively. This is a simple demonstration of binding. Further tests are envisioned to be developed for more complex binding/set-cover scenarios. 11.3.5 Noise Tests Tests were performed on the 26 patterns with random exponential noise of mean amplitude  added to the prototypical stimulus. As part of tests NN were trained on patterns with  = 0.25 and  = 0.15, NN  = 0.",Theoretical Foundations of Artificial General Intelligence,chapter 11
"25, and NN  = 0.15 respectively. Lets look at test performance (from Achler, Omar, Amir, 2008). The number of variables and training epochs for NN are similar to as Section 11.3.1 with 100,551 total. Again we do not apply variables and training for WEKA at this point. All NNs that were well-trained for noise performed better than RF. However RF was more noise resistant without training than NN nave. 11.3.6 Scoring Tests Together The proposed test suite includes: superposition, occluding superposition, counting, binding, and noise. However at this point the most complete results available compare      214 Theoretical Foundations of Articial General Intelligence Table 11.3 (a) Comparison of performance in given random noise. NNs trained on noise improved performance. Nave RF performs better than nave NN, but NN WEKA performed best. Input Noise Level  0.",Theoretical Foundations of Artificial General Intelligence,chapter 11
"1 0.15 0.2 0.25 0.3 0.35 0.4 RF 100% 100% 95% 85% 68% 47% 29% NN nave 100% 78% 43% 24% 12% 9% 8% NN = 0.15 100% 100% 100% 81% 53% 28% 17% NN = 0.25 100% 100% 100% 97% 79% 49% 28% NN WEKA 100% 100% 100% 100% 98% 91% 76% Table 11.3 (b) Total score for the 182 tests (top row), followed by IQ values based on those scores, then the number of parameters and training episodes for each algorithm. AI_IQ scores (bottom).",Theoretical Foundations of Artificial General Intelligence,chapter 11
"* indicates estimated number of parameters. RF NN nave NN = 0.15 NN = 0.25 NN WEKA Score (%) 75 39 68 79 95 IQ 103 76 98 106 117 parameters 565 100,551 100,551 100,551 565* AI_IQ 120 86 86 86 129 RF and NN WEKA in: superposition, occluding superposition, binding, and noise. The total score weighing all weighted tests equally is RF 91.3; NN WEKA 51.4*. AI_IQ score: RF 111; NN WEKA 89. Again, these numbers do not properly account for the number of parameters and training in WEKA. Ideally the all resources should be included including hidden layer variables and WEKA training. However the goal here is to indicate how exibility can be evaluated and quantied. From this perspective this limited demonstration is sufcient.",Theoretical Foundations of Artificial General Intelligence,chapter 11
"This score and demonstration shows how exibility can be evaluated across a set of tests with combinatorial difculties. 11.3.7 Conclusion from Tests The test battery demonstrates fundamental differences in exibility between a method using M: RF; and methods using W: NN, SVM. Despite the black box designation for WEKA, RF still performed better than NN and SVM. This is demonstrated whether the mixtures are summed together (Achler, 2009) or combined as a union (Achler, Omar, Amir, 2008). The mixtures can contain multiple additions of the same pattern (repeats) and the networks are able to determine the pattern mixtures numerosity (Achler, Vural,      Articial General Intelligence Begins with Recognition 215 Amir, 2009). They are also able to resolve certain binding scenarios: i.e. to determine whether subcomponent parts belong together (Achler & Amir, 2008).",Theoretical Foundations of Artificial General Intelligence,chapter 11
"Thus recognition based on M is quantied as more exible. Comparing performance between generative models, using the least-squares method, equation 11.4 versus 11.5, we obtain the same results except for the union cases and some instances of the binding cases. The instances of binding and unions that did not settle on the same solutions had input patterns outside the xed points or linear combinations of the xed points. In those cases both models converged, however differing results can be expected since no guarantees are made outside of the xed points. The signicance of the differences between methods is beyond the scope of this paper and will be discussed in future work. 11.4 Summary It remains difcult to evaluate recognition exibility using benchmark tests while discouraging methods which may improve performance by being narrowly optimized for the tests. We developed a set of metrics composed of tests and evaluation criteria that focus on exibility.",Theoretical Foundations of Artificial General Intelligence,chapter 11
"The benchmark tests are designed to frustrate narrowly optimized methods by using tests that present combinatorial difculties. The metrics are designed to frustrate narrowly optimized methods by penalizing performance based on the amount of resources used, such as parameters and the number of training instances. We introduce several types of classiers and showed how this evaluation system works. Most notably, we compared methods that utilize feedforward connections during testing with methods that use lateral competition and feedforward-feedback connections during testing. We ran into several difculties with the general simulation package, WEKA. It was not designed to test a large number of tests. However, despite such limitations the ndings were sufcient to demonstrate how this metric system operates. The conclusion of this study indicates that feedforward-feedback methods may be a more exible conguration for classiers. This is a topic that is still evolving and a work in progress.",Theoretical Foundations of Artificial General Intelligence,chapter 11
"     216 Theoretical Foundations of Articial General Intelligence Acknowledgments This work was supported in part by the Notational Geospatial Agency and Los Alamos National Labs. I would like to thank Eyal Amir, Tanya Berger-Wolf, Luis Bettencourt, Garrett Kenyon, Peter Loxley, Cyrus Omar, Dervis Vural, and those who helped review this work for valuable comments. Bibliography [1] Achler T. (2009). Using Non-Oscillatory Dynamics to Disambiguate Simultaneous Patterns. IEEE IJCNN 2009: 3570. [2] Achler T., in press 2012, Towards Bridging the Gap Between Pattern Recognition and Symbolic Representation Within Neural Networks, Workshop on Neural-Symbolic Learning and Reasoning AAAI. [3] Achler T. Amir E. (2008). Input Feedback Networks: Classication and Inference Based on Network Structure.",Theoretical Foundations of Artificial General Intelligence,chapter 11
"Articial General Intelligence 1: 1526. [4] Achler T., Amir E., (2008). Hybrid Classication and Symbolic-Like Manipulation Using SelfRegulatory Feedback Networks, Proc 4th Int Neural-Symbolic Learning & Reasoning Workshop. [5] Achler T., Bettencourt L., (2011). Evaluating the Contribution of Top-Down Feedback and Post-Learning Reconstruction, Biologically Inspired Cognitive Architectures AAAI Proceedings. [6] Achler T., Omar C., Amir E. (2008). Shedding Weights: More With Less. IJCNN 2008: 3020 3027. [7] Achler T. Vural D., Amir E. (2009). Counting Objects with Biologically Inspired RegulatoryFeedback Networks. IEEE IJCNN 2009: 3640. [8] Boden M. (2006).",Theoretical Foundations of Artificial General Intelligence,chapter 11
"A guide to recurrent neural networks and backpropagation. <http://www.itee.uq.edu.au/~mikael/papers/rn_dallas.pdf>. [9] Feigenson L., Dehaene S., & Spelke E., (2004). Core systems of number. Trends Cogn. Sci. 8 (7): 30714. [10] Gray C.M., Konig P., Engel A.K., Singer W. (1989). Oscillatory responses in cat visual cortex exhibit inter-columnar synchronization which reects global stimulus properties. Nature 338 (6213): 3347. [11] Hinton G.E., & Salakhutdinov R.R. (2006). Reducing the dimensionality of data with neural networks. Science, 313 (5786), 504507 [12] Hopeld J.J.",Theoretical Foundations of Artificial General Intelligence,chapter 11
"(1982) Neural networks and physical systems with emergent collective computational abilities, PNAS, vol. 79 no. 8 pp. 25542558. [13] Hyvrinen A., Hurri J., Hoyer P.O. (2009). Natural Image Statistics, Springer-Verlag. [14] Legg S. and Hutter M. Universal intelligence: A denition of machine intelligence. Minds & Machines, 17 (4):391444, 2007. [15] Olshausen B.A., Field D.J., (1997) Sparse coding with an overcomplete basis set: A strategy employed by V1? Vision Research 37:33113325. [16] Rosenblatt F. (1962). Principles of neurodynamics; perceptrons and the theory of brain mechanisms. Washington, Spartan Books. [17] Rumelhart D.E., & McClelland J.L. (1986).",Theoretical Foundations of Artificial General Intelligence,chapter 11
"Parallel distributed processing: explorations in the microstructure of cognition. Cambridge, Mass.: MIT Press.      Bibliography 217 [18] Schmidhuber J. (1992). Learning complex, extended sequences using the principle of history compression. Neural Computation, 4 (2):234242. [19] Vapnik V.N. (1995). The nature of statistical learning theory. New York: Springer. [20] von der Malsburg C. (1999). The what and why of binding: the modelers perspective. Neuron, 24 (1), 95-104, 111125 [21] Williams R.J., Zipser D. (1994). Gradient-based learning algorithms for recurrent networks and their computational complexity. In Back-propagation: Theory, Architectures & Applications. Hillsdale, NJ: Erlbaum. [22] Witten I.H., & Frank E. (2005).",Theoretical Foundations of Artificial General Intelligence,chapter 11
"Data mining: practical machine learning tools and techniques (2nd ed.). Amsterdam; Boston, MA: Morgan Kaufman. [23] Zeiler M.D., Kirshnan D., Taylor G.W., Fergus R. (2010). Deconvolutional Networks, Computer Vision & Pattern Recognition CVPR.   ",Theoretical Foundations of Artificial General Intelligence,chapter 11
"  Chapter 12 Theory Blending as a Framework for Creativity in Systems for General Intelligence Maricarmen Martinez, Tarek R. Besold, Ahmed Abdel-Fattah, Helmar Gust, Martin Schmidt, Ulf Krumnack, and Kai-Uwe Khnberger Institute of Cognitive Science, University of Osnabrck, Albrechtstr. 28, 49076 Osnabrck, Germany {mmartine | tbesold | ahabdelfatta | hgust | martisch | krumnack | kkuehnbe}@uos.de Being creative is a central property of humans in solving problems, adapting to new states of affairs, applying successful strategies in previously unseen situations, or coming up with new conceptualizations. General intelligent systems should have the potential to realize such forms of creativity to a certain extent.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"We think that creativity and productivity issues can be best addressed by taking cognitive mechanisms into account, such as analogymaking, concept blending, computing generalizations and the like. In this chapter, we argue for the usage of such mechanisms for modeling creativity. We exemplify in detail the potential of such a mechanism like theory blending using a historical example from mathematics. Furthermore, we argue for the claim that modeling creativity by such mechanisms has a huge potential in a variety of domains. 12.1 Introduction Articial intelligence (AI) has shown remarkable success in many different application domains. Modern information technology, as most prominently exemplied by internet applications, control systems for machines, assistant systems for cars and planes, the generation of user proles in business processes, automatic transactions in nancial markets etc. would not be possible without the massive usage of AI technologies. In this sense, AI is a success story that triggered a signicant impact to economic developments and changes in social relations and social networks.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"Nevertheless, there is a gap between the original goals of the founders of AI as a scientic discipline and current systems implementing stateof-the-art AI technologies. Whereas the original dream was to develop general-purpose 219      220 Theoretical Foundations of Articial General Intelligence systems (like the famous general problem solver [19]), present AI systems are highly specialized, designed for a very particular domain, mostly without any generalization capabilities. Possible solutions for developing systems that approximate intelligence on a human scale are currently far from being achievable. This situation is unsatisfactory, at least if one does not want to give up the original motivation for the birth of AI as a discipline. Articial general intelligence (AGI) addresses precisely this gap between current AI systems and the obvious lack in providing solutions to the hard problem of modeling general intelligence, i.e. intelligence on a human scale. Higher cognitive abilities of humans fan out to the whole breadth of aspects that are usually examined in cognitive science.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"Humans are able to communicate in natural language, to understand and utter natural language sentences that they never heard or produced themselves, to solve previously unseen problems, to follow effectively strategies in order to achieve a certain goal, to learn very abstract theories (as in mathematics), to nd solutions to open problems in such abstract disciplines etc. Although research has been examining frameworks for modeling such abilities, there are currently no good theories for computing creativity and productivity aspects of human cognition. This chapter addresses the problem of how creativity and productivity issues can be modeled in articial systems. Specically, we propose to consider certain cognitive mechanisms as a good starting point for the development of intelligent systems. Such mechanisms are, for example, analogy-making, concept blending, and the computation of generalizations. In this text, we want to focus primarily on concept blending as an important cognitive mechanism for productivity. The text has the following structure: Section 12.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"2 relates creativity abilities of natural agents to cognitive mechanisms that can be considered as a basis for explaining these abilities. Section 12.3 discusses cross-domain reasoning mechanisms that are important for the creative transfer of information from one domain to another domain. By such mechanisms it is possible to associate two domains that are originally independent and unconnected from each other. In section 12.4, some basic formal aspects of blending are introduced and section 12.5 models in detail the historically important creative invention of the complex plane in mathematics by blending processes. Section 12.6 gives an outlook for next generation general intelligent systems and section 12.7 concludes the chapter.      Theory Blending as a Framework for Creativity in Systems for General Intelligence 221 12.2 Productivity and Cognitive Mechanisms Creativity occurs in many different contexts of human activity.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"It is not only a topic for artists or employees in the advertising industry, but also a topic in basic recognition processes, in the usage of natural language, in scientic disciplines, in solving problem tasks, in teaching situations, in developing new engineering solutions etc. The following list explains some examples in more detail.  Besides the fact that metaphors in natural language are a strong tool to express propositions, feelings, warnings, questions etc. that are sometimes hard to express directly, they are moreover a possibility to establish connections between different, seemingly unconnected domains in order to facilitate learning. For example, if a high school teacher utters a sentence like Gills are the lungs of sh she expresses that lungs have the function in (lets say) mammals like gills have in sh. This is a remarkable interpretation for a sentence that is literally just semantic nonsense. In order to generate such an interpretation, the interpreter needs to be creative.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"An analogy-based framework for modeling metaphors can be found in [12].  One of the classical domains for creativity of humans is the problem solving domain. Examples for problem solving are puzzles (like the tower of Hanoi problem), intelligence tests, scientic problem solving (like solving a problem in mathematics), or inventive problem solving in the business domain. To nd solutions for a problem, humans show a remarkable degree of creativity. A classical reference for analogy-based problem solving is [13].  Inventions in many domains are another prototypical example where creativity plays an important role. Besides the huge potential for business applications, also scientic disciplines are on a very basic level connected to inventions. For example, the human mind needs to be rather creative in order to come up with new concepts in rather abstract domains like mathematics.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"How is it possible that humans invent concepts like real numbers, complex numbers, Banach spaces, or operator semi-groups in Hilbert spaces? We will discuss the example of inventing the complex plane in mathematics in more detail below.  The abstract concept of nesting an object in another object (quite often a similar one), is rst of all a slightly abstract but simple idea. Usually it is referred to in design communities as the nested doll principle originating from the Russian Matryoshka doll, where dolls are contained in other dolls. The potential in building products out      222 Theoretical Foundations of Articial General Intelligence of this simple idea is remarkable: planetary gearing, nesting tables and bowls, trojans (computer virus), self-similar fractals etc. are just some examples where this concept is used for designing products.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"In order to generate a conceptualization of such a general idea you need to transfer an abstract principle to completely unrelated domains, especially to such domains where it is not already obvious how this principle can be instantiated. We claim that several forms of creativity in humans can be modeled by specic cognitive mechanisms. Examples of such mechanisms are analogy-making (transferring a conceptualization from one domain into another domain), concept blending (merging parts of conceptualizations of two domains into a new domain), computation of generalizations (abstractions), just to mention some of them. These mechanisms are obviously connected to other more standard cognitive mechanisms, like learning from sparse data or performing classical forms of inferences like deduction, induction, and abduction. We think that computational frameworks for the mentioned cognitive mechanisms are a way to implement creativity in machines. This may be considered as a necessary, although not a sufcient, step in order to achieve intelligence on a human scale, i.e.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"the ultimate goal of articial general intelligence. 12.3 Cross-Domain Reasoning In this text, we focus primarily on theory blending as a cognitive mechanism for modeling creativity. Nevertheless, from a more abstract perspective it is important to mention that blending, as well as analogy-making and the computation of generalizations are specic types of cross-domain reasoning. Establishing connections between seemingly unconnected domains is a highly important part of creativity as is obvious from the examples given in section 12.2 (compare for example the nested doll principle or metaphors in natural language). In this section, we introduce various cross-domain mechanisms, previously already specied in [17]. In each case, we illustrate the mechanism with an example taken from mathematics, without really going into the technical details (we provide the relevant references instead).",Theoretical Foundations of Artificial General Intelligence,chapter 12
"We hope that by the end of this section the reader will have an appreciation of why we think mathematics is a good domain to exemplify and study creativity from a cognitive point of view: mathematical thinking spans a broad spectrum of domains, from the very concrete to the very abstract and requires the creative use of cognitive abilities that are not specic to mathematics. Section 12.5 will revisit some of the mechanisms      Theory Blending as a Framework for Creativity in Systems for General Intelligence 223 introduced here, in the context of a worked-out historical example of mathematical creativity. All approaches to blending take knowledge to be organized in some form of domains, i.e. the underlying knowledge base provides concepts and facts in groups that can serve as input to the blending process. The different domains may in principle be incoherent and even mutually contradictory but they are nonetheless interconnected in a network organized by relations like generalization, analogy, similarity, projection, and instantiation.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"There are many examples in the literature on blending about conceptual integration in mathematics [1,6,15]. For example, [15] is a careful account of how complex mathematical notions are grounded on a rich network of more basic ones via metaphorical mappings and conceptual blends. The most basic linkages in this network consist of grounding metaphors through which mathematical notions acquire meaning in terms of everyday domains. For instance, basic arithmetic can be understood metaphorically in terms of four everyday domains: object collections, motion along a path, measuring with a unit rod, and Lego-like object constructions. Besides basic grounding metaphors there are linking metaphors and blends. These in turn can map domains that are not basic but either the target domains of metaphors or conceptual blends. The idea of the complex plane discussed below involves a blend of already non-basic algebraic, numerical, and geometric notions. We use Heuristic-Driven Theory Projection (HDTP) as the underlying framework.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"Originally developed for metaphor and analogy-making (see [12] and [21] for details), HDTP has been applied to many different domains and has been extended in various directions, among other things, to cover also theory blending. In HDTP, domains are represented via nite rst-order axiomatizations (dening domain theories) and intra-domain reasoning can be performed with classical logical calculi. Moreover, cross-domain reasoning can also be allowed, in many different ways. (1) Analogy-making: quite often identied as a central mechanism of cognition [7], the establishment of an analogical relation between two domains is based on identifying structural commonalities between them. Given an analogical relation, knowledge can be transferred between domains via analogical transfer. HDTP applies the syntactic mechanism of anti-unication [20] to nd generalizations of formulas and to propose an analogical relation (cf. Figure 12.1).",Theoretical Foundations of Artificial General Intelligence,chapter 12
"Several of the mechanisms listed next are actually supported by analogy-making and transfer. Analogical transfer results in structure enrichment of the target side. In the HDTP framework such enrichment usually corresponds to the addition of new axioms to the      224 Theoretical Foundations of Articial General Intelligence Generalization (G) KKKKKKKK K K ssssssssss SOURCE (S) analogical relation TARGET (T) Fig. 12.1 Analogy via generalization in HDTP target theory, but may also involve the addition of new rst-order symbols. When using a sortal logic, even new sorts can be added to the language of the target theory. There are cases in which analogical transfer is desired in order to create a new enriched domain, while keeping the original target domain unchanged. In such cases the generalization, source, target, and enriched domains are interconnected by a blend (see the next section for a depiction of how a blend interconnects four domains).",Theoretical Foundations of Artificial General Intelligence,chapter 12
"An example of this kind of blending, and of structure enrichment involving new sorts, is the blend of (partial formalizations of) the domains MEASURING STICK (MS) (a metaphor that grounds the idea of discreteness) and MOTION ALONG A PATH (MAP) (grounding the idea of continuity) discussed in [10]. The blended domain is an expansion of MAP in which a notion of unit (taken from MS) is added. It can be thought of as a partial axiomatization of the positive real number line with markings for the positive integers. Without going into the formal details, in this example the blended domain (theory) comes from enriching the rst-order theory by which MAP is represented with a subsort of whole units, and then importing the axioms of MS into it, in a form that relativizes them to the subsort.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"(2) Cross-domain generalization: As an example, an abstract approximation of naive arithmetic may be obtained by iterative pairwise generalizations of the basic grounding domains of Lakoff and Nez mentioned above. These basic domains are analogous in various aspects that can then be generalized1. The fact that HDTP always computes a generalization when it establishes an analogy between two domains was used in [11] to implement this example. Figure 12.2 shows that calculating an analogy between the rst generalization and yet a third basic domain produces an even more generalized proto-arithmetic domain. (3) Cross-domain specialization: in the HDTP framework, after a generalized theory G has been obtained, the process of translating terms or properties from G to one of the 1For example, joining object collections and putting together linear constructed Lego-objects are both commutative operations.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"     Theory Blending as a Framework for Creativity in Systems for General Intelligence 225 Generalization (ARITHMETIC-2) ;;;;;;;;;;;;;;;; Generalization (ARITHMETIC-1) D D D D D D D   transfer   xxxxx x OBJECT COLLECTION OBJECT CONSTRUCTION MOTION ALONG A PATH Fig. 12.2 Computing generalizations from Lakoff and Nez grounding metaphors for arithmetics domains that it generalizes can be thought of as a process of cross-domain specialization. A key concern is to nd conditions under which, say, the translation of any rst-order proof done inside G (say Arithmetic-1 in Figure 12.2) is also a proof inside each of the domains G generalizes (say Object Collection in Figure 12.2). A discussion on such conditions, formulated in the theory of institutions, can be found in [14].",Theoretical Foundations of Artificial General Intelligence,chapter 12
"(4) Detection of congruence relations: the detection of weak notions of equality (structural congruences) is pervasive in mathematics and beyond mathematics, as we will discuss. The issue is central even for the basic question of how humans can arrive at the notion of fractional numbers based only on basic grounding everyday domains like the four proposed in [15]. In [10], this challenge is approached using the fact that HDTP can in principle detect when some relation R in one of the input domains D1 and D2 is analogous to the equality relation in the other domain. This can then be used as a trigger for further processing, namely trying to establish if R is a congruence. If successful, the process should result in a diagram like Figure 12.3. D1 onto  onto  D2 isom  Q Fig. 12.3 Diagram of a quotient Q over a domain D1.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"Intuitively, at the level of models the elements of D1 are mapped to their equivalence classes in the quotient Q. While processes of quotient construction in the above sense may seem particular to mathematics, the facts show otherwise. Forming a quotient involves identifying a notion of      226 Theoretical Foundations of Articial General Intelligence weak equality on a domain D1, deriving from it a useful way to categorize the entities of D1, and obtaining a theory of the space of categories. Each one of these aspects is important from the perspective of creativity in general cognitive systems. For example, it is a fact that through history humans have created many novel representation systems, of diverse kinds. One case in mathematics is the positive real numbers as a representation system for segments.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"The positive number line can be thought of as a quotient domain (Q) of the domain of physical segments (D1) obtained thanks to the identication of segments of the same length (same image in the domain D2 of positive reals) with the unique segment in the number line that has that length. By virtue of the isomorphism between the positive number line and the positive reals, one can also see the quotient Q as a geometric, more concrete model of the theory of the reals. We conclude this section by noticing that the arrows in Figure 12.3 are labeled. The labels indicate semantic constraints on how the (unspecied but presupposed) universe of one domain can be mapped into another domain. The fact that we need the labels in Figure 12.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"3 to better approximate our intuition of what a quotient is, and the many examples we will see in our case study later, suggest that attaching semantic labels to the HDTP syntactic theory morphisms may turn out to be very useful. The semantic constraints of a theory morphism relating domain theories T1 and T2 may say things such as the universe T1 is about is embedded in the universe T2 is about. The complex plane example below will better illustrate what sorts of dynamics may be evoked by the kind of semantic labeling of morphisms we are proposing. 12.4 Basic Foundations of Theory Blending To provide a formal basis for relating different domains to create new possible conceptualizations in a blending process we start with Goguens version of concept blending as given in [9], in particular in the form presented by his gure 3 (p. 6) here represented in Fig. 12.4.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"The idea is that given two domain theories I1 and I2 representing two conceptualizations we look for a generalization G and then construct the blend space B in such a way as to preserve the correlations between I1 and I2 established by the generalization. The morphisms mapping the axioms of one theory to another are induced by signature morphisms mapping the symbols of the representation languages. Symbols in I1 and I2 coming from the same symbol in G correspond to each other in an analogical manner. Due to possible      Theory Blending as a Framework for Creativity in Systems for General Intelligence 227 B I1         I2 ???????? G         ??????? Fig. 12.4 The diagram of a blend. incompatibilities between I1 and I2 the morphisms to B may be partial, in that not all the axioms from I1 and I2 are mapped to B.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"Goguen uses a generalized push-out construction2 to specify the blend space B in such a way that the blend respects the relationship between I1 and I2 implicitly established by the generalization. That means that B is the smallest theory comprising as much as possible from I1 and I2 while reecting the commonalities of I1 and I2 encoded in the generalization G. A standard example discussed in [9] is that of the possible conceptual blends of the concepts HOUSE and BOAT into both BOATHOUSE and HOUSEBOAT (as well as other less obvious blends). In a very simplistic representation we can dene HOUSE as I1 = { HOUSE   LIVES-IN . RESIDENT} and BOAT as I2 = { BOAT   RIDES-ON . PASSENGER}. Parts of the conceptual spaces of HOUSE and BOAT can be aligned, e.g.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"RESIDENT  PASSENGER, LIVES-IN  RIDES-ON, HOUSE  BOAT (resulting in a conceptualization of HOUSEBOAT, while RESIDENT  BOAT results in a conceptualization of BOATHOUSE). Conceptual blends are created by combining features from the two spaces, while respecting the constructed alignments between them. The blend spaces coexist with the original spaces: HOUSE and BOAT are kept untouched although there might be new relations between BOATHOUSE and HOUSE or BOAT. Hence, the blend space is not the result of extending one of the given spaces with new features or properties, but constructing a new one. Conceptual blending gives us a way to understand how theories that would simply be inconsistent if crudely combined, can nevertheless be blended by taking appropriately chosen parts of the theories, while respecting common features. 2For a formal denition of the category theoretic construction of a push-out compare for example [16].",Theoretical Foundations of Artificial General Intelligence,chapter 12
"     228 Theoretical Foundations of Articial General Intelligence 12.5 The Complex Plane: A Challenging Historical Example For a long time, the mathematical community considered the complex numbers to be only algebraic scaffolding, tools that carefully used as if they were numbers could facilitate calculations and discoveries about real numbers. One problem was that, while real numbers could be naturally conceived as distances from a reference point in a line, it was not clear at all whether a number of the form a + bi (where a and b are real numbers and i2 = 1) could correspond to any intuitive notion of magnitude. The discovery of the complex plane played a very important role in the acceptance of complex numbers as proper numbers.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"It turned out that the complex number a + bi corresponds to an arrow (vector) starting from the origin of a cartesian system of coordinates and ending at the coordinate (a,b), and the basic arithmetical operations on complex numbers such as sum and product have geometric meaning. The complex plane representation of complex numbers is a prime historical example of creativity in mathematics, one that involves devising a novel domain (the complex plane) through which a known one (the domain of complex numbers as pure algebraic entities) can be understood in geometrical terms. In this section, we reconstruct in our terms J. R. Argands discovery of the complex plane in 1806, according to his own report [2]. Our aim is to illustrate a real case of discovery that involved the construction of a network of interrelated domains, obtained by either: (1) retrieval from memory, (2) analogical construction of a new domain, or (3) blending.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"The network of domains is shown in Figure 12.5. We will describe the way in which this graph is formed, presenting only minimal partial formalizations of the domains, and instead focusing on the motivations and constraints that guide the creative process in this case study. This section is partially based on ideas rst presented in [17]. According to [22], Argands discovery of the complex plane arose from thoughts not directly related to the complex numbers. His starting point was the observation that negative numbers may not seem real if you restrict yourself to magnitudes such as the size of object collections, but they do make sense when measuring weight in a two-plate scale. In such context, there is a natural reference point (an origin or zero) and a displacement of a plate with respect to that point in one of two possible directions: upwards or downwards.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"In general, creative production is in numerous (maybe most) occasions a serendipitous process, and one would like human-level articial intelligent systems to have the ability to be creative on the spot, as the opportunity arises, not only in settings where a determinate goal has been set. However, it is fair to say that current articial creative systems work by trying to achieve rather unconstrained but still pre-given goals (e.g. compose a narrative      Theory Blending as a Framework for Creativity in Systems for General Intelligence 229 REALS+ isom   add sign  NL+  add direction  SEGMENTS  add direction  onto  REALS isom   id  NL  vvvvvvvvvvv  id   DDDDDDDDDDD D D COMPLEX ARITHMETIC (CA) isom GGGGGGGGGG G G VECTORS onto {{{{{{{{{{{{{ COMPLEX PLANE (CP) Fig.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"12.5 Domains involved in Argands reasoning. Arrows indicate the direction of (partial) theory morphisms. Arrow labels indicate constraints at the level of models. Curved tails are like an injective label on the arrow. out a given set of old narratives, or compose a new musical piece in the style of certain composer). It is still a challenge to make them able, as Argand was, of posing interesting creative goals, desires, or questions out of casual observations. From a network-of-domains viewpoint, Argands observation starts forming the network in Figure 12.5 by recruiting the four domains in the upper left area. The number line representation (NL) is an already familiar domain, retrieved from memory, a geometric representation of the real numbers. Similarly, Argand remarks that the right side of the number line (NL+) is a geometric representation of the positive real numbers.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"To clarify the nature of the arrows (morphisms) in the diagram, take the example of the arrow from NL+ to NL. There are two aspects to this arrow. Its direction indicates that there is a partial morphism3 from the rst-order theory NL+ to the rst-order theory NL. This accounts for the purely syntactic aspect of the arrow. There are also two semantic markers that together indicate how the intended universes of the domains relate to each other. Namely, the curved tail says that there is a total embedding from the universe of NL+ to the universe of NL that preserves the truth of the formulas mapped by the syntactic morphism, and the add direction marker is a construction marker (a program, maybe) that says how the embedding is actually constructed. 3This morphism is a partial function mapping a subtheory of the domain into the codomain.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"     230 Theoretical Foundations of Articial General Intelligence As we will see, with the exception of the complex plane (CP) and the vector domain (VECTORS), which were Argands creation, all other nodes in Figure 12.5 and the arrows connecting them were previously known to him. Hence, the corresponding subgraph is part of a long-term memory of interrelated domains. The diamond formed by the novel domains CP and VECTORS plus the complex arithmetic domain (CA) and the number line (NL) domain is a blend that crowns the whole process we will now describe. Argands positive number line NL+ included notions of proportions (a:b::c:d) and geometric mean. The geometric mean of two segments a and b in NL+ is the unique x such that a:x::x:b.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"It can be obtained algebraically as the solution of xx = ab, or via a geometric construction in SEGMENTS, the result of which (a segment) is mapped back to NL+. Argand uses the initial four-domains network to pose his rst interesting question. Argands Challenge 1: Is there a notion of geometric mean in NL that extends that of NL+ and has also natural algebraic and geometric corresponding notions? To address the problem, Argand uses two consecutive analogical steps: (1) He detects the analogy between the add sign and add direction semantic markers: every real number is either positive or r for some positive real r, and each segment in the number line lies in the positive side of it or its (geometric) opposite. This highlights the domain of real numbers as a candidate for providing an algebraic interpretation of the extended notion of geometric mean.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"(2) Ensuring an analogy with the geometric mean in NL+, he proposes as denition that the magnitude of the geometric mean of two entities in NL is the geometric mean of their magnitudes and its direction is the geometric mean of their directions (the two possible directions are represented by the segments +1 and 1). The heart of the initial challenge can now be stated in terms of the newly created notion of geometric mean of directions. Argands Challenge 1a: Find a segment x such that 1:x::x:1. The solution must have both algebraic and geometric readings, as does everything in NL. Algebraically, moving to the isomorphic domain of reals, the problem is nding a real number x such that x  x = 1  1 = 1. No such x exists in the reals, but the domain of complex numbers is recruited as an extension of the reals where there is a solution4.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"On the geometric side, by analogy with the add direction construction, Argand creates from the domain of SEGMENTS a new domain of directed segments, or VECTORS, with 4It is here that by serendipity, Argand ends up thinking about the complex numbers in a geometric context.      Theory Blending as a Framework for Creativity in Systems for General Intelligence 231 types real, vector constants 0, 1: vector 0, 1, 90, 180, 360: real functions | |, angle: vector  real : vector  vector rotate, scale: vector  real  vector +v, project: vector  vector  vector +, , +360: real  real  real predicates proportion: vector  vector  vector  vector geomean: vector  vector  vector : vector  vector laws   v (rotate(rotate(v,90),90) = v) rotate(rotate(1,90),90) =",Theoretical Foundations of Artificial General Intelligence,chapter 12
"1   v (rotate(rotate(v,),) = rotate(v, +360 )) 90+360 90 = 180 v a b (v = scale(rotate(1,a),b)) v (v = scale(v,1)) v1v2v3v4 (proportion(v1,v2,v3,v4)  ab(v2 = rotate(scale(v1,a),b)v4 = rotate(scale(v3,a),b))) v1v2v3 (geomean(v1,v2,v3)  proportion(v1,v3,v3,v2)) (abelian group axioms for (+v,) and +360) (add also the missing vector space axioms for scale and +v) (congruence axioms for ) (imported axioms from the reals for +,) Fig. 12.6 Partial formalization of the VECTORS domain a designated unit vector.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"Vectors can be stretched by a real factor, added, rotated, and projected. Figure 12.6 has a very partial and redundant axiomatization of this domain. The +360 stands for real addition modulo 360 degrees, for adding angles. Argands realization that the geometric mean of 1, 1 should be a unitary vector perpendicular to 1 corresponds to the fact that from our VECTORS axioms one can prove geomean(1,1,rotate(1,90)). With this notion of geometric mean that makes sense algebraically and geometrically, Argand nally comes up with the key question about complex numbers. Argands Challenge 2: Can a geometric representation for the complex numbers be obtained from the vector plane? In our terms, the problem is to nd a quotient blend space CP with inputs CA and VECTORS and the additional constraint that NL must be embedded in it.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"To see how this could get started by using the HDTP framework, Figure 12.7 gives a partial axiomatization of the domain CA of complex numbers as objects of the form a + bi. When comparing the axiomatizations of CA and VECTORS, the HDTP setup can detect that the rst formulas in the domains are structurally the same. A generalization G will be created that, as shown in Figure 12.8, will include an axiom X(F(F(X,Y),Y) = X) plus substitutions that lead back from G to the CA and VECTORS. These substitutions will map the term F(X,Y) to the terms rotate(v,90) and i  z respectively.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"Given the intended      232 Theoretical Foundations of Articial General Intelligence types real, complex (with real being a subsort of complex) constants i: complex 0, 1: real functions re, im: complex  real +c, : complex  complex  complex +, : real  real  real : complex  complex laws z (i(iz) = z) ii = 1 z a b (z = a+bi  a = re(z)b = im(z)) z z (re(z+c z) = re(z)+re(z)) z z (im(z+c z) = im(z)+im(z)) z a b (z = a+bi)) z z (re(zz) = re(z)re(z)im(z)im(z)) z z (im(zz) = re(z)im(z)+im(z)re(z)) (abelian group axioms for +c) (add also the missing eld axioms",Theoretical Foundations of Artificial General Intelligence,chapter 12
"for +c and ) (imported axioms from the reals for +,) Fig. 12.7 Partial formalization of the CA domain (GENERALIZATION) X(F(F(X,Y),Y) = X) Frotate, Y90, Xv RRRRRRRRRRR R R F, Yi, Xz ppppppppppp (CA) z (i (i z) = z) analogical relation (VECTORS) v (rotate(rotate(v,90),90) = v) Fig. 12.8 Mapping from the generalization to CA and VECTORS. meaning of our axiomatizations, this corresponds to detecting that rotating vectors by 90 degrees is analogous to multiplying by i in the complex arithmetic domain. This is the key observation on which Argand initiated his construction of the complex plane.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"The generalization G will include other things such as an operation G with axioms of Abelian group that generalizes +v and +c. We skip the full details here. The example in this section shows the potential of the logic approach in modeling the role of very general cognitive mechanisms such as analogy-making, generalization, and blending in creativity. It illustrates ways in which our approach can suggest principled computational strategies for performing creative exploration and on the spot proposal of interesting creative tasks. Also, the strategies and issues shown here are general, relevant to many other subjects beyond mathematics. One example of this is the issue of construction of a context of relevant domains that may later facilitate the formation of blending domains. We saw that this can happen in at least two ways. First, such a context can provide the      Theory Blending as a Framework for Creativity in Systems for General Intelligence 233 motivation for nding a particular blend between two spaces.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"For instance, the motivation for the nal blend in this section was a meta-analogy: can we get a geometric representation just as there is one in the network for the positive reals? Second, a context of relevant domains can suggest adequacy constrains to be imposed on a the blend to be created. In our blend example, two constraints went beyond the general diagram of a blend: the relevant part of NL must be embedded into the new blended domain CP, and CP must be the bottom domain of a quotient (i.e. play the role of Q in Figure 12.3). 12.6 Outlook for Next Generation General Intelligent Systems The use of cognitively inspired techniques like conceptual blending or reasoning based on analogy and similarity in AI systems is (with few exceptions) to a large extent still in its very early infancy.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"In this section, continuing the overall argument from earlier sections, we will argue why accounting for an integration of these capacities when designing a general intelligent system might be a good and quite rewarding idea. Therefore, we will give an overview of different domains and scenarios, in which the availability of concept blending abilities can possibly make a signicant difference to state-of-the-art systems. Over the last decades, AI has made signicant progress in the eld of problem solving. Still, in most cases, the power of the current approaches is limited, as the danger of the often feared combinatorial explosion and the abyss of underspecication of problems are ubiquitous. For certain problems, conceptual blending might offer an alternate way to a solution, avoiding the classical mantraps AI researchers are struggling with.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"One of the most often cited examples is the Riddle of the Buddhist Monk [5]: A Buddhist monk is said to begin with the climb of a mountain at dawn, reaches the top at sunset, spends the night there, and again at dawn begins with the descent, reaching the foot of the mountain at sunset. Now, without making any further assumptions, it shall be decided whether there exists a place on the path which the monk occupies at the same hour of the day on both trips. Instead of computing for a sufciently ne-grained discretization of the monks path the placetime correspondences of both days and checking whether there is such a place (which would also only be possible under violation of the prohibition of further assumptions, as some estimation concerning the monks moving speed etc. would have to be made), conceptual blending offers an elegant solution to the problem.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"Instead of treating both days as separate time spans, they are blended into one, resulting in a scenario in which there seem to be two monks, one starting from the peak, one starting from the foot      234 Theoretical Foundations of Articial General Intelligence of the mountain, moving towards each other, and meeting exactly in the place the riddle asks for (thus not only giving an answer in terms of existence of the place, but also in a constructive way providing implicit information on its location). Although to the best of our knowledge there is not yet a clear characterization of the class of problems in which conceptual blending is rst choice, we are positive that a blending mechanism can nd application in a great number of practical problems. A second domain in which concept blending (and possibly also analogy, cf. [3]) may be of use is the area of rationality and rational behavior. For many years, researchers from different areas (e.g.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"game theory, psychology, logic, and probability theory) have tried to nd feasible theories and models for human reasoning and human-style rationality. Sadly, the results of these efforts, though being highly elaborate and theoretically wellfounded, mostly fall short when actually predicting human behavior. One of the classical examples in which most of the probability-based models reach their limits is Tversky and Kahnemans Linda Problem [23]: Linda is described as (among others) single, outspoken and very bright, with a degree in philosophy, concerned with issues of social justice, and with a past as anti-nuclear activist. Now, several additional characteristics are offered, and subjects are asked to rank these additional properties by estimated likelihood of occurrence. There, humans show to be prone to the conjunction fallacy, as a signicant number of subjects ranks the option Linda is a bank teller and is active in the feminist movement. higher than the single Linda is a bank teller.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"contradicting basic laws of probability. Again, conceptual blending offers a way out: Considering a blend of both concepts bank teller and feminist, maintaining some properties of a clich account of bank teller, and adding key properties from a clich feminist concept, which better t the initially given properties of Linda, subjects behavior can be explained and predicted (for a more detailed treatment cf. [17]). Thus, conceptual blending gives an intuitive and comparatively simple explanation to some of the classical challenges to existing theories of rationality, making it also an interesting candidate for inclusion in future general intelligent systems, which undoubtedly will have to deal with issues related to rationality and human behavior. Also in language understanding and production within an AI system, concept blending can provide additional functionality. Humans in many cases exhibit impressive capabilities in making sense of neologisms and previously unknown word combinations, whilst actual natural language interface systems in many cases are stretched to their limits, although the individual meanings of the combined terms might be known to the system.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"Concept blending here offers a guideline for combining these concepts into a blend, thus making      Theory Blending as a Framework for Creativity in Systems for General Intelligence 235 accessible also the resulting blended concept, as e.g. in Goguens by now classical example concerning the combined words HOUSEBOAT and BOATHOUSE mentioned in section 12.4 [8]. Therefore, an integration of concept blending faculties into a general intelligent system from a natural language interface point of view can be expected to be advantageous in both aspects, language production and language understanding: Whilst the output of the system might seem more human-like due to the occasional use of combined words based on blends, also the language input understanding part might prot from these capabilities, making use of blending techniques when trying to nd the meaning of unknown, possibly combined words. (For some further considerations concerning concept blending and noun noun combinations cf. [17].",Theoretical Foundations of Artificial General Intelligence,chapter 12
") A further main domain in which we expect great benet from an integration and development of concept blending capabilities into general intelligent systems is the branch of (articial) creativity. Although to the best of our knowledge until today research in articial general creativity and innovation has mostly been treated as an orphan within the AI community, a proper account of concept blending integrated into a general intelligent system might boost this branch of research in the future. Concept blending has already been identied as a key element in the concept generation stage in creative design processes [18]. It is undeniable that some form of concept blending is always present in most occurrences of human (productive) creativity over time, ranging from mythology and storytelling to product design and lifestyle.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"On the one hand, the former two provide numerous well-known examples, which by now sometimes even have become a xed part of human culture: In mythology, Pegasus is a blend between a horse and a bird and centaurs as mythological horsemen have to be considered straightforward blended concepts. More recent storytelling forms of concept blending, for instance, prominently feature in the famous 1865 novel, Alices Adventures In Wonderland, in which Lewis Carroll5 narrates various scenes that naturally call for blending by readers: In addition to the talking, dancing, wisely-speaking animals and the humanly-reied playing cards, living amingos appear as mallets and hedgehogs as balls, showing features and properties of both, their original concept and the (by the particular use) newly introduced concept. Ideas behind many modern science ction novels can also be seen as creatively generated blended concepts.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"For example, The Hunger Games [4] is a young adult science ction novel by Suzanne Collins, who claims that the idea of the novel occurred to her whilst channel surng on television. On one channel Collins observed people competing on a reality show and on another she 5Lewis Carroll is the pseudonym of the English author Charles Lutwidge Dodgson.      236 Theoretical Foundations of Articial General Intelligence saw footage of the Iraq War. The lines between the show competition and the war coverage began to blur, the two blended together in this very unsettling way, and the idea for the novel was formed. On the other hand, in recent product design, modern tablet computers can be seen as blends between journals and computers, keeping the size and format of a magazine and expanding it with the functionality of a laptop. So far, many of the examples mentioned in this chapter are related to higher-order cognitive abilities, e.g.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"problem solving, the invention of new mathematical concepts, or aspects of human rationality. Furthermore, we assumed that the input conceptualizations for the cognitive mechanisms are already given and need not to be generated by the approach. A natural question concerns the generation of such inputs for the system, in particular, if lower cognitive abilities are considered or highly structured background theories are not available, like in the case of infant learning. A nice example of such a set-up was indirectly already mentioned in Section 12.3, namely the grounding of the acquisition of a rudimentary number concept by children in real world domains including real world actions by the learning agent: in this context, we mentioned Lakoff and Nezs MOTION ALONG A PATH (MAP) and MEASURING STICK (MS) metaphor. Lakoff and Nez take an embodied and action-oriented stance, in the sense that the acquisition and rst approximations of a number concept by children is grounded in real world actions.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"The natural idea to model this is to take internal action sequences, like walking steps in the (MAP) metaphor, as initial representations for the system. Obviously, certain further action-oriented properties need to be taken into account, for example, the origin of the walking sequence or the possibility to walk back to the origin etc. Currently, we cannot give a formal approach of these issues, but we think that this type of grounding and the generation of on-the-y representations in such domains are in principle possible. Finally, we speculate about the potential breadth of applicability of concept blending as described in the HDTP framework and its covered types of reasoning. In Section 12.3 several cognitive mechanisms in the context of cross-domain reasoning mechanisms were already mentioned. These are summarized together with some additional mechanisms, like re-representation, frequency effects, and abduction, in Table 12.1. Although many interesting aspects of higher cognitive abilities are covered by these mechanisms sketched in Table 12.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"1, we do not claim that this list is in any sense complete. Besides the necessity to enable an AGI system with the mentioned mechanisms it is obvious that a system that is intended to model general intelligence in a holistic and complete sense, needs additionally several further low-level abilities. Obviously, the presented approach does not      Theory Blending as a Framework for Creativity in Systems for General Intelligence 237 Table 12.1 Many desirable functions of intelligent systems can be explained by cross-domain reasoning mechanisms. The left column lists mechanisms most of them introduced in this chapter and associates them with functions that can be based on them. Some mechanisms concern future extensions of the presented framework. The list is not intended to be complete.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"Cross-Domain Mechanism Function in Intelligent Systems Analogical Mapping Understanding of new domains; creation and comprehension of metaphorical expressions and analogies in general Analogical Transfer Problem solving; introduction of new concepts into a domain; creative invention of new concepts Generalization Learning of abstract concepts; compression of knowledge Specialization Applying abstract knowledge; problem solving by realizing a general strategy Congruence Relation Restructuring of a domain; identication of entities by a new concept Blending Creation of new concepts and theories; problem solving; human style rationality; understanding of compound nouns Re-representation Adaptation of the input domains in order to compute appropriate analogies and blend spaces Frequency Effects Probabilistic Extension for Modeling uncertainty Abduction Finding explanations of certain facts cover aspects like reactive behaviors, sensorimotor loops, or certain attention mechanisms. These challenges should be addressed by other mechanisms that are not directly related to analogy-making and concept blending, because the conceptual level is in such low-level cognitive abilities most often missing.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"The integration of higher and lower cognitive mechanisms could be achieved by a hybrid architecture, presently a rather standard approach for integrating heterogeneous methodologies. Taking all this evidence together, we consider concept blending as one of the cornerstones (maybe even a conditio sine qua non) of an architecture for general intelligent systems that also shall exhibit creative behavior and signicant capabilities in the eld of creativity, which in turn might possibly also open up new application areas and industry options to AI, as e.g. computational support systems for innovation and creativity in (product) design, architecture and product-creating arts.      238 Theoretical Foundations of Articial General Intelligence 12.7 Conclusions Articial general intelligence attempts to nd frameworks and to implement systems that can operate not only in a highly specialized domain, namely the domain they were built for, but that show also generalizability properties, i.e. they can operate even in domains they never experienced before.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"Besides advanced learning capabilities and many classical technologies from AI, such systems should have, as a necessary prerequisite, a creativity potential for nding solution strategies in unknown problem solving set-ups. We claim that the best way to tackle the creativity problem in systems for general intelligence is to consider the underlying cognitive mechanisms that allow humans to be creative to a remarkable degree. This chapter addresses this issue by proposing cognitive mechanisms like analogy-making and theory /concept blending as a source for creativity. Both mechanisms fall under a more general human ability, namely cross-domain reasoning, summarizing several non-classical forms of reasoning that enable the human being to associate domains that are usually considered to be disconnected. We exemplied in some detail the cognitive mechanism theory blending in a concrete example of the invention of the complex plane in mathematics. Although it may seem to be the case that the domain of mathematics is a relatively special case for a general mechanism, we think that the applicability of blending to other domains, problems, set-ups etc.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"is rather straightforward. Some ideas for this claim are provided in Section 12.6. We do not claim that the considerations presented in this chapter nally solve the task to make intelligent systems creative. They are rather a rst (but important) step towards a theory of creativity. Many formal and methodological challenges still need to be resolved for a uniform framework of creativity. Furthermore, the integration of several non-standard reasoning techniques in one architecture remains a challenge for the future. Although the mentioned HDTP framework integrates several of the mechanisms occurring in this chapter, further work is necessary towards an integration of the methodological variety. Bibliography [1] James Alexander. Blending in mathematics. Semiotica, 2011(187):148, 2011. [2] Jean-Robert Argand. Philosophie mathmatique. Essai sur une manire de reprsenter les quantits imaginaires, dans les constructions gomtriques.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"Annales de Mathmatiques pures et appliques, 4:133146, 1813. [3] T. R. Besold, H. Gust, U. Krumnack, A. Abdel-Fattah, M. Schmidt, and K.-U. Khnberger. An argument for an analogical perspective on rationality & decision-making. In Jan van Eijck and Rineke Verbrugge, editors, Proceedings of the Workshop on Reasoning About Other Minds:      Bibliography 239 Logical and Cognitive Perspectives (RAOM-2011), Groningen, The Netherlands, volume 751 of CEUR Workshop Proceedings, pages 2031. CEUR-WS.org, July 2011. [4] Suzanne Collins. The Hunger Games. Scholastic, 2010. [5] Gilles Fauconnier and Mark Turner. Conceptual integration networks.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"Cognitive Science, 22(2):133187, 1998. [6] Gilles Fauconnier and Mark Turner. The Way We Think: Conceptual Blending and the Minds Hidden Complexities. Basic Books, New York, 2002. [7] D. Gentner, K. Holyoak, and B. Kokinov, editors. The Analogical Mind: Perspectives from Cognitive Science. MIT Press, 2001. [8] Joseph Goguen. An introduction to algebraic semiotics, with application to user interface design. In Computation for Metaphors, Analogy, and Agents, volume 1562 of Lecture Notes in Computer Science, pages 242291. Springer, 1999. [9] Joseph Goguen. Mathematical models of cognitive space and time. In D. Andler, Y. Ogawa, M. Okada, and S.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"Watanabe, editors, Reasoning and Cognition: Proc. of the Interdisciplinary Conference on Reasoning and Cognition, pages 125128. Keio University Press, 2006. [10] Markus Guhe, Alison Pease, Alan Smaill, Maricarmen Martinez, Martin Schmidt, Helmar Gust, Kai-Uwe Khnberger, and Ulf Krumnack. A computational account of conceptual blending in basic mathematics. Journal of Cognitive Systems Research, 12(3):249265, 2011. [11] Markus Guhe, Alison Pease, Alan Smaill, Martin Schmidt, Helmar Gust, Kai-Uwe Khnberger, and Ulf Krumnack. Mathematical reasoning with higher-order anti-unifcation. In Proceedings of the 32st Annual Conference of the Cognitive Science Society, pages 19921997, 2010.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"[12] Helmar Gust, Kai-Uwe Khnberger, and Ute Schmid. Metaphors and Heuristic-Driven Theory Projection (HDTP). Theoretical Computer Science, 354:98117, 2006. [13] Douglas Hofstadter and the Fluid Analogies Research Group. Fluid Concepts and Creative Analogies. Computer Models of the Fundamental Mechanisms of Thought. Basic Books, New York, 1995. [14] U. Krumnack, H. Gust, A. Schwering, and K.-U. Khnberger. Remarks on the meaning of analogical relations. In M. Hutter, E. Baum, and E. Kitzelmann, editors, Articial General Intelligence, 3rd International Conference AGI, pages 6772. Atlantis Press, 2010. [15] George Lakoff and Rafael Nez. Where Mathematics Comes From: How the Embodied Mind Brings Mathematics into Being.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"Basic Books, New York, 2000. [16] Saunders Mac Lane. Categories for the Working Mathematician. Spinger, Berlin, 2nd edition, 1998. [17] M. Martinez, T. R. Besold, A. Abdel-Fattah, K.-U. Khnberger, H. Gust, M. Schmidt, and U. Krumnack. Towards a domain-independent computational framework for theory blending. In AAAI Technical Report of the AAAI Fall 2011 Symposium on Advances in Cognitive Systems, pages 210217, 2011. [18] Yukari Nagai. Concept blending and dissimilarity: factors for creative concept generation process. Design Studies, 30:648675, 2009. [19] Allen Newell and Herbert Simon. GPS, a program that simulates human thought. In E. Feigenbaum and J. Feldmann, editors, Computers and Thought, pages 279293.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"McGraw-Hill, 1963/1995. [20] Gordon D. Plotkin. A note on inductive generalization. Machine Intelligence, 5:153163, 1970. [21] Angela Schwering, Ulf Krumnack, Kai-Uwe Khnberger, and Helmar Gust. Syntactic principles of Heuristic-Driven Theory Projection. Journal of Cognitive Systems Research, 10(3):251 269, 2009. [22] Jaqueline Stedall. Mathematics emerging:a Sourcebook 15401900. Oxford University Press, Oxford, 2008. [23] A. Tversky and D. Kahneman. Extensional versus intuitive reasoning: The conjunction fallacy in probability judgement. Psychological Review, 90(4):293315, 1983.",Theoretical Foundations of Artificial General Intelligence,chapter 12
"  Chapter 13 Modeling Motivation and the Emergence of Affect in a Cognitive Agent Joscha Bach Berlin School of Mind and Brain, Humboldt University of Berlin Unter den Linden 6, 10199 Berlin, Germany joscha.bach@hu-berlin.de Emotion and motivation play a crucial role in directing and structuring intelligent action and perception. Here, we will look at the relationship between motives, affects and higherlevel emotions, and give directions for their realization in computational models, with a focus on enabling autonomous goal-setting and learning. 13.1 Introduction Any functional model that strives to explain the full breadth of mental function will have to tackle the question of understanding emotion, affective states and motivational dynamics (Sloman, 1981; Lisetti and Gmytrasiewicz, 2002). Consequently, the eld of affective computing has made considerable progress during the last decades. Today, several families of models are available for incorporation in agent implementations.",Theoretical Foundations of Artificial General Intelligence,chapter 13
"This chapter is not concerned with giving a comprehensive review of these approaches (for an overview on current emotion modeling, see Gratch, Marsella, and Petta, 2011; and for a look at its history Hudlicka and Fellous, 1996; Gratch and Marsella, 2005), but mainly with a particular approach: how to build a system that treats emotion and affect as emergent phenomena. I will also refrain from giving a particular implementation (if you are interested in that, see the authors work: Bach, 2009), but instead, I want to supply a general framework for emergent emotions and motivation, that could be adapted to various different agent architectures.",Theoretical Foundations of Artificial General Intelligence,chapter 13
"241      242 Theoretical Foundations of Articial General Intelligence Most emotion models start out from a set of pre-dened low-level or high-level emotions, which are characterized and implemented as stimulus related functional parameters (appraisals, see Roseman, 1991; Lazarus, 1991; Ellsworth and Scherer, 2003). These parameters give rise to complex behavioral tendencies, which can then be functionally classied (Ortony, Clore, and Collins, 1988; Plutchik, 1994). The term appraisal describes the relationship between stimulus and emotion; an appraisal is a valenced reaction to a situation, as the agent perceives it. In this view, emotions are triggered by a causal interpretation of the environment (Gratch and Marsella, 2004) with respect to the current goals, beliefs, intentions and relations of the agent.",Theoretical Foundations of Artificial General Intelligence,chapter 13
"By evaluating these, a frame of the appraisal and a corresponding affective state of the agent are set, which in turn enable it to cope with the situation. Here, coping subsumes the external and the cognitive behaviors with relation to the appraisal: actions and speech acts, as well as the modication of beliefs, intentions, goals and plans. This way, the agent inuences the external environment (the world accessible by action and communication) and the internal environment (its model of the world, along with its plans and goals) to address the issues according to their valence and context. Appraisal frame and affective state are the link between external and internal situational stimuli, and the internal and external response (see Figure 13.1).",Theoretical Foundations of Artificial General Intelligence,chapter 13
"Environment  Causal interpretation  (Goals, Beliefs, Causal Relations, Plans, Intentions)  Control Signals  Appraisal  Dialogue  Action  Coping  Explanation  Belief  Formation  Planning  Appraisal  Frames  Affective  State  Fig. 13.1 The role of appraisals in the cognitive system (Gratch and Marsella, 2004, p. 11)      Modeling Emotion and Affect 243 Appraisal models have a distinct engineering advantage: if you want to design an agent that expresses an emotion or behavioral tendency in a given situation, associating stimulus and behavior with a functional relationship is a straightforward approach. On the other hand, classical appraisal models are less suited to explain the internal cognitive dynamics and specic interactions of which emotional effects are the result, i.e.",Theoretical Foundations of Artificial General Intelligence,chapter 13
"they are more concerned with which emotion is expressed when, then what it is like to be in an emotional state, and thereby lose out not only in explanatory power, but also in functionality. In my view, neither emotion nor motivation are strict requirements for articial intelligent systems: as we will see, affective modulation and emotions may increase the efciency of the use of cognitive resources, or aid in communication, but that does hardly imply that cognition and communication would be impossible without these. (Likewise, while motivation may be a requirement for autonomous goal-directed behavior, non-autonomous AIs are entirely conceivable.) So, what exactly are the advantages of emotional systems? Emotions play a number of important roles in human cognition: They structure and lter perceptual content, according to current needs and processing requirements. They prime the retrieval of memory content, both by providing an associative context, and by informing the resolution and urgency of cognitive operations. They provide valenced feedback (e.g.",Theoretical Foundations of Artificial General Intelligence,chapter 13
"positive and negative reinforcement signals) for learning, deliberation and decisionmaking. Furthermore, emotions structure and inform social interaction between individuals, and self-reection of the individual in social contexts. The effect of emotional states on cognition amounts not just to an adaptation to external events, but in a modulation that enhances the efciency of the cognitive processing itself: emotional states may affect the mode of access to mental content, and dynamically prune representational structures and decision trees according to a motivational context and an environmental situation. Thus, any agent implementation that wants to model and perhaps utilize these benets will have to look beyond externally observable behavior and externalist emotion classication, but into the functionality of the affective and motivational system at the level of the agent architecture. In the following, I will explain how to anchor emotion and motivation within a cognitive architecture, based on the Psi theory, which originates in theoretical psychology (Drner, 1999; Drner et al.",Theoretical Foundations of Artificial General Intelligence,chapter 13
"2002) and its formalization in MicroPsi (Bach, 2003, 2007, 2009, 2011). MicroPsi does not treat affective states as distinct events or parameters, but as congurations of the cognitive system. Emotions are modeled as emergent phenomena. They can      244 Theoretical Foundations of Articial General Intelligence be classied and attributed by observing the agent (potentially even classied and observed internally, by the agent itself), but they are not pre-dened entities. Colloquially put, I will describe how to build a system that does not just simulate emotions, but that actually has emotionswithin the framework of a certain denition of having emotions, of course. Our model distinguishes between demands of the system, motives, cognitive modulators, affects and directed emotions.",Theoretical Foundations of Artificial General Intelligence,chapter 13
"Demands are pre-dened needs that give rise to goals, but whereas goals depend on the given environment, needs only depend on the agent itself and are part of its denition. When needs are signaled to the cognitive system, they may become motives, and give rise to intentions. Motives and intentions give relevance to objects of the agents perception, deliberation and action, and therefore dene higher-level emotions (such as pride, jealousy or anger). Higher-level emotions are not just characterized by relevance and object, but also by a valence (a representation of desirability) and an affective state. Affects are modulations of cognition, such as arousal, surprise, anxiety, and elation. While we will not characterize each affect or emotion individually and with all the detail that they deserve, this chapter will give an introduction on the relationship between motivation, affect and emotion, and should give you a fair idea how to implement them in an agent model.",Theoretical Foundations of Artificial General Intelligence,chapter 13
"On the following pages, we start out by characterizing emotions and affects, before we look at the relevant architectural components for realizing them. These components include a motivational system, decision making functionality, access to mental and perceptual content, cognitive modulation and facilities for reinforcement learning based on valence signals. 13.2 Emotion and affect Emotions are states and processes that inuence the allocation of physical and mental resources, perception, recognition, deliberation, learning and action. This is not a proper denition, of course: the same is true for a lot of other states that we would not intuitively call emotions. A raising or lowering of the blood pressure, for instance, does affect organisms in a similar way, but would not be called an emotion in itself. Emotions are phenomena that manifest on the cognitive and behavioral level, they are not identical to physiological mechanisms.",Theoretical Foundations of Artificial General Intelligence,chapter 13
"Furthermore, emotion needs to be distinguished from motivational phenomena: while hunger is cognitively represented (as an urge) and implies a valence and an effect on the allocation of mental and physical resources of an organism, it      Modeling Emotion and Affect 245 is not an emotion, but a motivator. Generally speaking, motivation determines what has to be done, while emotion determines how it has to be done. Dening emotions is a notoriously difcult and contested issue within psychology, so instead of trying my luck with a general denition, we might start with pointing out the following components (Diener, 1999) that a theory of emotion should account for:  Subjective experience (how it feels to be in an emotional state)  Physiological processes (neural, neurochemical and physiological mechanisms that facilitate emotion)  Expressive behavior (facial and bodily expression, movement patterns, modulation of voice and breath etc.)  Cognitive evaluations.",Theoretical Foundations of Artificial General Intelligence,chapter 13
"These components help us to characterize a range of affective phenomena, like emotional reexes (especially startling), undirected moods (euphoria, depression etc.), valenced affective states (joy, distress, anxiety), affects directed upon motivationally relevant states (jealousy, pity, pride) and affects directed upon motivationally relevant processes (disappointment, relief). Emotions are adaptive (i.e. the emotional states an organism is subjected to in a certain situation depend partly on its history of past experiences in similar situations), and the range of emotional states varies between individuals (Russel, 1995). On the other hand, it seems that emotions themselves are not the product of learning and largely invariant in dimensionality and expression (Ekman and Friesen, 1971; Izard, 1994). For instance, individuals may learn in what situations fear is appropriate or inappropriate, as well what kind of fear and what intensity.",Theoretical Foundations of Artificial General Intelligence,chapter 13
"But the ability to perceive fear itself is not acquired, rather, it stems from the way an organism is equipped to react to certain external or internal stimuli. Thus, it makes sense to develop general taxonomies of emotional states. For our purpose, we will make the following basic distinctions: (i) On the lowest level, we identify modulators of cognition, like arousal, and valence. The arousal is a cognitive parameter that controls the general activation and action readiness of an agent, and it is controlled by the urgency of its perceived demands. The valence is a reinforcement signal that emanates from changes in the perceived demands: a rapid lowering of a demand corresponds to a positive valence (pleasure), while a rapid increase in a demand corresponds to a negative valence (distress).      246 Theoretical Foundations of Articial General Intelligence (ii) Taken together, the individual modulators dene a conguration, a mode of cognitive processing.",Theoretical Foundations of Artificial General Intelligence,chapter 13
"I will call this conguration an affective state. Most affective states are valenced (i.e., perceived as desirable or undesirable), with the exception of surprise, which can have a neutral valence. (iii) Affects that have a mental representation (like a goal situation or another agent) as their object are called higher-level emotions. This includes emotions like disappointment, which is a negatively valenced affect directed upon a change in expectations, i.e. upon a process instead of a state. The relevance of the objects of affects is given by the motivational system of a cognitive agent. Without a corresponding motivator, a mental representation cannot elicit a valenced response, and is therefore emotionally irrelevant. Note that there are alternatives to this approach of capturing emotions, for instance:  Modeling emotions as explicit states. Thus, the emotional agent has a number of states it can adopt, possibly with varying intensity, and a set of state transition functions.",Theoretical Foundations of Artificial General Intelligence,chapter 13
"These states parameterize the modules of behavior, perception, deliberation and so on.  Modeling emotions by connecting them directly to stimuli, assessments or urges (like hunger or social needs) of the agent. (A similar approach has been suggested by Frijda, 1986.)  Disassembling emotions into compounds (sub-emotions, basic emotions), and modeling their co-occurrence. Some suggestions for suitable sets of primary emotions and/or emotion determinants have been made by some emotion psychologists (for instance Plutchik, 1994). Conversely, we are going to capture emotions implicitly by identifying the parameters that modify the agents cognitive behavior and are thus the correlates of the emotions. The manipulation of these parameters will modify the emotional setting of the agent, and lead to the emergence of affective states. 13.",Theoretical Foundations of Artificial General Intelligence,chapter 13
"3 Affective states emerging from cognitive modulation If affects are emergent over continuous basic parameters, they amount to areas in a continuous multidimensional space (with each dimension given by one of the modulators). One of the rst attempts to treat emotion as a continuous space was made by Wilhelm Wundt (1910). According to Wundt, every emotional state is characterized by three com     Modeling Emotion and Affect 247 ponents that can be organized into orthogonal dimensions. The rst dimension ranges from pleasure to displeasure, the second from arousal to calmness, and the last one from tension to relaxion (Figure 13.2), that is, every emotional state can be evaluated with respect to its positive or negative content, its stressfulness, and the strength it exhibits. Thus, an emotion may be pleasurable, intense and calm at the same time, but not pleasurable and displeasurable at once.",Theoretical Foundations of Artificial General Intelligence,chapter 13
"Wundts model has been re-invented by Charles Osgood in 1957, with an evaluation dimension (for pleasure/displeasure), arousal, and potency (for the strength of the emotion) (Osgood et al., 1957), and re-discovered by Ertel (1965) as valence, arousal, and potency. Wundts model does not capture the social aspects of emotion, so it has been sometimes amended to include extraversion/introversion, apprehension/disgust and so on, for instance by Traxel and Heide, who added submission/dominance as the third dimension to a valence/arousal model (Traxel and Heide 1961). Pleasure  Displeasure  Tension  Relaxation  Calmness  Arousal  Fig. 13.2 Dimensions of Wundts emotional space (see Wundt, 1910).",Theoretical Foundations of Artificial General Intelligence,chapter 13
"Note that arousal, valence and stress are themselves not emotions, but mental conguration parameters that are much closer to the physiological level than actual emotionsthey are modulators. Affects, i.e. undirected emotions, are regions within the space spanned by the modulator dimensions. The model implemented in MicroPsi suggests a very specic set of modulators, which are determined by additional architectural assumptions within MicroPsi. If your model is      248 Theoretical Foundations of Articial General Intelligence built on a different cognitive architecture, the possible set of suitable modulators will be different. MicroPsis modulators include:  Valence: events and stimuli that inuence demands of the agent are accompanied with a valence signal (pleasure or displeasure).  Arousal: this is related to the urgency of the currently active demands. High demands result in a high arousal.",Theoretical Foundations of Artificial General Intelligence,chapter 13
"The arousal has behavioral implications, it roughly corresponds to the physiological unspecic sympathicus syndrome in humans, and increases goal directedness. In biological systems, arousal also controls the allocation of physiological resources, for instance, it diverts oxygen to increase muscular responses at the cost of digestive processes.  Resolution level: speed and accuracy of perception, memory access and planning are inversely related. The resolution level controls this trade-off, in MicroPsi by adjusting the width and depth of activation spreading in the agents representations. A high resolution level results in slow processing, but a deep and detailed perceptual/memory exploration. Conversely, low resolution will speed up processing at the cost of accuracy. The resolution level is inversely proportional to the arousal.  Selection threshold: this parameter controls the stability of the dominant motive. It amounts to an adaptive stubbornness of the agent and avoids goal oscillation when demands change.",Theoretical Foundations of Artificial General Intelligence,chapter 13
"A high urgency and importance of the leading motive will increase the selection threshold, making it harder for competing motives to seize control.  Goal directedness, as mentioned before, depends on the level of arousal, and inuences the selection of action vs. deliberation/exploration strategies.  Securing rate: controls the rate of background checks of the agent, and thus the allocation of attention between perception and other cognitive processing. These dimensions account for behavioral tendencies and physiological correlates of affective states. Anger, for instance, is characterized by high arousal, low resolution, strong motive dominance, few background checks and strong goal-directedness; sadness by low arousal, high resolution, strong dominance, few background-checks and low goaldirectedness. The modulators also account for much of the hedonic aspect of emotions, i.e., the specic way emotion is reected via proprioception.",Theoretical Foundations of Artificial General Intelligence,chapter 13
"For example, the high arousal of anger will lead to heightened muscular tension and a down-regulation of digestion, which adds to the specic way anger feels to an individual.      Modeling Emotion and Affect 249 For a more detailed look at the relations between the modulator dimensions, see Figure 13.3. The states of the modulators (the proto-emotional parameters) are a function of the urgency and importance of motives, and of the ability to cope with the environment and the tasks that have to be fullled to satisfy the motives. As illustrated in Figure 13.3, a high urgency of the leading motive will decrease resolution and increase goal orientation and selection threshold (motive dominance), while less time is spent checking for changes in the background; whereas a high importance of the leading motive will increase the resolution level. Uncertainty and lack of experience (task specic competence) increase the rate of securing behavior.",Theoretical Foundations of Artificial General Intelligence,chapter 13
"A high level of condence (general competence) increases the selection threshold, and the arousal is proportional to the general demand situation (urgency and importance of all motives). Uncertainty is measured by comparing expectations with events as they happen, competence depends on the rate of success while attempting to execute goal-directed behavior. Furthermore, the modulator dynamics are controlled by the strength of the leading motive (importance), the time left to satisfy it (urgency), the current selection threshold and the expected chance to satisfy it (task specic competence). As mentioned before, motive importancies and urgencies are supplied by the motivational system. Importance: all Motives Arousal Goal Directedness Selection Threshold Securing Behavior Resolution Level leading Motive leading Motive specific to current task Experienced  Uncertainty Urgency: all Motives Competence: general + _ _ _ _ _ + + + + + + + + _ Importance: all Motives Fig. 13.",Theoretical Foundations of Artificial General Intelligence,chapter 13
"3 Dimensions of emotion according to the Psi theory (adopted from Hille, 1998).      250 Theoretical Foundations of Articial General Intelligence The six-dimensional sketch is not exhaustive; especially when looking at social emotions, at least the demands for afliation (external legitimacy signals) and honor (internal legitimacy, ethical conformance), which are motivational dimensions like competence and uncertainty reduction, would need to be added. The Psi theory is not a complete theory of human motivation yet. Extensions and modications to the modulator dimensions will have an effect on the scope of the emergent affective states. While this approach characterizes the nature of affects well (by treating them as the result of cognitive modulation), the resulting emotional categories are likely to exhibit differences to the set of human emotions.",Theoretical Foundations of Artificial General Intelligence,chapter 13
"This argument could be extended to animal cognition: while most vertebrates and all mammals certainly have emotions, in the sense that their cognition is modulated by valence, arousal, resolution level and so on, their emotions might be phenomenologically and categorically dissimilar to human emotions, because they have a different motivational system, different cognitive capabilities and organization, and perhaps even different modulators. Nonetheless, this simple model is already sufcient to address the affective level, both conceptual and with detailed implementation, and its explanatory power if sufcient to account for subtle differences, like the one between enthusiastic joy and bliss (both are characterized by strong positive valence, but bliss is accompanied by a low arousal and high resolution level). 13.",Theoretical Foundations of Artificial General Intelligence,chapter 13
"4 Higher-level emotions emerging from directing valenced affects When we talk about emotions, we do not just refer to affective congurations like joy and bliss, but mostly to reactions to complex situations, such as social success, impending danger, a goal reached due to the benecial inuence of another agent. These higher-level emotions are characterized by an affective state that is directed upon (i.e., functionally associated to) a motivationally relevant object. Perhaps the most successful and prominent model of this category is based on work by Ortony, Clore and Collins (1988). The OCC model is easily adapted for the simulation of believable behavior, and even though it does not account for more specic emotions, like jealousy or envy, its application and extension is rather straightforward.",Theoretical Foundations of Artificial General Intelligence,chapter 13
"Ortony, Clore and Collins distinguish three main classes of emotions with respect to their object, which is either the consequence of some event, an aspect of some thing, or the action of some agent. From this perspective, the difference between social emotions (the      Modeling Emotion and Affect 251 appraisal of the actions of oneself or other agents) and event-based emotions (hope, relief) becomes visible (Figure 13.4). Valenced reaction to  consequences of events  actions of agents  aspects of objects  pleased, displeased etc.  approving, disapproving etc.  liking, disliking, etc.  focusing on  focusing on  consequences   for other  consequences   for self  self (agent)  other agent  desirable  for other  undesirable  for other  prospects  relevant  prospects  irrelevant  happy for...",Theoretical Foundations of Artificial General Intelligence,chapter 13
"resentment  gloating  pity  fortunes of others  joy  distress  well-being  pride  shame  admiration  reproach  attribution  love  hatred  attraction  hope  fear  confirmed  disconfirmed  satisfaction  relief  disappointment  prospect-based  well-being/attribution compounds   gratification  remorse  gratitude  anger  confirm. of fears  prospects  Fig. 13.4 Taxonomy of higher-level emotions (after Ortony, Clore and Collins, 1988, p. 19). At the rst stage, the OCC model distinguishes for each group of emotions whether they are positively or negatively valenced; for events this is their degree of pleasurableness, resentment etc., for agents it is approval or disapproval, and for objects it is their desirability, and thus the degree of attraction or repulsion.",Theoretical Foundations of Artificial General Intelligence,chapter 13
"Event-related emotions are further separated depending on whether the consequences apply to others or oneself, and whether the event is already present or not. Present events may lead to joy or distress, anticipated events to hope and fear. If the anticipated events materialize, the reactions are either satisfaction or the conrmation of fear, if they do not occur, then the consequence is either disappointment or relief. Emotions with respect to events happening to others depend on the stance taken towards these othersif they are seen positively, reactions may be happiness for them, or pity (if the      252 Theoretical Foundations of Articial General Intelligence event has negative consequences). If the others are resented, then a positive outcome may lead to envy and resentment, a negative to gloating. Agent-directed emotions (attributions) depend on whether the agent in question is someone else (who may be admired or reproached), or one self (in which case the emotion could be pride or shame).",Theoretical Foundations of Artificial General Intelligence,chapter 13
"Of course, appraisals may also relate to the consequences of events that are caused by the actions of agents. The OCC taxonomy calls the resulting emotions attribution/wellbeing compounds: Here, if oneself is responsible, the reaction may be gratication or remorse, and if the culprit is someone else, it could be gratitude or anger. Every emotion can be specied in a formal language, by using threshold parameters to specify intervals of real-valued variables in a weight-matrix to describe  for events: their desirability for the agent itself, their desirability for others, their deservingness, their liking, the likelihood of their occurrence, the related effort, and whether they are realized,  for agents: their praiseworthiness, their cognitive relevance, the deviation of expectations,  for objects: their appeal and their familiarity. By setting the thresholds accordingly, the emotion model can be tuned to different applications, or to individual variances.",Theoretical Foundations of Artificial General Intelligence,chapter 13
"The OCC taxonomy is an engineering approach that constructs the emotion categories based on systematizing our common-sense understanding of emotion. It does not say much about how the cognitive appraisals are realized  this is left to the designer of an architecture for a believable agent, nor does it explain the reason or the behavioral result of the individual higher-level emotions. However, if used in conjunction with a motivational system, it characterizes the emotions that emerge from dynamically attaching relevance to situations, agents and expectations. 13.5 Generating relevance: the motivational system In my view, emotion is best understood as part of a larger cognitive architecture, including a motivational system that determines relevance. Desires and fears, affective reexes and mood changes correspond to needs, such as environmental exploration, identication and avoidance of danger, and the attainment of food, shelter, cooperation, procreation, and intellectual growth.",Theoretical Foundations of Artificial General Intelligence,chapter 13
"Since the best way to satisfy the individual needs varies with the en     Modeling Emotion and Affect 253 vironment, the motivational system is not aligned with particular goal situations, but with the needs themselves, through a set of drives. Let us call events that satisfy a need of the system a goal, or an appetitive event, and one that frustrates a need an aversive event (for instance, a failure or an accident). Since goals and aversive events are given by an open environment, they can not be part of the denition of the architecture, and the architecture must specify a set of drives according to the needs of the system. Drives are indicated to the system as urges, as signals that make a need apparent. An example of a need would be nutrition, which relates to a drive for seeking out food. On the cognitive level of the system, the activity of the drive is indicated as hunger.",Theoretical Foundations of Artificial General Intelligence,chapter 13
"The connection between urges and events is established by reinforcement learning. In our example, that connection will have to establish a representational link between the indicator for food and a consumptive action (i.e., the act of ingesting food), which in turn must refer to an environmental situation that made the food available. Whenever the urge for food becomes active in the future, the system may use the link to retrieve the environmental situation from memory and establish it as a goal. Needs All urges of the agent stem from a xed and nite number of hard-wired needs, implemented as parameters that tend to deviate from a target value. Because the agent strives to maintain the target value by pursuing suitable behaviors, its activity can be described as an attempt to maintain a dynamic homeostasis. The current agent model of the Psi theory suggests several physiological needs (fuel, water, intactness), two cognitive needs (certainty, competence) and a social need (afliation).",Theoretical Foundations of Artificial General Intelligence,chapter 13
"It is straightforward to extend this set, for instance by adding a need for warmth to the set of physiological demands. All behavior of Psi agents is directed towards a goal situation that is characterized by a consumptive action satisfying one of the needs. In addition to what the physical (or virtual) embodiment of the agent dictates, there are cognitive needs that direct the agents towards exploration and the avoidance of needless repetition. The needs of the agent are weighted against each other, so differences in importance can be represented.      254 Theoretical Foundations of Articial General Intelligence Physiological needs Fuel and water: In the MicroPsi simulations, water and fuel are used whenever an agent executes an action, especially locomotion. Certain areas of the environment caused the agent to loose water quickly, which associated them with additional negative reinforcement signals. Intactness: Environmental hazards may damage the body of the agent, creating an increased intactness need and thus leading to negative reinforcement signals (akin to pain).",Theoretical Foundations of Artificial General Intelligence,chapter 13
"If damaged, the agent may look for opportunities for repair, which in turn increase intactness. These simple needs can be extended at will, for instance by needs for shelter, for rest, for exercise, for certain types of nutrients etc. Cognitive needs Certainty: To direct agents towards the exploration of unknown objects and affairs, they possess an urge specically for the reduction of uncertainty in their assessment of situations, knowledge about objects and processes and in their expectations. Because the need for certainty is implemented similar to the physiological urges, the agent reacts to uncertainty just as it would to pain signals and will display a tendency to remove this condition. This is done by triggering explorative behavior.",Theoretical Foundations of Artificial General Intelligence,chapter 13
"Events leading to an urge for uncertainty reduction include:  the agent meets unknown objects or events,  for the recognized elements, there is no known connection to behaviorthe agent has no knowledge what to do with them,  there are problems to perceive the current situation at all,  there has been a breach of expectations; some event has turned out differently as anticipated,  over-complexity: the situation changes faster than the perceptual process can handle,  the anticipated chain of events is either too short or branches too much. Both conditions make predictions difcult. In each case, the uncertainty signal is weighted according to the relation to the appetitive or aversive relevance of the object of uncertainty.",Theoretical Foundations of Artificial General Intelligence,chapter 13
"The urge for certainty may be satised by certainty eventsthe opposite of uncertainty events:  the complete identication of objects and scenes,  complete embedding of recognized elements into agent behaviors,  fullled expectations (even negative ones),  a long and non-branching chain of expected events.      Modeling Emotion and Affect 255 Like all urge-satisfying events, certainty events create a positive reinforcement signal and reduce the respective need. Because the agent may anticipate the reward signals from successful uncertainty reduction, it can actively look for new uncertainties to explore (diversive exploration). Competence: When choosing an action, Psi agents weight the strength of the corresponding urge against the chance of success. The measure for the chance of success to satisfy a given urge using a known behavior program is called specic competence. If the agent has no knowledge on how to satisfy an urge, it has to resort to general competence as an estimate.",Theoretical Foundations of Artificial General Intelligence,chapter 13
"Thus, general competence amounts to something like self-condence of the agent, and it is an urge on its own. (Specic competencies are not urges.) The general competence of the agent reects its ability to overcome obstacles, which can be recognized as being sources of negative reinforcement signals, and to do that efciently, which is represented by positive reinforcement signals. Thus, the general competence of an agent is estimated as a oating average over the reinforcement signals and the inverted displeasure signals. The general competence is a heuristics on how well the agent expects to perform in unknown situations. As in the case of uncertainty, the agent learns to anticipate the positive reinforcement signals resulting from satisfying the competence urge. A main source of competence is the reduction of uncertainty. As a result, the agent actively aims for problems that allow to gain competence, but avoids overly demanding situations to escape the frustration of its competence urge.",Theoretical Foundations of Artificial General Intelligence,chapter 13
"Ideally, this leads the agent into an environment of medium difculty (measured by its current abilities to overcome obstacles). Aesthetics: Environmental situations and relationships can be represented in innitely many ways. Here aesthetics corresponds to a need for improving representations, mainly by increasing their sparseness, while maintaining or increasing their descriptive qualities. Social needs Afliation: Because the explorative and physiological desires of our agents are not sufcient to make them interested in each other, they have a need for positive social signals, so-called legitimacy signals. With a legitimacy signal (or l-signal for short), agents may signal each other okayness with regard to the social group. Legitimacy signals are an expression of the senders belief in the social acceptability of the receiver. The need for l-signals needs frequent replenishment and thus amounts to an urge to afliate with other agents. Agents can send l-signals to reward each other for successful cooperation.",Theoretical Foundations of Artificial General Intelligence,chapter 13
"Anti-l-signals are the counterpart of l-signals. An anti-l-signal (which basically amounts to a frown) punishes an agent by depleting its legitimacy reservoir.      256 Theoretical Foundations of Articial General Intelligence Agents may also be extended by internal l-signals, which measure the conformance to internalized social norms. Supplicative signals are pleas for help, i.e. promises to reward a cooperative action with l-signals or likewise cooperation in the future. Supplicative signals work like a specic kind of anti-l-signals, because they increase the legitimacy urge of the addressee when not answered. At the same time, they lead to (external and internal) l-signals when help is given. They can thus be used to trigger altruistic behavior. The need for l-signals should adapt to the particular environment of the agent, and may also vary strongly between agents, thus creating a wide range of types of social behavior.",Theoretical Foundations of Artificial General Intelligence,chapter 13
"By making the receivable amount of l-signals dependent of the priming towards particular other agents, Psi agents might be induced to display jealous behavior. Social needs can be extended by romantic and sexual needs. However, there is no explicit need for social power, because the model already captures social power as a specic need for competencethe competence to satisfy social needs. Even though the afliation model is still fragmentary, we found that it provides a good handle on the agents during experiments. The experimenter can attempt to induce the agents to actions simply by the prospect of a smile or frown, which is sometimes a good alternative to a more solid reward or punishment. Appetence and aversion In order for an urge to have an effect on the behavior on the agent, it does not matter whether it really has an effect on its (physical or simulated) body, but that it is represented in the proper way within the cognitive system.",Theoretical Foundations of Artificial General Intelligence,chapter 13
"Whenever the agent performs an action or is subjected to an event that reduces one of its urges, a reinforcement signal with a strength that is proportional to this reduction is created by the agents pleasure center. The naming of the pleasure and displeasure centers does not necessarily imply that the agent experiences something like pleasure or displeasure. The name refers to the fact thatlike in humanstheir purpose lies in signaling the reexive evaluation of positive or harmful effects according to physiological, cognitive or social needs. (Experiencing these signals would require an observation of these signals at certain levels of the perceptual system of the agent.)      Modeling Emotion and Affect 257 Motives A motive consists of an urge (that is, the value of an indicator for a need) and a goal that has been associated to this indicator.",Theoretical Foundations of Artificial General Intelligence,chapter 13
"The goal is a situation schema characterized by an action or event that has successfully reduced the urge in the past, and the goal situation tends to be the end element of a behavior program. The situations leading to the goal situationthat is, earlier stages in the connected occurrence schema or behavior program might become intermediate goals. To turn this sequence into an instance that may initiate a behavior, orient it towards a goal and keep it active, we need to add a connection to the pleasure/displeasure system. The result is a motivator and consists of:  a need sensor, connected to the pleasure/displeasure system in such a way, that an increase in the deviation of the need from the target value creates a displeasure signal, and a decrease results in a pleasure signal. This reinforcement signal should be proportional to the strength of the increment or decrement.",Theoretical Foundations of Artificial General Intelligence,chapter 13
"optionally, a feedback loop that attempts to normalize the need automatically  an urge indicator that becomes active if there is no way of automatically adjusting the need to its target value. The urge should be proportional to the need.  an associator (part of the pleasure/displeasure system) that creates a connection between the urge indicator and an episodic schema/behavior program, specically to the aversive or appetitive goal situation. The strength of the connection should be proportional to the pleasure/displeasure signal. Note that usually, an urge gets connected with more than one goal over time, since there are often many ways to satisfy or increase a particular urge. 13.6 Motive selection The motivational system denes the action control structures of a cognitive agent. Above the level of motives and demands, motives must be chosen, i.e. established as leading motives, or intentions. Intentions amount to selected motives, combined with a way to achieve the desired outcome.",Theoretical Foundations of Artificial General Intelligence,chapter 13
"Within MicroPsi, an intention refers to the set of representations that initiates, controls and structures the execution of an action. (It is not required that an intention be conscious, that it is directed onto an object etc.here, intentions are simply those things that make actions happen.) Intentions may form intention hierarchies, i.e. to reach a goal it might be necessary to establish sub-goals and pursue these. An intention can be seen as a set of a goal state,      258 Theoretical Foundations of Articial General Intelligence an execution state, an intention history (the protocol of operations that took place in its context), a plan, the urge associated with the goal state (which delivers the relevance), the estimated specic competency to fulll the intention (which is related to the probability of reaching the goal) and the time horizon during which the intention must be realized (Figure 13.5).",Theoretical Foundations of Artificial General Intelligence,chapter 13
"situation   image  past history  episodic schema (for instance restaurant script)  time estimate tterminus  currently active plans/automatisms  time estimate t    epistemic competence  general competence  competence indicator  motive strength  demand   indicator  urgency  goal  goal  Fig. 13.5 The structure of a motive. If a motive becomes active, it is not always selected immediately; sometimes it will not be selected at all, because it conicts with a stronger motive or the chances of success when pursuing the motive are too low. In the terminology of Belief-Desire-Intention agents (Bratman, 1987), motives amount to desires, selected motives give rise to goals and thus are intentions.",Theoretical Foundations of Artificial General Intelligence,chapter 13
"Active motives can be selected at any time, for instance, an agent seeking fuel could satisfy a weaker urge for water on the way, just because the water is readily available, and thus, the active motives, together with their related goals, behavior programs and so on, are called intention memory. The selection of a motive takes place according to a value by success probability principle, where the value of a motive is given by its importance (indicated by the respective urge), and the success probability depends on the competence of the agent to reach the particular goal. In some cases, the agent may not know a way to reach a goal (i.e., it has no epistemic competence related to that goal). If the agent performs well in general, that is, it has a high general competence, it should still consider selecting the related motive.",Theoretical Foundations of Artificial General Intelligence,chapter 13
"The chance      Modeling Emotion and Affect 259 to reach a particular goal might be estimated using the sum of the general competence and the epistemic competence for that goal. Thus, the motive strength to satisfy a need d is calculated as urgedd (generalCompetence+competencedd), i.e. the product of the strength of the urge and the combined competence. If the window of opportunity is limited, the motive strength should be enhanced with a third factor: urgency. The rationale behind urgency lies in the aversive goal created by the anticipated failure of meeting the deadline. The urgency of a motive related to a time limit could be estimated by dividing the time needed through the time left, and the motive strength for a motive with a deadline can be calculated using (urgedd +urgencydd)(generalCompetence+competencedd), i.e. as the combined urgency multiplied with the combined competence.",Theoretical Foundations of Artificial General Intelligence,chapter 13
"The time the agent has left to reach the goal can be inferred from episodic schemas stored in the agents current expectation horizon, while the necessary time to nish the goal oriented behavior can be determined from the behavior program. (Obviously, these estimates require a detailed anticipation of things to come, which may be difcult to obtain.) At each time, only one motive is selected for the execution of its related behavior program. There is a continuous competition between motives, to reect changes in the environment and the internal states of the agent. To avoid oscillations between motives, the switching between motives may be taxed with an additional cost: the selection threshold. As explained in Section 13.3, this amounts to a bonus that is added to the strength of the currently selected motive. The value of the selection threshold can be varied according to circumstances, rendering the agent opportunistic or stubborn. 13.",Theoretical Foundations of Artificial General Intelligence,chapter 13
"7 Putting it all together The functionality described in the previous sections amounts to structural components of a more general affective agent architecture. While this functionality constrains the design space considerably, I would like to add some more necessary detail to the sketch of the framework for cognitive agents, according to the Psi theory. If motive selection and affective modulation are indispensable, then we will need operations that can be objects of motives, representations that can capture them, and processes that can be modulated. Let me briey address these requirements (Figure 13.6). Our agent will need to have the following facilities:  A set of urges that signal demands of the agent.      260 Theoretical Foundations of Articial General Intelligence  A selection mechanism that rises the satisfaction of one of the urges to an intention (an active motive).  An action selection/planning mechanism that chooses actions to reach the goal associated with satisfying the urge.",Theoretical Foundations of Artificial General Intelligence,chapter 13
"An associative memory, containing a world model (situation image), a protocol memory (i.e., representations of the past), and a mental stage (a representation of counterfactual events, such as plans and expectations). The representations in this memory can be primed (pre-activated or biased for) by active motives.  Perceptual mechanisms that give rise to the world model.  Action execution mechanisms that actually perform the chosen actions.  A set of modulators that modify the access to memory content, and the way perception, action selection and action execution work.  A reinforcement learning mechanism that creates associations between urges, goal situations and aversive events, based on the effect that encountered situations have on the demands. These simple requirements leave room for a large variety of possible architectures, and yet they demonstrate how motivation, affect and higher-level emotions can emerge from low more basic functionality.",Theoretical Foundations of Artificial General Intelligence,chapter 13
"Obviously, emotions are not part of this architecture, they are not built into it explicitly, but they can be consistently attributed to it. I believe that this reects an important aspect of the nature of emotions that incidentally causes so much difculty in their denition and treatment. While emotions are an important part of the descriptions that we use to interpret others, and not least reect ourselves, they are probably not best understood as natural kinds. Rather, the experience of an emotion amounts to a perceptual gestalt (Castelfranchi and Miceli, 2009), an associated, internally perceived co-occurrence of aspects of modulation, proprioceptive correlates of this modulation, of valence, action tendencies and motivationally relevant mental content. Similarities in these perceptual gestalts allow us to group emotions into (by and large) distinct, (mostly) recognizable categories.",Theoretical Foundations of Artificial General Intelligence,chapter 13
"Attributing the same set of categories to other agents helps us to interpret and predict their behavior, expression, past, and future reactions. And yet, emotions are probably best characterized by decomposing them into their modulatory and motivational components.      Bibliography 261 Protocol and Situation Memory  Urges (Drives)  Currently active motive Motive selection  Perception  Action selection  Planning  Action  execution  Modulators  Pre-activation    Fig. 13.6 Minimal architectural requirements for emergent emotions. Acknowledgments This work was supported by a postdoctoral fellowship grant of the Berlin School of Mind and Brain, Humboldt University of Berlin. I would like to thank Pei Wang, the editor of this volume, and many of its contributors for inspiring and helpful discussions, and the anonymous reviewers for constructive feedback. Bibliography [1] Bach, J. (2003).",Theoretical Foundations of Artificial General Intelligence,chapter 13
"The MicroPsi Agent Architecture Proceedings of ICCM-5, International Conference on Cognitive Modeling, Bamberg, Germany, 1520. [2] Bach, J. (2007). Motivated, Emotional Agents in the MicroPsi Framework, in Proceedings of 8th European Conference on Cognitive Science, Delphi, Greece.      262 Theoretical Foundations of Articial General Intelligence [3] Bach, J. (2009). Principles of Synthetic Intelligence. Psi, an architecture of motivated cognition. Oxford University Press. [4] Bach, J. (2011). A Motivational System for Cognitive AI. In Schmidhuber, J., Thorisson, K.R., Looks, M. (eds.): Proceedings of Fourth Conference on Articial General Intelligence, Mountain View, CA. 232242. [5] Bratman, M. (1987). Intentions, Plans and Practical Reason. Harvard University Press.",Theoretical Foundations of Artificial General Intelligence,chapter 13
"[6] Castelfranchi, C., Miceli, M. (2009). The cognitive-motivational compound of emotional experience. Emotion Review, 1, 223231. [7] Diener, E. (1999). Special section: The structure of emotion. Journal of Personality and Social Psychology, 76, 803867. [8] Drner, D. (1999). Bauplan fr eine Seele. Reinbeck: Rowohlt. [9] Drner, D., Bartl, C., Detje, F., Gerdes, J., Halcour, (2002). Die Mechanik des Seelenwagens. Handlungsregulation. Verlag Hans Huber, Bern. [10] Ekman, P., Friesen, W. (1971). Constants across cultures in the face and emotion.",Theoretical Foundations of Artificial General Intelligence,chapter 13
"In: Journal of Personality and Social Psychology 17(2): 12429. [11] Ellsworth, P.C., Scherer, K.R. (2003). Appraisal processes in emotion. in Davidson, R.J., Goldsmith, H.H., Scherer, K.R. (Eds.) Handbook of the affective sciences. New York, Oxford University Press. [12] Ertel, S. (1965). EED Ertel-Eindrucksdifferential (PSYNDEX Tests Review). Zeitschrift fr experimentelle und Angewandte Psychologie, 12, 2258. [13] Frijda, N.H. (1986). The emotions. Cambridge, U.K., Cambridge University Press. [14] Frijda, N. (1987). Emotion, cognitive structure, and action tendency. Cognition and Emotion, 1, 115143. [15] Gratch, J.",Theoretical Foundations of Artificial General Intelligence,chapter 13
"Marsella, S. (2004). A framework for modeling emotion. Journal of Cognitive Systems Research, Volume 5, Issue 4, 2004, p. 269306. [16] Hudlicka, E., Fellous, J.-M. (1996). Review of computational models of emotion (Technical Report No. 9612). Psychometrix. Arlington, MA. [17] Izard, C.E. (1994). Innate and universal facial expressions: Evidence from developmental and cross-cultural research. Psychological Bulletin, 115, 288299. [18] Lazarus, R. (1991). Emotion and Adaptation, NY, Oxford University Press. [19] Lisetti, C., Gmytrasiewicz, P. (2002). Can a rational agent afford to be affectless? A formal approach. Applied Articial Intelligence, 16, 577609. [20] Marsella, S.",Theoretical Foundations of Artificial General Intelligence,chapter 13
"Gratch, J., Petta, P. (in press): Computational Models of Emotion. In Scherer, K.R., Bnziger, T., Roesch, E. (eds.). A blueprint for an affectively competent agent: Crossfertilization between Emotion Psychology, Affective Neuroscience, and Affective Computing. Oxford University Press. [21] Sloman, A. (1981). Why robots will have emotions. Proceedings IJCAI. [22] Ortony, A., Clore, G.L., Collins, A. (1988). The cognitive structure of emotions. Cambridge, U.K., Cambridge University Press. [23] Osgood, C.E., Suci, G.J. Tannenbaum, P.H. (1957). The measurement of meaning. Urbana: University of Illinois Press. [24] Plutchik, R. (1994). The Psychology and Biology of Emotion.",Theoretical Foundations of Artificial General Intelligence,chapter 13
"New York: Harper Collins. [25] Roseman, I.J. (1991). Appraisal determinants of discrete emotions. In: Cognition and Emotion, 3, 161200. [26] Russel, J.A. (1995). Facial expressions of emotion. What lies beyond minimal universality. Psychological Bulletin, 118, 379391. [27] Wundt, W. (1910). Gefhlselemente des Seelenlebens. In: Grundzge der physiologischen Psychologie II. Leipzig: Engelmann D.R. Bates, Phys. Rev., 492 (1950).",Theoretical Foundations of Artificial General Intelligence,chapter 13
"  Chapter 14 AGI and Machine Consciousness Antonio Chella 1 and Riccardo Manzotti 2 1 DICGIM University of Palermo, Viale delle Scienze, Building 6, 90128 Palermo, Italy 2 Institute of Consumption, Communication and Behavior, IULM University, Via Carlo Bo, 8, 16033 Milano, Italy antonio.chella@unipa.it, riccardo.manzotti@iulm.it Could consciousness be a theoretical time bomb, ticking away in the belly of AI? Who can say? John Haugeland [24] (p. 247) This review discusses some of main issues to be addressed to design a conscious AGI agent: the agents sense of the body, the interaction with the environment, the agents sense of time, the free will of the agent, the capability for the agent to have some form of experience, and nally the relationship between consciousness and creativity.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"14.1 Introduction Articial General Intelligence aims at nding computational models for the deep basic mechanisms of intelligence [50]. The classical view of intelligence from the standpoint of psychology is usually related with the g factor and the psychometric approach (see Neisser et al. [45] for a review). Alternative views have been proposed by Gardner [22] and by Sternberg [61]. Legg and Hutter [36] provided several different denitions related to the different aspects of intelligence, from psychology to philosophy to cognitive sciences. 263      264 Theoretical Foundations of Articial General Intelligence However, whatever intelligence is, one may wonders if an agent can actually be intelligent without facing the problem of consciousness (for an introductory textbook on consciousness see Blackmore [6]). The relationship between consciousness and intelligence appears to be subtle. On the one side the need of awareness for an intelligent agent is obviously given for granted.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"On the other side, it is suspected that many cognitive processes that are necessary for acting intelligently may be unconscious and happen in the absence of consciousness. However, it is undeniable that consciousness is closely related with the broader unpredictable and less automatic forms of intelligence. Briey, we may distinguish between two main aspects of intelligence: an aspect that we may call syntactic and another one that we may call semantic. The rst one is related with the capability to suitably manipulate by combining and recombining a xed set of symbols by brute force and according to some heuristics. Many successful AI systems belong to this category. The second aspect is related to the smarter capabilities required to generate meaning and to ground symbols. While the syntactic manipulation of symbols could occur without consciousness, it is claimed that the meaningful intelligent act does not seem to be possible without consciousness. If we want to build a real smart AGI based agent, we have to face the problem of consciousness.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"In the following, we review some of the main issues in the current research eld of machine consciousness that are most strictly related with the problem of building an AGI system. In particular, Section 14.2 briey outlines the main philosophical positions about consciousness, Section 14.3 introduces the framework of machine consciousness, while Section 14.4 and Section 14.5 discuss the sense of the body and the interactions with the environment for a conscious AGI agent. Section 14.6 analyzes the problem of agents time and Section 14.7 the free will for an AGI system. Section 14.8 discusses the capability for an AGI agent to have some form of experience, and nally, Section 14.9 analyzes some of the relationships between consciousness and creativity in the framework of an AGI system. 14.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"2 Consciousness The prestigious journal Science, in a special collection of articles published beginning July 2005, celebrated the journals 125th anniversary with a look forward at the most compelling puzzles and questions facing scientists today. The rst one was the question about      AGI and Machine Consciousness 265 what the universe is made of, and the second one concerned the biological basis of consciousness [43]. Therefore, the scientic interest towards the understanding of consciousness is far higher than other scientic questions. But what is consciousness? Consciousness is an elusive concept which resists to denitions, as the Oxford Companion to Philosophy admits [27]. However, consciousness is not the only one concept in science which is difcult to dene: the same is true for other concepts as e.g., life or intelligence, not to mention the fundamental quantities of physics as space, time, matter, and nevertheless science has been able to make important steps in dealing with these elusive concepts.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"John Searle [57] proposed the following vivid denition of consciousness (p. 559): Consciousness consists of inner, qualitative, subjective states and processes of sentience or awareness. Consciousness, so dened, begins when we wake in the morning from a dreamless sleep and continues until we fall asleep again, die, go into a coma, or otherwise become unconscious. Searle evidences that conscious states are inner, qualitative and subjective, and many denitions proposed in the cognitive science literature concur in considering consciousness as tightly related with our subjective experience of the world. The philosopher Thomas Nagel, in a famous essay on what it is like to be a bat [44], discussed the difculty of studying conscious experience from a scientic point of view. In fact, he argues that we know everything about sonars and how they work, but we can never experience the sense of distance as a bat does by using sonar principles.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"The sonar experience is purely subjective and the physical analysis of the bats brain is compatible with absence of experience. Therefore, Nagel concludes that experience cannot be reduced to the physical domain. Ned Block [7] contrasts P-consciousness and A-consciousness. P-consciousness is the phenomenal aspect of consciousness and refers to subjective experience, while Aconsciousness is related with the function of consciousness. Therefore, A-consciousness deals with different cognitive capabilities such as reasoning, action, control of speech and so on, and may be the subject of scientic studies. P-consciousness, according to Block, is subject to an explanatory gap, i.e., how to step from physical and biological descriptions of processes in the brain to qualitative experience. David Chalmers [10] claimed a similar view. He distinguished between the easy and the hard problem of consciousness.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"The easy problems are related with how the brain is able to categorize and react to stimuli, how it is able to integrates information, how it may report internal states, how it may pay attention, and so on. Instead, the hard problem      266 Theoretical Foundations of Articial General Intelligence of consciousness is about the explanation of the subjective aspect of experience. This is, according to Chalmers, related to the information-processing capabilities of the brain. In fact, he suggests a double-aspect of information that may have a physical and a phenomenal aspect. Information processing is physical, while experience is suggested to arise from the phenomenal aspect of information. The aforementioned philosophical views are essentially forms of dualism, i.e., they consider two different levels: the physical domain where the brain resides, and the mental domain where experience resides. These two levels are irreducible, i.e., it is not possible to explain experience, which belongs to mental domain, in terms of entities in the physical domain.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"A related position is epiphenomenalism, that states that although experience is generated by processes in the brain, it is essentially useless, i.e. it does not add anything to ones behavior. Frank Jackson [29] describes this position by means of a famous mental experiment that allows us to better clarify the distinction between knowledge of a thing and subjective experience of the same thing, e.g., a color. He describes the ctional case of Mary: she is a brilliant scientist and she knows everything about colors; but, for some reason, Mary has never seen a color. In fact, she sees the world by means of a black and white monitor and she lives in a black and white room. What happens when eventually Mary leaves the room? She experiences true colors, but, by denition, nothing new is added to her knowledge of colors and of the world.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"A further view is put forward by Francis Crick who, in his book The Astonishing Hypothesis [18], outlines a reductionist view about consciousness (p. 3): The Astonishing Hypothesis is that You, your joys and your sorrows, your memories and your ambitions, your sense of personal identity and free will, are in fact no more that the behavior of a vast assembly of nerve cells and their associated molecules. As Lewis Carrolls Alice might have phrased it: Youre nothing but a pack of neurons. According to this view there are no levels other than the physical level. The mental level is only a way to explain the physical processes happening in the brain. As Christof Koch [31] discusses in details, a serious scientic study of consciousness should focus on the neural correlates of consciousness, i.e., the processes in the brain that are correlated with subjective experience.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"Following this line of thought, Koch proposes a general framework for studying consciousness from the point of view of neuroscience. Another interesting approach to consciousness from the point of view of neuroscience is due to Giulio Tononi [64]. According to Tononi, consciousness is strictly related with information integration. Experience, e.g., information integration, is suggested to be a fun     AGI and Machine Consciousness 267 damental quantity as mass, charge, energy. Interestingly, according to Tononi, any physical system may have subjective experience to the extent that it is capable to integrate information. Thus it may be possible, at least in principle, to build a really conscious artifact. Finally, Daniel Dennett [20] carried out a view that is often considered to be on the verge of eliminativism. Dennett claims that consciousness does not exists as a real process but it is a sort of umbrella concept that incorporates several different and sometimes unrelated physical processes.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"Therefore, a study of consciousness should focus on the different processes under this overarching concept. He introduced the term heterophenomenology as to indicate the study of consciousness from a rigorous third person point of view. According to this view, the alleged subjective experience may be studied in terms of explaining objective reports as verbal reports, PET, MRI, and so on. 14.3 Machine Consciousness The discipline of machine consciousness aims at studying the problems of consciousness, briey introduced in the previous section, by means of the design and implementation of conscious machines. In recent years there has been a growing interest in machine consciousness [1214]. The popular journal Scientic American in the June 2010 issue reviewed the twelve events that will change everything, and one of these events, considered likely before 2050, is machine self-awareness [23].",Theoretical Foundations of Artificial General Intelligence,chapter 14
"This interest in machine consciousness is mainly motivated by the belief that the approach based on the construction of conscious artifacts can shed new light on the many critical aspects that affect mainstream studies of cognitive sciences, philosophy of mind and also AI and neuroscience. As a working denition of conscious AGI agent we may consider the following one: a conscious AGI agent is an artifact that has some form of subjective experience, and it employs it in many cognitive tasks. Therefore, a conscious AGI agent has both Pconsciousness and A-consciousness. The study of conscious AGI agents is an intriguing eld of enquiry for at least two reasons. First, it takes consciousness as a real physical phenomenon to be explained at the physical level and with practical effects on behavior, a debated standpoint as stated in the previous section. Secondly, it hypothesizes the possibility to reproduce, by means of artifacts, the most intimate of mental aspects, namely, the subjective experience.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"     268 Theoretical Foundations of Articial General Intelligence Although many argued against the possibility of a conscious agent, so far no one has conclusively argued against such a possibility. In fact, arguments which deny the possibility of machine consciousness often would deny the possibility of human consciousness as well. Contrary to AI and functionalism, some scholars regard the functional view of the mind as insufcient to endorse the design of a conscious agent. Here, the commonly raised arguments against strong AI, such as the Searles Chinese Room argument [56], may lose some of their strength: although most available computational systems are Turing machines that instantiate the von Neumanns blue print, other non-conventional architectures are becoming available and more are going to be designed and implemented in the near future. The recent progresses in the elds of neuromorphic computing [28] and DNA computing [1], just to mention two main cases, may open new ways towards the implementation of a conscious system.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"Therefore a true conscious AGI agent could be implemented in some non-conventional hardware. Furthermore, it is an open issue whether AGI agents are reducible to a pure Turing machine view, when considered as a complex system made up by the interaction between the agent itself and its environment. Roughly speaking, machine consciousness lies in the middle ground between the extremes of the strong biological position (i.e., only biological brains are conscious) and liberal functionalism position (i.e., any behaviorally equivalent functional systems is conscious). Machine consciousness proponents maintain that biological position is too narrow and yet they concede that some kind of physical constraints will be unavoidable (hence no multiple feasibility) in order to build a conscious agent. A common objection to machine consciousness emphasizes the fact that biological entities may have unique characteristics that cannot be reproduced in artifacts. If this objection is true, machine consciousness may not be feasible.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"However, this contrast between biological and articial entities has often been over exaggerated, especially in relation to the problems of consciousness. So far, nobody was able to satisfactorily prove that the biological entities may have characteristics that can not be reproduced in articial entities with respect to consciousness. Instead, at a meeting on machine consciousness in 2001 organized by the Swartz Foundation at Cold Spring Harbor Laboratories, the conclusion of Christof Koch1 was that: we know of no fundamental law or principle operating in this universe that forbids the existence of subjective feelings in artifacts designed or evolved by humans. 1http://www.theswartzfoundation.org/abstracts/2001_summary.asp      AGI and Machine Consciousness 269 Consider the well-known distinction introduced by Searle [56] related to weak AI and strong AI.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"According to weak AI position, a computer may be an useful tool for the study of the mind but it is not itself a mind, while for the strong AI position, also known as computationalism, an appropriately programmed computer is a real mind itself and it has real effective cognitive capabilities. Recently, some scholars working in the eld of machine consciousness emphasized the behavioral role of consciousness to avoid the problem of phenomenal experience (see e.g. Seth [58]). They, paraphrasing Searle, suggested that it is possible to distinguish between weak machine consciousness and strong machine consciousness. The former approach deals with AGI agents which behaved as if they were conscious, at least in some respects. Such a view avoids any commitment to the hard problem of consciousness. The latter approach explicitly deals with the possibility of designing and implementing AGI agents capable of real conscious experience.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"This distinction is also mirrored in the previously discussed distinction between P-consciousness and Aconsciousness, where, roughly speaking, a weak AGI conscious agent corresponds to an agent that has A-consciousness only, while a strong AGI conscious agent corresponds to an agent that has P-consciousness and A-consciousness. Although the distinction between weak and strong AGI agents may set a useful working hypothesis, it may suggest a misleading view. Setting aside experience, i.e., Pconsciousness, something relevant could be missed for the understanding of cognition. Skipping the hard problem of consciousness could be not a viable option in the business of making real conscious AGI agents. Further, the distinction between weak and strong AGI systems may be misleading because it mirrors a dichotomy between true conscious agents and as if conscious agents. Yet, human beings are conscious and there is evidence that most animals exhibiting behavioral signs of consciousness are phenomenally conscious. It is a fact that intelligent human beings have phenomenal consciousness.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"They experience pains, pleasures, colors, shapes, sounds, and many more other phenomena. They feel emotions, feelings of various sort, bodily and visceral sensations. Arguably, they also have experiences of thoughts and of some cognitive processes. In summary, it would be very strange whether natural selection had gone at such great length to provide us with consciousness if there was a way to get all the advantages of a conscious being like intelligence without actually producing it. Thus we cannot help but wonder whether it is possible to design an AGI agent without dealing with the hard problem of consciousness.      270 Theoretical Foundations of Articial General Intelligence 14.4 Agents Body A crucial aspect for a conscious AGI agent is the relationships with its own body, as pointed out by Metzinger [42].",Theoretical Foundations of Artificial General Intelligence,chapter 14
"Although it cannot be underestimated the importance of the interface between a robot and its environment, as well as the importance of an efcient body, it is far from clear whether this aspect is intrinsically necessary to the occurrence of consciousness. Having a body does not seem to be a sufcient condition for consciousness. Arguably, it could be a necessary condition. Apart from intuitive cases, when is an agent truly embodied? On one hand, there is no such a thing as a non-embodied agent, since even the most classic AI system has to be implemented as a set of instructions running inside a physical device as a computer. On the other hand, a complex and sophisticated robot such as ASIMO by Honda2 is far from being conscious, as it is actually controlled by carefully hard-wired behavioral rules. There are many biological agents that would apparently score very well on embodiment but yet do not seem good candidate for consciousness. Take insects for instance.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"They show impressive morphological structures that allow them to perform outstandingly well without very sophisticated cognitive capabilities. It should be noticed that having a body could also inuence higher cognitive processes more strictly related with intelligence. In their seminal and highly debated book, Lakoff and Nez [35] discuss in detail and with many examples how mathematical concepts and reasonings could be deeply rooted in the human body and in its interactions with environment. In this eld, an effective robot able to build an internal model of its own body and environment has been proposed by Holland and Goodman [25]. The system is based on a neural network that controls a Khepera minirobot and it is able to simulate perceptual activities in a simplied environment. Following the same principles, Holland et al. [26] discuss the robot CRONOS, a very complex anthropomimetic robot whose operations are controlled by SIMNOS, a 3D simulator of the robot and of its environment.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"ECCEROBOT3 is the new complex incarnation of this anthropomimetic robot. Chella and Macaluso [11] present a model of robot perception based on a comparison loop between the actual and the expected robot sensory data generated by a 3D model of the robot body. The perception loop process is operating in CiceRobot, a robot that offered guided tours at the Archaeological Museum of Agrigento, Italy. 2http://asimo.honda.com 3http://eccerobot.org      AGI and Machine Consciousness 271 Shanahan [59, 60] discusses a cognitive robot architecture based on the Global Workspace Theory [2] in which the planning operations are performed by simulating the interactions of the robot with the external environment. He also discusses about implementations of the architecture based on different kinds of neural networks.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"While the previously discussed robots start with their own basic model of the body and they had to learn how to control it, Bongard et al. [9] discuss a starsh robot which is able to build a model of its own body from scratch. The body model is then employed to make the robot walk. The body model changes consequently if some damages occur to the robots real body. In summary, the notion of embodiment is far more complex than the simple idea of controlling a robots body. It refers to the kind of development and causal processes engaged between a robot, its body, and its environment. This step appears to be unavoidable for a truly conscious AGI agent. 14.5 Interactions with the Environment Besides having a body, a conscious AGI agent needs to be situated in an environment, Yet the necessity of situatedness is not totally uncontroversial.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"For instance, authors like Metzinger [41] argued that consciousness needs a purely virtual inner world created inside a system which, to all respects, lacks any direct contact with the environment. If consciousness requires a body in the environment, real or simulated one, we should be able to point out what is to be situated. What kind of architecture and individual history is sufcient for being situated? A classic example of interaction with the environment is the case of passive dynamic walker, extensively discussed by Pfeifer and Bongard [51]. A fruitful approach towards situatedness is represented by those implementations that outsource part of the cognitive processes to the environment and explicitly consider the agent as a part of the environment. An interesting idea in this eld is the morphological computation introduced by Paul [49]. She built different robots shaped in such a way so that the robot is able to perform simple logic operations as a XOR by means of their interactions with environment.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"About the interaction between body and environment, there are two powerful conceptual attractors in the discussion on consciousness. They are going to exert their strength in the machine consciousness arena, too. Where is the mind and its content located? Inside or      272 Theoretical Foundations of Articial General Intelligence outside the body of the agent? So far, neither options proved entirely satisfactory and the debates keeps running. It would be very simple if we could locate consciousness inside of the body of the agent and thus inside our conscious AGI robots. So, the mind should somehow depend on what takes place exclusively inside of the body of the robot. Therefore, the mental world must be inside of the agent from the beginning or it must be concocted inside. This position can broadly be labeled as internalism. However, such a view is not fully convincing since the robots mental states (broadly speaking) are about something that often appears as being external to the body.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"In fact, mental states typically address external states of affairs (whether they are concepts, thoughts, percepts, objects, events). In fact, consciousness refers to the external world. Then, we could reframe our model of the agents mind such as to include also the external world in the agents mind. Such an externalist change in our perspective would endorse those views that consider the sense of the body and the interaction with external environment as main conditions for a conscious AGI agent [15,5355]. Initial suggestions have been presented on how to implement an externalist agent. The most relevant ones are due to ORegan and No [4648]. They discuss the enactive process of visual awareness as based on sensorimotor contingencies. Following this approach, an AGI agent should be equipped by a pool of sensorimotor contingencies so that entities in the environment activate the related contingencies that dene the interaction schemas between the robot and the entity itself.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"Some contingencies may be pre-programmed in the agent by design (phylogenetic contingencies), but during the working life, the agent may acquire novel contingencies and therefore novel way of interacting with the environment. Moreover, the agent should acquire new ways of mastery, i.e., new ways to use and combine contingencies, in order to generate its own goal tasks and motivations (ontogenetic contingencies). A mathematical analysis of the enactive theory applied to a simple robot in a simulated environment is presented in Philipona et al. [52]. Manzotti and Tagliasco [40] discuss the theory of enlarged mind as an externalist theory covering the phenomenal and the functional aspects of consciousness with respect to a robot vision system. Following this line, Manzotti [39] analyzed the human and robotic conscious perception as a process that unies the activity in the brain and the perceived events in the external world.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"     AGI and Machine Consciousness 273 In summary, consciousness involves some kind of developmental integration with the environment such that what the robot is and does is a result of the deep interactions with the environment. A conscious AGI agent should be an agent that changes in some non-trivial way as a result of its tight coupling with the environment. 14.6 Time Conscious experience is located in time. We experience the ow of time in a characteristic way which is both continuous and discrete. On one hand, there is the ow of time in which we oat seamlessly. On the other hand, our cognitive processes require time to produce conscious experience and they are located in time. Surprisingly, there is evidence from the famous Libets studies [37] showing that we are visually aware of something only half a second after our eyes have received the relevant information. The classic notion of time from physics ts very loosely with our experience of time. Only the instantaneous present is real.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"Everything has to t in such Euclidean temporal point. For instance, speed is nothing more than the value of a derivative and can be dened at every instant. We are expected to occupy only an ever-shifting temporal point with no width. Such an instantaneous present cannot accommodate the long lasting and contentrich conscious experience of present. Neuroscience faces similar problems. According to the neural view of the mind, every cognitive and conscious process is instantiated by patterns of neural activity. This apparently innocuous hypothesis hides a problem. If a neural activity is distributed in time (as it has to be since neural activity consists in temporally distributed series of spikes), there must be some strong sense in which something taking place in different instants of time belong to the same cognitive or conscious process. But what glues together the rst and the last spike of a neural activity leading a subject to perceive a face? Simply suggesting that they occur inside the same window of neural activity is like explaining a mystery with another mystery.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"What is a temporal window? And how does it t with our physical picture of time? Indeed, it seems to be at odds with the instantaneous present of physics. In the case of AGI agents, this issue is extremely counterintuitive. For instance, let us suppose that a certain computation is identical with a given conscious experience. What would happen if we purposefully slow down the taking place of the same computation, as imagined in the famous science ction book Permutation City [21]? Certainly, we can      274 Theoretical Foundations of Articial General Intelligence envisage an articial environment where the same computation is performed at an altered time (for instance we could simply slow down the internal clock of such a machine). Would the AGI agent have identical conscious experience but spread in a longer span of time? Some initial interesting experiments have been described by Madl et al. [38] in the framework of the LIDA system [3].",Theoretical Foundations of Artificial General Intelligence,chapter 14
"They report experiments that simulate the timing of the cognitive cycle of LIDA; interestingly, the performances of the system are similar to humans just when the simulated timing of the cognitive cycle is comparable with the human timing. A related issue is the problem of the present. As in the case of brains, what denes a temporal window? Why are certain states part of the present? Does it depend on certain causal connections with behavior or is it the effect of some intrinsic property of computations? It is even possible that we would need to change our basic notion of time. 14.7 Free Will Another issue that does not t with the picture of a classic AI system is the fact that a conscious AGI agent should be capable of a unied will assumed as free (see, among others, Wegner [67] and Walter [66]). A classic argument against free will in human and hence in an artifact is the following [30].",Theoretical Foundations of Artificial General Intelligence,chapter 14
"If a subject is nothing but the micro-particles constituting it (and their state), all causal powers are drained by the smallest constituents. If the argument holds, there will be no space left for any high level causal will. All reality ought to reduce causally to what is done by the micro-particles who would be in total charge of what happens. No causation would be possible and no space would remain for the will of a subject to interfere on the course of events. Yet, we have a strong intuition that we are capable of willing something and that our conscious will is going to make a difference in the course of events. Many philosophers strongly argued in favor of the efcacy of conscious will. Another threat to free will comes from previously cited Libets studies [37], according to which we are conscious of our free choices only after 300 ms our brain has made them.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"Although Libet left open the possibility that we can veto the deliberations of our brains, there is an open debate about the interpretation of these experimental results. In short, an open problem is whether a complex agent as a whole can have any kind of causal power over its constituents. Since consciousness seems to be strongly related      AGI and Machine Consciousness 275 with the agent as a whole, we need some theory capable of addressing the relation between wholes and parts. For an AGI agent, this issue is difcult as ever. The classic approach and several design strategies (from the traditional divide et impera to sophisticated object-oriented programming languages) suggest to conceive AGI agents as made of separate and autonomous modules. Then, AGI agents would be classic examples of physical systems where the parts completely drain the causal power of the system as a whole. From this point of view, these systems would be completely unsuited to endorse a conscious will.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"However, there are some possible approaches that can provide a new route. A possible approach [64] is based on recent proposals that stressed the connectivity between elementary computational units of an agent. According to such proposals it is possible to implement networks of computational units whose behavior is not reducible to any part of the network, but rather it stems out of the integrated information of the system as a whole. Another approach [40] stresses the roles of suitable feedback loops that could do more than classic control feedbacks. Here, the idea is to implement machines capable of taking into account their whole history, also in a summarized way, in order to decide what to do. Thus, the behavior of an AGI agent would be the result of its past history as a whole. There would not be separate modules dictating what the system has to do, but rather the past history as a whole would have effect in every choice. 14.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"8 Experience The more complex problem for consciousness is: how can a physical system like an AGI agent produce something similar to our subjective experience? During a jam session, the sound waves generated by the musical instruments strike our ears and we experience a sax solo accompanied by bass, drums and piano. At sunset, our retinas are struck by rays of light and we have the experience of a symphony of colors. Swallow molecules of various kinds and, therefore, feel the taste of a delicious wine. It is well known that Galileo Galilei suggested that smells, tastes, colors and sounds do not exist outside the body of a conscious subject (the living animal). Thus experience would be created by the subject in some unknown way. A possible hypothesis concerns the separation between the domain of experience, namely, the subjective content, and the domain of objective physical events. This hypoth     276 Theoretical Foundations of Articial General Intelligence esis is at the basis of science itself.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"The claim is that physical reality can be adequately described only by the quantitative point of view in a third person perspective while ignoring any qualitative aspects. After all, in a physics textbook there are many mathematical equations that describe a purely quantitative reality. There is no room for quality content, feelings or emotions. Yet many scholars (see Strawson [63]) have questioned the validity of such a distinction as well as the degree of real understanding of the nature of the physical world. Whether the mental world is a special construct generated by some feature of the nervous systems of mammals is still an open question, as briey summarized in Section 14.2. It is fair to stress that there is neither empirical evidence nor theoretical arguments supporting such a view. In the lack of a better theory, we could also take into consideration the idea inspired by the previously mentioned externalism view that the physical world comprehends also those features that we usually attribute to the mental domain.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"A physicalist must be held that if something is real, and we assume consciousness is real, it has to be physical. Hence, in principle, a device can envisage it. In the case of AGI agents, how is it possible to overcome the distinction between function and experience? As previously outlined, a typical AGI agent is made up by a set of interconnected modules, each operating in a certain way. How the operation of some or all of the interconnected modules should generate conscious experience? However, the same question could be transferred to the activity of neurons. Each neuron, taken alone, does not work differently from a software module or a chip. But it could remains a possibility: it is not the problem of the physical world, but of our theories of the physical world. AGI systems are part of the same physical world that produce consciousness in human subjects, so they may exploit the same properties and characteristics that are relevant for conscious experience.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"In this regard, Giulio Tononi [65] proposed the information integration theory, briey introduced in Section 14.2. According to Tononi, the degree of conscious experience is related with the amount of integrated information. The primary task of the brain is to integrate information and, noteworthy, this process is the same whether it takes place in humans or in artifacts like AGI agents. According to this theory, conscious experience has two main characteristics. On the one side, conscious experience is differentiated because the potential set of different conscious states is huge. On the other side, conscious experience is integrated; in fact a conscious state is experienced as a single entity. Therefore, the substrate of conscious experience must be an integrated entity able to differentiate among a      AGI and Machine Consciousness 277 big set of different states and whose informational state is greater than the sum of the informational states of the component sub-entities.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"Tononi provides a formal methodology for the static case [64] and for the dynamic case [4,5] of the theory. According to this theory, Koch and Tononi [32] discuss a potential new Turing test based on the integration of information: articial systems should be able to mimic the human being not in language skills (as in the classic version of Turing test), but rather in the ability to integrate information from different sources, for example in the generation of the explanation of a picture. We must emphasize the fact that the implementation of a true AGI agent able to perform information integration is a real technological challenge. In fact, as previously stated, the typical software engineering techniques for the construction of AGI agents are essentially based on the design of a system through the decomposition of the system into easier subsystems. Each subsystem then will communicate with the others subsystems through well-dened interfaces so that the interaction between the subsystems happen in a controlled way.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"Tononis theory rather requires maximum interaction between the subsystems to allow an effective integration. Therefore new engineering techniques are required to design conscious AGI agents. Information integration theory could represent a rst step towards a theoretically wellfounded approach to machine consciousness. The idea of being able to nd the consciousness equations that, like Maxwells equations in physics, explains consciousness in living beings and in artifacts is a kind of ultimate goal for scholars of consciousness. 14.9 Creativity Can an AGI system be so creative to the point that its creations could be indistinguishable from those of a human being? According to Sternberg [62], creativity is the ability to produce something that is new and appropriate. The result of a creative process is not reducible to some sort of deterministic reasoning. No creative activity seems to identify a specic chain of activity, but an emerging holistic result [33].",Theoretical Foundations of Artificial General Intelligence,chapter 14
"Therefore, a creative AGI should be able to generate novel artifacts not by following preprogrammed instructions, as in typical industrial robots, but on the contrary by means of a real creative act. The problem of creativity in artifacts has been widely debated for example in the eld of automatic music composition. The software system EMI by David Cope [16] produces impressive results: even for an experienced listener it is difcult to distinguish musical      278 Theoretical Foundations of Articial General Intelligence compositions created by these programs from those ones created by a human composer. There is no doubt that these systems may capture some main aspects of the creative process, at least in music. However, one may wonders if an AGI system can actually be creative without being conscious. In this regard, Damasio [19] suggests a close connection between consciousness and creativity. Also Cope [17] discusses the relationship between consciousness and creativity.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"Although he does not take a clear position on this matter, he seem to favor the view according to which consciousness is not necessary for creative process. In fact, Cope asks if a creative agent should need to be aware of being creating something and if it needs to experience its own creations. According to Boden [8], the argument of consciousness is typically adopted to support the thesis according to which an articial agent can never be conscious and therefore it can never be really creative. In this respect, a conscious AGI system may be a breakthrough towards a real creative agent. The relationship between consciousness and creativity is difcult and complex. On the one side some scholars claim the need of awareness of the creative act. On the other side, it is suspected that, similarly to intelligence, many processes that are necessary for the creative act may happen in the absence of consciousness. However it is undeniable that consciousness is closely linked with the broader unpredictable and less automatic forms of creativity.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"In addition, we could distinguish between the mere production of new combinations and the aware creation of new content. If the wind would create (like the monkeys on a keyboard) a form which is indistinguishable from the Pieta by Michelangelo, it would be a creative act? Many authors would debate this argument. Let us consider as an example the design of an AGI system able to improvise jazz. The aspect of corporeality seems to be fundamental to the jazz performance. Auditory feedback is not sufcient to explain the characteristics of a performance: making music is essentially a full body activity [34]. The movement of the hands on the instrument, the touch and the strength needed for the instrument to play, the vibrations of the instrument propagated through the ngers of the player, the vibration of the air perceived by the players body, are all examples of feedbacks guiding the musician during the performance. The player receives different types of bodily feedbacks, e.g.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"through the receptors of the skin and through the receptors of the tendons and muscles.      AGI and Machine Consciousness 279 In addition to having a body, an artist, during a jam session, is typically situated in a group where she has a continuous exchange of information. The artist receives and provides continuous feedbacks with the other players of the group, and sometimes even with the audience, in the case of live performances. The classical view, often theorized in textbooks of jazz improvisation, suggests that during a solo, the player follows his own musical path largely made up by a suitable musical sequence of previously learned patterns. This is a partial view of an effective jazz improvisation.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"Undoubtedly, the musician has a repertoire of musical patterns, but she is also able to freely deviate from its path depending on her past experience and sensitivity, and according to the feedback she receives from other musicians or the audience, for example from suggestions from the rhythm section or due to signals of appreciation from the listeners. Finally, an AGI system aware of its jazz improvisation should be able to integrate during time the information generated by the instrument, the instruments of its group as well as information from its own body. Therefore, many of the challenges previously reviewed for a conscious AGI agent, as the agents body, the interaction with environment, the sense of time, the capability to take free decisions and to have experiences, are all challenges for a truly creative AGI system. 14.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"10 Conclusions The list of problems related with machine consciousness that have not been properly treated here is long: the problem of meaning, the generation of mental images, the problem of representation, the problem of higher order consciousness, and so on. These are issues of great importance for the creation of a conscious AGI agent, although some of them may overlap in part with the arguments discussed above. The sense of the body of an agent, its interactions with the environment, the problem of agents time, the free will of the agent and the capability for the agent to have some form of experience and creativity are all issues relevant to the problem of building a conscious AGI agent. Machine consciousness is, at the same time, a theoretical and technological challenge that forces us to deal with old problems by means of new and innovative approaches.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"It is possible that research in machine consciousness will push to re-examine in a novel way      280 Theoretical Foundations of Articial General Intelligence many hanging threads from classic AI, as Haugeland summarizes in a provocative way in the quotation at the beginning of this review. Bibliography [1] M. Amos, Theoretical and Experimental DNA Computation. vol. XIII, Theoretical Computer Science, (Springer, Heidelberg, 2005). [2] B. Baars, A Cognitive Theory of Consciousness. (Cambridge University Press, Cambridge, MA, 1988). [3] B. Baars and S. Franklin, Consciousness is computational: The LIDA model of global workspace theory, International Journal of Machine Consciousness. 1(1), 2332, (2009). [4] D. Balduzzi and G.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"Tononi, Integrated information in discrete dynamical systems: Motivation and theoretical framework, PLoS Computational Biology. 4(6), e1000091, (2008). [5] D. Balduzzi and G. Tononi, Qualia: The geometry of integrated information, PLoS Computational Biology. 5(8), e1000462, (2009). [6] S. Blackmore, Consciousness An Introduction (second edition). (Hodder Education, London, 2010). [7] N. Block, On a confusion about a function of consciousness, Behavioral and Brain Sciences. 18(02), 227247, (1995). [8] M. Boden, The Creative Mind: Myths and Mechanisms Second edition. (Routledge, London, 2004). [9] J. Bongard, V. Zykov, and H.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"Lipson, Resilient machines through continuous self-modeling, Science. 314, 11181121, (2006). [10] D. Chalmers, The Conscious Mind. (Oxford University Press, New York, NY, 1996). [11] A. Chella and I. Macaluso, The perception loop in CiceRobot, a museum guide robot, Neurocomputing. 72, 760766, (2009). [12] A. Chella and R. Manzotti, Eds., Articial Consciousness. (Imprint Academic, Exeter, UK, 2007). [13] A. Chella and R. Manzotti, Machine consciousness: A manifesto for robotics, International Journal of Machine Consciousness. 1(1), 3351, (2009). [14] A. Chella and R. Manzotti. Articial consciousness. In eds. V.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"Cutsuridis, A. Hussain, and J. Taylor, Perception-Action Cycle: Models, Architectures, and Hardware, vol. 1, Springer Series in Cognitive and Neural Systems, pp. 637671. Springer, New York, NY, (2011). [15] A. Clark, Supersizing The Mind. (Oxford University Press, Oxford, UK, 2008). [16] D. Cope, Computer modeling of musical intelligence in EMI, Computer Music Journal. 16(2), 6983, (1992). [17] D. Cope, Computer Models of Musical Creativity. (MIT Press, Cambridge, MA, 2005). [18] F. Crick, The Astonishing Hypothesis. (Simon and Schuster Ltd, 1994). [19] A. Damasio, The Feeling of What Happens: Body and Emotion in the Making of Consciousness.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"(Harcourt Brace, New York, 1999). [20] D. Dennett, Consciousness Explained. (Little, Brown, New York, 1991). [21] G. Egan, Permutation City. (Weidenfeld Military, 1994). [22] H. Gardner, Frames of Mind: The Theory of Multiple Intelligences. (Basic Books, New York, NY, 1983). [23] L. Greenemeier, Machine self-awareness, Scientic American. 302(6), 2829 (June, 2010). [24] J. Haugeland, Articial Intelligence: The Very Idea. (MIT Press, Bradford Book, Cambridge, MA, 1985).      Bibliography 281 [25] O. Holland and R. Goodman, Robots with internal models a route to machine consciousness?, Journal of Consciousness Studies.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"10(4-5), 77109, (2003). [26] O. Holland, R. Knight, and R. Newcombe. A robot-based approach to machine consciousness. In eds. A. Chella and R. Manzotti, Articial Consciousness, pp. 156173. Imprint Academic, (2007). [27] T. Honderich, Ed., The Oxford Companion To Philosophy New edition. (Oxford University Press, Oxford, UK, 2005). [28] G. Indiveri and T. Horiuchi, Frontiers in neuromorphic engineering, Frontiers in Neuroscience. 5, fnins.2011.00118, (2011). [29] F. Jackson, Epiphenomenal qualia, The Philosophical Quarterly. 32(127), 127136, (1982). [30] J. Kim, Mind in a Physical World.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"(MIT Press, Bradford Books, Cambridge, MA, 2000). [31] C. Koch, The Quest for Consciousness. (Roberts and Co., Engewood, CO, 2004). [32] C. Koch and G. Tononi, Can machines be conscious?, IEEE Spectrum. pp. 4751 (June, 2008). [33] A. Koestler, The Act of Creation. (Penguin Books, New York, 1964). [34] J. Krueger, Enacting musical experience, Journal of Consciousness Studies. 16, 98123, (2009). [35] G. Lakoff and Nunez, Where Mathematics Comes From: How the Embodied Mind Brings Mathematics into Being. (Basic Books, New York, 2000). [36] S. Legg and M. Hutter. A collection of denitions of intelligence. In eds. B.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"Goertzel and P. Wang, Advances in Articial General Intelligence: Concepts, Architectures and Algorithms, pp. 1724, Amsterdam, The Netherlands, (2006). IOS Press. [37] B. Libet, Mind Time. (Harvard University Press, Cambridge, MA, 2004). [38] T. Madl, B. Baars, and S. Franklin, The timing of the cognitive cycle, PLoS ONE. 6(4), e14803, (2011). [39] R. Manzotti, A process oriented view of conscious perception, Journal of Consciousness Studies. 13(6), 741, (2006). [40] R. Manzotti and V. Tagliasco, From behaviour-based robots to motivation-based robots, Robotics and Autonomous Systems. 51, 175190, (2005). [41] T. Metzinger, Being No One.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"(MIT Press, Bradford Books, Cambridge, MA, 2003). [42] T. Metzinger, The Ego Tunnel. (Basic Books, New York, 2009). [43] G. Miller, What is the biological basis of consciousness, Science. 309, 79 (July, 2005). [44] T. Nagel, What is it like to be a bat?, The Philosophical Review. 83(4), 435450, (1974). [45] U. Neisser, G. Boodoo, T. Bouchard, A. Wade Boykin, S. Ceci, D. Halpern, J. Loehlin, R. Perloff, R. Sternberg, and S. Urbina, Intelligence: Knowns and unknowns, American Psychologist. 51(2), 77101, (1996). [46] A. No, Action in Perception.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"(MIT Press, Bradford Books, Cambridge, MA, 2004). [47] J. ORegan, Why Red Doesnt Sound Like a Bell. (Oxford University Press, Oxford, UK, 2011). [48] J. ORegan and A. No, A sensorimotor account of vision and visual consciousness, Behavioral and Brain Sciences. 24, 9391031, (2001). [49] C. Paul, Morphological computation a basis for the analysis of morphology and control requirements, Robotics and Autonomous Systems. 54, 619630, (2006). [50] C. Pennachin and B. Goertzel. Contemporary approaches to articial general intelligence. In eds. B. Goertzel and C. Pennachin, Articial General Intelligence, pp. 130. Springer, Berlin, (2007). [51] R. Pfeifer and J.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"Bongard, How the Body Shapes the Way We Think. (MIT Press, Bradford Books, Cambridge, MA, 2007). [52] D. Philipona, J. ORegan, and A. No, Is there something out there? Inferring space from sensorimotor dependencies, Neural Computation. 15, 20292049, (2003). [53] T. Rockwell, Neither Brain nor Ghost. (MIT Press, Cambridge, MA, 2005). [54] M. Rowlands, Externalism  Putting Mind and World Back Together Again. (McGill-Queens University Press, Montreal and Kingston, 2003).      282 Theoretical Foundations of Articial General Intelligence [55] M. Rowlands, The New Science of the Mind. (MIT Press, Bradford Books, Cambridge, MA, 2010). [56] J.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"Searle, Minds, brains, and programs, Behavioral and Brain Sciences. 3(3), 417457, (1980). [57] J. Searle, Consciousness, Annual Review of Neuroscience. 23, 557578, (2000). [58] A. Seth, The strength of weak articial consciousness, International Journal of Machine Consciousness. 1(1), 7182, (2009). [59] M. Shanahan, A cognitive architecture that combines internal simulation with a global workspac, Consciousness and Cognition. 15, 433449, (2006). [60] M. Shanahan, Embodiment and the Inner Life. (Oxford University Press, Oxford, UK, 2010). [61] R. Sternberg, Beyond IQ: A Triarchic Theory of Human Intelligence. (Cambridge University Press, 1985). [62] R.",Theoretical Foundations of Artificial General Intelligence,chapter 14
"Sternberg, Ed., Handbook of Creativity. (Cambridge University Press, Cambridge, UK, 1998). [63] G. Strawson, Does physicalism entail panpsychism?, Journal of Consciousness Studies. 13, 331, (2006). [64] G. Tononi, An information integration theory of consciousness, BMC Neuroscience. 5(42), (2004). [65] G. Tononi, Consciousness as integrated information: a provisional manifesto, Biological Bulletin. 215, 216242, (2008). [66] H. Walter, Neurophilosophy of Free Will. (MIT Press, Bradford Books, Cambridge, MA, 2001). [67] D. Wegner, The Illusion of Conscious Will. (MIT Press, Bradford Books, Cambridge, MA, 2002).",Theoretical Foundations of Artificial General Intelligence,chapter 14
"  Chapter 15 Human and Machine Consciousness as a Boundary Effect in the Concept Analysis Mechanism Richard Loosemore Mathematical and Physical Sciences, Wells College, Aurora, NY 13026, U.S.A. rloosemore@wells.edu To solve the hard problem of consciousness we observe that any cognitive system of sufcient power must get into difculty when it tries to analyze consciousness concepts, because the mechanism that does the analysis will bottom out in such a way as to make the system declare these concepts to be both real and ineffable. Rather than use this observation to dismiss consciousness as an artifact, we propose a unifying interpretation that allows consciousness to be explicable at a meta level, while at the same time being mysterious and inexplicable on its own terms. This implies that science must concede that there are some aspects of the world that deserve to be called real, but which are beyond explanation.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"We conclude that some future thinking machines will, inevitably, have the same subjective consciousness that we do. Some testable predictions are derived from this theory. 15.1 Introduction The scope of this chapter is dened by the following questions:  When we use the term consciousness what exactly are we trying to talk about?  How does consciousness relate to the functioning of the human brain?  If an articial general intelligence (AGI) behaved as if it had consciousness, would we be justied in saying that it was conscious?  Are any of the above questions answerable in a scientically objective manner? The ultimate goal is to answer the third question, about machine consciousness, but in order to make meaningful statements about the consciousness of articial thinking systems, we need rst to settle the question of what consciousness is in a human being.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"And before 283      284 Theoretical Foundations of Articial General Intelligence we can answer that question, we need to be clear about whatever it is we are trying to refer to when we use the term consciousness. Finally, behind all of these questions there is the problem of whether we can explain any of the features of consciousness in an objective way, without stepping outside the domain of consensus-based scientic enquiry and becoming lost in a wilderness of subjective opinion. To anyone familiar with the enormous literature on the subject of consciousness, this might seem a tall order. But, with due deference to the many intellectual giants who have applied themselves to this issue without delivering a widely accepted solution, I would like to suggest that the problem of consciousness is actually much simpler than it appears on the surface. What makes it seem difcult is the fact that the true answer can only be found by asking a slightly different question than the one usually asked.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"Instead of asking directly for an explanation of the thing, we need to ask why we have such peculiar difculty stating what exactly the thing is. Understanding the nature of the difculty reveals so much about the problem that the path to a solution then becomes clear. 15.1.1 The Hard Problem of Consciousness One of the most troublesome aspects of the literature on the problem of consciousness is the widespread confusion about what exactly the word consciousness denotes. In his inuential book on the subject, Chalmers [2] resolved some of this confusion when he drew attention to the fact that the word is often used for concepts that do not contain any deep philosophical mystery. These straightforward senses include:  The ability to introspect or report mental states.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"A y and a human can both jump out of the way of a looming object, but a human can consciously think and talk about many aspects of the episode, whereas the y simply does not have enough neural machinery to build internal models of its action. By itself, though, this ability to build internal models is not philosophically interesting.  Someone who is asleep can be described as not being conscious, but in this case the word is only used for a temporary condition, not a structural incapacity.  We occasionally say that a person consciously did something, when what we really mean is that the person did it deliberately.  If a person knows a fact we sometimes say that they are conscious of the fact. In contrast to these senses (and others in a similar vein), there is one meaning for the word consciousness that is so enigmatic that it is almost impossible to express.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"This      Human and Machine Consciousness as a Boundary Effect in the Concept Analysis Mechanism 285 is the subjective quality of our experience of the world. For example, the core thing that makes our sensation of redness different from our sensation of greenness, but which we cannot talk about with other people in any kind of objective way. These so-called qualia the quality of our tastes, pains, aches, visual and auditory imagery, feelings of pleasure and sense of selfare all experiences that we can talk about with other people who say they experience them, but which we cannot describe to a creature that does not claim to experience them. When a person who is red-green color blind asks what difference they would see between red and green if they had a full complement of color receptors, the only answer we can give is It is like the difference between your color red/green and the color blue, only different.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"To the extent that this answer leaves out something important, that omitted thing is part of the problem of consciousness. The terms phenomenology or phenomenal consciousness are also used to describe these core facts about being a conscious creature. This is in contrast to the psychology of being a thinking creature: we can analyze the mechanisms of thought, memory, attention, problem solving, object recognition, and so on, but in doing so we still (apparently) say nothing about what it is like to be a thing that engages in cognitive activity. One way to drive this point home is to notice that it is logically possible to conceive of a creature that is identical to one of us, right down to the last atom, but which does not actually experience this inner life of the mind.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"Such a creaturea philosophical zombie would behave as if it did have its own phenomenology (indeed its behavior, ex hypothesi, would be absolutely identical to its normal twin) but it would not experience any of the subjective sensations that we experience when we use our minds. It can be argued that if such a thing is logically possible, then we have a duty to explain what it means to say that there is a thing that we possess, or a thing that is an attribute of what we are, that marks the difference between one of us and our zombie twin [1, 5]. If it is conceivable that a thing could be absent, then there must be a thing there that can be the subject of questions. That thingabsent in the zombie but present in ourselvesis consciousness.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"In order to make a clear distinction between the puzzle of this kind of consciousness, versus the relatively mundane senses of the word listed earlier, Chalmers [2] labeled this the hard problem of consciousness. The other questionsfor example, about the neural facts that distinguish waking from sleepingmay be interesting in their own right, but they do not involve deep philosophical issues, and should not be confused with the hard problem.      286 Theoretical Foundations of Articial General Intelligence Many philosophers would say that these subjective aspects of consciousness are so far removed from normal science that if anyone proposed an objective, scientic explanation for the hard problem of consciousness they would be missing the point in a quite fundamental way. Such an explanation would have to start with a bridge between the ideas of objective and subjective, and since the entire scientic enterprise is, almost by denition, about explaining objectively veriable phenomena, it seems almost incoherent to propose a scientic (i.e.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"non-subjective) explanation for consciousness (which exists only in virtue of its pure subjectivity). The story so far is that there is confusion in the literature about the exact denition of consciousness because it is ambiguous between several senses, with only one of the senses presenting a deep philosophical challenge. This ambiguity is only part of the confusion, however, because there are many cases where a piece of research begins by declaring that it will address the hard problem (for example, there is explicit language that refers to the mystery of subjective experience), but then shifts into one of the other senses, without touching the central question at all. This is especially true of neuroscience studies that purport to be about the neural correlate of consciousness: more often than not the actual content of the study turns out to devolve on the question of which neural signals are present when the subject is awake, or engaging in intentional acts, and so on.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"The eventual goal of the present chapter is to answer questions about whether machines can be said to be conscious, so it should be clear that the hard problem, and only the hard problem, is at issue here. Knowing that an articial intelligence has certain circuits active when it is attending to the world, but inactive when it is not, is of no relevance. Similarly, if we know that wires from a red color-detection module are active, this tells us the cognitive level fact that the machine is detecting red, but it does not tell us if the machine is experiencing a sensation of redness, in anything like the way that we experience redness. It is this subjective experience of rednessas well as all the other aspects of phenomenologythat we need to resolve.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"What does it mean to say that a human experiences a subjective phenomenal consciousness, and is it possible to be sure that an articial intelligence of sufcient completeness would (or would not) have the same phenomenal experience? 15.1.2 A Problem within the Hard Problem We now focus on the fact that even after we separate the hard problem of consciousness from all the non-hard, or easy problems, there is still some embarrassing vagueness in      Human and Machine Consciousness as a Boundary Effect in the Concept Analysis Mechanism 287 the denition of the hard problem itself. The trouble is that when we try to say what we mean by the hard problem, we inevitably end up by saying that something is missing from other explanations. We do not say Here is a thing to be explained, we say We have the feeling that there is something that is not being addressed, in any psychological or physical account of what happens when humans (or machines) are sentient.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"It seems impossible to articulate what we actually want to see explainedwe can only say that we consider all current accounts (as well as every conceivable future account) of the mechanisms of cognition to be not relevant to phenomenology. The situation can perhaps be summarized in the form of a dialectic: Skeptic: If you give us an objective denition for terms such as consciousness and phenomenology, then and only then can we start to build an explanation of those things; but unless someone can say exactly what they mean by these terms, they are not really saying anything positive at all, only complaining about some indenable thing that ought to be there. Phenomenologist: We understand your need for an objective denition for the thing that we want explained, but unfortunately that thing seems to be intrinsically beyond the reach of objective denition, while at the same time being just as deserving of explanation as anything else in the universe.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"The difculty we have in supplying an objective denition should not be taken as grounds for dismissing the problemrather, this lack of objective denition IS the problem! If we step back for a moment and observe this conict from a distance, we might be tempted to ask a kind of meta-question. Why should the problem of consciousness have this peculiar indeniteness to it? This new question is not the same as the problem of consciousness itself, because someone could conceivably write down a solution to the problem of consciousness tomorrow, and have it accepted by popular acclamation as the solution, and yet we could still turn around and ask: Yes, but now please explain why the problem was so hard to even articulate! That questionregarding the fact that this problem is different from all other problems because we cannot seem to dene it in positive termsmight still be askable, even after the problem itself had been solved. 15.1.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"3 An Outline of the Solution In fact, this meta-question needs to be addressed rst, because it is the key to the mystery. I would like to propose that we can trace this slipperiness back to a specic cause: all intelligent systems must contain certain mechanisms in order to be fully intelligent, and a      288 Theoretical Foundations of Articial General Intelligence side effect of these mechanisms is that some questions (to wit, the exact class of questions that correspond to consciousness) can neither be dened nor properly answered. When we pose questions to ourselves we engage certain cognitive mechanisms whose job is to analyze the cognitive structures corresponding to concepts. If we take a careful look at what those mechanisms do, we notice that there are some situations in which they drive the philosophers brain into a paradoxical mixed state in which she declares a certain aspect of the world to be both real and intrinsically inexplicable.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"In effect, there are certain concepts that, when analyzed, throw a monkey wrench into the analysis mechanism. That is a precis of the rst phase of the argument. But then there is a secondand in many ways more importantphase of the argument, in which we look at the reality of the particular concepts that break the cognitive mechanism responsible for explaining the world. Although phase one of the argument seemed to explain consciousness as a malfunction or short-circuit in the cognitive mechanism that builds explanations, in this second part we make an unusual turn into a new compromise, neither dualist nor physicalist, that resolves the problem of consciousness in a somewhat unorthodox way. 15.2 The Nature of Explanation All facets of consciousness have one thing in common: they involve some particular types of introspection, because we look inside at our subjective experience of the world (qualia, sense of self, and so on) and ask what these experiences amount to.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"In order to analyze the nature of these introspections we need to take one step back and ask what happens when we think about any concept, not just those that involve subjective experience. 15.2.1 The Analysis Mechanism In any intelligent systemeither a biological mind or a sufciently complete articial general intelligence (AGI) systemthere has to be a powerful mechanism that enables the system to analyze its own concepts. The system has to be able to explicitly think about what it knows, and to deconstruct that knowledge in many ways. If the degree of intelligence is high enough, the scope of this analysis mechanism (as it will henceforth be called) must be extremely broad. It must be able to ask questions about basic-level concepts, and then ask further questions about the constituent concepts that dene basic-level concepts, and then continue asking questions all the way down to the deepest levels of its knowledge.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"     Human and Machine Consciousness as a Boundary Effect in the Concept Analysis Mechanism 289 AGI systems will surely have this analysis mechanism at some point in the future, because it is a crucial part of the general in articial general intelligence, but since there is currently no consensus about how to do this, we need to come up with a language that allows us to talk about the kind of things that such a mechanism might get up to. For the purposes of this chapter I am going to use a language derived from my own approach to AGIwhat I have called elsewhere a molecular framework for cognition [6,7]. It is important to emphasize that there are no critical features of the argument that hinge on the exact details of this molecular framework. In fact, the framework is so general that any other AGI formalism could, in principle, be translated into the MF style.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"However, the molecular framework is arguably more explicit about what the analysis mechanism does, so by using the language of the framework we get the benet of a more concrete picture of its workings. Some AGI formalisms will undoubtedly take a different approach, so to avoid confusion about the role played by the MF in this chapter, I will make the following claim, which has the status of a postulate about the future development of theories of intelligence:  Postulate (Analysis Mechanism Equivalence): Any intelligent system with the ability to ask questions about the meaning of concepts, with the same scope and degree of detail as the average human mind, will necessarily have an equivalent to the analysis mechanism described here. Different forms of the analysis mechanism will be proposed by different people, but the intended force of the above postulate is that in spite of all those differences, all (or most) of those analysis mechanisms will have the crucial features on which this explanation of consciousness depends.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"So the use of the molecular framework in this chapter does nothing to compromise the core of the argument. 15.2.2 The Molecular Framework The Molecular Framework (MF) is a generic model of the core processes inside any system that engages in intelligent thought. It is designed to be both a description of human cognition and a way to characterize a broad range of AGI architectures. The basic units of knowledge, in this framework, are what cognitive psychologists and AGI programmers loosely refer to as concepts, and these can stand for things [chair], processes [sitting], relationships [on], actions [describe], and so on.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"     290 Theoretical Foundations of Articial General Intelligence The computational entities that encode concepts are found in two places in the system: the background (long-term memory, where there is effectively one entity per concept) and the foreground, which is roughly equivalent to working memory, or the contents of consciousness, since it contains the particular subset of concepts that the system is using in its current thoughts and all aspects of its current model of the world. The concept-entities in the foreground are referred to here as atoms, while those in the background are called elements. This choice of terminology is designed to make it clear that, in the simplest form of the molecular framework, each concept is represented by just one element in the background, whereas there can be many instances of that concept in the foreground. If the system happens to be thinking about several instances of the [chair] concept there would be several [chair] atoms in the foreground, but there would only be one [chair] element in the background.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"For the purposes of this chapter we will almost exclusively be concerned with atoms, and (therefore) with events happening in the foreground. The contents of the foreground could be visualized as a space in which atoms link together to form clusters that represent models of the state of the world. One cluster might represent what the system is seeing right now, while another might represent sounds that are currently being heard, and yet another might represent some abstract thoughts that the system is entertaining (which may not have any connection to what is happening in its environment at that moment). The function of the foreground, then, is to hold models of the world. Theorists differ in their preference for atoms that are either active or passive. A passive approach would have all the important mechanisms on the outside, so that the atoms were mere tokens manipulated by those mechanisms.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"An active approach, on the other hand, would have few, if any, external mechanisms that manipulate atoms, but instead would have all the interesting machinery in and between the atoms. In the present case we will adopt the active, self-organized point of view: the atoms themselves do (virtually) all the work of interacting with, and operating on, one another. This choice makes no difference to the argument, but it gives a clearer picture of some claims about semantics that come later. Two other ingredients that need to be mentioned in this cognitive framework are external sensory input and the systems model of itself. Sensory information originates at the sensory receptors (retina, proprioceptive detectors, ears, etc.), is then pre-processed in some way, and nally arrives at the edge of the foreground, where it causes atoms      Human and Machine Consciousness as a Boundary Effect in the Concept Analysis Mechanism 291 representing primitive sensory features to become active.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"Because of this inward ow of information (from the sensory organs to the edge of the foreground and then on into the interior region of the foreground), those atoms that are near the edge of the foreground will tend to represent more concrete, low-level concepts, while atoms nearer the center will be concerned with more high-level, abstract ideas. The self-model is a structure (a large cluster of atoms), somewhere near the center of the foreground, that represents the system itself. It could be argued that this self-model is present in the foreground almost all of the time because when the mind is representing some aspect of the world, it usually keeps a representation of its own ongoing existence as part of that world. There are uctuations in the size of the self model, and there may be occasions when it is almost absent, but most of the time we seem to maintain a model of at least the minimal aspects of our self, such as our being located in a particular place.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"Although the self-model proper is a representation of the system, somewhere near to it there would also be a part of the system that has the authority to initiate and control actions taken by the system: this could be described as the Make It So place. Finally, note that there are a variety of operators at work in the foreground, whose role is to make changes to clusters of atoms. The atoms themselves do some of this work, by trying to activate other atoms with which they are consistent. So, for example, a [cat] atom that is linked to a [crouching-posture] atom will tend to activate an atom representing [pounce]. But there will also be operators that do such things as concept creation (making a new atom to encode a new conjunction of known atoms), elaboration (where some existing atoms are encouraged to bring in others that can represent more detailed aspects of what they are already representing), various forms of analogy building, and so on.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"This cognitive framework depicts the process of thought as a collective effect of the interaction of all these atoms and operators. The foreground is a molecular soup in which atoms assemble themselves (with the help of operators) into semi-stable, dynamically changing structures. Hence the use of the term molecular framework to describe this approach to the modeling of cognition. 15.2.3 Explanation in General Atoms can play two distinct roles in the foreground, mirroring the distinction between use and mention of words. When the word cat appears in a sentence like The cat is on the chair, it is being used to refer to a cat, but when the same word appears in a sentence like The word cat has three letters, the word itself, not the concept, is being mentioned.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"     292 Theoretical Foundations of Articial General Intelligence In much the same way, if the foreground has atoms representing a chair in the outside world a [chair] atom will be part of the representation of that outside situation, and in this case the [chair] atom is simply being used to stand for something. But if the system asks itself What is a chair?, there will be one [chair] atom that stands as the target of the cluster of atoms representing the question. There is a strong difference, for the system, between representing a particular chair, and trying to ask questions about the concept of a chair. In this case the [chair] atom is being mentioned or referenced in the cluster of atoms that encode the question. It helps to picture the target atom as being placed in a special zone, or bubble, attached to the cluster of atoms that represent the questionwhatever is inside the bubble is playing the special role of being examined, or mentioned.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"This is in contrast to the ordinary role that most atoms play when they are in the foreground, which is merely to be used as part of a representation. So, when an atom, [x], becomes the target of a What is x? question, the [x] atom will be placed inside the bubble, then it will be elaborated and unpacked in various ways. What exactly does it mean to elaborate or unpack the atom? In effect, the atom is provoked into activating the other atoms that it would normally expect to see around it, if it were part of an ordinary representation in the foreground. Thus, the [chair] atom will cause atoms like [legs], [back], [seat], [sitting], [furniture] to be activated. And note that all of these activated atoms will be within the bubble that holds the target of the question. What the question-cluster is doing is building a model of the meaning of [chair], inside the bubble.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"The various features and connotations of the [chair] concept try to link with one another to form a coherent cluster, and this coherent cluster inside the bubble is a model of the meaning, or denition, of the target concept. One important aspect of this [meaning-of-chair] cluster is that the unpacking process tends to encourage more basic atoms to be activated. So the concepts that make up the nal answer to the question will tend to be those that are subordinate features of the target atom.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"This is clearly just a matter of looking in the opposite direction from the one that is normally followed when an atom is being recognized: usually the activation of a cluster of atoms like [legs], [back] and [seat] will tend to cause the activation of the [chair] atom (this being the essence of the recognition process), so in order to get the meaning of [chair], what needs to happen is for the [chair] atom to follow the links backwards and divulge which other atoms would normally cause it to be activated. We can call this set of elaboration and unpacking operations the analysis mechanism. Although it is convenient to refer to it as a single thing, the analysis mechanism is not really      Human and Machine Consciousness as a Boundary Effect in the Concept Analysis Mechanism 293 a single entity, it is an open-ended toolkit of exible, context-dependent operators. More like a loosely-dened segment of an ecology than a single creature.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"However, at the core of all these operators there will still be one basic component that grabs the target atom and starts following links to extract the other atoms that constitute the evidence (the features) that normally allow this atom to be activated. All other aspects of the analysis mechanism come into play after this automatic unpacking event. If this were about narrow AI, rather than AGI, we might stop here and say that the essence of explanation was contained in the above account of how a [chair] concept is broken down into more detailed components. In an AGI system, however, the analysis mechanisms will have extensive connections to a large constellation of other structures and operators, including representations of, among other things:  The person who asked the question that is being considered;  That persons intentions, when they asked the question;  Knowledge about what kinds of explanation are appropriate in what contexts;  The protocols for constructing sentences that deliver an answer;  The status and reliability of the knowledge in question.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"In other words, there is a world of difference between a dictionary lookup mechanism that regurgitates the denition of chair (something that might be adequate in a narrow AI system), and the massive burst of representational activity that is triggered when a human or an AGI is asked What is a chair?. The mental representation of that one question can be vastly different between cases where (say) the questioner is a young infant, a nonnative-speaker learning the English language, and a professor who sets an exam question for a class of carpentry or philosophy students. 15.2.4 Explaining Subjective Concepts In the case of human cognition, what happens when we try to answer a question about our subjective experience of the color red? In this case the analysis mechanism gets into trouble, because any questions about the essence of the color red will eventually reach down to a [redness] concept that is directly attached to an incoming signal line, and which therefore has no precursors.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"When the analysis mechanism tries to follow downward links to more basic atoms, it nds that this particular atom does not have any! The [redness] concept cannot be unpacked like most other concepts, because it lies at the very edge of the foreground: this is the place at which atoms are no longer used to represent parts of the      294 Theoretical Foundations of Articial General Intelligence world. Outside the foreground there are various peripheral processing mechanisms, such as the primitive visual analysis machinery, but these are not within the scope of the operators that can play with atoms in the foreground itself. As far as the foreground is concerned the [redness] atom is activated by outside signals, not by other atoms internal to the foreground. Notice that because of the rich set of processes mentioned above, the situation here is much worse than simply not knowing the meaning of a particular word.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"If we are asked to dene a word we have never heard of, we can still talk about the letters or phonemes in the word, or specify where in the dictionary we would be able to nd it, and so on. In the case of color qualia, though, the amount of analysis that can be done is precisely zero, so the analysis mechanism returns nothing. Or does it return nothing? What exactly would we expect the analysis mechanism to do in this situation? Bear in mind that the mechanism itself is not intelligent (the global result of all these operations might be intelligent, but the individual mechanisms are just automatic), so it cannot know that the [red] concept is a special case that needs to be handled differently. So we would expect the mechanism to go right ahead and go through the motions of producing an answer.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"Something will come out of the end of the process, even if that something is an empty container where a cluster of atoms (representing the answer to the question) should have been. So if the analysis mechanism does as much as it can, we would expect it to return an atom representing the concept [subjective-essence-of-the-color-red], but this atom is extremely unusual because it contains nothing that would allow it to be analyzed. And any further attempt to apply the analysis mechanism to this atom will yield just another atom of the same element. The system can only solve its problem by creating a unique type of atom whose only feature is itself. This bottoming-outof the analysis mechanism causes the cognitive system to eventually report that There is denitely something that it is like to be experiencing the subjective essence of red, but that something is ineffable and inexplicable.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"What it is saying is that there is a perfectly valid concept inside the foregroundthe one that encodes the raw fact of rednessbut that the analysis of this concept leads beyond the edge of the foreground (out into the sensory apparatus that supplies the foreground with visual signals), where the analysis mechanism is not able to go. This is the only way it can summarize the peculiar circumstance of analyzing [red] and getting [red] back as an answer. This same short-circuit in the analysis mechanism is common to all of the consciousness questions. For qualia, the mechanism hits a dead end when it tries to probe the sensory      Human and Machine Consciousness as a Boundary Effect in the Concept Analysis Mechanism 295 atoms at the edge of the foreground. In the case of emotions there are patterns of activation coming from deeper centers in the brain, which are also (arguably) beyond the reach of the foreground.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"For the concept of self, there is a core representation of the self that cannot be analyzed further because its purpose is to represent, literally, itself. The analysis mechanism can only operate within the foreground, and it seems that all aspects of subjective phenomenology are associated with atoms that lie right on the boundary. In every case where this happens it is not really a failure of the mechanism, in the sense that something is broken, it is just an unavoidable consequence of the fact that the cognitive system is powerful enough to recursively answer questions about its own knowledge. If this were really a failure due to a badly designed mechanism, then it might be possible to build a different type of intelligent system that did not have this problem. Perhaps it would be possible to design around this problem, but it seems just as likely that any attempt to build a system capable of analyzing its own knowledge without limitations will have a boundary that causes the same short-circuit.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"Attempts to get the system to cope gracefully with this problem may only move the boundary to some other place, because any x that is powerful enough to make the system not sense a problem, for these special concepts, is likely to have the unwanted side effect of causing the system to be limited in the depth of its analytic thought. If a system has the ability to powerfully analyze its own concepts, then, it will have to notice the fact that some concepts are different because the cannot be analyzed further. If we try to imagine a cognitive system that is, somehow, not capable of representing the difference between these two classes of concepts, we surely get into all kinds of trouble. The system can be asked the direct question When you look at the color red, what is the difference between that and the color blue? Because my friend here, who has never been able to see the color blue, would like to know.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"In the face of that direct question, the system is not only supposed to nd no difference between its internal ability to handle the analysis of the [redness] concept and its handling of others, like the [chair] concept, it is also supposed to somehow not notice that its verbal reply contains the peculiarly empty phrase Uh, I cannot think of any way to describe the difference. At some level, it must surely be possible for us to draw the attention of this hypothetical cognitive system to the fact that it is drawing a blank for some kinds of concept and not for othersand as soon as we can draw its attention to that fact, it is on a slippery slope toward the admission that there is a drastic difference between subjective phenomenology and objective concepts. There is something approaching a logical incoherence in the idea that a cognitive system can have      296 Theoretical Foundations of Articial General Intelligence a powerful (i.e.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"human-level) analysis mechanism but also be immune to the failure mode described above. 15.2.5 The That Misses The Point Objection The principal philosophical objection to the above argument is that it misses the point. It explains only the locutions that philosophers produce when talking about consciousness, not the actual experiences they have. The proposed explanation looks like it has slipped from being about the phenomenology, at the beginning, to being about the psychology (the cognitive mechanisms that cause people to say the things they do about consciousness) at the end. That would make this entire proposal into a discussion of a non-hard problem, because the philosopher can listen to the above account and yet still say Yes, but why would that short circuit in my psychological mechanism cause this particular feeling in my phenomenology? Here we come to the crux of the proposed explanation of consciousness. Everything said so far could, indeed, be taken as just another example of a non-hard sidetracking of the core question.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"What makes this a real attempt o address the hard problem of consciousness is the fact that there is a aw in the above objection, because it involves an implicit usage of the very mechanism that is supposed to be causing the trouble. So if someone says There is something missing from this argument, because when I look at my subjective experience I see things (my qualia!) that are not referenced in any way by the argument, what they are doing is asking for an explanation of (say) color qualia that is just as satisfactory as explanations of ordinary concepts, and they are noticing that the proposed explanation is inferior because it leaves something out. But this withinthe-system comparison of consciousness with ordinary concepts is precisely the kind of thought process that will invoke the analysis mechanism! The analysis mechanism inside the mind of the philosopher who raises this objection will then come back with the verdict that the proposed explanation fails to describe the nature of conscious experience, just as other attempts to explain consciousness have failed.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"The proposed explanation, then, can only be internally consistent with itself if the philosopher nds the explanation wanting. There is something wickedly recursive about this situation. The proposed explanation does not address the question of why the phenomenology of the color red should be the way that it isso in a certain respect the explanation could be said to have failed. But at the core of the explanation itself is the prediction that when the explanation is processed through      Human and Machine Consciousness as a Boundary Effect in the Concept Analysis Mechanism 297 the head of a philosopher who tries to nd objections to it, the explanation must necessarily cause the philosophers own analysis mechanism to become short-circuited, resulting in a verdict that the explanation delivers no account of the phenomenology. Do all of the philosophical objections to this argument fall into the same category (i.e. they depend for their force on a deployment of the analysis mechanism that is mentioned in the argument)? I claim that they do, for the following reason.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"The way that Chalmers [2] formulated it, there is a certain simplicity to the hard problem, because whenever an objection is lodged against any proposed resolution of the problem, the objection always works its way back to the same nal point: the proposed explanation fails to make contact with the phenomenological mystery. In other words, the buck always stops with Yes, but there is still something missing from this explanation. Now, the way that I interpret all of these different proposed explanations for consciousnessand the matching objections raised by philosophers who say that the explanation fails to account for the hard problem is that these various proposals may differ in the way that they approach that nal step, but that in the end it is only the nal step that matters.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"In other words, I am not aware of any objection to the explanation proposed in this chapter that does not rely for its force on that nal step, when the philosophical objection deploys the analysis mechanism, and thereby concludes that the proposal does not work because the analysis mechanism in the head of the philosopher returned a null result. And if (as I claim) all such objections eventually come back to that same place, they can all be dealt with in the same way. But this still leaves something of an impasse. The argument does indeed say nothing about the nature of conscious experience, qua subjective experience, but it does say why it cannot supply an explanation. Is explaining why we cannot explain something the same as explaining it? This is the question to be considered next. 15.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"3 The Real Meaning of Meaning This may seem a rather unsatisfactory solution to the problem of consciousness, because it appears to say that our most immediate, subjective experience of the world is an artifact of the operation of the brain. The proposed explanation of consciousness is that subjective phenomenology is a thing that intelligent systems must say they experience (because their analysis mechanism would not function correctly otherwise)but this seems to put consciousness in the same category as visual artifacts, illusions, hallucinations and      298 Theoretical Foundations of Articial General Intelligence the like. But something is surely wrong with this conclusion: it would be bizarre to treat something that dominates every aspect of our waking lives as if it were an artifact. I believe that condemning consciousness as an artifact is the wrong conclusion to draw from the above explanation.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"I am now going to make a case that all of the various subjective phenomena associated with consciousness should be considered just as real as any other phenomena in the universe, but that science and philosophy must concede that consciousness has the special status of being unanalyzable. The appropriate conclusion is that consciousness can be predicted to occur under certain circumstances (namely, when an intelligent system has the kind of powerful analysis mechanism described earlier), but that there are strict limits to what we can say about its nature. We are obliged to say that these things are real, but even though they are real they are beyond the reach of science. 15.3.1 Getting to the Bottom of Semantics The crucial question that we need to decide is what status we should give to the atoms in a cognitive system that have the peculiar property of making the analysis mechanism return a verdict of this is real, but nothing can be said about it.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"To answer this question in a convincing way, we need to understand the criteria we use when we decide:  The realness or validity of different concepts (their epistemology);  The meaning of concepts, or the relationships between concepts and things in the world (their semantics and ontology);  The validity of concepts that are used in scientic explanations. We cannot simply wave our hands and pick a set of criteria to apply to these things, we need to have some convincing reasons to make one choice or another. Who adjudicates the question of which concepts are real and which are artifacts? On what basis can we conclude that some concepts (e.g. the phenomenological essence of redness) can be dismissed as not real or artifactual? There seem to be two options here. One would involve taking an already welldeveloped theory of semantics or ontologyan off-the-shelf theory, so to speakand then applying it to the present case.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"The second would be to take a detailed look at the various semantic/ontological frameworks that are available and nd out which one is grounded most rmly; which one is secure enough in its foundations to be the true theory of meaning.      Human and Machine Consciousness as a Boundary Effect in the Concept Analysis Mechanism 299 Unfortunately, both of these options lead us into a trap. The trap works roughly as follows. Suppose that we put forward a Theory of Meaning (lets call it Theory X), in the hope that Theory X will be so ontologically complete that it gives us the correct or valid method for deciding which concepts are real and which are artifacts; which concepts are scientically valid and which are illusory/insufcient/incoherent. Having made that choice, we can be sure of one thing: given how difcult it is to construct a Theory of Meaning, there will be some fairly abstract concepts involved in this theory.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"And as a result the theory itself will come under scrutiny for its conceptual coherence. Lying at the root of this theory there will be some assumptions that support the rest of the theory. Are those assumptions justied? Are they valid, sufcient or coherent? Are they necessary truths? You can see where this is leading: any Theory of Meaning that purports to be the way to decide whether or not concepts have true meaning (refer to actual things in the world) is bound to be a potential subject of its own mechanism. But in that case the theory would end up justifying its own validity by referring to criteria that it already assumes to be correct. The conclusion to draw from these considerations is that any Theory X that claims to supply absolute standards for evaluating the realness or validity of concepts cannot be consistent. There is no such thing as an objective theory of meaning.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"This circularity or question-begging problem applies equally to issues like the meaning of meaning and explanations of the concept of explanation, and it aficts anyone who proposes that the universe can be discovered to contain some absolute, objective standards for the meanings of things, or for the fundamental nature of explanatory force. 15.3.2 Extreme Cognitive Semantics There is only one attitude to ontology and semantics that seems capable of escaping from this trap, and that is an approach that could be labeled Extreme Cognitive Semanticsthe idea that there is no absolute, objective standard for the mapping between symbols inside a cognitive system and things in the world, because this mapping is entirely determined by the purely contingent fact of the design of real cognitive systems [3, 8]. There is no such thing as the pure, objective meaning of the symbols that cognitive systems use, there is only the way that cognitive systems actually do, as a matter of fact, use them.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"Meanings are determined by the ugly, inelegant design of cognitive systems, and that is the end of it.      300 Theoretical Foundations of Articial General Intelligence How does this impact our attempt to decide the status of those atoms that cause our analysis mechanisms to bottom out? The rst conclusion should be that, since the meanings and status of all atoms are governed by the way that cognitive systems actually use them, we should give far less weight to an externally-imposed formalismlike the possible-worlds semantics popular in articial intelligence [4]which says that subjective concepts point to nothing in the real world (or in functions dened over possible worlds) and are therefore ctitious. Secondand in much the same veinwe can note that the atoms in question are such an unusual and extreme case, that formalisms like traditional semantics should not even be expected to handle them.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"This puts the shoe on the other foot: it is not that these semantic formalisms are capable of dismissing the consciousness-concepts and therefore the latter are invalid, it is rather that the formalisms are too weak to be used for such extreme cases, and therefore they have no jurisdiction in the matter. Finally, we can use the Extreme Cognitive Semantics viewpoint to ask if there is a way to make sense of the idea that various concepts possess different degrees of realness. In order to do this, we need to look at how concepts are judged to be or not be real in ordinary usage. Ordinary usage of this concept seems to have two main aspects. The rst involves the precise content of a concept and how it connects to other concepts. So, unicorns are not real because they connect to our other concepts in ways that clearly involve them residing only in stories. The second criterion that we use to judge the realness of a concept is the directness and immediacy of its phenomenology.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"Tangible, smellable, seeable things that lie close at hand are always more real. Abstract concepts are less real. Interestingly, the consciousness atoms that we have been considering in this argument ([redness], [self] and so on) score very differently on these two measures of realness. They connect poorly to other concepts on their downward side because we cannot unpack them. But on the other hand they are the most immediate, closest, most tangible concepts of all, because they dene what it means to be immediate and tangible. When we say that a concept is more real the more concrete and tangible it is, what we actually mean is that it gets more real the closer it gets to the most basic of all concepts. In a sense there is a hierarchy of realness among our concepts, with those concepts that are phenomenologically rich being the most immediate and real, and with a decrease in that richness and immediacy as we go toward more abstract concepts.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"     Human and Machine Consciousness as a Boundary Effect in the Concept Analysis Mechanism 301 15.3.3 Implications What can we conclude from this analysis? I believe that the second of these two criteria of realness is the one that should dominate. We normally consider the concepts that are closest to our phenomenology to be the ones that are the best-connected and most thoroughly consistent with the rest of our conceptual systems. But the concepts associated with consciousness are an exception to that rule: they have the most immediacy, but a complete lack of connections going to other concepts that explain what they are. If we are forced to choose which of the two criteria is more important, it seems most coherent to treat immediacy as the real arbiter of what counts as real.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"Perhaps the best way to summarize the reason why this should be so is to consider the fact that in ordinary usage realness of a concept is to some extent inherited: if a concept is dened in terms of others that are considered very real, then it will be all the more real. But then it would make little sense to say that all concepts obey the rule that they are more real, valid and tangible, the closer they are to the phenomenological concepts at the root of the tree ... but that the last layer of concepts down at the root are themselves not real. Given these considerations, I maintain that the correct explanation for consciousness is that all of its various phenomenological facets deserve to be called as real as any other concept we have, because there are no meaningful objective standards that we can apply to judge them otherwise. But while they deserve to be called real they also have the unique status of being beyond the reach of scientic inquiry.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"We can talk about the circumstances under which they arise, but we can never analyze their intrinsic nature. Science should admit that these phenomena are, in a profound and specialized sense, mysteries that lie beyond our reach. This seems to me a unique and unusual compromise between materialist and dualist conceptions of mind. Minds are a consequence of a certain kind of computation; but they also contain some mysteries that can never be explained in a conventional way. We cannot give scientic explanations for subjective phenomena, but we can say exactly why we cannot do so. In the end, we can both explain consciousness and not explain it. 15.4 Some Falsiable Predictions This theory of consciousness can be used to make some falsiable predictions.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"We are not yet in a position to make empirical tests of these predictions, because the tests would seem to require the kind of nanotechnology that would let us rewire our brains on the y,      302 Theoretical Foundations of Articial General Intelligence but the tests can be lodged in the record, against the day that some experimentalist can take up the challenge of implementing them. The uniqueness of these predictions lies in the fact that there is a boundary (the edge of the foreground) at which the analysis mechanism gets into trouble. In each case, the prediction is that these phenomena will occur at exactly that boundary, and nowhere else. Bear in mind, however, that we do not yet know where this boundary actually lies, in the implementation that is the human brain.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"If we are able to construct AGI systems that function at the human level of intelligence, with a full complement of cognitive mechanisms that includes the analysis mechanism described earlier, then these predictions will be testable by asking the AGI what it experiences in each of the following cases. 15.4.1 Prediction 1: Blindsight Some kinds of brain damage cause people to experience blindsight, a condition in which the person reports little or no conscious awareness of a certain visual stimulus, while at the same time they can sometimes act on the stimulus as if it were visible [9]. The prediction in this case is that some of the visual pathways in the human brain will be found to lie within the scope of the analysis mechanism, while others will be found to lie outside. The ones outside the scope of the analysis mechanism will be precisely those that, when spared after damage, allow visual awareness without consciousness. 15.4.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"2 Prediction 2: New Qualia If we built three sets of new color receptors in the eyes, with sensitivity to three bands in the ultraviolet range, and if we built enough brain wiring to supply the foreground with new concept-atoms triggered by these receptors, this should give rise to three new color qualia. After acclimatizing to the new qualia, we could then swap connections on the old color receptors and the new UV pathways, at a point that lies just outside the scope of the analysis mechanism. The prediction here is that the two sets of color qualia will be swapped in such a way that the new qualia will be associated with the old visible-light colors, and that this will only occur if the swap happens beyond the analysis mechanism. If we then removed all trace of the new UV pathways and retinal receptors, outside the foreground (beyond the reach of the analysis mechanism), then the old color qualia would disappear, leaving only the new qualia.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"The subject will have a ghost of a memory of the old color qualia, because the old concept atoms will still be there, but those atoms will only      Human and Machine Consciousness as a Boundary Effect in the Concept Analysis Mechanism 303 be seen in imagination. And if we later reintroduce a set of three color receptors and do the whole procedure again, we can bring back the old color qualia if we are careful to ensure that the new visual receptors trigger the foreground concept-atoms previously used for the visible-light colors. The subject would suddenly see the old qualia again 15.4.3 Prediction 3: Synaesthetic Qualia Take the system described above (after the rst installation of new qualia) and arrange for a cello timbre to excite the old concept-atoms that would have caused red qualia. Cello sounds will now cause the system to have a disembodied feeling of redness. 15.4.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"4 Prediction 4: Mind Melds Join two minds so that B has access to the visual sensorium of A, using new conceptatoms in Bs head to encode the incoming information from A. B would say that she knew what As qualia were like, because she would be experiencing new qualia. If B were getting sounds from As brain, but these were triggering entirely new atoms designed especially to encode the signals, B would say that A did not experience sound the way she did, but in an entirely new way. If, on the other hand, the incoming signals from A triggered the same sound atoms that B uses (with no new atom types being created), then B will report that she is hearing all of As sonic input mixed in with her own. In much the same way, B could be given an extra region of her foreground periphery exclusively devoted to the visual stream coming from A.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"She would then say that she had two heads, but that she could only attend to one of them at a time. With new atoms for the colors, again, she would report that Bs qualia differed from her own. Note that any absolute comparison between the way that different people experience the world is not possible. The reported qualia in these mind-meld cases would be entirely dependent on choices of how to cross-wire the systems. 15.5 Conclusion The simplest explanation for consciousness is that the various phenomena involved have an irreducible dual aspect to them. On the one hand, they are explicable because we can understand that they are the result of a powerful cognitive system using its analysis mechanism to probe concepts that happen to be beyond its reach.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"But on the other hand, these concepts deserve to be treated as the most immediate and real objects in the universe,      304 Theoretical Foundations of Articial General Intelligence because they dene the very foundation of what it means for something to be real. These consciousness conceptssuch as the subjective phenomenological experience of the color redcannot be explained by any further scientic analysis. Rather than try to resolve this situation by allowing one interpretation to trump the other, it seems more parsimonious to conclude that both are true at the same time, and that the subjective aspects of experience belong to a new category of their own: they are real but inexplicable, and no further scientic analysis of them will be able to penetrate their essential nature.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"According to this analysis, an Articial General Intelligence designed in such a way that it had the same problems with its analysis mechanism that we humans do (and I have argued that this would mean any fully sentient computer capable of a near-human degree of intelligence, because the analysis mechanism plays such a critical role in all types of general intelligence) would experience consciousness for the same reasons that we do. We could never prove this statement the way that we prove statements about objective concepts, but that is part of what it means to say that consciousness concepts have a special status (they are real, but beyond analysis). The only way to be consistent about our interpretation of these phenomena is to say that, insofar as we can say anything at all about consciousness, we can be sure that the right kind of articial general intelligence would experience a subjective phenomenology comparable in scope to human subjective consciousness. Bibliography [1] Cambell, K. K. (1970).",Theoretical Foundations of Artificial General Intelligence,chapter 15
"Body and Mind (Doubleday, New York). [2] Chalmers, D. J. (1996). The Conscious Mind: In Search of a Fundamental Theory (Oxford University Press, Oxford). [3] Croft, W. and Cruse, D. A. (2004). Cognitive Linguistics (Cambridge University Press, Cambridge). [4] Dowty, D. R., Wall, R. E., and Peters, S. (1981). Introduction to Montague Semantics (D. Reidel, Dordrecht). [5] Kirk, D. (1974). Zombies versus materialists, Aristotelian Society 48 (suppl.), pp. 13552. [6] Loosemore, R. P. W. (2007). Complex Systems, Articial Intelligence and Theoretical Psychology, in B. Goertzel and P. Wang (eds.",Theoretical Foundations of Artificial General Intelligence,chapter 15
"Advances in Articial General Intelligence: Concepts, Architectures and Algorithms (IOS Press, Amsterdam), pp. 159173. [7] Loosemore, R. P. W. and Harley, T. A. (2010). Brains and Minds: On the Usefulness of Localization Data to Cognitive Psychology, in M. Bunzl and S. J. Hanson (eds.), Foundational Issues in Human Brain Mapping (MIT Press, Cambridge, MA), pp. 217240. [8] Smith, L. B. and Smith, L. K. (1997). Perceiving and Remembering: Category Stability, Variability, and Development, in K. Lamberts and D. Shanks (eds.), Knowledge, Concepts, and Categories (Cambridge University Press, Cambridge). [9] Weiskrantz, L. (1986). Blindsight: A Case Study and Implications (Oxford University Press, Oxford).",Theoretical Foundations of Artificial General Intelligence,chapter 15
"  Chapter 16 Theories of Articial Intelligence  Meta-theoretical considerations Pei Wang Temple University, Philadelphia, USA http://www.cis.temple.edu/pwang/ This chapter addresses several central meta-theoretical issues of AI and AGI. After analyzing the nature of the eld, three criteria for desired theories are proposed: correctness, concreteness, and compactness. The criteria are claried in the AI context, and using them, the current situation in the eld is evaluated. 16.1 The problem of AI theory Though it is a common practice for a eld of science or engineering to be guided and identied by the corresponding theories, the eld of Articial Intelligence (AI) seems to be an exception.",Theoretical Foundations of Artificial General Intelligence,chapter 16
"After more than half of a century since its formation, AI still has no widely accepted theory, and in the related discussions the following opinions are often heard:  The best model of intelligence is the human brain itself (and all theories are merely poor approximations...)  There is no need for any new theory, since AI can be built according to X (depending on who said it, the X can be mathematical logic, probability theory, theory of computation, ...)  A theory of AI has to be established piece by piece, and we are starting from Y (depending on who said it, the Y can be search, reasoning, learning, perception, actions, ...)  There cannot be any good theory of intelligence (since intelligence is so complicated, though our work is obviously central to it ...)  305      306 Theoretical Foundations of Articial General Intelligence  Theoretical debating is a waste of time (and we should focus on practical applications.",Theoretical Foundations of Artificial General Intelligence,chapter 16
"For example, an intelligent system should be able to ...)  A good theory only comes at the end of the research (so dont worry about it now, and it will come as long as we continue the current research on ...) There is a historical reason for this situation. Though the idea of thinking machine can be traced further back in history, the eld of AI was started from the realization that computers, though initially designed to do numerical calculations, can be made to carry out other mental activities, such as theorem proving and game playing, which are hard intellectual problems that are usually considered as demanding intelligence [12,28]. This problem-oriented attitude toward AI focuses on the problem-solving capability of a computer system, while does not care much for the underlying theory. Consequently, the early works in AI often showed the Look, ma, no hands syndrome  A paper reports that a computer has been programmed to do what no computer program has previously done, and that constitutes the report.",Theoretical Foundations of Artificial General Intelligence,chapter 16
"How science has been advanced by this work or other people are aided in their work may be unapparent. [25]. For such a work, the question, Wheres the AI? is a tough one to answer [40]. To many AI researchers, the lack of a common theory is not an issue at all. As said by Minsky [30], Our minds contains processes that enable us to solve problems we consider difcult. Intelligence is our name for whichever of those processes we dont yet understand. According to this opinion, a theory of AI is impossible by denition, since we cannot have a theory for those processes we dont yet understand  as soon as we have a good theory for such a process, it is no longer considered as AI anymore [30,40].",Theoretical Foundations of Artificial General Intelligence,chapter 16
"To get out of this annoying situation, in mainstream AI intelligence is treated as the collaboration of a group of loosely coupled functions, each of them can be separately specied in computational and algorithmic terms, implemented in computers, and used to solve certain practical problems [24, 39]. In an inuential AI textbook by Russell and Norvig [39], it is written that in the late 1980s AI adopts the scientic method, since It is now more common to build on existing theories than to propose brand new ones .... However, it is not mentioned that none of those existing theories were proposed with intelligence as the subject matter, nor has shown the potential of solving the problem of intelligence as a whole. Though in this way the eld has produced valuable results in the past decades, it still suffers from internal fragmentation [5] and paradigmatic mess [11], largely due to the      Theories of Articial Intelligence 307 lack of a common theoretical foundation.",Theoretical Foundations of Artificial General Intelligence,chapter 16
"There have been many debates on the nature or objective of the eld, or on what type of theory it should or can have [8,21,42,50]. Though the pursuit of unied theories of AI is widely considered as futile in the eld, there is still a small number of AI researchers who believe that such a theory is possible, and worthwhile to be investigated. The best known work in this direction is the Unied Theories of Cognition by Newell [31], in which he argued for the necessity for AI and cognitive science to have unied theories, and proposed his theory, which attempts to cover both AI and human intelligence. Similar attempts include the works of Albus [1] and Pollock [36]. In recent years, the term Articial General Intelligence (AGI) is adopted by a group of AI researchers to emphasize the general-purpose and holistic nature of the intelligence they are after [17, 49].",Theoretical Foundations of Artificial General Intelligence,chapter 16
"Since AGI treats intelligence as a whole, there are more efforts to establish unied theories [3, 4, 6, 9, 13, 15, 20, 41, 46], though none of them is mature or convincing enough to obtain wide acceptance in the eld at the current moment [7]. Even though the AGI community is more AI-theory-oriented than mainstream AI, not every AGI project is based on some theory about intelligence. As in mainstream AI, a project is often guided by one, or more than one, of the following considerations: Practical problem-solving demands: Since intelligence is displayed in the problemsolving capability of a system, many projects target problems that currently can be solved by humans only. Such a system is usually designed and analyzed according to the theory of computation [19,24].",Theoretical Foundations of Artificial General Intelligence,chapter 16
"Knowledge about human intelligence: Since the human mind has the best-known form of intelligence, many projects aim at duplicating certain aspects of the human mind or brain. Such a system is usually designed and analyzed according to the theories in psychology or neuroscience [31,38]. Available normative models: Since intelligence intuitively means to do the right thing, many projects are designed and analyzed as models of rationality or optimization, according to mathematical theories like classical logic and probability theory, though usually with extensions and/or revisions [26,35]. Even the AGI projects that are based on certain theories on AI are moving in very different directions, mainly because of the difference in their theoretical foundations, as well as the inuence of the above considerations. This collection provides a representative example of the diversity in the theoretical study of AGI.",Theoretical Foundations of Artificial General Intelligence,chapter 16
"     308 Theoretical Foundations of Articial General Intelligence Consequently, currently in the eld of AI/AGI there are very different opinions on research goal [23, 47], roadmap [16,27], evaluation criteria [2, 22], etc. Though each researcher can and should make decisions on the above issues for his/her own project, for the eld as a whole this paradigmatic mess makes comparison and cooperation difcult, if not impossible.",Theoretical Foundations of Artificial General Intelligence,chapter 16
"In this chapter, I will not promote my own theory of AI (which is described in my previous publications [45, 46, 48]), nor to evaluate the other theories one by one, but to address the major meta-level issues about AI theories, such as  What is the nature of an AI theory?  How to evaluate an AI theory?  Why do we lack a good theory? This chapter attempts to clarify the related issues, so as to pave the way to a solid theoretical foundation for AGI, which is also the original and ultimate form of AI. For this reason, in the following AI is mainly used to mean AGI, rather than the current mainstream practice. 16.2 Nature and content of AI theories In a eld of science or engineering, a theory usually refers to a system of concepts and statements on the subject matter of the eld.",Theoretical Foundations of Artificial General Intelligence,chapter 16
"Generally speaking, there are two types of theory: Descriptive theory: Such a theory starts with certain observations in the eld. The theory provides a generalization and explanation of the observations, as well as predictions for future events, so as to guide peoples behaviors. The theories in natural science are the best examples of this type. Normative theory: Such a theory starts with certain assumptions, then derives conclusions from them. When the assumptions are accepted as applicable in a eld, all the conclusions should also be accepted as true. Mathematics and engineering theories are the best examples of this type.1 1In elds like economics and law, a normative theory or model species what people should do, often for ethical reasons. It is not what the word means here. Instead, in this chapter a normative theory species what people should do for rational reasons. This usage is common in the study of human reasoning and decision making, for example see [14].",Theoretical Foundations of Artificial General Intelligence,chapter 16
"     Theories of Articial Intelligence 309 Though it is possible for these two types of theory to interweave (in the sense that parts of a theory may belong to the other type), for a theory as a whole its type is still usually clear. For example, modern physics uses a lot of mathematics in it, but it does not change the overall descriptive nature of the theories in physics. On the contrary, computer science is mainly based on normative theories on how to build and use computer systems, even though empirical methods are widely used to test the systems.2 What makes a Theory of AI special on this aspect is that it needs to be both descriptive and normative, in a certain sense. AI studies the similarity and the difference between The Computer and the Brain, as suggested by the title of [44].",Theoretical Foundations of Artificial General Intelligence,chapter 16
"This research is directly driven by the observation that though the computer systems can take over humans mental labor in many situations (and often do a better job), there are nevertheless still many features of the human mental activities that have not been reproduced by computers. An AI theory should provide a bridge over this gap between the Brain and the Computer, so as to guide the designing and building of computer systems that are similar to the human mind in its mental power. Intelligence is simply the word whose intuitive meaning is the closest to the capability or property to be duplicated from the brain to the computer, though some people may prefer to use other words like cognition, mind, or thinking. The choice of word here does not change the nature of this problem too much. Given this objective, an AI theory must identify the (known or potential) similarities between two entities, the Brain and the Computer, which are very different on many aspects.",Theoretical Foundations of Artificial General Intelligence,chapter 16
"Furthermore, human intelligence is an existing phenomenon, while computer intelligence is something to be built, for which an accurate description does not exist at this moment. Consequently, an AI theory should be descriptive with respect to human intelligence (not in all details, but in basic principles, functions and mechanisms), and at the same time, be normative to computer intelligence. That is, on one hand, the theory should summarize and explain how the human mind works, at a proper level and scope of description; on the other hand, it should guide the design and development of computer systems, so as to make them just like the human mind, at the same level and scope of description. A theory for this eld is surely centered at the concept of intelligence.",Theoretical Foundations of Artificial General Intelligence,chapter 16
"Accurately speaking, there are three concepts involved here: Human Intelligence (HI), the intelligence as displayed by human beings; Computer Intelligence (CI), the intelligence as to be displayed by computer systems; 2On this topic, I disagree with Newell and Simons opinion on Computer science as empirical inquiry [32].      310 Theoretical Foundations of Articial General Intelligence General Intelligence (GI), the general and common description of both HI and CI. For the current discussion, HI can also be referred to as natural intelligence, CI as articial intelligence, and GI simply as intelligence, which also covers other concepts like animal intelligence, collective intelligence, alien intelligence, etc., as special cases [48]. Roughly speaking, the content of the theory must cover certain mechanisms in the human mind (as the HI), then generalize and abstract them (to be the GI), and nally specify them in a computational form (to become the CI).",Theoretical Foundations of Artificial General Intelligence,chapter 16
"No matter what names are used, the distinction and relationship among the three concepts are necessary for an AI theory, because the theory needs to identify the common properties between human beings and computer systems, while still to acknowledge their differences in other aspects.3 Now it is easy to see that in an AI theory, the part about HI is mostly descriptive, that about CI is mostly normative, and that about GI is both. The human mind is a phenomenon that has been studied by many branches of science from different perspectives and with different focuses. There have been many theories about it in psychology, neuroscience, biology, philosophy, linguistics, anthropology, sociology, etc. When talking about HI, what AI researchers usually do is to selectively acquire concepts and conclusions from the other elds, and to reorganize them in a systematic way. As a result, we get a theory that summarizes certain observed phenomenon of the human mind.",Theoretical Foundations of Artificial General Intelligence,chapter 16
"Such a theory is fundamentally synthetic and empirical, in that its conclusions are summaries of common knowledge on how the human mind works, so it is veried by comparing its conclusions to actual human (mental) activities. Here the procedure is basically the same as in natural science. The only special thing is the selectivity coming from the (different) understandings of the concept intelligence: different researchers may include different phenomena within the scope of HI, which has no natural boundary. On the contrary, a theory about CI has to be normative, since this phenomenon does not exist naturally, and the main function of the theory is to tell the practitioners how to produce it. As a normative theory, its basic assumptions come from two major sources: knowledge of intelligence that describes what should be done, and knowledge of computer that describes what can be done.",Theoretical Foundations of Artificial General Intelligence,chapter 16
"Combined together, this knowledge can guide the whole 3Some people may argue that AI researchers are only responsible for the CI part of the picture, because the HI part should be provided by psychologists, and the GI part should be covered by a theory of general intelligence, contributed by philosophers, logicians, mathematicians, and other researchers working on general and abstract systems. Though there is some truth in this argument, at the current time there is no established theory of GI that we AI researchers can accept as guidance, so we have to work on the whole picture, even though part of it is beyond our career training.      Theories of Articial Intelligence 311 design and development process, by specifying the design objective, selecting some theoretical and technical tools, drawing a blueprint of the systems architecture, planning a development roadmap, evaluating the progress, and verifying the results. Here the procedure is basically the same as in engineering.",Theoretical Foundations of Artificial General Intelligence,chapter 16
"The only special thing is the selectivity coming from the (different) understandings of the concept intelligence: different researchers may dene the concept differently, which will change everything in the following development. As the common generalization of HI and CI, a theory of GI is both descriptive and normative. On one hand, the theory should explain how human intelligence works as a special case, and on the other hand, it should describe how intelligence works in general, so as to guide how an intelligent computer system should be designed. Therefore, this theory should be presented in a medium-neutrallanguage that does not assume the special details of either the human brain or the computer hardware. At the same time, since it is less restricted by the low-level constraints, this part of the theory gives the researchers the largest freedom, compared to the HI and the CI part.",Theoretical Foundations of Artificial General Intelligence,chapter 16
"Consequently, this is also where the existing theories differ most from each other  the differences among the theories are not much on how the brain, mind, or computer works, but on where the brain and the machine should be similar to each other [47]. In the textbook by Russell and Norvig [39], different approaches toward AI are categorized according to whether they are designed to be thinking or acting humanly or rationally. It seems that the former is mainly guided by descriptive theories, while the latter by normative theories. Though such a difference indeed exists, it is more subtle than what these two words suggest. Since the basic assumptions and principles of all models of rationality come from abstraction and idealization of the human thinking process, rationally thinking/acting is actually a special type of humanly thinking/acting.",Theoretical Foundations of Artificial General Intelligence,chapter 16
"For example, though the Universal AI model AIXI by Hutter [20] is presented in a highly abstract and mathematical form, its understanding of intelligence is still inspired and justied according to certain opinions about the notion in psychology [23]. On the other extreme, though Hawkins HTM model of intelligence is based on certain neuroscientic ndings, it is not an attempt to model the human brain in all aspects and in all details, but to selectively emulate certain mechanisms that are believed to be the crux of intelligence [18].",Theoretical Foundations of Artificial General Intelligence,chapter 16
"Therefore, the difference between AIXI and HTM, as well as among the other AGI models, is not on whether to learn from the human brain/mind (the answer is always yes, since it is the best-known form of intelligence), or whether to idealize and simplify the knowledge obtained from the human brain/mind (the answer is also always yes, since a computer      312 Theoretical Foundations of Articial General Intelligence cannot become identical to the brain in all aspects), but on where to focus and how much to abstract and generalize.",Theoretical Foundations of Artificial General Intelligence,chapter 16
"From the same knowledge about the human mind, there are many meaningful ways to establish a notion of HI, by focusing on different aspects of the phenomena; from the same notion of HI, there are many meaningful ways to establish a notion of GI, by describing intelligence on different levels, with different granularities and scopes; from the same notion of GI, there are many meaningful ways to establish a notion of CI, by assuming different hardware/software platforms and working environments. The systems developed according to different notions will surely have different properties and practical applications, and are similar to the human mind in different senses. Unless one commits to a particular denition of intelligence, there is no absolute standard to decide which of these ways is the correct way to establish a theory of AI. The current collection to which this chapter belongs provides a concrete example for this situation: though the chapter authors all use the notion of intelligence, and are explaining related phenomena, the theories they proposed are very different.",Theoretical Foundations of Artificial General Intelligence,chapter 16
"It is not necessarily the case that at most one of the theory is correct or really captures intelligence as it is, while all the others are wrong, since each of them represents a certain perspective; nor can the issue be resolved by pooling the perspectives altogether, because they are often incommensurable, due to the usage of different concepts. This diversity is a major source of difculty in theoretical discussions of AI. 16.3 Desired properties of a theory Though there are reasons for different AI theories to be proposed, it does not mean that all of them are equally good. The following three desired properties of a scientic theory are proposed and discussed in my own theory of intelligence [48] (Section 6.2):  Correctness: A theory should be supported by available evidence.  Concreteness: A theory should be instructive in problem solving.  Compactness: A theory should be simple.",Theoretical Foundations of Artificial General Intelligence,chapter 16
"Though these properties are proposed for scientic theories in general, in this chapter they will be discussed in the context of AI. Especially, let us see what they mean for an AI theory that is both descriptive (for human minds) and normative (for computer systems).      Theories of Articial Intelligence 313 Correctness Since the best-known form of intelligence is human intelligence, an AI theory is correct if it is supported by the available knowledge about the human mind. In this aspect, AI is not that different from any natural science, in that the correctness of a theory is veried empirically, rather than proved according to some priori postulates. Since the study of the human mind has been going on in many disciplines for a long time, AI researchers often do not need to carry out their own research on human subjects, but to inherit the conclusions from the related disciplines, including (though not limited to) psychology, linguistics, philosophy, neuroscience, and anthropology.",Theoretical Foundations of Artificial General Intelligence,chapter 16
"This task is not as simple as it sounds, since an AI theory cannot simply copy the concepts and statements from the related disciplines  to form the HI part of the theory, selection is needed; to form the GI part of the theory, generalization is needed. Intelligence is not related to every aspect of a human being, and AI is not an attempt to clone a human. Even though the concept of intelligence has many different understandings, it is mainly about the mental properties of human beings, rather than their physical or biological properties (though those properties have impacts in the content of human thought). Furthermore, even only for lexical considerations, the notion of Intelligence should be more general than the notion of Human Intelligence, so as to cover the non-human forms of intelligence. Therefore, an AI theory needs to decide the boundary of its empirical evidence, by indicating which processes and mechanisms in the human mind/brain/body is directly relevant to AI, and which of them are not.",Theoretical Foundations of Artificial General Intelligence,chapter 16
"In other words, an AI theory must specify the scope and extent to which a computer is (or will be) similar to a human. The following two extreme positions on this issue are obviously improper  if HI is specied in such a tight way that is bounded to all aspects of a human being, non-human intelligence would be impossible by denition; if HI is specied in such a loose way that the current computer systems are already intelligent by denition, AI will be trivialized and deserves no attention. This task uniquely belongs to AI theories, because even though there are many studies on the human mind in the past in the related disciplines, little effort is made to separate the conclusions about intelligence (or cognition, mind) in general from those about human intelligence (or human cognition, human mind) in specic. For example, though there is a huge literature on the psychological study of human intelligence, which is obviously related to AI, an AI theory cannot use the conclusions indiscriminately.",Theoretical Foundations of Artificial General Intelligence,chapter 16
"This is because the notion of intelligence is used in psychology as an      314 Theoretical Foundations of Articial General Intelligence attribute where the difference among human beings is studied, while in AI it is an attribute where the difference between humans and computers is much more important. Many common properties among human beings are taken for granted in psychology, so they are rarely addressed in psychological theories. On the contrary, these properties are exactly what AI tries to reproduce, so they cannot be omitted in AI theories. For this reason, it is not helpful to directly use human IQ tests to evaluate the intelligence of a computer system. Similarly, the correctness of an AI theory cannot be judged in the same way as a theory in a related empirical discipline, such as psychology. On the other hand, the humancomputer difference cannot be used as an excuse for an AI theory to contain conclusions that are clearly inconsistent with the existing knowledge of the human mind.",Theoretical Foundations of Artificial General Intelligence,chapter 16
"In the current context, even a theorem proved in a formal theory is not necessarily correct as a conclusion about intelligence, unless the axioms of the theory can be justied as acceptable in AI. If a normal human being is not intelligent according to an AI theory, then the theory is not really about intelligence as we know it, but about something else. This is especially the case for the GI part of the theory  even though generalization and simplication are necessary and inevitable, overgeneralization and oversimplication can cause serious distortion in a theory, to the extent that it is no longer relevant to the original problem. For an AI theory to be correct, it does not need to explain every phenomenon of the human mind, but only those that are considered as essential for HI by the theory. Though each theory may select different phenomena, there are some obvious features that should be satised by every theory of intelligence.",Theoretical Foundations of Artificial General Intelligence,chapter 16
"Suggested features are exemplied by the requirement of being general [7], or being adaptive and can work with insufcient knowledge and resources [48]. At the current time, the correctness of an AI theory is usually a matter of degree. The existence of certain counterevidence rarely falsies a theory completely (as suggested by Popper [37]), though it does decrease its correctness, and therefore its competitiveness when compared with other theories. We will return to this topic later. Concreteness The practical value of a scientic theory shows in the guidance it provides to human activities. In the current context, this requirement focuses on the relation between an AI theory (especially the CI part) and the computer systems developed according to it.",Theoretical Foundations of Artificial General Intelligence,chapter 16
"     Theories of Articial Intelligence 315 Since the objective of AI is to build thinking machines, the content of an AI theory needs to be concrete enough to be applicable into system design and development, even though it does not have to specify all the technical details. This requirement means that a pure descriptive theory about how human intelligence works will not qualify as a good AI theory. In the theoretical discussions of AI and Cognitive Science, there are some theories that sound quite correct. However, they are very general, and use fuzzy and ambiguous concepts, so seem to be able to explain everything. What is missing in these theories, however, is the ability of making concrete, accurate, and constructive suggestions on how to build AI systems. Similarly, it is a serious problem if a theory of AI proposes a design of AI systems, but some key steps in it cannot be implemented  for example, the AIXI model is uncomputable [20].",Theoretical Foundations of Artificial General Intelligence,chapter 16
"Such a result cannot be treated as an unfortunate reality about intelligence, because the involved notion of intelligence is a construct in the theory, rather than a naturally existing phenomenon objectively described by the theory. The human mind has provided an existing proof for the possibility of intelligence, so there is no reason to generalize it into a notion that cannot be realized in a physical system. In summary, a good AI theory should include a description of intelligence using the terminology provided by the existing computer science and technology. That is, the theory not only needs to tell people what should be done, but also how to do it. To guide the building of AI systems does not necessarily mean these systems come with practical problem-solving capability. It again depends on the working denition of intelligence. According some opinion, intelligence means to be able to solve humansolvable problems [33], so an AI theory should cover the solutions to various practical problems.",Theoretical Foundations of Artificial General Intelligence,chapter 16
"However, there are also theories that do not take intelligence as problemsolving capability, but learning capability [46]. According to such a theory, when an AI system is just built, it may have little problem-solving ability, like a human baby. What it has is the potential to acquire problem-solving ability via its interaction with the environment. The requirement of concreteness allows both of the previous understandings of intelligence  no matter how this concept is interpreted, it needs to be realized in computer systems. To insist that the CI part of an AI theory must be presented using concrete (even computational) concepts does not mean that the theory of AI can be replaced by the existing theories of computer science. Not all computer systems are intelligent, and AI is a special type of computer systems that is designed according to a special theory.",Theoretical Foundations of Artificial General Intelligence,chapter 16
"It is just like that a      316 Theoretical Foundations of Articial General Intelligence theory of architecture cannot be replaced by a theory of physics, even though every building is constructed from physical components with physical relations among them. The claim that AI needs no theory beyond computer science [19] cannot explain the obvious difference between the human mind and the conventional computers. Compactness While the previous two properties (correctness and concreteness) are about the external relation between an AI theory and outside systems (human and computer, respectively), compactness is a property of the internal structure of the theory. Here the word compactness is used to mean the conceptual simplicity of a theorys content, not merely on its size measured literally. Since scientic theories are used to guide human behaviors, simple theories are preferred, because they are easier to use and to maintain (to verify, to revise, to extend, etc.).",Theoretical Foundations of Artificial General Intelligence,chapter 16
"This opinion is well known in various forms, such as Occams Razor or Machs Economy of Thought, and is accepted as a cornerstone in several AGI theories [4,20,41]. To establish a compact theory in a complicated domain, two common techniques are axiomatization and formalization. Axiomatization works by compressing the core of the theory into a small number of fundamental concepts and statements, to be taken as the basic notions and axioms of the theory. The other notions are dened recursively from the basic ones, and the other statements are proved from the axioms as theorems. Consequently, in principle the theory can be reduced to its axioms. Besides efciency in usage, axiomatization also simplies the verication of the theorys consistency and applicability. Formalization works by representing the notions in a theory by symbols in an articially formed language, rather than by words in a naturally formed language.",Theoretical Foundations of Artificial General Intelligence,chapter 16
"Consequently, the notions have relatively clear and unambiguous meaning. The same theory can also be applied to different situations, by giving its symbols different interpretations. Even though it looks unintuitive to outsiders, a formal theory is actually easier to use for various purposes. Axiomatization and formalization are typically used in mathematics, as well as in logic, computer science, and other normative theories. The same idea can also be applied to empirical science to various degrees, though because the very nature of those theories, they cannot be fully axiomatized (because they must open to new evidence) or fully formalized      Theories of Articial Intelligence 317 (because their key concepts already have concrete meaning associated, and cannot be taken as symbols waiting to be interpreted). Since a theory of AI has empirical content, it cannot be fully axiomatized or formalized, neither.",Theoretical Foundations of Artificial General Intelligence,chapter 16
"Even so, it is still highly desired for it to move in that direction as far as possible, by condensing its empirical content into a small set of assumptions and postulations, then deriving the other part of the theory from it using justiable inference rules. To a large extent, it is what a Serious Computational Science demands, with the requirements of being cohesive and theorem-guided [7]. 16.4 Relations among the properties To summarize the previous discussions, a good AI theory should provide a correct description about how intelligence works (using evidence from human intelligence), give concrete instructions on how to produce intelligence in computer systems (using feasible techniques), and have a compact internal structure (using partial axiomatization and formalization). These three requirements are independent, in the sense that in general there is no (positive or negative) correlation among them. All the three Cs are desired in a theory, for different reasons, and one cannot be reduced into, or replaced by, the others.",Theoretical Foundations of Artificial General Intelligence,chapter 16
"For example, a simpler theory is not necessarily more correct or less correct, when compared with other theories. On this topic, one usual misconception is about Occams Razor, which is often phrased as Simpler theories are preferred, because they are more likely to be correct. This is not proper, since the original form of this idea was just something like Simpler theories are preferred, and it is not hard to nd examples where simpler theories are actually less correct. A common source of this misconception is the assumption that the only desired feature of a scientic theory is its correctness (or truth)  in that case, if simpler theories are preferred, the preference must come from its correctness. However, generally speaking, compactness (or simplicity) is a feature that is preferred for its own sake, rather than as an indicator of correctness.",Theoretical Foundations of Artificial General Intelligence,chapter 16
"It is like when we compare several products, we prefer cheaper ones when everything else is about the same, though it does not mean that we prefer cheaper products because they usually have higher quality. Here price and quality are two separate factors inuencing our overall preference, and additional information is needed to specify their relationship.4 4Some people may argue that a simpler theory is more correct because it is less likely to be wrong, but if a theory becomes simpler by saying less, such a simplication will make the theory covers less territory, so it will also      318 Theoretical Foundations of Articial General Intelligence In certain special situations, it is possible for the requirements to be taken as correlated. One such treatment is Solomonoffs universal prior, which assumes that without domain knowledge, the simpler hypotheses have higher probability to be correct [43].",Theoretical Foundations of Artificial General Intelligence,chapter 16
"Though Solomonoffs model of induction has its theoretical and practical values, the soundness of its application to a specic domain depends on whether the assumptions of the model, including the above one, can be satised (exactly or approximately) in the domain. For the related AGI models (such as AIXI [20]), such justications should be provided, rather than taken for granted. After all, we often meet simple explanations of complex phenomena that turn out to be wrong, and Occams Razor cannot be used as an argument for the correctness of a theory (though it can be an argument for why a theory is preferred). For the same reason, a formal theory is not necessarily more correct than an informal one, though the former is indeed preferred when the other features of the two theories are similar.",Theoretical Foundations of Artificial General Intelligence,chapter 16
"These three Cs are arguably complete, because altogether they fully cover the subject matter: the descriptive ingredients of the theory need to be correct, the normative ingredients need to be concrete, and the whole theory needs to be compact. Of course, each of the three can be further specied with more details, while all of them must be possessed by a theory that is about intelligence, rather than only about one part or one aspect of it. All three Cs can only be relatively satised. As a result, though probably there will not be a perfect theory of AI, there are surely better theories and not-so-good ones. When a theory is superior to another one in all three dimensions, it is generally better. If it is superior in one aspect, but inferior in another, then whether it is better for the current purpose depends on how big the differences are, as well as on the focus of the comparison.",Theoretical Foundations of Artificial General Intelligence,chapter 16
"Intuitively speaking, we can think the overall score on the competitiveness of an AI theory as a multiplication of the three scores it obtains on the three Cs, though we do not have numerical measurements for the scores yet. In this way, an acceptable theory must be acceptable in all the three dimensions. Even if a theory is excellent in two aspects, it still can be useless for AI if it is terrible in the third. 16.5 Issues on the properties In the current theoretical explorations in AGI, a common problem is to focus on some desired property of a theory, while ignoring the others. have less supporting evidence. To simply remove some conclusions from a theory does not make it more correct, unless correctness is dened according to Poppers falsication theory about science [37], so the existence of supporting evidence does not contribute to the correctness of a theory. Such a denition is not accepted here.",Theoretical Foundations of Artificial General Intelligence,chapter 16
"     Theories of Articial Intelligence 319 Issues on correctness typically happen in formal or computational models of intelligence. Sometimes people think as long as they make it clear that a model is based on idealized assumptions, they can assume whatever they want (usually the assumptions required by their available theoretical or technical tools). For example, Schmidhuber thought that for AI systems, the assumption of Markovian environments is too strong, so We will concentrate on a much weaker and therefore much more general assumption, namely, that the environments responses are sampled from a computable probability distribution. If even this weak assumption were not true then we could not even formally specify the environment, leave alone writing reasonable scientic papers about it. [41] It is true that every formal and computational model is based on some idealized assumptions, which are usually never fully satised in realistic situations.",Theoretical Foundations of Artificial General Intelligence,chapter 16
"However, this should not be taken as an excuse to base the model on highly unrealistic assumptions or assumptions that can only be satised in special situations. Since the conclusions of the model are largely determined by its assumptions, an improper assumption may completely change the nature of the problem, and consequently the model will not be about intelligence (as we know it) at all, but about something else. One cannot force people to accept a new denition of intelligence simply because there is a formal or computational model for it  it reminds us the famous remark of Abraham Maslow: If you only have a hammer, you tend to see every problem as a nail. We do want AI to become a serious science, but to change the problem into a more manageable one is not the way to get there. On the other hand, to overemphasize correctness at the cost of the other requirements also leads to problems.",Theoretical Foundations of Artificial General Intelligence,chapter 16
"The Model Fit Imperative analyzed by Cassimatis (Chapter 2 of this book) is a typical example. A theory of AI is not responsible for explaining or reproducing all the details of human intelligence. The most biologically (or psychologically) accurate model of the human brain (or mind) is not necessarily the best model for AI. Issues on concreteness typically happen in theories that have rich philosophical content. Though philosophical discussions are inevitable in AI theories, to only present a theory at that level of description is often useless, and such a discussion quickly degenerates into word games, which is why among AI researchers this is a philosophical problem is often a way to say It doesnt matter or You can say whatever you want about it. Similarly, if some theory contains descriptions that nobody knows how to implement or even to approximate, such a theory will not be very useful for AI.",Theoretical Foundations of Artificial General Intelligence,chapter 16
"Just to solve the AI problem in principle is not enough, unless those principles clearly lead to technical decisions in design and development, even if not in all details.      320 Theoretical Foundations of Articial General Intelligence Issues on compactness widely exist in AGI projects that are mainly guided by psychological/biological inspirations or problem-solving capabilities. Such a project is usually based on a theory that basically treats intelligence as a collection of cognitive functions that are organized into a cognitive architecture (see Chapters 7 and 8 of this book). One problem about this approach is that the functions recognized in the human mind are not necessarily carried out by separate processes or mechanisms. In a psychological theory, sometimes it is reasonable to concentrate on one aspect of intelligence, but such a practice is not always acceptable in an engineering plan to realize intelligence, since to reproduce a single mechanism of intelligence may be impossible, given its dependency on the other mechanisms.",Theoretical Foundations of Artificial General Intelligence,chapter 16
"For example, reasoning and learning may be two aspects of the same process [29, 46]; perceiving may be better considered as a way of acting [34]; analogy may be inseparable from high-level perception [10]. Though from an engineering point of view, a modular architecture may be used in an AI system, the identication and specication of the modules must follow an AI theory  the functions and modules should be the theorems of a theory that are derived from a small number of axioms or principles, so as to guarantee the coherence and integrity of the system as a whole. Without such an internal structure, a theory of AI looks like a grab bag of ideas  even when each idea in it looks correct and concrete, there is still no guarantee that the ideas are indeed consistent, nor guidance on how to decide if on a design issue different ideas point to different directions.",Theoretical Foundations of Artificial General Intelligence,chapter 16
"Such an architecture often looks arbitrary, without convincing reason for the partition of the overall function into the modules. Consequently, the engineering practice will probably be full of trial-and-error, which should not happen if the theory is well-organized. 16.6 Conclusion A major obstacle of progress in AI research is theoretical nihilism  facing the wellknown difculty in establishing a theory of AI, the research community as a whole has not made enough effort in this task, but instead either follows some other theories developed for certain related, though very different, problems, or carries out the research based on intuitions or practical considerations, with the hope that the theoretical problems can be eventually solved or avoided using technical tricks. Though AI is indeed a very hard problem, and it is unlikely to get a perfect (or even satisfactory) theory very soon, to give up on the effort or to depend on an improper substitute      Bibliography 321 is not a good alternative.",Theoretical Foundations of Artificial General Intelligence,chapter 16
"Even though the research can go ahead without the guidance of a theory, it may run in a wrong direction, or into dead alleys. Even an imperfect theory is still better than no theory at all, and a theory developed in another domain does not necessarily keep its authority in AI, no matter how successful it is in its original domain. Given the special situation in the eld, an AI theory must be descriptive with respect to the human mind, and be normative with respect to computer systems. To achieve this objective, it should construct a notion of general intelligence, which does not depend in the details of either the biological brain or the electrical computer. The desired properties of such a theory can be summarized by the Three Cs: Correctness, Concreteness, and Compactness, and the overall quality of the theory depends on all the three aspects.",Theoretical Foundations of Artificial General Intelligence,chapter 16
"Among the existing theoretical works, many issues are caused by focusing only on one (or two) of the properties, while largely ignoring the other(s). To a large extent, the above issues come from the scienceengineering duality of AI. A theory of AI is similar to a theory of natural science in certain aspects, while that of engineering in other aspects. We cannot work in this eld like typical natural scientists, because intelligent computers are not existing phenomena for us to study, but something to be created; on the other hand, we cannot work like typical engineers, because we are not sure what we want to build, but have to nd that out by studying the human mind. The theoretical challenge is to nd a minimum description of the human mind at a certain level, then, with it as specication, to build computer systems in which people will nd most of the features of intelligence, in the everyday sense of the word.",Theoretical Foundations of Artificial General Intelligence,chapter 16
"Though the task is hard, there is no convincing argument for its impossibility. What the eld needs is to spend more energy in theoretical exploration, while keeping a clear idea about what kind of theory we are looking for, which is what this chapter attempts to clarify. Acknowledgements Thanks to Joscha Bach for the helpful comments. Bibliography [1] Albus, J. S. (1991). Outline for a theory of intelligence, IEEE Transactions on Systems, Man, and Cybernetics 21, 3, pp. 473509. [2] Alvarado, N., Adams, S. S., Burbeck, S. and Latta, C. (2002). Beyond the Turing Test: Performance metrics for evaluating a computer simulation of the human mind, in Proceedings of the 2nd International Conference on Development and Learning, pp. 147152.      322 Theoretical Foundations of Articial General Intelligence [3] Bach, J.",Theoretical Foundations of Artificial General Intelligence,chapter 16
"(2009). Principles of Synthetic Intelligence PSI: An Architecture of Motivated Cognition (Oxford University Press, Oxford). [4] Baum, E. B. (2004). What is Thought? (MIT Press, Cambridge, Massachusetts). [5] Brachman, R. J. (2006). (AA)AI  more than the sum of its parts, 2005 AAAI Presidential Address, AI Magazine 27, 4, pp. 1934. [6] Bringsjord, S. (2008). The logicist manifesto: At long last let logic-based articial intelligence become a eld unto itself, Journal of Applied Logic 6, 4, pp. 502525. [7] Bringsjord, S. and Sundar G, N. (2009). Toward a serious computational science of intelligence, Call for Papers for an AGI 2010 Workshop. [8] Bundy, A.",Theoretical Foundations of Artificial General Intelligence,chapter 16
"and Ohlsson, S. (1990). The nature of AI principles, in The foundation of articial intelligencea sourcebook (Cambridge University Press, New York), pp. 135154. [9] Cassimatis, N. L. (2006). Articial intelligence and cognitive science have the same problem, in Papers from the AAAI Spring Symposium on Between a Rock and a Hard Place: Cognitive Science Principles Meet AI-Hard Problems, pp. 2732. [10] Chalmers, D. J., French, R. M. and Hofstadter, D. R. (1992). High-level perception, representation, and analogy: a critique of articial intelligence methodology, Journal of Experimental & Theoretical Articial Intelligence 4, pp. 185211. [11] Chandrasekaran, B. (1990).",Theoretical Foundations of Artificial General Intelligence,chapter 16
"What kind of information processing is intelligence? in The foundation of articial intelligencea sourcebook (Cambridge University Press, New York), pp. 1446. [12] Feigenbaum, E. A. and Feldman, J. (1963). Computers and Thought (McGraw-Hill, New York). [13] Franklin, S. (2007). A foundational architecture for articial general intelligence, in B. Goertzel and P. Wang (eds.), Advance of Articial General Intelligence (IOS Press, Amsterdam), pp. 36 54. [14] Gabbay, D. M. and Woods, J. (2003). Normative models of rational agency: The theoretical disutility of certain approaches, Logic Journal of the IGPL 11, 6, pp. 597613. [15] Goertzel, B. (2009).",Theoretical Foundations of Artificial General Intelligence,chapter 16
"Toward a general theory of general intelligence, Dynamical Psychology, URL: http://goertzel.org/dynapsyc/dynacon.html#2009. [16] Goertzel, B., Arel, I. and Scheutz, M. (2009). Toward a roadmap for human-level articial general intelligence: Embedding HLAI systems in broad, approachable, physical or virtual contexts, Articial General Intelligence Roadmap Initiative, URL: http://www.agiroadmap.org/images/HLAIR.pdf. [17] Goertzel, B. and Pennachin, C. (eds.) (2007). Articial General Intelligence (Springer, New York). [18] Hawkins, J. and Blakeslee, S. (2004). On Intelligence (Times Books, New York). [19] Hayes, P. and Ford, K. (1995).",Theoretical Foundations of Artificial General Intelligence,chapter 16
"Turing Test considered harmful, in Proceedings of the Fourteenth International Joint Conference on Articial Intelligence, pp. 972977. [20] Hutter, M. (2005). Universal Articial Intelligence: Sequential Decisions based on Algorithmic Probability (Springer, Berlin). [21] Kirsh, D. (1991). Foundations of AI: the big issues, Articial Intelligence 47, pp. 330. [22] Laird, J. E., Wray, R. E., Marinier, R. P. and Langley, P. (2009). Claims and challenges in evaluating human-level intelligent systems, in Proceedings of the Second Conference on Articial General Intelligence, pp. 9196. [23] Legg, S. and Hutter, M. (2007). Universal intelligence: a denition of machine intelligence, Minds & Machines 17, 4, pp. 391444. [24] Marr, D.",Theoretical Foundations of Artificial General Intelligence,chapter 16
"(1982). Vision: A Computational Investigation into the Human Representation and Processing of Visual Information (W. H. Freeman & Co., San Francisco). [25] McCarthy, J. (1984). We need better standards for AI research, AI Magazine 5, 3, pp. 78. [26] McCarthy, J. (1988). Mathematical logic in articial intelligence, Ddalus 117, 1, pp. 297311. [27] McCarthy, J. (2007). From here to human-level AI, Articial Intelligence 171, pp. 11741182.      Bibliography 323 [28] McCarthy, J., Minsky, M., Rochester, N. and Shannon, C. (1955). A Proposal for the Dartmouth Summer Research Project on Articial Intelligence, URL: http://wwwformal.stanford.edu/jmc/history/dartmouth.html. [29] Michalski, R.",Theoretical Foundations of Artificial General Intelligence,chapter 16
"S. (1993). Inference theory of learning as a conceptual basis for multistrategy learning, Machine Learning 11, pp. 111151. [30] Minsky, M. (1985). The Society of Mind (Simon and Schuster, New York). [31] Newell, A. (1990). Unied Theories of Cognition (Harvard University Press, Cambridge, Massachusetts). [32] Newell, A. and Simon, H. A. (1976). Computer science as empirical inquiry: symbols and search, Communications of the ACM 19, 3, pp. 113126. [33] Nilsson, N. J. (2005). Human-level articial intelligence? Be serious! AI Magazine 26, 4, pp. 6875. [34] No, A. (2004). Action in Perception (MIT Press, Cambridge, Massachusetts). [35] Pearl, J.",Theoretical Foundations of Artificial General Intelligence,chapter 16
"(1988). Probabilistic Reasoning in Intelligent Systems (Morgan Kaufmann Publishers, San Mateo, California). [36] Pollock, J. L. (2006). Thinking about Acting: Logical Foundations for Rational Decision Making (Oxford University Press, USA, New York). [37] Popper, K. R. (1959). The Logic of Scientic Discovery (Basic Books, New York). [38] Rumelhart, D. E. and McClelland, J. L. (eds.) (1986). Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 1, Foundations (MIT Press, Cambridge, Massachusetts). [39] Russell, S. and Norvig, P. (2010). Articial Intelligence: A Modern Approach, 3rd edn. (Prentice Hall, Upper Saddle River, New Jersey). [40] Schank, R. C.",Theoretical Foundations of Artificial General Intelligence,chapter 16
"(1991). Where is the AI? AI Magazine 12, 4, pp. 3849. [41] Schmidhuber, J. (2007). The new AI: General & sound & relevant for physics, in B. Goertzel and C. Pennachin (eds.), Articial General Intelligence (Springer, Berlin), pp. 175198. [42] Simon, T. W. (1990). Articial methodology meets philosophy, in The foundation of articial intelligencea sourcebook (Cambridge University Press, New York), pp. 155164. [43] Solomonoff, R. J. (1964). A formal theory of inductive inference. Part I and II, Information and Control 7, 1-2, pp. 122,224254. [44] von Neumann, J. (1958). The Computer and the Brain (Yale University Press, New Haven, CT).",Theoretical Foundations of Artificial General Intelligence,chapter 16
"[45] Wang, P. (1995). Non-Axiomatic Reasoning System: Exploring the Essence of Intelligence, Ph.D. thesis, Indiana University. [46] Wang, P. (2006). Rigid Flexibility: The Logic of Intelligence (Springer, Dordrecht). [47] Wang, P. (2008). What do you mean by AI, in Proceedings of the First Conference on Articial General Intelligence, pp. 362373. [48] Wang, P. (2010). A General Theory of Intelligence, An on-line book under development. URL: http://sites.google.com/site/narswang/EBook. [49] Wang, P. and Goertzel, B. (2007). Introduction: Aspects of articial general intelligence, in B. Goertzel and P. Wang (eds.), Advance of Articial General Intelligence (IOS Press, Amsterdam), pp. 116.",Theoretical Foundations of Artificial General Intelligence,chapter 16
"[50] Wilks, Y. (1990). One small head: models and theories, in The foundation of articial intelligencea sourcebook (Cambridge University Press, New York), pp. 121134.   ",Theoretical Foundations of Artificial General Intelligence,chapter 16
"  Chapter 1: Introduction Abstract In this chapter we provide an overview of probabilistic logic networks (PLN), including our motivations for developing PLN and the guiding principles underlying PLN. We discuss foundational choices we made, introduce PLN knowledge representation, and briefly introduce inference rules and truth-values. We also place PLN in context with other approaches to uncertain inference. 1.1 Motivations This book presents Probabilistic Logic Networks (PLN), a systematic and pragmatic framework for computationally carrying out uncertain reasoning – reasoning about uncertain data, and/or reasoning involving uncertain conclusions. We begin with a few comments about why we believe this is such an interesting and important domain of investigation. First of all, we hold to a philosophical perspective in which “reasoning” – properly understood – plays a central role in cognitive activity.", Probabilistic Logic Networks,chapter 1
"We realize that other perspectives exist; in particular, logical reasoning is sometimes construed as a special kind of cognition that humans carry out only occasionally, as a deviation from their usual (intuitive, emotional, pragmatic, sensorimotor, etc.) modes of thought. However, we consider this alternative view to be valid only according to a very limited definition of “logic.” Construed properly, we suggest, logical reasoning may be understood as the basic framework underlying all forms of cognition, including those conventionally thought of as illogical and irrational. The key to this kind of extended understanding of logic, we argue, is the formulation of an appropriately general theory of uncertain reasoning – where what is meant by the latter phrase is: reasoning based on uncertain knowledge, and/or reasoning leading to uncertain conclusions (whether from certain or uncertain knowledge).", Probabilistic Logic Networks,chapter 1
"Moving from certain to uncertain reasoning opens up a Pandora’s box of possibilities, including the ability to encompass within logic things such as induction, abduction, analogy and speculation, and reasoning about tim e and causality. While not necessarily pertinent to the technical details of PLN, it is perhaps worth noting that the authors’ main focus in exploring uncertain inference has been its pertinence to our broader work on artificial general intelligence (AGI). As elaborated in (Goertzel and Pennachin 2007; Goertzel and Wang 2007; Wang et al 2008), AGI refers to the construction of intelligent systems that can carry out a variety of complex goals in complex environments, based on a rich contextual understanding of themselves, their tasks and their environments.", Probabilistic Logic Networks,chapter 1
"AGI was the     2 Probabilistic Logic Networks original motivating goal of theAI research field, but at the moment it is one among multiple streams of AI research, living alongside other subfields focused on more narrow and specialized problem-solving. One viable approach to achieving powerful AGI, we believe, is to create integrative software systems with uncertain inference at their core. Specifically, PLN has been developed within the context of a larger artificial intelligence project, the Novamente Cognition Engine or NCE (Goertzel 2006), which seeks to achieve general forms of cognition by integrating PLN with several other processes. Recently, the NCE has spawned an opensource sister project called OpenCog, as well (Hart and Goertzel 2008). In the final two chapters we will briefly discuss the implementation of PLN within the NCE, and give a few relevant details of the NCE architecture.", Probabilistic Logic Networks,chapter 1
"However, the vast majority of the discussion of PLN here is independent of the utilization of PLN as a component of the NCE. PLN stands as a conceptual and mathematical construct in its own right, with potential usefulness in a wide variety of AI and AGI applications. We also feel that the mathematical and conceptual aspects of PLN have the potential to be useful outside the AI context, both as purely mathematical content and as guidance for understanding the nature of probabilistic inference in humans and other natural intelligences. These aspects are not emphasized here but we may address them more thoroughly in future works. Of course, there is nothing new about the idea that uncertain inference is broadly important and relevant to AI and other domains. Over the past few decades a number of lines of research have been pursued, aimed at formalizing uncertain inference in a manner capable of application across the broad scope of varieties of cognition.", Probabilistic Logic Networks,chapter 1
"PLN incorporates ideas derived from many of these other lines of inquiry, including standard ones like Bayesian probability theory (Jaynes, 2003), fuzzy logic (Zadeh 1989), and less standard ones like the theory of imprecise probabilities (Walley 1991), term logic (Sommers and Englebretsen 2000), Pei Wang’s Non-Axiomatic Reasoning System (NARS) (Wang 1996), and algorithmic information theory (Chaitin 1987). For various reasons, which will come out as the book proceeds, we have found each of these prior attempts (and other ones, from which we have not seen fit to appropriate ideas, some of which we will mention below) unsatisfactory as a holistic approach to uncertain inference or as a guide to the creation of an uncertain inference component for use in integrative AGI systems.", Probabilistic Logic Networks,chapter 1
"Among the general high-level requirements underlying the development of PLN have been the following: • To enable uncertainty-savvy versions of all known varieties of logical reasoning; including, for instance, higher-order reasoning involving quantifiers, higher-order functions, and so forth. • To reduce to crisp “theorem prover” style behavior in the limiting case where uncertainty tends to zero. • To encompass inductive and abductive as well as deductive reasoning.     Chapter 1: Introduction 3 • To agree with probability theory in those reasoning cases where probability theory, in its current state of development, provides solutions within reasonable calculational effort based on assumptions that are plausible in the context of real-world embodied software systems. • To gracefully incorporate heuristics not explicitly based on probability theory, in cases where probability theory, at its current state of development, does not provide adequate pragmatic solutions.", Probabilistic Logic Networks,chapter 1
"• To provide “scalable” reasoning, in the sense of being able to carry out inferences involving at least billions of premises. Of course, when the number of premises is fewer, more intensive and accurate reasoning may be carried out. • To easily accept input from, and send input to, natural language processing software systems. The practical application of PLN is still at an early stage. Based on our evidence so far, however, we have found PLN to fulfill the above requirements adequately well, and our intuition remains that it will be found to do so in general. We stress, however, that PLN is an evolving framework, consisting of a conceptual core fleshed out by a heterogeneous combination of components. As PLN applications continue to be developed, it seems likely that various PLN components will be further refined and perhaps some of them replaced entirely.", Probabilistic Logic Networks,chapter 1
"We have found the current component parts of PLN acceptable for our work so far, but we have also frequently been aware of more sophisticated alternative approaches to various subproblems (some drawn from the literature, and some our own inventions), and have avoided pursuing many of such due to a desire for initial simplicity. The overall structure of PLN theory is relatively simple, and may be described as follows. First, PLN involves some important choices regarding knowledge representation, which lead to specific “schematic forms” for logical inference rules. The knowledge representation may be thought of as a definition of a set of “logical term types” and “logical relationship types,” leading to a novel way of graphically modeling bodies of knowledge. It is this graphical interpretation of PLN knowledge representation that led to the “network” part of the name “Probabilistic Logic Networks.", Probabilistic Logic Networks,chapter 1
"It is worth noting that the networks used to recognize knowledge in PLN are weighted directed hypergraphs (Bollobas 1998) much more general than, for example, the binary directed acyclic graphs used in Bayesian network theory (Pearl 1988). Next, PLN involves specific mathematical formulas for calculating the probability value of the conclusion of an inference rule based on the probability values of the premises plus (in some cases) appropriate background assumptions. It also involves a particular approach to estimating the confidence values with which these probability values are held (weight of evidence, or second-order uncertainty). Finally, the implementation of PLN in software requires important choices regarding the structural representation of inference rules, and also regarding “inference control” – the strategies required to decide what inferences to do in what order, in each particular practical situation.     4 Probabilistic Logic Networks 1.1.", Probabilistic Logic Networks,chapter 1
"1 Why Probability Theory? In the next few sections of this Introduction we review the conceptual foundations of PLN in a little more detail, beginning with the question: Why choose probability theory as a foundation for the “uncertain” part of uncertain inference? We note that while probability theory is the foundation of PLN, not all aspects of PLN are based strictly on probability theory. The mathematics of probability theory (and its interconnection with other aspects of mathematics) has not yet been developed to the point where it is feasible to use explicitly probabilistic methods to handle every aspect of the uncertain inference process.", Probabilistic Logic Networks,chapter 1
"Some researchers have reacted to this situation by disregarding probability theory altogether and introducing different conceptual foundations for uncertain inference, such as DempsterShafer theory (Dempster 1968; Shafer 1976), Pei Wang’s Non-Axiomatic Reasoning System (Wang 1996), possibility theory (Zadeh 1978) and fuzzy set theory (Zadeh 1965). Others have reacted by working within a rigidly probabilistic framework, but limiting the scope of their work fairly severely based on the limitations of the available probabilistic mathematics, avoiding venturing into the more ambiguous domain of creating heuristics oriented toward making probabilistic inference more scalable and pragmatically applicable (this, for instance, is how we would characterize the mainstream work in probabilistic logic as summarized in Hailperin 1996; more comments on this below).", Probabilistic Logic Networks,chapter 1
"Finally, a third reaction – and the one PLN embodies – is to create reasoning systems based on a probabilistic foundation and then layer non-probabilistic ideas on top of this foundation when this is the most convenient way to arrive at useful practical results. Our faith in probability theory as the ultimately “right” way to quantify uncertainty lies not only in the demonstrated practical applications of probability theory to date, but also in Cox’s (1961) axiomatic development of probability theory and ensuing refinements (Hardy 2002), and associated mathematical arguments due to de Finetti (1937) and others. These theorists have shown that if one makes some very basic assumptions about the nature of uncertainty quantification, the rules of elementary probability theory emerge as if by magic. In this section we briefly review these ideas, as they form a key part of the conceptual foundation of the PLN framework.", Probabilistic Logic Networks,chapter 1
"Cox’s original demonstration involved describing a number of properties that should commonsensically hold for any quantification of the “plausibility” of a proposition, and then showing that these properties imply that plausibility must be a scaled version of conventional probability. The properties he specified are, in 1 particular , 1 The following list of properties is paraphrased from the Wikipedia entry for “Cox’s Theorem.”     Chapter 1: Introduction 5 1. The plausibility of a proposition determines the plausibility of the proposition’s negation; either decreases as the other increases. Because “a double negative is an affirmative,” this becomes a functional equation f(f(x))= x saying that the function f that maps the probability of a proposition to the probability of the proposition’s negation is an involution; i.e., it is its own ! inverse. 2.", Probabilistic Logic Networks,chapter 1
"The plausibility of the conjunction [A & B] of two propositions A, B, depends only on the plausibility of B and that of A given that B is true. (From this Cox eventually infers that multiplication of probabilities is associative, and then that it may as well be ordinary multiplication of real numbers.) Because of the associative nature of the “and” operation in propositional logic, this becomes a functional equation saying that the function g such that P(A and B)=g(P(A),P(BA)) is an associative binary operation. All strictly increasing associative binary operations on the real numbers are isomorphic to multiplication of ! numbers in the interval [0, 1]. This function therefore may be taken to be multiplication. 3. Suppose [A & B] is equivalent to [C & D].", Probabilistic Logic Networks,chapter 1
"If we acquire new information A and then acquire further new information B, and update all probabilities each time, the updated probabilities will be the same as if we had first acquired new information C and then acquired further new information D. In view of the fact that multiplication of probabilities can be taken to be ordinary multiplication of real numbers, this becomes a functional equation "" f(z)% "" f(y)% yf$ ’ =zf$ ’ # y & # z & where f is as above. !C ox’s theorem states, in essence, that any measure of plausibility that possesses the above three properties must be a rescaling of standard probability.", Probabilistic Logic Networks,chapter 1
"While it is impressive that so much (the machinery of probability theory) can be derived from so little (Cox’s very commonsensical assumptions),     6 Probabilistic Logic Networks mathematician Michael Hardy (2002) has expressed the opinion that in fact Cox’s axioms are too strong, and has provided significantly weaker conditions that lead to the same end result as Cox’s three properties. Hardy’s conditions are more abstract and difficult to state without introducing a lot of mathematical mechanism, but essentially he studies mappings from propositions into ordered “plausibility” values, and he shows that if any such mapping obeys the properties of 1. If x implies y then f(x) < f(y) 2. If f(x) < f(y) then f(¬x) > f(¬y), where ¬ represents “not” 3.", Probabilistic Logic Networks,chapter 1
"If f(x|z) <= f(y|z) and f(x|¬z) <= f(y|¬z) then f(x) < f(y) 4. For all x, y either f(x) ! f(y) or f(y) ! f(x) then it maps propositions into scaled probability values. Note that this property list mixes up absolute probabilities f() with conditional probabilities f(|), but this is not a problem because Hardy considers f(x) as equivalent to f(x|U) where U is the assumed universe of discourse. Hardy expresses regret that his fourth property is required; however, Youssef’s (1994) work related to Cox’s axioms suggests that it is probably there in his mathematics for a very good conceptual reason. Youssef has shown that it is feasible to drop Cox’s assumption that uncertainty must be quantified using real numbers, but retain Cox’s other assumptions.", Probabilistic Logic Networks,chapter 1
"He shows it is possible, consistent with Cox’s other assumptions, to quantify uncertainty using “numbers” drawn from the complex, quaternion, or octonion number fields. Further, he argues that complex-valued “probabilities” are the right way to model quantum-level phenomena that have not been collapsed (decohered) into classical phenomena. We believe his line of argument is correct and quite possibly profound, yet it does not seem to cast doubt on the position of standard real-valued probability theory as the correct mathematics for reasoning about ordinary, decohered physical systems. If one wishes to reason about the uncertainty existing in pure, pre-decoherence quantum systems or other exotic states of being, then arguably these probability theories defined over different base fields than the real numbers may be applicable. Next, while we are avid probabilists, we must distinguish ourselves from the most ardent advocates of the “Bayesian” approach to probabilistic inference.", Probabilistic Logic Networks,chapter 1
"We understand the weakness of the traditional approach to statistics with its reliance on often unmotivated assumptions regarding the functional forms of probability distributions. On the other hand, we don’t feel that the solution is to replace these assumptions with other, often unmotivated assumptions about prior probability distributions. Bayes’ rule is an important part of probability theory, but the way that the Bayesian-statistical approach applies it is not always the most useful way. A major example of the shortcomings of the standard Bayesian approach lies in the domain of confidence assessment, an important aspect of PLN already mentioned above. As Wang (2001) has argued in detail, the standard Bayesian approach does not offer any generally viable way to assess or reason about the “second-order uncertainty” involved in a given uncertainty value.", Probabilistic Logic Networks,chapter 1
"Walley (1991)     Chapter 1: Introduction 7 sought to redress this problem via a subtler approach that avoids assuming a single prior distribution, and makes a weaker assumption involving drawing a prior from a parametrized family of possible prior distributions; others have followed up his work in interesting ways (Weichselberger 2003), but this line of research has not yet been developed to the point of yielding robustly applicable mathematics. Within PLN, we introduce a spectrum of approaches to confidence assessment ranging from indefinite probabilities (essentially a hybridization of Walley’s imprecise probabilities with Bayesian credible intervals) to frankly non-probabilistic heuristics inspired partly by Wang’s work. By utilizing this wide range of approaches, PLN can more gracefully assess confidence in diverse settings, providing pragmatic solutions where the Walley-type approach (in spite of its purer probabilism) currently fails.", Probabilistic Logic Networks,chapter 1
"Though Cox’s theorem and related results argue convincingly that probability theory is the correct approach to reasoning under uncertainty, the particular ways of applying probability theory that have emerged in the contemporary scientific community (such as the “Bayesian approach”) all rely on specific assumptions beyond those embodied in the axioms of probability theory. Some of these assumptions are explicit mathematical ones, and others are implicit assumptions about how to proceed in setting up a given problem in probabilistic terms; for instance, how to translate an intuitive understanding and/or a collection of quantitative data into mathematical probabilistic form. 1.", Probabilistic Logic Networks,chapter 1
"2 PLN in the Context of Traditional Approaches to Probabilistic Logic So, supposing one buys the notion that logic, adequately broadly construed, is essential (perhaps even central) to cognition; that appropriate integration of uncertainty into logic is an important aspect of construing logic in an adequately broad way; and also that probability theory is the correct foundation for treatment of uncertainty, what then? There is already a fairly well fleshed-out theory of probabilistic logic, so why does one need a substantial body of new theory such as Probabilistic Logic Networks? The problem is that the traditional theories in the area of probabilistic logic don’t directly provide a set of tools one can use to structure a broadly-applicable, powerful software system for probabilistic inferencing. They provide a number of interesting and important theorems and ideas, but are not sufficiently pragmatic in orientation, and also fail to cover some cognitively key aspects of uncertain inference such as intensional inference.", Probabilistic Logic Networks,chapter 1
"Halpern’s (2003) book provides a clearly written, reasonably thorough overview of recent theories in probabilistic logic. The early chapters of Hailperin (1996) gives some complementary historical and theoretical background. Alongside other approaches such as possibility theory, Halpern gives an excellent sum-     8 Probabilistic Logic Networks mary of what in PLN terms would be called “first-order extensional probabilistic logic” – the interpretation and manipulation of simple logic formulas involving absolute and conditional probabilities among sets. Shortcomings of this work from a pragmatic AI perspective include: • No guidance is provided as to which heuristic independence assumptions are most cognitively natural to introduce in order to deal with (the usual) situations where adequate data regarding dependencies is unavailable. Rather, exact probabilistic logic formulas are introduced, into which one can, if one wishes, articulate independence assumptions and then derive their consequences.", Probabilistic Logic Networks,chapter 1
"• Adequate methods for handling “second order uncertainty” are not presented, but this is critical for dealing with real-world inference situations where available data is incomplete and/or erroneous. Hailperin (1996) deals with this by looking at interval probabilities, but this turns out to rarely be useful in practice because the intervals corresponding to inference results are generally far too wide. Walley’s (1991) imprecise probabilities are more powerful but have a similar weakness, and we will discuss them in more detail in Chapter 4; they also have not been integrated into any sort of powerful, general, probabilistic logic framework, though integrating them into PLN if one wished to do so would not be problematic, as will become clear.", Probabilistic Logic Networks,chapter 1
"• Various sorts of truth-values are considered, including single values, intervals, and whole probability distributions, but the problem of finding the right way to summarize a probability distribution for logical inference without utilizing too much memory or sacrificing too much information has not been adequately resolved (and this is what we have tried to resolve with the “indefinite probabilities” utilized in PLN). • The general probabilistic handling of intensional, temporal, and causal inference is not addressed. Of course, these topics are handled in various specialized theories; e.g., Pearl’s causal networks (2000), but there is no general theory of probabilistic intensional, temporal, or causal logic; yet the majority of commonsense logical inference involves these types of reasoning. • The existing approaches to intermixing probabilistic truth-values with existential and universal quantification are conceptually flawed and often do not yield pragmatically useful results.", Probabilistic Logic Networks,chapter 1
"All in all, in terms of Halpern’s general formalism for what we call first-order extensional logic, what PLN constitutes is • A specific compacted representation of sets of probability distributions (the indefinite truth-value) • A specific way of deploying heuristic independence assumptions; e.g., within the PLN deduction and revision rules     Chapter 1: Introduction 9 • A way of counting the amount of evidence used in an inference (which is used in the revision rule, which itself uses amount of evidence together with heuristic independence assumptions) But much of the value of PLN lies in the ease with which it extends beyond first-order extensional logic.", Probabilistic Logic Networks,chapter 1
"Due to the nature of the conceptual and mathematical formalism involved, the same essential inference rules and formulas used for firstorder extensional logic are extended far more broadly, to deal with intensional, temporal, and causal logic, and to deal with abstract higher-order inference involving complex predicates, higher-order functions, and universal, existential, and fuzzy quantifiers. 1.2.1 Why Term Logic? One of the major ways in which PLN differs from traditional approaches to probabilistic logic (and one of the secrets of PLN’s power) is its reliance on a formulation of logic called “term logic.” The use of term logic is essential, for instance, to PLN’s introduction of cognitively natural independence assumptions and to PLN’s easy extension of first-order extensional inference rules to more general and abstract domains. Predicate logic and term logic are two different but related forms of logic, each of which can be used both for crisp and uncertain logic.", Probabilistic Logic Networks,chapter 1
"Predicate logic is the most familiar kind, where the basic entity under consideration is the “predicate,” a function that maps argument variables into Boolean truth-values. The argument variables are quantified universally or existentially. On the other hand, in term logic, which dates back at least to Aristotle and his notion of the syllogism, the basic element is a subject-Predicate statement, denotable A ! B where ! denotes a notion of inheritance or specialization. Logical inferences take the form of “syllogistic rules,” which give patterns for combining statements with matching terms. (We don’t use the ! notation much in PLN, because it’s not sufficiently precise for PLN purposes, since PLN introduces many varieties of inheritance; but we will use the ! notation in this section because here we are speaking about inheritance in term logic in general rather than about PLN in particular.", Probabilistic Logic Networks,chapter 1
"Examples are the deduction, induction, and abduction rules:     10 Probabilistic Logic Networks A ! B A B ! C |B A! C C Deduction A ! B A A ! C |B B ! C C Induction A ! C B ! C A |A ! B B C Abduction When we get to defining the truth-value formulas corresponding to these inference rules, we will observe that deduction is infallible in the case of absolutely certain premises, but uncertain in the case of probabilistic premises; while abduction and induction are always fallible, even given certain premises. In fact we will derive abduction and induction from the combination of deduction with a simple rule called inversion     Chapter 1: Introduction 11 A ! B |B ! A whose truth-value formula derives from Bayes rule. Predicate logic has proved to deal more easily with deduction than with induction, abduction, and other uncertain, fallible inference rules.", Probabilistic Logic Networks,chapter 1
"On the other hand, term logic can deal quite elegantly and simply with all forms of inference. Furthermore, the predicate logic formulation of deduction proves less amenable to “probabilization” than the term logic formulation. It is for these reasons, among others, that the foundation of PLN is drawn from term logic rather than from predicate logic. PLN begins with a term logic foundation, and then adds on elements of probabilistic and combinatory logic, as well as some aspects of predicate logic, to form a complete inference system, tailored for easy integration with software components embodying other (not explicitly logical) aspects of intelligence. Sommers and Engelbretsen (2000) have given an excellent defense of the value of term logic for crisp logical inference, demonstrating that many pragmatic inferences are far simpler in term logic formalism than they are in predicate logic formalism.", Probabilistic Logic Networks,chapter 1
"On the other hand, the pioneer in the domain of uncertain term logic is Pei Wang (Wang 1996), to whose NARS uncertain term logic based reasoning system PLN owes a considerable debt. To frame the issue in terms of our above discussion of PLN’s relation to traditional probabilistic logic approaches, we may say we have found that the formulation of appropriate heuristics to guide probabilistic inference in cases where adequate dependency information is not available, and appropriate methods to extend first-order extensional inference rules and formulas to handle other sorts of inference, are both significantly easier in a term logic rather than predicate logic context.", Probabilistic Logic Networks,chapter 1
"In these respects, the use of term logic in PLN is roughly a probabilization of the use of term logic in NARS; but of course, there are many deep conceptual and mathematical differences between PLN and NARS, so that the correspondence between the two theories in the end is more historical and theory-structural, rather than a precise correspondence on the level of content. 1.3 PLN Knowledge Representation and Inference Rules In the next few sections of this Introduction, we review the main topics covered in the book, giving an assemblage of hints as to the material to come. First, Chapter 2 describes the knowledge representation underlying PLN, without yet saying anything specific about the management of numbers quantifying uncertainties. A few tricky issues occur here, meriting conceptual discussion.", Probabilistic Logic Networks,chapter 1
"Even though PLN knowledge representation is not to do with uncertain inference per se, we have found that without getting the knowledge representation right, it is very difficult to define uncertain inference rules in an intuitive way. The biggest influence     12 Probabilistic Logic Networks on PLN’s knowledge representation has been Wang’s NARS framework, but there are also some significant deviations from Wang’s approach. PLN knowledge representation is conveniently understood according to two dichotomies: extensional vs. intensional, and first-order vs. higher-order. The former is a conceptual (philosophical/cognitive) distinction between logical relationships that treat concepts according to their members versus those that treat concepts according to their properties. In PLN extensional knowledge is treated as more basic, and intensional knowledge is defined in terms of extensional knowledge via the addition of a specific mathematics of intension (somewhat related to information theory).", Probabilistic Logic Networks,chapter 1
"This is different from the standard probabilistic approach, which contains no specific methods for handling intension and also differs from, e.g., Wang’s approach in which intension and extension are treated as completely symmetric, with neither of them being more basic or derivable from the other. The first-order versus higher-order distinction, on the other hand, is essentially a mathematical one. First-order, extensional PLN is a variant of standard term logic, as originally introduced by Aristotle in his Logic and recently elaborated by theorists such as Wang (1996) and Sommers and Engelbretsen (2000). First-order PLN involves logical relationships between terms representing concepts, such as Inheritance cat animal ExtensionalInheritance Pixel_444 Contour_7565 (where the notation is used that R A B denotes a logical relationship of type R between arguments A and B).", Probabilistic Logic Networks,chapter 1
"A typical first-order PLN inference rule is the standard term-logic deduction rule A ! B B ! C |A ! C which in PLN looks like ExtensionalInheritance A B ExtensionalInheritance B C |ExtensionalInheritance A C As well as purely logical relationships, first-order PLN also includes a fuzzy set membership relationship, and specifically addresses the relationship between fuzzy set membership and logical inheritance, which is closely tied to the PLN concept of intension. In the following text we will sometimes use the acronym FOI to refer to PLN First Order Inference.     Chapter 1: Introduction 13 Higher-order PLN, on the other hand (sometimes called HOI, for Higher Order Inference), has to do with functions and their arguments. Much of higher-order PLN is structurally parallel to first-order PLN; for instance, implication between statements is largely parallel to inheritance between terms.", Probabilistic Logic Networks,chapter 1
"However, a key difference is that most of higher-order PLN involves either variables or higher-order functions (functions taking functions as their arguments). So for instance one might have ExtensionalImplication Inheritance $X cat Evaluation eat ($X, mice) (using the notation that R A B denotes the logical relationship R applied to the arguments A and B). Here Evaluation is a relationship that holds between a predicate and its argument-list, so that, e.g., Evaluation eat (Sylvester, mice) means that the list (Sylvester, mice) lies within the set of ordered pairs characterizing the eat relationship. The parallel of the first-order extensional deduction rule given above would be a rule ExtensionalImplication A B ExtensionalImplication B C |ExtensionalImplication A C where the difference is that in the higher-order inference case the tokens A, B, and C denote either variable-bearing expressions or higher-order functions.", Probabilistic Logic Networks,chapter 1
"Some higher-order inference rules involve universal or existential quantifiers as well. While first-order PLN adheres closely to the term logic framework, higherorder PLN is better described as a mix of term logic, predicate logic, and combinatory logic. The knowledge representation is kept flexible, as this seems to lead to the simplest and most straightforward set of inference rules.     14 Probabilistic Logic Networks 1.4 Truth-value Formulas We have cited above the conceptual reasons why we have made PLN a probabilistic inference framework, rather than using one of the other approaches to uncertainty quantification available in the literature. However, though we believe in the value of probabilities we do not believe that the conventional way of using probabilities to represent the truth-values of propositions is adequate for pragmatic computational purposes.", Probabilistic Logic Networks,chapter 1
"One of the less conventional aspects of PLN is the quantification of uncertainty using truth-values that contain at least two components, and usually more (in distinction from the typical truth-value used in probability theory, which is a single number: a probability). Our approach here is related to earlier multi-component truth-value approaches due to Keynes (2004), Wang (2006), Walley (1991), and others, but is unique in its particulars. The simplest kind of PLN truth-value, called a SimpleTruthValue, consists of a pair of numbers <s,w> called a strength and a confidence. The strength value is a probability; the confidence value is a measure of the amount of certainty attached to the strength value. Confidence values are normalized into [0,1]. For instance <.6,1> means a probability of .6 known with absolute certainty. <.6,.2> means a probability of .6 known with a very low degree of certainty. <.", Probabilistic Logic Networks,chapter 1
"6,0> means a probability of .6 known with a zero degree of certainty, which indicates a meaningless strength value, and is equivalent to <x,0> for any other probability value x. Another type of truth-value, more commonly used as the default within PLN, is the IndefiniteTruthValue. We introduce the mathematical and philosophical foundations of IndefiniteTruthValues in Chapter 4. Essentially a hybridization of Walley’s imprecise probabilities and Bayesian credible intervals, indefinite probabilities quantify truth-values in terms of four numbers <L, U, b, k>: an interval [L,U], a credibility level b, and an integer k called the “lookahead.” IndefiniteTruthValues provide a natural and general method for calculating the “weightof-evidence” underlying the conclusions of uncertain inferences. We ardently believe that this approach to uncertainty quantification may be adequate to serve as an ingredient of powerful artificial general intelligence.", Probabilistic Logic Networks,chapter 1
"Beyond the SimpleTruthValues and IndefiniteTruthValues mentioned above, more advanced types of PLN truth-value also exist, principally “distributional truth-values” in which the strength value is replaced by a matrix approximation to an entire probability. Note that this, then, provides for three different granularities of approximations to an entire probability distribution. A distribution can be most simply approximated by a single number, somewhat better approximated by a probability interval, and even better approximated by an entire matrix. Chapter 5 takes the various inference rules defined in Chapter 2, and associates a “strength value formula” with each of them (a formula determining the strength of the conclusion based on the strengths of the premises). For example, the deduction rule mentioned above is associated with two strength formulas, one based on an independence assumption and the other based on a different “concept geome-     Chapter 1: Introduction 15 try” based assumption.", Probabilistic Logic Networks,chapter 1
"The independence-assumption-based deduction strength formula looks like B <s > B C <s > C ExtensionalInheritance A B <s > AB ExtensionalInheritance B C <s > BC |ExtensionalInheritance A C <s > AC s = s s + (1-s ) ( s – s s ) / (1- s ) AC AB BC AB C B BC B This particular rule is a straightforward consequence of elementary probability theory. Some of the other formulas are equally straightforward, but some are subtler and require heuristic reasoning beyond standard probabilistic tools like independence assumptions. Since simple truth-values are the simplest and least informative of our truth-value types; they provide quick, but less accurate, assessments of the resulting strength and confidence values. We reconsider these strength formulas again in Chapter 6, extending the rules to IndefiniteTruthValues. We also illustrate in detail how indefinite truth-values provide a natural approach to measuring weight-of-evidence.", Probabilistic Logic Networks,chapter 1
"IndefiniteTruthValues can be thought of as approximations to entire distributions, and so provide an intermediate level of accuracy of strength and confidence. As shown in Chapter 7, PLN inference formulas may also be modified to handle entire distributional truth-values. Distributional truth-values provide more information than the other truth-value types. As a result, they may also be used to yield even more accurate assessments of strength and confidence. The sensitivity to error of several inference rule formulas for various parameter values is explored in Chapter 8. There we provide a fairly detailed mathematical and graphical examination of error magnification. We also study the possibility of deterministic chaos arising from PLN inference. We introduce higher-order inference (HOI) in Chapter 10, where we describe the basic HOI rules and strength formulas for both simple truth-values and indefinite truth-values.", Probabilistic Logic Networks,chapter 1
"We consider both crisp and fuzzy quantifiers, using indefinite probabilities, in Chapter 11; treat intensional inference in Chapter 12; and inference control in Chapter 13. Finally, we tackle the topics of temporal and causal inference in Chapter 14. 1.5 Implementing and Applying PLN The goal underlying the theoretical development of PLN has been the creation of practical software systems carrying out complex, useful inferences based on uncertain knowledge and drawing uncertain conclusions. Toward that end we have implemented most of the PLN theory described in this book as will briefly be de-     16 Probabilistic Logic Networks scribed in Chapter 13, and used this implementation to carry out simple inference experiments involving integration with external software components such as a natural language comprehension engine and a 3D simulation world. Chapter 14 reviews some extensions made to basic PLN in the context of these practical applications, which enable PLN to handle reasoning about temporal and causal implications.", Probabilistic Logic Networks,chapter 1
"Causal inference in particular turns out to be conceptually interesting, and the approach we have conceived relates causality and intension in a satisfying way. By far the most difficult aspect of designing a PLN implementation is inference control, which we discuss in Chapter 13. This is really a foundational conceptual issue rather than an implementational matter per se. The PLN framework just tells you what inferences can be drawn; it doesn’t tell you what order to draw them in, in which contexts. Our PLN implementation utilizes the standard modalities of forward-chaining and backward-chaining inference control. However, the vivid presence of uncertainty throughout the PLN system makes these algorithms more challenging to use than in a standard crisp inference context. Put simply, the search trees expand unacceptably fast, so one is almost immediately faced with the need to use clever, experience-based heuristics to perform pruning.", Probabilistic Logic Networks,chapter 1
"The issue of inference control leads into deep cognitive science issues that we briefly mention here but do not fully explore, because that would lead too far afield from the focus of the book, which is PLN in itself. One key conceptual point that we seek to communicate, however, is that uncertain inference rules and formulas, on their own, do not compose a comprehensive approach to artificial intelligence. To achieve the latter, sophisticated inference control is also required, and controlling uncertain inference is difficult – in practice, we have found, requiring ideas that go beyond the domain of uncertain inference itself. In principle, one could take a purely probability-theoretic approach to inference control – choosing inference steps based on the ones that are most likely to yield successful conclusions based on probabilistic integration of all the available evidence. However, in practice this does not seem feasible given the current state of development of applied probability theory.", Probabilistic Logic Networks,chapter 1
"Instead, in our work with PLN so far, we have taken a heuristic and integrative approach, using other non-explicitly-probabilistic algorithms to help prune the search trees implicit in PLN inference control. As for applications, we have applied PLN to the output of a natural language processing subsystem, using it to combine premises extracted from different biomedical research abstracts to form conclusions embodying medical knowledge not contained in any of the component abstracts. We have also used PLN to learn rules controlling the behavior of a humanoid agent in a 3D simulation world; for instance, PLN learns to play “fetch” based on simple reinforcement learning stimuli. Our current research involves extending PLN’s performance in both these areas, and bringing the two areas together by using PLN to help the Novamente Cognition Engine carry out complex simulation-world tasks involving a combination of physical activity and linguistic communication.", Probabilistic Logic Networks,chapter 1
"Quite probably this ongoing research will involve various improvements to be made to the PLN framework itself. Our goal in articulating PLN has not been to present an ultimate     Chapter 1: Introduction 17 itself. Our goal in articulating PLN has not been to present an ultimate and final approach to uncertain inference, but rather to present a workable approach that is suitable for carrying out uncertain inference comprehensively and reasonably well in practical contexts. As probability theory and allied branches of mathematics develop, and as more experience is gained applying PLN in practical contexts, we expect the theory to evolve and improve. 1.6 Relationship of PLN to Other Approaches to Uncertain Inference Finally, having sketched the broad contours of PLN theory and related it to more traditional approaches to probabilistic logic, we now briefly discuss the relationship between PLN and other approaches to logical inference. First, the debt of PLN to various standard frameworks for crisp logical inference is clear.", Probabilistic Logic Networks,chapter 1
"PLN’s knowledge representation, as will be made clear in Chapter 2, is an opportunistically assembled amalgam of formalisms chosen from term logic, predicate logic and combinatory logic. Rather than seeking a pure and minimal formalism, we have thought more like programming language designers and sought a logical formalism that allows maximally compact and comprehensible representation of a wide variety of useful logical structures. Regarding uncertainty, as noted above, as well as explicit approaches to the problem of unifying probability and logic the scientific literature contains a number of other relevant ideas, including different ways to quantify uncertainty and to manipulate uncertainty once quantified. There are non-probabilistic methods like fuzzy logic, possibility theory, and NARS.", Probabilistic Logic Networks,chapter 1
"And there is a variety of probabilistic approaches to knowledge representation and reasoning that fall short of being fullon “probabilistic logics,” including the currently popular Bayes nets, which will be discussed in more depth below, and Walley’s theory of imprecise probabilities (Walley 1991), which has led to a significant literature (ISIPTA 2001, 2003, 2005, 2007), and has had a significant inspirational value in the design of PLN’s approach to confidence estimation, as will be reviewed in detail in Chapters 4, 6, and 10. Overall, regarding the representation of uncertainty, PLN owes the most to Pei Wang’s NARS approach and Walley’s theory of imprecise probabilities. Fuzzy set theory ideas are also utilized in the specific context of the PLN Member relationship.", Probabilistic Logic Networks,chapter 1
"However, we have not found any of these prior approaches to uncertainty quantification to be fully adequate, and so the PLN approach draws from them ample inspiration but not very many mathematical details. We now review the relationship of PLN to a few specific approaches to uncertainty quantification and probabilistic inference in a little more detail. In all cases the comments given here are high-level and preliminary, and the ideas discussed     18 Probabilistic Logic Networks will be much clearer to the reader after they have read the later chapters of this book and understand PLN more fully. 1.6.1 PLN and Fuzzy Set Theory Fuzzy set theory has proved a pragmatically useful approach to quantifying many kinds of relationships (Zadeh 1965, 1978), but we believe that its utility is fundamentally limited.", Probabilistic Logic Networks,chapter 1
"Ultimately, we suggest, the fuzzy set membership degree is not a way of quantifying uncertainty – it is quantifying something else: it is quantifying partial membership. Fuzzy set membership is used in PLN as the semantics of the truth-values of special logical relationship types called Member relationships. These fuzzy Member relationships may be used within PLN inference, but they are not considered the same as logical relationships such as Inheritance or Similarity relationships whose truth-values quantify degrees of uncertainty. Some (though nowhere near all) of the fuzzy set literature appears to us to be semantically confused regarding the difference between uncertainty and partial membership. In PLN we clearly distinguish between • Jim belongs to degree .6 to the fuzzy set of tall people. (MemberLink semantics) • Jim shares .6 of the properties shared by people belonging to the set of tall people (where the different properties may be weighted). (IntensionalInheritanceLink semantics) • Jim has a .", Probabilistic Logic Networks,chapter 1
"6 chance of being judged as belonging to the set of tall people, once more information about Jim is obtained (where this may be weighted as to the degree of membership that is expected to be estimated once the additional information is obtained). (IntensionalInheritanceLink, aka Subset Link, semantics) • Jim has an overall .6 amount of tallness, defined as a weighted average of extensional and intensional information. (Inheritance Link semantics) We suggest that the fuzzy, MemberLink semantics is not that often useful, but do recognize there are cases where it is valuable; e.g., if one wishes to declare that a stepparent and stepchild are family members with fuzzy degree .8 rather than 1.", Probabilistic Logic Networks,chapter 1
"In terms of the above discussion of the foundations of probability theory we note that partial membership assignments need not obey Cox’s axioms and need not be probabilities – which is fine, as they are doing something different, but also limits the facility with which they can be manipulated. In PLN, intensional probabilities are used for many of the purposes commonly associated with fuzzy membership values, and this has the advantage of keeping more things within a probabilistic framework.     Chapter 1: Introduction 19 1.6.2 PLN and NARS Pei Wang’s NARS approach has already been discussed above and will pop up again here and there throughout the text; furthermore, Appendix A1 presents a comparison of some of the first-order PLN truth-value formulas with corresponding NARS formulas.", Probabilistic Logic Networks,chapter 1
"As already noted, there is a long historical relationship between PLN and NARS; PLN began as part of a collaboration with NARS’s creator Pei Wang as an attempt to create a probabilistic analogue to NARS. PLN long ago diverged from its roots in NARS and has grown in a very different direction, but there remain many similarities. Beneath all the detailed similarities and differences, however, there is a deep and significant difference between the two, which is semantic: PLN’s semantics is probabilistic, whereas NARS’s semantics is intentionally and definitively not. PLN and NARS have a similar division into first-order versus higher-order inference, and have first-order components that are strictly based on term logic. However, PLN’s higher-order inference introduces predicate and combinatory logic ideas, whereas NARS’s higher-order inference is also purely term logic based.", Probabilistic Logic Networks,chapter 1
"Both PLN and NARS include induction, deduction, and abduction in their first-order components, with identical graphical structures; in PLN, however, induction and abduction are derived from deduction via Bayes rule, whereas in NARS they have their own completely independent truth-value functions. Both PLN and NARS utilize multi-component truth-values, but the semantics of each component is subtly different, as will be reviewed in appropriate points in the text to follow. 1.6.3 PLN and Bayes Nets Bayes nets are perhaps the most popular contemporary approach to uncertain inference. Because of this, we here offer a few more detailed comments on the general relationship between PLN and Bayes nets. Of course, the actual relationship is somewhat subtle and will be clear to the reader only after completing the exposition of PLN.", Probabilistic Logic Networks,chapter 1
"Traditional Bayesian nets assume a tree structure for events, which is unrealistic in general, but in recent years there has been a batch of work on “loopy Bayesian networks” in which standard Bayesian net information propagation is applied to potentially cyclic graphs of conditional probability. Some interesting alternatives to the loopy Bayesian approach have also been proposed, including one that uses a more advanced optimization algorithm within the Bayesian net framework. Bayes nets don’t really contain anything comparable to the generality of PLN higher-order inference. However, in the grand scheme of things, first-order PLN is not all that tremendously different from loopy Bayesian nets and related schemes. In both cases one is dealing with graphs whose relationships denote conditional     20 Probabilistic Logic Networks probabilities, and in both cases one is using a kind of iterative relaxation method to arrive at a meaningful overall network state.", Probabilistic Logic Networks,chapter 1
"If one took a forest of loopy Bayes nets with imprecise probabilities, and then added some formalism to interface it with fuzzy, predicate, and combinatory logic, then one might wind up with something reasonably similar to PLN. We have not taken such an approach but have rather followed the path that seemed to us more natural, which was to explicitly shape a probabilistic inference framework based on the requirements that we found important for our work on integrative AI. There are many ways of embodying probability theory in a set of data structures and algorithms. Bayes nets are just one approach. PLN is another approach and has been designed for a different purpose: to allow basic probabilistic inference to interact with other kinds of inference such as intensional inference, fuzzy inference, and higher-order inference using quantifiers, variables, and combinators.", Probabilistic Logic Networks,chapter 1
"We have found that for the purpose of interfacing basic probabilistic inference with these other sorts of inference, the PLN approach is a lot more convenient than Bayes nets or other more conventional approaches. Another key conceptual difference has to do with a PLN parameter called the “context.” In terms of probability theory, one can think of a context as a universe of discourse. Rather than attempting to determine a (possibly non-existent) universal probability distribution that has desired properties within each local domain, PLN creates local probability distributions based on local contexts. The context parameter can be set to Universal (everything the system has ever seen), Local (only the information directly involved in a given inference), or many levels in between. Yet another major conceptual difference is that PLN handles multivariable truth-values. Its minimal truth-value object has two components: strength and weight of evidence. Alternatively, it can use probability distributions (or discrete approximations thereof) as truth-values.", Probabilistic Logic Networks,chapter 1
"This makes a large difference in the handling of various realistic inference situations. For instance, the treatment of “weight of evidence” in PLN is not a purely mathematical issue, but reflects a basic conceptual issue, which is that (unlike most probabilistic methods) PLN does not assume that all probabilities are estimated from the same sample space. It makes this assumption provisionally in some cases, but it doesn’t make it axiomatically and comprehensively. With the context set to Universal, and with attention restricted to the strength component of truth-values, what we have in PLN-FOI is – speaking conceptually rather than mathematically – a different way of doing the same thing that loopy Bayes networks (BN) and its competitors are trying to do. PLN, loopy BN, and other related methods are all viewable as optimization algorithms trying to relax into a condition giving the “correct probability distribution,” and at some risk of settling into local optima instead.", Probabilistic Logic Networks,chapter 1
"But the ability to use more flexible truth-values, and to use local contexts as appropriate, makes a very substantial difference in practice. This is the kind of difference that becomes extremely apparent when one seeks to integrate probabilistic inference with other cognitive processes. And it’s the kind of difference that is important when trying to extend one’s reasoning sys-     Chapter 1: Introduction 21 tem from simple inferences to extremely general higher-order inference – an extension that has succeeded within PLN, but has not been successfully carried out within these other frameworks. 1.7 Toward Pragmatic Probabilistic Inference Perhaps the best way to sum up the differences between PLN and prior approaches to (crisp or uncertain) inference is to refer back to the list of requirements given toward the start of this Introduction.", Probabilistic Logic Networks,chapter 1
"These requirements are basically oriented toward the need for an approach to uncertain inference that is adequate to serve as the core of a general-purpose cognition process – an approach that can handle any kind of inference effectively, efficiently, and in an uncertainty-savvy way. Existing approaches to crisp inference are not satisfactory for the purposes of general, pragmatic, real-world cognition, because they don’t handle uncertainty efficiently and gracefully. Of course, one can represent uncertainties in predicate logic – one can represent anything in predicate logic – but representing them in a way that leads to usefully rapid and convenient inference incorporating uncertainties intelligently is another matter. On the other hand, prior approaches to uncertain inference have universally failed the test of comprehensiveness. Some approaches, such as Bayes nets and fuzzy set theory, are good at what they do but carry out only very limited functions compared to what is necessary to fulfill the inference needs of a generalpurpose cognitive engine.", Probabilistic Logic Networks,chapter 1
"Others, such as imprecise probability theory, are elegant and rigorous but are so complex that the mathematics needed to apply them in practical situations has not yet been resolved. Others, such as NARS and Dempster-Shafer theory, appear to us to have fundamental conceptual flaws in spite of their interesting properties. And still others, such as traditional probabilistic logic as summarized by Halpern and Hailperin, fail to provide techniques able to deal with the scale, incompleteness, and erroneousness typifying real-world inference situations. In sum, we do not propose PLN as an ultimate and perfect uncertain inference framework, only as an adequate one – but we do suggest that, in its adequacy, PLN distinguishes itself from the alternatives currently available. As noted above, we suspect that the particulars of the PLN framework will evolve considerably as PLN is utilized for more and more pragmatic inference tasks, both on its own and within integrative AI systems.", Probabilistic Logic Networks,chapter 1
"  Chapter 2: Knowledge Representation Abstract In chapter 2, we review the basic formalism of PLN knowledge representation in a way that is relatively independent of the particularities of PLN truthvalue manipulation. Much of this material has nothing explicitly to do with probability theory or uncertainty management; it merely describes a set of conventions for representing logical knowledge. However, we also define some of the elements of PLN truth-value calculation here, insofar as is necessary to define the essential meanings of some of the basic PLN constructs. 2.1 Basic Terminology and Notation The basic players in PLN knowledge representation are entities called terms and relationships (atomic formulae). The term Atom will refer to any element of the set containing both terms and relationships. The hierarchy of PLN Atoms begins with a finite set S of elementary terms. (In an AI context, these may be taken as referring to atomic perceptions or actions, and mathematical structures.", Probabilistic Logic Networks,chapter 2
"The set of ordered and unordered subsets of S is then constructed, and its elements are also considered as terms. Relationships are then defined as tuples of terms, and higherorder relationships are defined as predicates or functions acting on terms or relationships. Atoms are associated with various data items, including • Labels indicating type; e.g., a term may be a Concept term or a Number term; a relationship may be an Inheritance relationship or a Member relationship • Packages of numbers representing “truth-value” (more on that later) • In some cases, Atom-type-specific data (e.g., Number terms are associated with numbers; Word terms are associated with character strings) We will sometimes refer to uncertain truth-values here in a completely abstract way, via notation such as <t>. However, we will also use some specific truthvalue types in a concrete way: • “strength” truth-values, which consist of single numbers; e.g., <s> or <.8>.", Probabilistic Logic Networks,chapter 2
"Usually strength values denote probabilities but this is not always the case. The letter s will be habitually used to denote strength values. • SimpleTruthValues, which consist of pairs of numbers. These pairs come in two forms: o the default, <s,w>, where s is a strength and w is a “weight of evidence” – the latter being a number in [0,1] that tells you,     24 Probabilistic Logic Networks qualitatively, how much you should believe the strength estimate. The letter w will habitually be used to denote weight of evidence values. o <s,N>, where N is a “count” – a positive number telling you, qualitatively, the total amount of evidence that was evaluated in order to assess s. There is a heuristic formula interrelating w and N, w=N/(N+k) where k is an adjustable parameter. The letter N will habitually be used to denote count.", Probabilistic Logic Networks,chapter 2
"If the count version rather than the weight of evidence version is being used, this will be explicitly indicated, as the former version is the default. • IndefiniteTruthValues, which quantify truth-values in terms of four numbers <[L,U],b,k>, an interval [L,U], a credibility level b, and an integer k called the lookahead. While the semantics of IndefiniteTruthValues are fairly complex, roughly speaking they quantify the idea that after k more observations there is a probability b that the conclusion of the inference will appear to lie in the final interval [L,U]. The value of the integer k will often be considered a systemwide constant. In this case, IndefiniteTruthValues will be characterized more simply via the three numbers <[L,U], b>. • DistributionalTruthValues, which are discretized approximations to entire probability distributions.", Probabilistic Logic Networks,chapter 2
"When using DistributionalTruthValues, PLN deduction reduces simply to matrix multiplication, and PLN inversion reduces to matrix inversion.1 The semantics of these truth-values will be reviewed in more depth in later chapters, but the basic gist may be intuitable from the above brief comments. PLN inference rules are associated with particular types of terms and relationships; for example, the deduction rule mentioned in the Introduction is associated with ExtensionalInheritance and Inheritance relationships.", Probabilistic Logic Networks,chapter 2
"At the highest level we may divide the set of PLN relationships into the following categories, each of which corresponds to a set of different particular relationship types: • Fuzzy membership (the Member relationship) • First-order logical relationships • Higher-order logical relationships • Containers (lists and sets) • Function execution (the ExecutionOutput relationship) To denote a relationship of type R, between Atoms A and B, with truth-value t, we write R A B <t> If A and B have long names, we may use the alternate notation 1 We have so far developed two flavors of DistributionalTruthValues, namely StepFunctionTruthValues and PolynomialTruthValues.     Chapter 2: Knowledge Representation 25 R <t> A B which lends itself to visually comprehensible nesting; e.g., R <t> A R1 C D Similarly, to denote a term A with truth-value t, we write A <t> For example, to say that A inherits from B with probability .", Probabilistic Logic Networks,chapter 2
"8, we write Inheritance A B <.8> To say that A inherits from B with IndefiniteTruthValue represented by <[.8,.9], .95>, we write Inheritance A B <[.8,.9],.95> (roughly, as noted above, the [.8, .9] interval represents an interval probability and the .95 represents a credibility level). We will also sometimes use object-field notation for truth-value elements, obtaining, for example, the strength value object associated with an Atom (Inheritance A B).strength = [.8,.9] or the entire truth-value, using .tv (Inheritance A B).tv = <[.8,.9], .9, 20>. Finally, we will sometimes use a semi-natural-language notation, which will be introduced a little later on, when we first get into constructs of sufficient complexity to require such a notation. 2.", Probabilistic Logic Networks,chapter 2
"2 Context PLN TruthValues are defined relative to a Context. The default Context is the entire universe, but this is not usually a very useful Context to consider. For instance, many terms may be thought of as denoting categories; in this case, the strength of a term in a Context denotes the probability that an arbitrary entity in the Context is a member of the category denoted by the term.     26 Probabilistic Logic Networks Contextual relationships are denoted by Context relationships, introduced in Chapter 10 The semantics of Context C R A B <t> is simply R (A AND C) (B AND C) <t> Most of the discussion in following chapters will be carried out without explicit discussion of the role of context, and yet due to the above equivalence the conclusions will be usable in the context of contextual inference. 2.3 Fuzzy Set Membership As a necessary preliminary for discussing the PLN logical relationships, we now turn to the Member relationship.", Probabilistic Logic Networks,chapter 2
"The relationship Member A B <t> spans two terms where the target B cannot be an atomic term or a relationship, but must be a term denoting a set (be it a set of atomic terms, a set of composite terms, a set of relationships, etc.). In essence, the Member relationship of PLN is the familiar “fuzzy set membership” (Zadeh 1989). For instance, we may say Member Ben Goertzel_family <1> Member Tochtli Goertzel_family <.5> (Tochtli is the dog of a Goertzel family member.) When a Member relationship has a value between 0 and 1, as in the latter example, it is interpreted as a fuzzy value rather than a probabilistic value.", Probabilistic Logic Networks,chapter 2
"PLN is compatible with many different algebras for combining fuzzy truth-values, including the standard min and max operators according to which if Member Ben A <r> Member Ben B <s> then Member Ben (A OR B) <max(r,s)> Member Ben (A AND B) <min(r,s)> When to use fuzzy set membership versus probabilistic inheritance is a somewhat subtle issue that will be discussed later on. For instance, the fuzzy set community is fond of constructs such as Member Ben tall <.75>     Chapter 2: Knowledge Representation 27 which indicates that Ben is somewhat tall. But, while this is a correct PLN construct, it is also interesting in PLN to say IntensionalInheritance Ben tall <.75> which states that (roughly speaking) Ben shares .75 of the characteristic properties of tall things.", Probabilistic Logic Networks,chapter 2
"This representation allows some useful inferences that the Member relationship does not; for instance, inheritance relationships are probabilistically transitive whereas Member relationships come without any comparably useful uncertain transitivity algebra. (As a parenthetical note, both of these are actually bad examples; they should really be Context People Member Ben tall <.75> Context People IntensionalInheritance Ben tall <.75> because Ben’s .75 tallness, however you define it, is not meaningful in comparison to the standard of the universe, but only in comparison to the standard of humans.) We extend this notion of fuzzy set membership to other truth-value types as well. For instance, using IndefiniteTruthValues MemberLink Astro Jetson_family <[.8,1],.95,2> would mean that after 2 more observations of Astro the assessed fuzzy membership value for MemberLink Astro Jetson_family would lie within [.8,1] with confidence .95. 2.", Probabilistic Logic Networks,chapter 2
"4 First-Order Logical Relationships In this section, we begin our review of the PLN first-order logical relationship types, which are the following: • Relationships representing first-order conditional probabilities: o Subset (extensional) o Inheritance (mixed) o IntensionalInheritance (intensional) • Relationships representing symmetrized first-order conditional probabilities: o ExtensionalSimilarity (extensional) o Similarity (mixed) o IntensionalSimilarity (intensional)     28 Probabilistic Logic Networks • Relationships representing higher-order conditional probabilities: o ExtensionalImplication o Implication (mixed) o IntensionalImplication • Relationships representing symmetrized higher-order conditional probabilities: o ExtensionalEquivalence o Equivalence (mixed) o IntensionalInheritance The semantics of the higher-order logical relationships will be described briefly in Section 2.6 of this chapter and in more depth later on.", Probabilistic Logic Networks,chapter 2
"The truth-value formulas for inference on these higher-order relationships are the same as those for the corresponding first-order relationships. PLN-HOI (higher-order inference) also involves a number of other relationships, such as Boolean operators (AND, OR and NOT), the SatisfyingSet operator, and an infinite spectrum of quantifiers spanning the range from ForAll to ThereExists. 2.4.1 The Semantics of Inheritance We now explain in detail the semantics of the key PLN relationship type, Inheritance. Since inheritance in PLN represents the synthesis of extensional and intensional information, we will begin by considering extensional and intensional inheritance in their pure forms. 2.4.1.1 Subset Relationships Firstly, a Subset relationship represents a probabilistic subset relationship; i.e., purely extensional inheritance.", Probabilistic Logic Networks,chapter 2
"If we have Subset A B <s> (where s, a strength value, is a single number in [0,1]) where A and B are two terms denoting sets, this means P(B|A) = s or more precisely P(x in B | x in A) = s If “in” is defined in terms of crisp Member relationships (with strength in each case either 0 or 1) this means P( Member x B <1> | Member x A <1>) = s     Chapter 2: Knowledge Representation 29 On the other hand, if “in” is defined in terms of fuzzy Member relationships then one must define s as "" f((Member x B).strength, (Member x A).strength) s= x ""(Member x A).strength x where f(x,y) denotes the fuzzy set intersection function.", Probabilistic Logic Networks,chapter 2
"Two options in this regard are ! f(x,y) = min(x,y) f(x,y) = x*y In our current practical work with PLN we’re using the min function. As before, we treat other truth-value types in an analogous manner. For example, we interpret Subset A B <[L,U] 0.9, 20> as P(x in B| x in A)![ L, U] with confidence 0.9 after 20 more observations, where “in” is defined in terms of either crisp or fuzzy Member relationships as above. 2.4.1.2 Intensional Inheritance The Subset relationship is what we call an extensional relationship – it relates two sets according to their members. PLN also deals with intensional relationships – relationships that relate sets according to the patterns that are associated with them. The mathematics of intensionality will be given in a later chapter, but here we will review the conceptual fundamentals.", Probabilistic Logic Networks,chapter 2
"First, we review the general notions of intension and extension. These have been defined in various ways by various logical philosophers, but the essential concepts are simple. The distinction is very similar to that between a word’s denotation and its connotation. For instance, consider the concept “bachelor.” The extension of “bachelor” is typically taken to be all and only the bachelors in the world (a very large set). In practical terms, it means all bachelors that are known to a given reasoning system, or specifically hypothesized by that system. On the other hand, the intension of “bachelor” is the set of properties of “bachelor,” including principally the property of being a man, and the property of being unmarried.", Probabilistic Logic Networks,chapter 2
"Some theorists would have it that the intension of “bachelor” consists solely of these two properties, which are “necessary and sufficient conditions” for bachelorhood; PLN’s notion of intension is more flexible, and it may include necessary and sufficient conditions but also other properties, such as the fact that most bachelors have legs, that they frequently eat in restaurants, etc. These other properties allow us to understand how the concept of “bachelor” might be stretched in     30 Probabilistic Logic Networks some contexts; for instance, if one read the sentence “Jane Smith was more of a bachelor than any of the men in her apartment building,” one could make a lot more sense of it using the concept “bachelors’” full PLN intension than one could make using only the necessary-and-sufficient-condition intension.", Probabilistic Logic Networks,chapter 2
"The essential idea underlying PLN’s treatment of intension is to associate both fish and whale with sets of properties, which are formalized as sets of patterns – fish and whale , the sets of patterns associated with fish and whales. We then PAT PAT interpret IntensionalInheritance whale fish <.7> as Subset whale fish <.7> PAT PAT We then define Inheritance proper as the disjunction of intensional and extensional (subset) inheritance; i.e., Inheritance A B <tv> is defined as OR <tv> Subset A B IntensionalInheritance A B The nature of reasoning on Inheritance and IntensionalInheritance relationships will be reviewed in Chapter 12; prior to that we will use Subset and related extensional relationships in most of our examples. Why do we think intensional relationships are worth introducing into PLN? This is a cognitive science rather than a mathematical question.", Probabilistic Logic Networks,chapter 2
"We hypothesize that most human inference is done not using subset relationships, but rather using composite Inheritance relationships. And, consistent with this claim, we suggest that in most cases the natural language relation “is a” should be interpreted as an Inheritance relation between individuals and sets of individuals, or between sets of individuals – not as a Subset relationship. For instance, “Stripedog is a cat” as conventionally interpreted is a combination extensional/intensional statement, as is “Cats are animals.” This statement means not only that examples of cats are examples of animals, but also that patterns in cats tend to be patterns in animals. The idea that inheritance and implication in human language and cognition mix up intension and extension is not an original one – for example, it has been argued for extensively and convincingly by Pei Wang in his writings on NARS.", Probabilistic Logic Networks,chapter 2
"However, embodying this conceptual insight Wang has outlined a different mathematics that     Chapter 2: Knowledge Representation 31 we find awkward because it manages uncertainty in a non-probabilistic way. His approach seems to us to contradict the common sense embodied in Cox’s axioms, and also to lead to counterintuitive results in many practical cases. On the other hand, our approach is consistent with probability theory but introduces measures of association and pattern-intensity as additional concepts, and integrates them into the overall probabilistic framework of PLN. Philosophically, one may ask why a pattern-based approach to intensional inference makes sense. Why, in accordance with Cox’s axioms, isn’t straightforward probability theory enough? The problem is – to wax semi-poetic for a moment – that the universe we live in is a special place, and accurately reasoning about it requires making special assumptions that are very difficult and computationally expensive to explicitly encode into probability theory.", Probabilistic Logic Networks,chapter 2
"One special aspect of our world is what Charles Peirce referred to as “the tendency to take habits” (Peirce,1931-1958): the fact that “patterns tend to spread”; i.e., if two things are somehow related to each other, the odds are that there are a bunch of other patterns relating the two things. To encode this tendency observed by Peirce in probabilistic reasoning one must calculate P(A|B) in each case based on looking at the number of other conditional probabilities that are related to it via various patterns. But this is exactly what intensional inference, as defined in PLN, does. This philosophical explanation may seem somewhat abstruse – until one realizes how closely it ties in with human commonsense inference and with the notion of inheritance as utilized in natural language. Much more is said on this topic in Goertzel (2006). 2.4.1.", Probabilistic Logic Networks,chapter 2
"3 Symmetric Logical Relationships Inheritance is an asymmetric relationship; one may also define a corresponding symmetric relationship. Specifically one may conceptualize three such relationships, corresponding to Subset, IntensionalInheritance, and Inheritance: ExtensionalSimilarity A B <tv> tv.s = purely extensional estimate of P(x !B & x ! A | x ! A OR x !B) IntensionalSimilarity A B <tv> tv.s = purely intensional estimate of P(x !B & x ! A | x ! A OR x !B) Similarity A B <tv> tv.", Probabilistic Logic Networks,chapter 2
"= intensional/extensional estimate of P(x !B & x ! A | x ! A OR x !B) In each of these conceptual formulas, ! denotes respectively for each case SubSet, IntensionalInheritance and (mixed) Inheritance     32 Probabilistic Logic Networks Elementary probability theory allows us to create truth-value formulas for these symmetric logical relationships from the truth-value formulas for the corresponding asymmetric ones. Therefore we will not say much about these symmetric relationships in this book; yet in practical commonsense reasoning they are very common. 2.5 Term Truth-Values We have discussed the truth-values of first-order logical relationships. Now we turn to a related topic, the truth-values of PLN terms. Compared to relationship truth-values, term truth-values are mathematically simpler but conceptually no less subtle.", Probabilistic Logic Networks,chapter 2
"Most simply, the truth-value of an entity A may be interpreted as the truthvalue of a certain Subset relationship: A <tv> means Subset Universe A <tv> That is, the A.tv denotes the percentage of the “universe” that falls into category A. This is simple enough mathematically, but the question is: what is this “universe”? It doesn’t have to be the actual physical universe, it can actually be any set, considered as the “universal set” (in the sense of probability theory) for a collection of inferences. In effect, then, what we’ve called the Universe is really a kind of “implicit context.” This interpretation will become clear in Section 2.2 and Chapter 10 when we explicitly discuss contextual inference. Sometimes one specifically wants to do inference within a narrow local context. Other times, one wants to do inference relative to the universe as a whole, and it’s in this case that things get tricky.", Probabilistic Logic Networks,chapter 2
"In fact, this is one of the main issues that caused Pei Wang, the creator of the non-probabilistic NARS system that partially inspired PLN, to declare probability theory an unsound foundation for modeling human inference or designing computational inference systems. His NARS inference framework is not probability-theory based and hence does not require the positing of a universal set U. Our attitude is not to abandon probability theory because of its U-dependence, but rather to explicitly acknowledge that probabilistic inference is contextdependent, and to acknowledge that context selection for inference is an important aspect of cognition. When a mind wants to apply probabilistic reasoning, nothing tells it a priori how to set this particular parameter (the size of the universal set, |U|), which makes a big difference in the results that reasoning gives. Rather, we believe, the context for an inference must generally be determined by noninferential cognitive processes, aided by appropriate inference rules.", Probabilistic Logic Networks,chapter 2
"  Chapter 2: Knowledge Representation 33 There are two extreme cases: Universal and Local context. In the Universal case, pragmatically speaking, the set U is set equal to everything the system has ever seen or heard of. (This may of course be construed in various different ways in practical AI systems.) In the Local case, the set U is set equal to the union of the premises involved in a given inference, with nothing else acknowledged at all. According to the algebra of the PLN deduction rule, as will be elaborated below, local contexts tend to support more speculative inferences, whereas in the Universal context only the best supported of inferences come out with nontrivial strength. In some cases, the context for one probabilistic inference may be figured out by another, separate, probabilistic inference process. This can’t be a universal solution to the problem, however, because it would lead to an infinite regress.", Probabilistic Logic Networks,chapter 2
"Ultimately one has to bottom out the regress, either by assuming a universal set U is given a priori via “hard wiring” (perhaps a hard-wired function that sets U adaptively based on experience) or by positing that U is determined by non-inferential processes. If one wants to choose a single all-purpose U, one has to err on the side of inclusiveness. For instance, |U| can be set to the sum of the counts of all Atoms in the system. Or it can be set to a multiple of this, to account for the fact that the system cannot afford to explicitly represent all the entities it knows indirectly to exist. It is not always optimal to choose a universal and maximal context size, however. Sometimes one wants to carry out inference that is specifically restricted to a certain context, and in that case choosing a smaller U is necessary in order to get useful results.", Probabilistic Logic Networks,chapter 2
"For instance, if one is in the USA and is reasoning about the price of furniture, one may wish to reason only in the context of the USA, ignoring all information about the rest of the world. Later on we will describe the best approach we have conceived for defining U in practice, which is based on an equation called the “optimal universe size formula.” This approach assumes that one has defined a set of terms that one wants to consider as a context (e.g., the set of terms pertaining to events or entities in the USA, or properties of the USA). One also must assume that for some terms A, B and C in this context-set, one has information about the triple-intersections P(A!B!C). Given these assumptions, a formula may be derived that yields the U-value that is optimal, in the sense of giving the minimum error for PLN deduction in that context.", Probabilistic Logic Networks,chapter 2
"Note that some arbitrariness is still left here; one must somewhere obtain the context definition; e.g., decide that it’s intelligent to define U relative to the United States of America, or relative to the entire system’s entire experience, etc. This formula for deriving the value of U is based on values called “count values”, representing numbers of observations underlying truth value estimates, and closely related to the confidence components of truth values. This means that the challenge of defining U ultimately bottoms out in the problem of count/confidence updating. In an integrative AI architecture, for example, two sorts of processes may be used for updating the confidence components of Atoms’ TruthValues. Inference can be used to modify count values as well as strength values, which cov-     34 Probabilistic Logic Networks ers the case where entities are inferred to exist rather than observed to exist.", Probabilistic Logic Networks,chapter 2
"And in an architecture incorporating natural language processing, one can utilize “semantic mapping schemata,” which translate perceived linguistic utterances into sets of Atoms, and which may explicitly update the confidence components of truth values. To take a crude example, if a sentence says “I am very sure cats are only slightly friendly”, this translates into a truth value with a low strength and high confidence attached to the Atoms representing the statement “cats are friendly.” An important question there is: What process learns these cognitive schemata carrying out semantic mapping? If they are learned by probabilistic inference, then they must be learned within some universal set U. The pragmatic answer we have settled on in our own work is that inference applied to schema learning basically occurs using a local context, in which the schema known to the system are assumed to be all there are.", Probabilistic Logic Networks,chapter 2
"Some of these schemata learned with a local context are then used to manipulate the count variables of other Atoms, thus creating a larger context for other applications of inference within the system. In our practical applications of PLN, we have found it is not that often that the most universal U known to the system is used. More often than not, inference involves some relatively localized context. For example, if the system is reasoning about objects on Earth it should use a U relativized to Earth’s surface, rather than using the U it has inferred for the entire physical universe. P(air) is very small in the context of the whole physical universe, but much larger in the context of the Earth. Every time the inference system is invoked it must assume a certain context size |U|, and there are no rigid mathematical rules for doing this.", Probabilistic Logic Networks,chapter 2
"Rather, this parameter-setting task is a job for cognitive schema, which are learned by a host of processes in conjunction, including inference conducted with respect to the implicit local context generically associated with schema learning. 2.6 Higher-Order Logical Relationships The first-order logical relationships reviewed above are all relationships between basic terms. But the same sorts of probabilistic logical relationships may be seen to hold between more complex expressions involving variables (or variableequivalent constructs like SatisfyingSets). ExtensionalImplication, for example, is a standard logical implication between predicates. In PLN-HOI we have the notion of a predicate similar to standard predicate logic as a function that maps arguments into truth-values. We have an Evaluation relationship so that, e.g., for the predicate isMale, Evaluation isMale Ben_Goertzel <1> Evaluation isMale Izabela_Goertzel <0> Evaluation isMale Hermaphroditus <0.", Probabilistic Logic Networks,chapter 2
"5> So if we have the relationship     Chapter 2: Knowledge Representation 35 ExtensionalImplication isMale hasPenis <.99> this means isMale($X) implies hasPenis($X) <.99> or in other words ExtensionalImplication <.99> Evaluation isMale $X Evaluation hasPenis $X or P( Evaluation hasPenis $X | Evaluation isMale $X) = .99 Note that we have introduced a new notational convention here: the names of variables that are arguments of Predicates are preceded by the $ sign. This convention will be used throughout the book. Regarding the treatment of the non-crisp truth-values of the Evaluation relationships, the same considerations apply here as with Subset and Member relationships. Essentially we are doing fuzzy set theory here and may use the min(,) function between the Evaluation relationship strengths.", Probabilistic Logic Networks,chapter 2
"As this example indicates, the semantics of higher-order PLN relationships thus basically boils down to the semantics of first-order PLN relationships. To make this observation formal we must introduce the SatisfyingSet operator. We define the SatisfyingSet of a Predicate as follows: the SatisfyingSet of a Predicate is the set whose members are the elements that satisfy the Predicate. Formally, that is: S = SatisfyingSet P means ( Member $X S ).tv = ( Evaluation P $X).tv In PLN, generally speaking, one must consider not only Predicates that are explicitly embodied in Predicate objects, but also Predicates defined implicitly by relationship types; e.g., predicates like P($x) = Inheritance $x A This means that relationships between relationships may be considered as a special case of relationships between predicates.", Probabilistic Logic Networks,chapter 2
"In any case, given an individual Predicate h we can construct SatisfyingSet(h), and we can create an average over a whole set of Predicates h, B ! SatisfyingSet(h) ! A ! SatisfyingSet(h)     36 Probabilistic Logic Networks Thus, information about h(x) for various x !B and x ! A and various Predicates h can be used to estimate the strengths of subset relationships between sets. Also note that in dealing with SatisfyingSets we will often have use for the Predicate NonEmpty, which returns 1 if its argument is nonempty and 0 if its argument is the empty set. For instance, Evaluation NonEmpty (SatisfyingSet (eats_bugs)) <1> means that indeed, there is somebody or something out there who eats bugs. The main point of SatisfyingSets is that we can use them to map from higherorder into first-order.", Probabilistic Logic Networks,chapter 2
"A SatisfyingSet maps Evaluation relationships into Member relationships, and hence has the side effect of mapping higher-order relations into ordinary first-order relations between sets. In other words, by introducing this one higher-order relationship (SatisfyingSet) as a primitive we can automatically get all other higher-order relationships as consequences. So using SatisfyingSets we don’t need to introduce special higher-order relationships into PLN at all. However, it turns out to be convenient to introduce them anyway, even though they are “just” shorthand for expressions using SatisfyingSets and first-order logical relationships. To understand the reduction of higher-order relations to first-order relations using SatisfyingSets, let R and R denote two (potentially different) relationship 1 2 types and let X denote an Atom-valued variable, potentially restricted to some subclass of Atoms such as a particular term or relationship type.", Probabilistic Logic Networks,chapter 2
"For example, we may construct the following higher-order relationship types: ExtensionalImplication R A X 1 R B X 2 equals Subset SatisfyingSet(R A X) 1 SatisfyingSet(R B X) 2 Implication R A X 1 R B X 2 equals Inheritance SatisfyingSet(R A X) 1 SatisfyingSet(R B X) 2 ExtensionalEquivalence R A X 1     Chapter 2: Knowledge Representation 37 R B X 2 equals ExtensionalSimilarity SatisfyingSet(R A X) 1 SatisfyingSet(R B X) 2 Equivalence R A X 1 R B X 2 equals Similarity SatisfyingSet(R A X) 1 SatisfyingSet(R B X) 2 Higher-order purely intensional symmetric and asymmetric logical relationships are omitted in the table, but may be defined analogously.", Probabilistic Logic Networks,chapter 2
"To illustrate how these higher-order relations work, consider an example higher-order relationship, expressed in first-order logic notation as !X (Member Ben X) ! (Inheritance scientists X) This comes out in PLN notation as: ExtensionalImplication Member Ben X Inheritance scientists X (“if Ben is a member of the group X, then X must contain scientists”), or equivalently Subset SatisfyingSet (Member Ben) SatisfyingSet (Inheritance scientists) (“the set of groups X that satisfies the constraint ‘MemberRelationship Ben X’ is a subset of the set of groups X that satisfies the constraint ‘Inheritance scientists X.’”) While the above examples have concerned single-variable relationships, the same concepts and formalism work for the multiple-variable case, via the mechanism of using a single list-valued variable to contain a list of component variables. 2.", Probabilistic Logic Networks,chapter 2
"7 N-ary Logical Relationships The next representational issue we will address here has to do with relationships that have more than one argument. We don’t just want to be able to say that     38 Probabilistic Logic Networks cat inherits from animal, we want to be able to say that cats eat mice, that flies give diseases to people, and so forth. We want to express complex n-ary relations, and then reason on them. In PLN there are two ways to express an n-ary relation: using list relationships, and using higher-order functions. Each has its strengths and weaknesses, so the two are used in parallel. For instance, the list approach is often more natural for inferences using n-ary relationships between simple terms, whereas the higherorder function approach is often more natural for many aspects of inference involving complex Predicates. 2.7.1 The List/Set Approach The List approach to representing n-ary relations is very simple.", Probabilistic Logic Networks,chapter 2
"An n-ary relation f with the arguments x , …, x is represented as a Predicate, where 1 n Evaluation f (x , …, x ) 1 n So for instance, the relationship “Ben kicks Ken” becomes roughly Evaluation kicks (Ben, Ken) This doesn’t take the temporal aspect of the statement into account, but we will ignore that for the moment (the issue will be taken up later on). In some cases one has a relationship that is symmetric with respect to its arguments. One way to represent this is to use a Set object for arguments. For instance, to say “A fuses with B” we may say Evaluation fuse {A, B} where {A, B} is a Set. This kind of representation is particularly useful when one is dealing with a relationship with a large number of arguments, as often occurs with the processing of perceptual data. 2.7.", Probabilistic Logic Networks,chapter 2
"2 Curried Functions Another way to represent n-ary functions – in the spirit of Haskell rather than LISP – is using function currying. For example, a different representation of “Ben kicks Ken” is Evaluation (kick Ben) Ken where in this case the interpretation is that (kick Ben) is a function that outputs a Predicate function that tells whether its argument is kicked by Ben or not.", Probabilistic Logic Networks,chapter 2
"Strictly, of course, the kick in this example is not the same as the kick in the argument list example; a more correct notation would be, for instance,     Chapter 2: Knowledge Representation 39 Evaluation kick_List (Ben, Ken) Evaluation (kick_curry Ben) Ken In a practical PLN system these two functions will have to be represented by different predicates, with the equivalence relation Equivalence Evaluation (kick_curry $x) $y Evaluation kick_List ($x, $y) and/or one or more of the listification relations kick_List = listify kick_curry kick_curry = unlistify kick_List stored in the system to allow conversion back and forth. Another representation is then Evaluation (kick_curry_2 Ken) Ben which corresponds intuitively to the passive voice “Ken was kicked by Ben.", Probabilistic Logic Networks,chapter 2
We then have the conceptual equivalences kick_curry = “kicks” kick_curry_2 = “is kicked by” Note that the relation between kick_curry and kick_curry_2 is trivially representable using the C combinator (note that in this book we use the notational convention that combinators are underlined) by kick_curry = C kick_curry_2 Mathematical properties of Predicates are easily expressed in this notation., Probabilistic Logic Networks,chapter 2
"For instance, to say that the Predicate fuse is symmetric we need only use the higherorder relationship EquivalenceRelationship fuse C fuse or we could simply say Inheritance fuse symmetric where the Predicate symmetric is defined by ExtensionalEquivalence Evaluation symmetric $X Equivalence $X C $X  ", Probabilistic Logic Networks,chapter 2
"  Chapter 3: Experiential Semantics Abstract Chapter 3 is a brief chapter in which we discuss the conceptual interpretation of the terms used in PLN, according to the scheme we have deployed when utilizing PLN to carry out inferences regarding the experience of an embodied agent in a simulated world. This is what we call “experiential semantics.” The PLN mathematics may also be applied using different semantic assumptions, for instance in a logical theorem-proving context. But the development of PLN has been carried out primarily in the context of experiential semantics, and that will be our focus here. 3.1 Introduction Most of the material in this book is mathematical and formal rather than philosophical in nature. Ultimately, however, the mathematics of uncertain logic is only useful when incorporated into a practical context involving non-logical as well as logical aspects; and the integration of logic with non-logic necessarily requires the conceptual interpretation of logical entities.", Probabilistic Logic Networks,chapter 3
"The basic idea of experiential semantics is that the interpretation of PLN terms and relationships should almost always be made in terms of the observations made by a specific system in interacting with the world. (Some of these observations may, of course, be observations of the system itself.) The numerous examples given in later (and prior) sections regarding “individuals” such as people, cats and so forth, can’t really be properly interpreted without attention to this fact, and in particular to the experiential semantics of individuals to be presented here. What makes PLN’s experience-based semantics subtle, however, is that there are many PLN terms and relationships that don’t refer directly to anything in the world the PLN reasoning system is observing. But even for the most abstract relationships and concepts expressed in PLN, the semantics must ultimately be grounded in observations.     42 Probabilistic Logic Networks 3.", Probabilistic Logic Networks,chapter 3
"2 Semantics of Observations In experiential semantics, we are considering PLN as a reasoning system intended for usage by an embodied AI agent: one with perceptions and actions as well as cognitions. The first step toward concretizing this perspective is to define what we mean by observations. While PLN is mostly about statements with probabilistic truth values, at the most basic semantic level it begins with Boolean truth-valued statements. We call these basic Boolean truth-valued statements “elementary observations.” Elementary observations may be positive or negative. A positive observation is one that occurred; a negative observation is one that did not occur. Elementary observations have the property of unity: each positive elementary observation occurs once. This occurrence may be effectively instantaneous or it may be spread over a long period of time. But each elementary observation occurs once and is associated with one particular set of time-points.", Probabilistic Logic Networks,chapter 3
"In the experiential-semantics approach, PLN’s probabilistic statements may all ultimately be interpreted as statements about sets of elementary observations. In set-theoretic language, these elementary observations are “atoms” and PLN Concept terms are sets built up from these “atoms.” However we won’t use that terminology much here, since in PLN the term Atom is used to refer to any PLN term or relationship. Instead, we will call these “elementary terms.” In an experiential-semantics approach to PLN, these are the basic terms out of which the other PLN terms and relationships are built. For example, in the context of an AI system with a camera eye sensor, an elementary observation might be the observation A defined by A = ""the color of the pixel at location (100, 105) is blue at time 2:13:22PM on Tuesday January 6, 2004.", Probabilistic Logic Networks,chapter 3
"Each elementary observation may be said to contain a certain number of bits of information; for instance, an observation of a color pixel contains more bits of information than an observation of a black-and-white pixel. 3.2.1 Inference on Elementary Observations Having defined elementary observations, one may wish to draw implication relationships between them. For instance, if we define the elementary observations     Chapter 3: Experiential Semantics 43 B = ""blue was observed by me at time 2:13:22PM on Tuesday January 6, 2004"" C = ""blue was observed by me on Tuesday January 6, 2004"" then we may observe that A implies B B implies C A implies C However, the semantics of this sort of implication is somewhat subtle. Since each elementary observation occurs only once, there is no statistical basis on which to create implications between them.", Probabilistic Logic Networks,chapter 3
"To interpret implications between elementary observations, one has to look across multiple “possible universes,” and observe that for instance, in any possible universe in which A holds, C also holds. This is a valid form of implication, but it’s a subtle one and occurs as a later development in PLN semantics, rather than at a foundational level. 3.2.2 Inference on Sets of Elementary Observations Next, sets of elementary observations may be formed and their unions and intersections may be found, and on this basis probabilistic logical relationships between these sets may be constructed. For instance, if X = the set of all observations of dark blue Y = the set of all observations of blue then it’s easy to assign values to P(X|Y) and P(Y|X) based on experience.", Probabilistic Logic Networks,chapter 3
"P(X|Y) = the percentage of observations of blue that are also observations of dark blue P(Y|X) = 1, because all observations of dark blue are observations of blue Probabilistic inference on sets of observations becomes interesting because, in real-world intelligence, each reasoning system collects far more observations than it can retain or efficiently access. Thus it may retain the fact that P(dark blue | blue) = .3     44 Probabilistic Logic Networks without retaining the specific examples on which this observation is founded. The existence of “ungrounded” probabilistic relationships such as this leads to the need for probabilistic inference using methods like the PLN rules. In this context we may introduce the notion of “intension,” considered broadly as “the set of attributes possessed by a term.” This is usually discussed in contrast to “extension,” which is considered as the elements in the set denoted by a term.", Probabilistic Logic Networks,chapter 3
"In the absence of ungrounded probabilities, inference on sets of observations can be purely extensional. However, if a reasoning system has lost information about the elements of a set of observations, but still knows other sets the set belongs to, or various conditional probabilities relating the set to other sets, then it may reason about the set using this indirect information rather than the (forgotten) members of the set – and this reasoning may be considered “intensional.” This is a relatively simple case of intensionality, as compared to intensionality among individuals and sets thereof, which will be discussed below. 3.3 Semantics of Individuals While the basic semantics of PLN is founded on observations, most of the concrete PLN examples we will give in these pages involve individual entities (people, animals, countries, and so forth) rather than directly involving observations.", Probabilistic Logic Networks,chapter 3
"The focus on individuals in this text reflects the level at which linguistic discourse generally operates, and shouldn’t be taken as a reflection of the level of applicability of PLN: PLN is as applicable to perception-and-action-level elementary observations as it is to abstract inferences about individuals and categories of individuals. However, reasoning about individuals is obviously a very important aspect of commonsense reasoning; and so, in this section, we give an explicit treatment of the semantics of individuals. Reasoning regarding individuals is a somewhat subtler issue than reasoning regarding observations, because the notion of an “individual” is not really a fundamental concept in mathematical logic. Conventional approaches to formalizing commonsense inference tend to confuse things by taking individuals as logical atoms. In fact, in the human mind or the mind of any AI system with a genuine comprehension of the world, individuals are complex cognitive constructs. Observations are much more naturally taken as logical atoms, from a mathematical and philosophical and cognitive-science point of view.", Probabilistic Logic Networks,chapter 3
"However, from a practical commonsense reasoning point of view, if one takes elementary observations as logical atoms, then inference regarding individuals can easily become horribly complex, because the representation of a pragmatically interesting individual in terms of elementary observations is generally extremely complex. PLN works around this problem by synthesizing individual-level and observation-level inference in a way that allows individual-level inference to occur based implicitly on observation-level semantics. This is a subtle point that is easy to miss when looking at practical PLN inference examples, in which individual- and observation-level semantics are freely intermixed in a con-     Chapter 3: Experiential Semantics 45 sistent way. This free intermixture is only possible because the conceptual foundations of PLN have been set up in a proper way. Let’s consider an example of an individual: the orange cat named Stripedog1 who is sitting near me (Ben Goertzel) as I write these words.", Probabilistic Logic Networks,chapter 3
"What is Stripedog? In PLN terms, Stripedog is first of all a complex predicate formed from elementary observations. Given a set of elementary observations, my mind can evaluate whether this set of observations is indicative of Stripedog’s presence or not. It does so by evaluating a certain predicate whose input argument is a set of elementary observations and whose output is a truth value indicating the “degree of Stripedogness” of the observation set. At this point we may introduce the notion of the “usefulness” of an argument for a predicate, which will be important later on. If a predicate P is applied to an observation set S, and an observation O lies in S, then we say that O is important for (P, S) if removing O from S would alter the strength of the truth value of P as applied to S. Otherwise O is unimportant for (P, S).", Probabilistic Logic Networks,chapter 3
"We will say that an observation set S is an identifier for P if one of two conditions holds: Positive identifier: S contains no elements that are unimportant for (P, S), or Negative identifier: P applied to S gives a value of 0 Of course, in addition to the observational model of Stripedog mentioned above, I also have an abstract model of Stripedog in my mind. According to this abstract model, Stripedog is a certain pattern of arrangement of molecules. The individual molecules arranged in the Stripedogish pattern are constantly disappearing and being replaced, but the overall pattern of arrangement is retained.", Probabilistic Logic Networks,chapter 3
"This abstract model of Stripedog exists in my mind because I have an abstract model of the everyday physical world in my mind, and I have some (largely ungrounded) implications that tell me that when a Stripedoggish elementary observation set is presented to me, this implies that a Stripedoggish pattern of arrangement of molecules is existent in the physical world that I hypothesize to be around me. The Stripedog-recognizing predicate, call it F , has a SatisfyingSet that Stripedog we may denote simply as stripedog, defined by ExtensionalEquivalence Member $X stripedog AND Evaluation F $X Stripedog Evaluation isIdentifier ($X, F ) Stripedog The predicate isIdentifier(S,P) returns True if and only if the set S is an identifier for the predicate P, in the sense defined above.", Probabilistic Logic Networks,chapter 3
"1 In case you’re curious, “Stripedog” is a colloquialism for “badger” -- a word that Ben Goertzel’s son Zebulon discovered in the Redwall books by Brian Jacques, and decided was an excellent name for a cute little orange kitten-cat.     46 Probabilistic Logic Networks This set, stripedog, is the set of all observation sets that are identifiers for the individual cat named Stripedog, with a fuzzy truth value function defined by the extent to which an observation set is identified as being an observation of Stripedog. 3.3.1 Properties of Individuals Now, there may be many predicates that imply and/or are implied by F to Stripedog various degrees.", Probabilistic Logic Networks,chapter 3
"For instance there’s a predicate F that says true whenever living_being a living being is observed; clearly Implication F F Stripedog living_being holds with a strength near 1 (a strength 1 so far, based on direct observation, since Stripedog has not yet been seen dead; but a strength <1 based on inference since it’s inferred that it’s not impossible, though unlikely, to see him dead). And, Implication F F <.8,.99> Stripedog orange holds as well – the .8 being because when Stripedog is seen at night, he doesn’t look particularly orange. Those predicates that are probabilistically implied by the Stripedog-defining predicate are what we call properties of Stripedog. Note that if Implication F G holds, then Inheritance (SatisfyingSet F) (SatisfyingSet G) holds.", Probabilistic Logic Networks,chapter 3
"So properties of Stripedog correspond to observation sets that include observations of Stripedog plus other, non-Stripedoggish observations. 3.4 Experiential Semantics and Term Probabilities Another conceptual issue that arises in PLN related to experiential semantics is the use of term probabilities. It is reasonable to doubt whether a construct such as P(cat) or P(Stripedog) makes any common sense – as opposed to conditional probabilities denoting the probabilities of these entities in some particular contexts. In fact we believe there is a strong cognitive reason for a commonsense rea-     Chapter 3: Experiential Semantics 47 soning engine to use default term probabilities, and one that ties in with experiential semantics and merits explicit articulation. The key point is that the default probability of a term represents the probability relative to the context of the entire universe as intellectually understood by the reasoning system.", Probabilistic Logic Networks,chapter 3
"This would be nebulous to define and of extremely limited utility. Rather, a term probability should represent the probability of the class denoted by the term relative to the organism’s (direct and indirect) experience. An analogy to everyday human experience may be worthwhile. Outside of formal contexts like science and mathematics, human organisms carry out commonsense reasoning within the default context of their everyday embodied life. So for instance in our everyday thinking we assume as a default that cats are more common than three-eared wombats. Though we can override this if the specific context calls for it. So, we conjecture, the term probability of “cat” in a typical human mind is shorthand for “the term probability of cat in the default context of my everyday life” – but it is not represented anything like this; rather, the “everyday life” context is left implicit.", Probabilistic Logic Networks,chapter 3
"Formalistically, we could summarize the above discussion by saying that: The default term probability of X is the weighted average of the probability of X across all contexts C, where each context C is weighted by its importance to the organism. In this sense, default term probabilities become more heuristic than contextspecific probabilities. And they also require concepts outside PLN for their definition, relying on the embedding of PLN in some broader embodied cognition framework such as the NCE. The conceptual reason why this kind of default node probability is useful is that doing all reasoning contextually is expensive, as there are so many contexts. So as an approximation, assuming a default experiential context is very useful. But the subtlety is that for an organism that can read, speak, listen, and so forth, the “everyday experiential context” needs to go beyond what is directly experienced with the senses. 3.", Probabilistic Logic Networks,chapter 3
"5 Conclusion In this brief chapter, beginning with elementary observations, we have built up to individuals and their properties. The semantic, conceptual notions presented here need not be invoked explicitly when reviewing the bulk of the material in this book, which concerns the mathematics of uncertain truth value estimation in PLN. However, when interpreting examples involving terms with names like “Ben” and “cat”, it is important to remember that in the context of the reasoning carried out by an embodied agent, such terms are not elementary indecomposables but rather complex constructs built up in a subtle way from a large body of elementary observations.", Probabilistic Logic Networks,chapter 3
"  Chapter 4: Indefinite Truth Values Abstract In this chapter we develop a new approach to quantifying uncertainty via a hybridization of Walley’s theory of imprecise probabilities and Bayesian credible intervals. This “indefinite probability” approach provides a general method for calculating the “weight-of-evidence” underlying the conclusions of uncertain inferences. Moreover, both Walley’s imprecise beta-binomial model and standard Bayesian inference can be viewed mathematically as special cases of the more general indefinite probability model. 4.1 Introduction One of the major issues with probability theory as standardly utilized involves the very quantification of the uncertainty associated with statements that serve as premises or conclusions of inference.", Probabilistic Logic Networks,chapter 4
"Using a single number to quantify the uncertainty of a statement is often not sufficient, a point made very eloquently by Wang (Wang 2004), who argues in detail that the standard Bayesian approach does not offer any generally viable way to assess or reason about the “second-order uncertainty” involved in a given probability assignment. Probability theory provides richer mechanisms than this: one may assign a probability distribution to a statement, instead of a single probability value. But what if one doesn’t have the data to fill in a probability distribution in detail? What is the (probabilistically) best approach to take in the case where a single number is not enough but the available data doesn’t provide detailed distributional information? Current probability theory does not address this issue adequately. Yet this is a critical question if one wants to apply probability theory in a general intelligence context. In short, one needs methods of quantifying uncertainty at an intermediate level of detail between single probability numbers and fully known probability distributions.", Probabilistic Logic Networks,chapter 4
"This is what we mean by the question: What should an uncertain truth-value be, so that a general intelligence may use it for pragmatic reasoning? 4.2 From Imprecise Probabilities to Indefinite Probabilities Walley’s (Walley 1991) theory of imprecise probabilities seeks to address this issue, via defining interval probabilities, with interpretations in terms of families of probability distributions. The idea of interval probabilities was originally intro-     50 Probabilistic Logic Networks duced by Keynes (Keynes 1921, 2004), but Walley’s version is more rigorous, grounded in the theory of envelopes of probability distributions. Walley’s intervals, so-called “imprecise probabilities,” are satisfyingly natural and consistent in the way they handle uncertain and incomplete information. However, in spite of a fair amount of attention over the years, this line of research has not yet been developed to the point of yielding robustly applicable mathematics.", Probabilistic Logic Networks,chapter 4
"Using a parametrized envelope of (beta-distribution) priors rather than assuming a single prior as would be typical in the Bayesian approach, Walley (Walley 1991, 1996) concludes that it is plausible to represent probabilities as intervals of "" m m+k% the form , . In this formula n represents the total number of observa$ ’ # n+k n+k& tions, m represents the number of positive observations, and k is a parameter that Walley calls s and derives as a parameter of the beta distribution. Walley calls this parameter the learning parameter, while we will refer to it as the lookahead pa!r ameter. Note that the width of the interval of probabilities is inversely related to the number of observations n, so that the more evidence one has, the narrower the interval. The parameter k determines how rapidly this narrowing occurs. An interval of this sort is what Walley calls an “imprecise probability.", Probabilistic Logic Networks,chapter 4
"Walley’s approach comes along with a host of elegant mathematics including a Generalized Bayes’ Theorem. However it is not the only approach to interval probabilities. For instance, one alternative is Weichselberger’s (Weichselberger, 2003) axiomatic approach, which works with sets of probabilities of the form [L, U] and implies that Walley’s generalization of Bayes’ rule is not the correct one. One practical issue with using interval probabilities like Walley’s or Weichselberger’s in the context of probabilistic inference rules (such as those used in PLN) is the pessimism implicit in interval arithmetic. If one takes traditional probabilistic calculations and simplistically replaces the probabilities with intervals, then one finds that the intervals rapidly expand to [0, 1]. This fact simply reflects the fact that the intervals represent “worst case” bounds.", Probabilistic Logic Networks,chapter 4
"This same problem also affects Walley’s and Weichselberger’s more sophisticated approaches, and other approaches in the imprecise probabilities literature. The indefinite probabilities approach presented here circumvents these practical problems by utilizing interval probabilities that have a different sort of semantics – closely related to, but not the same as, those of Walley’s interval probabilities. Indefinite probabilities, as we consider them here, are represented by quadruples of the form <(L, U], b, k>– thus, they contain two additional numbers beyond the [L, U] interval truth values proposed by Keynes, and one number beyond the <(L, U], k> formalism proposed by Walley. The semantics involved in assigning such a truth value to a statement S is, roughly, “I assign a probability of b to the hypothesis that, after I have observed k more pieces of evidence, the truth value I assign to S will lie in the interval [L, U].", Probabilistic Logic Networks,chapter 4
"” In the practical examples presented here we will hold k constant and thus will deal with truth value triples <(L, U], b>. The inclusion of the value b, which defines the credibility level according to which [L, U] is a credible interval (for hypothesized future assignments of the     Chapter 4: Indefinite Truth Values 51 probability of S, after observing k more pieces of evidence), is what allows our intervals to generally remain narrower than those produced by existing imprecise probability approaches. If b=1, then our approach essentially reduces to imprecise probabilities, and in pragmatic inference contexts tends to produce intervals [L, U] that approach [0, 1]. The use of b<1 allows the inferential production of narrower intervals, which are more useful in a real-world inference context.", Probabilistic Logic Networks,chapter 4
"In practice, to execute inferences using indefinite probabilities we make heuristic distributional assumptions, assuming a “second-order” distribution which has [L, U] as a (100*b)% credible interval, and then “first-order” distributions whose means are drawn from the second-order distribution. These distributions are to be viewed as heuristic approximations intended to estimate unknown probability values existing in hypothetical future situations. The utility of the indefinite probability approach may be dependent on the appropriateness of the particular distributional assumptions to the given application situation. But in practice we have found that a handful of distributional forms seem to suffice to cover commonsense inferences (beta and bimodal forms seem good enough for nearly all cases; and here we will give only examples covering the beta distribution case).", Probabilistic Logic Networks,chapter 4
"Because the semantics of indefinite probabilities is different from that of ordinary probabilities, or imprecise probabilities, or for example NARS truth values, it is not possible to say objectively that any one of these approaches is “better” than the other one, as a mathematical formalism. Each approach is better than the others at mathematically embodying its own conceptual assumptions. From an AGI perspective, the value of an approach to quantifying uncertainty lies in its usefulness when integrated with a pragmatic probabilistic reasoning engine. While complicated and dependent on many factors, this is nevertheless the sort of evaluation that we consider most meaningful. Section 4.3 deals with the conceptual foundations of indefinite probabilities, clarifying their semantics in the context of Bayesian and frequentist philosophies of probability. Section 4.4 outlines the pragmatic computational method we use for doing probabilistic and heuristic inference using indefinite probabilities. 4.", Probabilistic Logic Networks,chapter 4
"3 The Semantics of Uncertainty The main goal of this chapter is to present indefinite probabilities as a pragmatic tool for uncertain inference, oriented toward utilization in AGI systems. Before getting practical, however, we will pause in this section to discuss the conceptual, semantic foundations of the “indefinite probability” notion. In the course of developing the indefinite probabilities approach, we found that the thorniest aspects lay not in the mathematics or software implementation, but rather in the conceptual interpretation of the truth values and their roles in inference. In the philosophy of probability, there are two main approaches to interpreting the meaning of probability values, commonly labeled frequentist and Bayesian (Stanford Encyclopedia of Philosophy 2003). There are many shades of meaning     52 Probabilistic Logic Networks to each interpretation, but the essential difference is easy to understand.", Probabilistic Logic Networks,chapter 4
"The frequentist approach holds that a probability should be interpreted as the limit of the relative frequency of an event-category, calculated over a series of events as the length of the series tends to infinity. The subjectivist or Bayesian approach holds that a probability should be interpreted as the degree of belief in a statement, held by some observer; or in other words, as an estimate of how strongly an observer believes the evidence available to him supports the statement in question. Early proponents of the subjectivist view were Ramsey (Ramsey 1931) and de Finetti (de Finetti 1974-75), who argued that for an individual to display self-consistent betting behavior they would need to assess degrees of belief according to the laws of probability theory. More recently Cox’s theorem (Cox 1946) and related mathematics (Hardy 2002) have come into prominence as providing a rigorous foundation for subjectivist probability.", Probabilistic Logic Networks,chapter 4
"Roughly speaking, this mathematical work shows that if the observer assessing subjective probabilities is to be logically consistent, then their plausibility estimates must obey the standard rules of probability. From a philosophy-of-AI point of view, neither the frequentist nor the subjectivist interpretations, as commonly presented, is fully satisfactory. However, for reasons to be briefly explained here, we find the subjectivist interpretation more acceptable, and will consider indefinite probabilities within a subjectivist context, utilizing relative frequency calculations for pragmatic purposes but giving them an explicitly subjectivist rather than frequentist interpretation. The frequentist interpretation is conceptually problematic in that it assigns probabilities only in terms of limits of sequences, not in terms of finite amounts of data. Furthermore, it has well-known difficulties with the assignment of probabilities to unique events that are not readily thought of as elements of ensembles.", Probabilistic Logic Networks,chapter 4
"For instance, what was the probability, in 1999, of the statement S holding that “A great depression will be brought about by the Y2K problem”? Yes, this probability can be cast in terms of relative frequencies in various ways. For instance, one can define it as a relative frequency across a set of hypothetical “possible worlds”: across all possible worlds similar to our own, in how many of them did the Y2K problem bring about a great depression? But it’s not particularly natural to assume that this is what an intelligence must do in order to assign a probability to S. It would be absurd to claim that, in order to assign a probability to S, an intelligence must explicitly reason in terms of an ensemble of possible worlds. Rather, the claim must be that whatever reasoning a mind does to evaluate the probability of S may be implicitly interpreted in terms of possible worlds. This is not completely senseless, but is a bit of an irritating conceptual stretch.", Probabilistic Logic Networks,chapter 4
"The subjectivist approach, on the other hand, is normally conceptually founded either on rational betting behaviors or on Cox’s theorem and its generalizations, both of which are somewhat idealistic. No intelligent agent operating within a plausible amount of resources can embody fully self-consistent betting behavior in complex situations. The irrationality of human betting behavior is well known; to an extent this is due to emotional rea-     Chapter 4: Indefinite Truth Values 53 sons, but there are also practical limitations on the complexity of the situation in which any finite mind can figure out the correct betting strategy. And similarly, it is too much to expect any severely resource-constrained intelligence to be fully self-consistent in the sense that the assumptions of Cox’s theorem require.", Probabilistic Logic Networks,chapter 4
"In order to use Cox’s theorem to justify the use of probability theory by practical intelligences, it seems to us, one would need to take another step beyond Cox, and argue that if an AI system is going to have a “mostly sensible” measure of plausibility (i.e., if its deviation from Cox’s axioms are not too great), then its intrinsic plausibility measure must be similar to probability. We consider this to be a viable line of argument, but will pursue this point in another paper – to enlarge on such matters here would take us too far afield. Walley’s approach to representing uncertainty is based explicitly on a Bayesian, subjectivist interpretation; though whether his mathematics has an alternate frequentist interpretation is something he has not explored, to our knowledge.", Probabilistic Logic Networks,chapter 4
"Similarly, our approach here is to take a subjectivist perspective on the foundational semantics of indefinite probabilities (although we don’t consider this critical to our approach; quite likely it could be given a frequentist interpretation as well). Within our basic subjectivist interpretation, however, we will frequently utilize relative frequency calculations when convenient for pragmatic reasoning. This is conceptually consistent because within the subjectivist perspective there is still a role for relative frequency calculations, so long as they are properly interpreted. Specifically, when handling a conditional probability P(A|B), it may be the case that there is a decomposition B=B +...+B so that the B are mutually exclusive and 1 n i equiprobable, and each of P(A|B) is either 0 or 1. In this case the laws of probabili ity tell us P(A|B) = P(A|B ) P(B | B) + ...", Probabilistic Logic Networks,chapter 4
"+ P(A|B ) P(B |B) = (P(A|B ) + ... + 1 1 n n 1 P(A|B ))/n, which is exactly a relative frequency. So, in the case of statements that n are decomposable in this sense, the Bayesian interpretation implies a relative frequency based interpretation (but not a “frequentist” interpretation in the classical sense). For decomposable statements, plausibility values may be regarded as the means of probability distributions, where the distributions may be derived via subsampling (sampling subsets C of {B ,...,B }, calculating P(A|C) for each subset, 1 n and taking the distribution of these values; as in the statistical technique known as bootstrapping). In the case of the “Y2K” statement and other similar statements regarding unique instances, one option is to think about decomposability across possible worlds, which is conceptually controversial. 4.", Probabilistic Logic Networks,chapter 4
"4 Indefinite Probability We concur with the subjectivist maxim that a probability can usefully be interpreted as an estimate of the plausibility of a statement, made by some observer. However, we suggest introducing into this notion a more careful consideration of the role of evidence in the assessment of plausibility. We introduce a distinction that we feel is critical, between     54 Probabilistic Logic Networks • the ordinary (or “definite”) plausibility of a statement, interpreted as the degree to which the evidence already (directly or indirectly) collected by a particular observer supports the statement. • the “indefinite plausibility” of a statement, interpreted as the degree to which the observer believes that the overall body of evidence potentially available to him supports the statement. The indefinite plausibility is related to the ordinary plausibility, but also takes into account the potentially limited nature of the store of evidence collected by the observer at a given point in time.", Probabilistic Logic Networks,chapter 4
"While the ordinary plausibility is effectively represented as a single number, the indefinite plausibility is more usefully represented in a more complex form. We suggest to represent an indefinite plausibility as a quadruple <(L, U], b, k>, which when attached to a statement S has the semantics “I assign an ordinary plausibility of b to the statement that ‘Once k more items of evidence are collected, the ordinary plausibility of the statement S will lie in the interval [L, U]’.” Note that indefinite plausibility is thus defined as “second-order plausibility” – a plausibility of a plausibility. As we shall see in later sections of the paper, for most computational purposes it seems acceptable to leave the parameter k in the background, assuming it is the same for both the premises and the conclusion of an inference.", Probabilistic Logic Networks,chapter 4
"So in the following we will speak mainly of indefinite probabilities as <(L, U], b> triples, for sake of simplicity. The possibility does exist, however, that in future work inference algorithms will be designed that utilize k explicitly. Now, suppose we buy the Bayesian argument that ordinary plausibility is best represented in terms of probability. Then it follows that indefinite plausibility is best represented in terms of second-order probability; i.e., as “I assign probability b to the statement that ‘Once k more items of evidence have been collected, the probability of the truth of S based on this evidence will lie in the interval [L, U]’.” 4.4.1 An Interpretation in Terms of Betting Behavior To justify the above definition of indefinite probability more formally, one approach is to revert to betting arguments of the type made by de Finetti in his work on the foundations of probability.", Probabilistic Logic Networks,chapter 4
"As will be expounded below, for computational purposes we have taken a pragmatic frequentist approach based on underlying distributional assumptions. However, for purposes of conceptual clarity, a more subjectivist de Finetti style justification is nevertheless of interest. So, in this subsection we will describe a “betting scenario” that leads naturally to a definition of indefinite probabilities. Suppose we have a category C of discrete events; e.g., a set of tosses of a certain coin, which has heads on one side and tails on the other. Next, suppose we have a predicate S, which is either True or False (Boolean values) for each event within the above event-category C. For example, if C is a set of tosses of a certain     Chapter 4: Indefinite Truth Values 55 coin, then S could be the event “Heads.” S is a function from events into Boolean values.", Probabilistic Logic Networks,chapter 4
"If we have an agent A, and the agent A has observed the evaluation of S on n different events, then we will say that n is the amount of evidence that A has observed regarding S; or we will say that A has made n observations regarding S. Now consider a situation with three agents: the House, the Gambler, and the Meta-gambler. As the name indicates, House is going to run a gambling operation, involving generating repeated events in category C, and proposing bets regarding the outcome of future events in C. More interestingly, House is also going to propose bets to Meta-gambler regarding the behavior of Gambler. Specifically, suppose House behaves as follows. After Gambler makes n observations regarding S, House offers Gambler the opportunity to make what we‘ll call a “de Finetti” type bet regarding the outcome of the next observation of S.", Probabilistic Logic Networks,chapter 4
"That is, House offers Gambler the opportunity: You must set the price of a promise to pay $1 if the next observation of S comes out True, and $0 if it does not. You must commit that I will be able to choose to either buy such a promise from you at the price you have set, or to require you to buy such a promise from me. In other words: you set the odds, but I decide which side of the bet will be yours. Assuming Gambler does not want to lose money, the price Gambler sets in such a bet is the “operational subjective probability” that Gambler assigns that the next observation of S will come out True. As an aside, House might also offer Gambler the opportunity to bet on sequences of observations; e.g.", Probabilistic Logic Networks,chapter 4
"it might offer similar “de Finetti” price-setting opportunities regarding predicates like “The next 5 observations of S made will be in the ordered pattern (True, True, True, False, True).” In this case, things become interesting if we suppose Gambler thinks that: For each sequence Z of {True, False} values emerging from repeated observation of S, any permutation of Z has the same (operational subjective) probability as Z. Then, Gambler thinks that the series of observations of S is “exchangeable,” which means intuitively that S’s subjective probability estimates are really estimates of the “underlying probability of S being true on a random occasion.” Various mathematical conclusions follow from the assumption that Gambler does not want to lose money, combined with the assumption that Gambler believes in exchangeability. Next, let’s bring Meta-gambler into the picture. Suppose that House, Gambler and Meta-gambler have all together been watching n observations of S.", Probabilistic Logic Networks,chapter 4
"Now, House is going to offer Meta-gambler a special opportunity. Namely, he is going to bring Meta-gambler into the back room for a period of time. During this period of time, House and Gambler will be partaking in a gambling process involving the predicate S. Specifically, while Meta-gambler is in the back room, House is going to show Gambler k new observations of S. Then, after the k’th observation, House is going to come drag Meta-gambler out of the back room, away from the pleasures of the flesh and back to the place where gambling on S occurs.     56 Probabilistic Logic Networks House then offers Gambler the opportunity to set the price of yet another de Finetti style bet on yet another observation of S. Before Gambler gets to set his price, though, Meta-gambler is going to be given the opportunity of placing a bet regarding what price Gambler is going to set.", Probabilistic Logic Networks,chapter 4
"Specifically, House is going to allow Meta-gambler to set the price of a de Finetti style bet on a proposition of Metagambler’s choice, of the form: Q = “Gambler is going to bet an amount p that lies in the interval [L, U]” For instance Meta-gambler might propose Let Q be the proposition that Gambler is going to bet an amount lying in [.4, .6] on this next observation of S. I’ll set at 30 cents the price of a promise defined as follows: To pay $1 if Q comes out True, and $0 if it does not. I will commit that you will be able to choose either to buy such a promise from me at this price, or to require me to buy such a promise from you. I.e., Meta-Gambler sets the price corresponding to Q, but House gets to determine which side of the bet to take.", Probabilistic Logic Networks,chapter 4
"Let us denote the price set by Meta-gambler as b; and let us assume that Meta-gambler does not want to lose money. Then, b is Meta-gambler’s subjective probability assigned to the statement that: “Gambler’s subjective probability for the next observation of S being True lies in [L, U].” But, recall from earlier that the indefinite probability <[L, U], b, k> attached to S means that: “The estimated odds are b that after k more observations of S, the estimated probability of S will lie in [L, U].” or in other words “[L, U] is a b-level credible interval for the estimated probability of S after k more observations.” In the context of an AI system reasoning using indefinite probabilities, there is no explicit separation between the Gambler and the Meta-gambler; the same AI system makes both levels of estimate.", Probabilistic Logic Networks,chapter 4
"But this is of course not problematic, so long as the two components (first-order probability estimation and b-estimation) are carried out separately. One might argue that this formalization in terms of betting behavior doesn’t really add anything practical to the indefinite probabilities framework as already formulated. At minimum, however, it does make the relationship between indefinite probabilities and the classical subjective interpretation of probabilities quite clear. 4.4.2 A Pragmatic Frequentist Interpretation Next, it is not hard to see how the above-presented interpretation of an indefinite plausibility can be provided with an alternate justification in relative frequency     Chapter 4: Indefinite Truth Values 57 terms, in the case where one has a statement S that is decomposable in the sense described above. Suppose that, based on a certain finite amount of evidence about the frequency of a statement S, one wants to guess what one’s frequency estimate will be once one has seen a lot more evidence.", Probabilistic Logic Networks,chapter 4
"This guessing process will result in a probability distribution across frequency estimates – which may itself be interpreted as a frequency via a “possible worlds” interpretation. One may think about “the frequency, averaged across all possible worlds, that we live in a world in which the observed frequency of S after k more observations will lie in interval I.” So, then, one may interpret <[L, U], b, N> as meaning “b is the frequency of possible worlds in which the observed frequency of S, after I’ve gathered k more pieces of evidence, will lie in the interval [L, U].” This interpretation is not as conceptually compelling as the betting-based interpretation given above – because bets are real things, whereas these fictitious possible worlds are a bit slipperier. However, we make use of this frequency-based interpretation of indefinite probabilities in the practical computational implementation of indefinite probability presented in the following sections – without, of course, sacrificing the general Bayesian interpretation of the indefinite probability approach.", Probabilistic Logic Networks,chapter 4
"In the end, we consider the various interpretations of probability to be in the main complementary rather than contradictory, providing different perspectives on the same very useful mathematics. Moving on, then: To adopt a pragmatic frequency-based interpretation of the second-order plausibility in the definition of indefinite plausibility, we interpret “I assign probability b to the statement that ‘Once k more items of evidence are collected, the probability of the truth of S based on this evidence will lie in the interval [L, U]’” to mean “b is the frequency, across all possible worlds in which I have gathered k more items of evidence about S, of worlds in which the statement ‘the estimated probability of S lies in the interval [L, U]’ is true.” This frequency-based interpretation allows us to talk about a probability distribution consisting of probabilities assigned to values of ‘the estimated probability of S,’ evaluated across various possible worlds.", Probabilistic Logic Networks,chapter 4
"This probability distribution is what, in the later sections of the paper, we call the “second-order distribution.” For calculational purposes, we assume a particular distributional form for this second-order distribution. Next, for the purpose of computational implementation, we make the heuristic assumption that the statement S under consideration is decomposable, so that in each possible world, “the estimated probability of S” may be interpreted as the mean of a probability distribution. For calculational purposes, in our current implementation we assume a particular distributional form for these probability distributions, which we refer to as “the first-order distributions.” The adoption of a frequency-based interpretation for the second-order plausibility seems hard to avoid if one wants to do practical calculations using the indefinite probabilities approach. On the other hand, the adoption of a frequency-based interpretation for the first-order plausibilities is an avoidable convenience, which is appropriate only in some situations.", Probabilistic Logic Networks,chapter 4
"We will discuss below how the process of reasoning using indefinite probabilities can be simplified, at the cost of decreased     58 Probabilistic Logic Networks robustness, in cases where decomposability of the first order probabilities is not a plausible assumption. So, to summarize, in order to make the indefinite probabilities approach computationally tractable, we begin by restricting attention to some particular family D of probability distributions. Then, we interpret an interval probability attached to a statement as an assertion that: “There is probability b that the subjective probability of the statement, after I have made k more observations, will appear to be drawn from a distribution with a mean in this interval.", Probabilistic Logic Networks,chapter 4
"Then, finally, given this semantics and a logical inference rule, one can ask questions such as: “If each of the premises of my inference corresponds to some interval, so that there is probability b that after k more observations the distribution governing the premise will appear to have a mean in that interval, then what is an interval so that b of the family of distributions of the conclusion have means lying in that interval?” We may then give this final interval the interpretation that, after k more observations, there is a probability b that the conclusion of the inference will appear to lie in this final interval. (Note that, as mentioned above, the parameter k essentially “cancels out” during inference, so that one doesn’t need to explicitly account for it during most inference operations, so long as one is willing to assume it is the same in the premises and the conclusion.", Probabilistic Logic Networks,chapter 4
"In essence, this strategy merges the idea of imprecise probabilities with the Bayesian concept of credible intervals; thus the name “indefinite probabilities” (“definite” having the meaning of “precise,” but also the meaning of “contained within specific boundaries” – Walley’s probabilities are contained within specific boundaries, whereas ours are not). As hinted above, however, the above descriptions mask the complexity of the actual truth-value objects. In the indefinite probabilities approach, in practice, each IndefiniteTruthValue object is also endowed with three additional parameters: • An indicator of whether [L, U] should be considered as a symmetric or asymmetric credible interval. • A family of “second-order” distributions, used to govern the second-order plausibilities described above. • A family of “first-order” distributions, used to govern the first-order plausibilities described above.", Probabilistic Logic Networks,chapter 4
"Combined with these additional parameters, each truth-value object essentially provides a compact representation of a single second-order probability distribution with a particular, complex structure. 4.5 Truth-Value Conversions In our current implementation, we usually use <[L, U], b, k> IndefiniteTruthValues for inference. For other purposes however, it is necessary to convert these     Chapter 4: Indefinite Truth Values 59 truth values into SimpleTruthValues, in either (s,w) or (s,n) form. We now derive a conversion formula for translating indefinite truth values into simple truth values. In order to carry out the derivation additional assumptions must be made, which is why the formula derived here must be considered “heuristic” from the point of view of applications. When the underlying distributional assumptions apply, the formula is exact, but these assumptions may not always be realistic. 4.5.", Probabilistic Logic Networks,chapter 4
"1 Calculation of Approximate Conversion Formulas To derive conversion formulas we assume that the distributions underlying the means within the [L, U] intervals of the indefinite truth values are beta distributions. Due to the conjugacy of the beta and binomial distributions, this means we can model these means as corresponding to Bernoulli trials. In order to derive approximate formulas, we first consider the problem “backwards”: Given b, n, and k, how can we derive [L, U] from an assumption of an underlying Bernoulli process with unknown probability p? We then reverse the process to obtain an approach for deriving n given b, k and [L, U]. Theorem: Suppose there were x successes in the first n trials of a binomial process with an unknown fixed probability p for success. Suppose further that the prior probability density f(p=a) is uniform.", Probabilistic Logic Networks,chapter 4
"Then the probability that there will be (x+X) successes in the first (n+k) trials is given by "" k%"" n% (n+1)$ ’$ ’ # X&# x& , "" k+n% (k+n+1)$ ’ # X+x& "" % where $ ’ indicates the binomial coefficient. # & ! Proof: The probability P(y successes in n+k trialsx successes in n trials) is the same as ! P(y"" x successes in k trials). Letting X = y-x and assuming probability p for "" k% X k(X success on each trial, then this probability would be given by $ ’ p (1( p) .", Probabilistic Logic Networks,chapter 4
"# X& ! The probability densities for p can be found from Bayes’ theorem, which states ! !     60 Probabilistic Logic Networks "" x % f$ s= p= a’ f(p= a) "" x% # n & f$ p= as= ’ = . (1) # n& "" x% f$ s= ’ # n& Here f() denotes the appropriate probability density. Now f(p=a)=1 and since n "" x% is fixed, f$ s= ’ = f(x successes in n trials), so ! # n& f"" x% )1"" n% px(1( p)n( x 1 $ s= ’ = $ ’ dp= .", Probabilistic Logic Networks,chapter 4
"(2) # n& 0 # x& n+1 ! Hence, ! P(X successes in k trials) =(n+1)(1"" k% ’"" n% pX+x p)k+n)(X+x) $ $ ’ (1) dp (3) 0 # X&# x& "" k%"" n% (n+1)$ ’$ ’ # X&# x& = . "" k+n% (k+n+1)$ ’ # X+ x& The theorem gives a distribution based on n, k and x; and then, applying b, we can find a symmetric credible interval [L, U] about s=x/n based on this distribu! tion. Due to small deviations arising from integer approximations, given L, U, b and k, the reverse process is somewhat trickier. We now outline two approximate inverse procedures. We first exhibit a heuristic algorithmic approach.", Probabilistic Logic Networks,chapter 4
"From the results of this heuristic approach we then develop an approximate inverse function. 4.5.1.1 Heuristic Algorithmic Approach ""(n+k)L+0.5# ""(n+k)U+0.5# 1. Let L = , and U = . 1 n+k 1 n+k ! !     Chapter 4: Indefinite Truth Values 61 "" k%"" n% $ ’$ ’ n+1 (m=l+r# m&# x& 2. Calculate b = , where 1 k+n+1 m=l "" k+n% $ ’ # m+ x& ""n(L+U)+0.5# x = , l=""(n+k)L+0.5#, and 2 r=(n+ !k) (U 1""L 1) . 3.", Probabilistic Logic Networks,chapter 4
"Form the function v(n)=d[<[L1, U1], b1>, <[L, U], b>], where d[a,b] is the standard Euclidean distance from a to b. ! ! 4. Find the value of n that minimizes the function v(n). ! Aside from small deviations arising from integer approximations, n depends inversely on the width U-L of the interval [L, U]. To find the n-value in step 4 we initially perform a search by setting n=2j for a sequence of j values, until we obtain a b value that indicates we have surpassed the correct n value. We then perform a binary search between this maximum value N and N/2. We thus guarantee that the actual algorithm is of order O(log n). 4.5.1.", Probabilistic Logic Networks,chapter 4
"2 Approximate Inverse Function Approach As an alternate and faster approach to finding n, we develop a function of L, U, k, and b that provides a reasonable approximation for n. We begin by plotting the cumulative probabilities given by equation (3) for various values of n, by following the first two steps of the heuristic approach above. Aside from small deviations caused by the discrete nature of the cumulative distribution functions, each graph can be approximately modeled by a function of the form # b& ln % ( $ A’ prob=1""b=1""Ae""Bn . Inverting, we obtain n ="" . B For simplicity, we model the dependence of the coefficients A andB upon the values of L, U, and k, linearly. From the data we gathered this appears to be a reasonable assumption, though we have not yet derived an analysis of the error of ! these approximations.", Probabilistic Logic Networks,chapter 4
"We will use th!e notation A= AU+ A , B=BU+B , 1 2 1 2 A = A L+ A , B =B L+B , A = A k+ A!, and! B =B k+B , i i1 i2 i i1 i2 ij ij1 ij2 ij ij1 ij2 where 1""i,j,k ""2. Putting everything together we end up with A=(A L+ A )U+(A L+ A ) 11 12 21 !22 ! ! =[(!A k+ A )L+!A k+ A ]U+[(A !k + A )L+ A k+ A ] 111 112 121 122 211 212 221 222 ! and !     62 Probabilistic Logic Networks B=(B L+B )U+(B L+B ) 11 12 21 22 =[(B k+B )L", Probabilistic Logic Networks,chapter 4
+B k+B ]U+[(B k+B )L+B k+B ] 111 112 121 122 211 212 221 222 Finding the values of the coefficients A and B yields the following values: ij A =-0.00875486 A =-2.35064019 A =0.002463011 111 112 121 ! A =-0.220372781 A =0.010727656 A =2.803020516 122 211 212 A =-0.003647227 A =0.437068392 221 222 B =0.003032946 B =-0.399778839 B =-0.004302594 ! 111 ! 112 ! 121 B =0.930153781 B =0.002803518 B =-0.593689012 ! 122 ! 211 ! 212 B =-0.000265616 B =-0., Probabilistic Logic Networks,chapter 4
"071902027 ! 221 ! 222 ! ! ! Observing that the dependence upon k is relatively negligible compared to the de! pendence upon L!, U , and b, we can alternat!ive ly eliminate the k-dependence and ! use instead f!ix ed values for A =-2.922447948, A =-0.072074201, 11 12 A =3.422902859, A =0.252879708, B =-0.261903438, B =0.716893418, 21 22 11 12 B =-0.39831749, and B =-0.107482534. 21 22 4.5.2 Further Development of Indefinite Probabilities In this chapter we have presented the basic idea of indefinite truth values. The purpose of the indefinite truth value idea, of course, lies in its utilization in inference, which is left for later chapters.", Probabilistic Logic Networks,chapter 4
"But our hope is that in this chapter we have conveyed the essential semantics of indefinite probabilities, which is utilized, along the mathematics, in the integration of indefinite probabilities with inference rules. Some of the inferential applications of indefinite probabilities we encounter in later chapters will be fairly straightforward, such as their propagation through deductive and Bayesian inference. Others will be subtler, such as their application in the context of intensional or quantifier inference. In all cases, however, we have found the indefinite probability notion useful as a summary measure of truth value in a pragmatic inference context. In some cases, of course, a summary approximation won’t do and one actually needs to retain one or more full probability distributions rather than just a few numbers giving a rough indication. But in reality one can’t always use a full representation of this nature due to restrictions on data, memory, and processing power; and thus we have placed indefinite probabilities in a central role within PLN.", Probabilistic Logic Networks,chapter 4
"As compared with simpler summary truth values such as single probability numbers or (probability, weight of evidence) pairs, they seem to provide a better compromise between compactness and accuracy.  ", Probabilistic Logic Networks,chapter 4
"  Chapter 5: First-Order Extensional Inference — Rules and Strength Formulas Abstract In this chapter we launch into the “meat” of PLN: the specific PLN inference rules, and the corresponding truth-value formulas used to determine the strength of the conclusion of an inference rule from the strengths of the premises. Inference rules and corresponding truth-value strength formulas comprise a large topic; in this chapter we deal with a particular sub-case of the problem: first-order extensional inference. 5.1 Introduction Recall that first-order inference refers to inference on relationships between terms (rather than on predicates, or relationships between relationships), and that extensional inference refers to inference that treats terms as denoting sets with members (as opposed to intensional inference, which treats terms as denoting entities with properties).", Probabilistic Logic Networks,chapter 5
"These first-order extensional rules and truth-value formulas turn out to be the core PLN rules truth-value formulas, in the sense that most of the rules and formulas for handling higher-order and/or intensional inference and/or weight of evidence are derived as re-interpretations of the first-order extensional rules and associated strength formulas. Higher-order inference, intensional inference, and weight of evidence formulas are handled separately in later chapters. 5.2 Independence-Assumption-Based PLN Deduction In this section we present one version of the PLN strength formula for firstorder extensional deduction (abbreviated to “deduction” in the remainder of the chapter). First we give some conceptual discussion related to this inference formula; then we provide the algebraic formula. The formula itself is quite simple; however it is important to fully understand the concepts underlying it, because it embodies simplifying assumptions that introduce errors in some cases.", Probabilistic Logic Networks,chapter 5
"We will also be reviewing an alternate strength formula for first-order extensional deduction, in a later section of this chapter, which mitigates these errors in certain circumstances.     64 Probabilistic Logic Networks Conceptually, the situation handled by the first-order extensional deduction formula is depicted in the following Venn diagram: First-order extensional deduction, as well as the related inference forms we call induction and abduction in PLN, may be cast in the form: Given information about the size of some regions in the Venn diagram, make guesses about the size of other regions. 5.2.1 Conceptual Underpinnings of PLN Deduction In this subsection, as a preliminary to presenting the PLN inference formulas in detail, we will discuss the concepts underlying PLN in a more abstract way, using simple inference examples to demonstrate what it is we really mean by “probabilistic deduction.", Probabilistic Logic Networks,chapter 5
"This conceptual view is not needed for the actual calculations involved in PLN, but it’s essential to understanding the semantics underlying these calculations, and is also used in the proof of the PLN deduction formula. Let’s consider some simple examples regarding Subset relationships. Supposing, in the above diagram, we know Subset A B <.5> Subset B C <.5> What conclusions can we draw from these two relationships? What we want is to derive a relation of the form Subset A C <tv> from the two given premises. When we do this however, we are necessarily doing probabilistic, estimative inference, not direct truth-value evaluation. To see why, suppose B is a set with two elements; e.g.", Probabilistic Logic Networks,chapter 5
"B = {x ,x } 1 2 so that Member x B <1> 1 Member x B <1> 2     Chapter 5: First Order Extensional Inference 65 Let’s consider two of the many possible cases that might underlie the above Subset relationships: Case 1 (A!B and B!C are identical) A = C B x 3 x 1 x 2 A = C = {x , x } 1 3 B = {x ,x } 1 2 In this case, direct truth-value evaluation yields Subset A C <1> Case 2 (A!B and B!C are disjoint) A C x x 3 4 x x 2 1 A = {x , x } 1 3 B B = {x , x } 1 2 C = {x , x } 2 4 In this case, direct truth-value evaluation yields Subset A", Probabilistic Logic Networks,chapter 5
"C <0>     66 Probabilistic Logic Networks The problem is that, just given the two premises Subset A B <.5> Subset B C <.5> we don’t know which of the two above cases holds – or if in fact it’s a completely different situation underlying the Subset relationships. Different possible situations, underlying the same pair of Subset relationships, may result in completely different truth-values <tv> for the conclusion Subset A C <tv> So no exact computation of the truth-value of the conclusion is possible. Instead, all that’s possible is an estimate of the truth-value of the latter, obtained by averaging over possible situations consistent with the two given premises. The PLN deduction strength formula to be given below is merely an efficient way of carrying out this averaging, in an approximate way. We will derive theorems giving compact formulas for the expected (average) truth-values to be obtained as the answer to the above deduction problem.", Probabilistic Logic Networks,chapter 5
"We approach this averaging process in two ways. First, we derive inference formulas for an “independence-based PLN,” in which we average over all possible sets satisfying given premises. Later, we introduce a more general and more realistic “concept-geometry” approach. In the concept-geometry approach, we restrict attention to only those sets having particular shapes. The idea here is that concepts tend to be better approximated by these shapes than by random sets. One important point regarding PLN inference is that the evidence sets used to compute the truth-values of two premises need not be overlapping. We can freely combine two relationships that were derived based on different sets of observations. This is important because many real-world cases fit this description. For instance, when a person reasons Inheritance gulls birds Inheritance birds fly |Inheritance gulls fly it may well be the case that the two premises were formed based on different sets of evidence.", Probabilistic Logic Networks,chapter 5
"The gulls that were observed to be birds may well be different from the birds that were observed to fly. A simple example of non-overlapping evidence, related to our above Subset example with the sets A, B, and C, is: Case 3 (non-overlapping evidence sets) A = {x , x x x } 1 4, 7, 8 B = {x , x , x , x x , x } 1 2 3 4, 5 6 C = {x , x , x } 2 3 4 In this case, direct truth-value evaluation yields     Chapter 5: First Order Extensional Inference 67 Subset A C <.25> Now, suppose that truth-value estimates for the two relations Subset A B <.5> Subset B C <.5> were obtained as follows: Subset A B <.", Probabilistic Logic Networks,chapter 5
"5> was derived by observing only {x , x }, whereas 1 7 Subset B C <.5> was derived by observing only {x , x } 2 6 In Case 3, the two premise Subsets were derived based on completely different sets of evidence. But the inference rules don’t care; they can make their estimates anyway. We will present this “not caring” in a formal way a little later when we discuss the weight of evidence rules for first-order inference. Now let’s step through the basic logic by which PLN deals with these examples of inference involving Subsets, with all their concomitant subcases. Basically, as noted above, what PLN does is to estimate the outcome of an average over all possible cases. The PLN formulas carry out this estimation in a generally plausible way, based on the information available.", Probabilistic Logic Networks,chapter 5
"More formally, what does this mean? First we’ll present a slight simplification, then un-simplify it in two stages. In addition to these simplifications, we will also assume a strength-only representation of truth-values, deferring consideration of more complex truth-value types until later. In the simplified version what PLN says in this example is, basically: Let’s assume a universe U that is a finite size Usize. Let V denote the set of all triples of sets {A, B, C} in this universe, such that Subset A B <.5> Subset B C <.5> holds. For each triple in V, we can compute the value s so that Subset A C <s> The average of all these strength-estimates is the estimated truth-value strength of the Subset.", Probabilistic Logic Networks,chapter 5
"Note that this average includes cases where A!B and B!C have no overlap at all, cases where A!B and B!C are identical, cases where A and C are identical – all possible cases given the assumed finite universe U and the constraints posed by the premise Subsets. If the two premise Subsets are drawn from different evidence sets, this doesn’t matter – the degree of evidence-set overlap is     68 Probabilistic Logic Networks just one among many unknown properties of the evidence sets underlying the premise truth-values. Now, how is this picture a simplification? The first way is that we haven’t introduced all the potentially available information. We may have knowledge about the truth-values of A, B, and C, as well as about the truth-values of the Subset relationships. In fact, this is the usual case. Suppose that s , s , and s are the strengths A B C of the truth-values of the Atoms A, B, and C.", Probabilistic Logic Networks,chapter 5
"In that case, we can redefine the set V specified above; we can define it as the set of all triples of sets {A, B, C} so that |A| = s Usize A |B| = s Usize B |C| = s Usize C Subset A B <.5> Subset B C <.5> hold. Here Usize is the “universe size,” the size of the total space of entities under consideration. We can then compute the truth-value of Subset A C <tv> as the average of the estimates obtained for all triples in V. Next, the other way in which the above picture is a simplification is that it assumes the strength values of the premises (and the strength values of the Atoms A, B, and C) are exactly known.", Probabilistic Logic Networks,chapter 5
"In fact, these will usually be estimated values; and if we’re using indefinite truth-values, distributional truth-values, or confidencebearing truth-values, knowledge about the “estimation error” may be available. In these cases, we are not simply forming a set V as above. Instead, we are looking at a probability distribution V over all triples {A, B, C} where A, B, and C are subsets of U, and the quantities |A| (determined from s = A.TruthValue.strength) A |B| (determined from s = B.TruthValue.strength) B |C| (determined from s = C.TruthValue.strength) C (Subset A B).TruthValue.strength (Subset B C).TruthValue.strength are drawn from the probability distributions specified by the given truth-values.", Probabilistic Logic Networks,chapter 5
"We will deal with this uncertainty below by doing a sensitivity analysis of the PLN inference formulas, estimating for each formula the error that may ensue from uncertain inputs, and discounting the weight of evidence associated with the conclusion based on this estimated error. Finally, one technical point that comes up in PLN is that the quantitative results of truth-value estimates depend on the finite universe size Usize that is assumed. This parameter is also called the “context size.” Basically, the smaller Usize is, the more speculative the inferences are. In the example given above, the minimum usable Usize is Usize = |U| = |A| + |B| + |C|. This parameter setting is good if one wants to do speculative inductive or abductive inference. If one wants to minimize error, at cost of also minimizing creativity, one should set Usize as large as possi-     Chapter 5: First Order Extensional Inference 69 ble.", Probabilistic Logic Networks,chapter 5
"Using the PLN formulas, there is no additional computational cost in assuming a large Usize; the choice of a Usize value can be based purely on the desired inferential adventurousness. The semantics of the universal set will be discussed further in a later subsection. 5.2.2 The Independence-Assumption-Based Deduction Rule Now we proceed to give a heuristic derivation of one of the two truth-value strength formulas commonly associated with the PLN deduction rule.", Probabilistic Logic Networks,chapter 5
"In this inference rule, we want to compute the “strength” s of the relationship AC Subset A C <s > AC which we interpret as P(A""C) s =P(CA)= AC P(A) given the data: ! s =P(BA) AB s =P(CB) BC s =P(A) A s =P(B) B s =P(C) C Essentially, that means we want to guess the size of the Venn diagram region A!C given information about the sizes of the other regions A, B, C, A!B, ! B!C. As illustrated in the above example, in the following discussion we will sometimes use the notation: • s ’s are strengths of Subset relationships (e.g., Subset i j <s >) ij ij • s’s are strengths of terms (i.e.", Probabilistic Logic Networks,chapter 5
"i <s >) i i This notation is handy for the presentation of algebraic relationships involving Atom strengths, a situation that often arises with PLN. We will also use N and N ij i to denote relationship and term “counts” respectively, and d and d to denote relaij j tionship and term “weights of evidence.”     70 Probabilistic Logic Networks Whenever the set of values {s , s , s , s , s } is consistent (i.e., when it A B C AB BC corresponds to some possible sets A, B, and C) then the PLN deduction strength formula becomes (1!"" s )(s ""s s ) s =s s + AB C B BC (2.1) AC AB BC 1""s B Here we will give a relatively simple heuristic proof of this formula, and then (more tediously) a fully rigorous demonstration.", Probabilistic Logic Networks,chapter 5
"! As we shall see a little later, formulas for inductive and abductive reasoning involve similar problems, where one is given some conditional and absolute probabilities and needs to derive other, indirectly related ones. For induction, one starts with s , s , s , s , s and wishes to derive s . For abduction, one starts with s , BA BC A B C AC AB s , s , s , s and wants to derive s . The inference formulas involving similarity CB A B C AC relations may be similarly formulated. 5.2.2.1 Heuristic Derivation of the Independence-Assumption-Based Deduction Rule The heuristic derivation that we’ll give here relies on a heuristic independence assumption. The rigorous derivation given afterward replaces the appeal to an independence assumption with an averaging over all possible worlds consistent with the constraints given in the premises. But of course, even the rigorous proof embodies some a priori assumptions.", Probabilistic Logic Networks,chapter 5
"It assumes that the only constraints are the ones implicitly posed by the truth-values of the premise terms and relationships, and that every possible world consistent with these truth-values is equally likely. If there is knowledge of probabilistic dependency, this constitutes a bias on the space of possible worlds, which renders the assumption of unbiased independence invalid. Knowledge of dependency can be taken into account by modifying the inference formula, in a way that will be discussed below. The danger is where there is a dependency that is unknown; in this case the results of inference will not be accurate, an unavoidable problem. Assume we’re given P(A), P(B), P(C), P(B|A), P(C|B), defined relative to some universal set U. We want to derive P(C|A); i.e., the formula for P(C|A) = s , AC which was cited above. We begin the derivation by observing that P(C""A) P(CA)= .", Probabilistic Logic Networks,chapter 5
"P(A) Because P(A) is assumed given, we may focus on finding P(C!A) in terms of the other given quantities. !     Chapter 5: First Order Extensional Inference 71 We know P(C""A)=P(B)P((C""A)B)+P(U#B)P((C""A)U#B) . This follows because in general ! P(B)P(X B)+P(U""B)P(XU""B) P(B)P(X#B) P(U""B)P(X#(U""B)) = + P(B) P(U""B) =P(X#B)+P(X#(U""B)) =P(X#B)+[P(X)""P(X#B)]=P(X).", Probabilistic Logic Networks,chapter 5
"So we can say, for instance, ! P(C""A""B) P((C""A)B)= P(B) P((C""B)""(A""B)) = P(B) (C""B)""(A""B) = . B Now, we can’t go further than this without making an independence assumption. But if we assume C and A are independent (in both B and U-B), we can simplify ! these terms. To introduce the independence assumption heuristically, we will introduce a “bag full of balls” problem. Consider a bag with N balls in it, b black ones and w white ones. We are going to pick n balls out of it, one after another, and we want to know the chance p(k) that k of them are black. The solution to this problem is known to be the hypergeometric distribution. Specifically, ""b#""N !b# ""N# p(k)= $ %$ % $ %.", Probabilistic Logic Networks,chapter 5
"&k n!k n ’& ’ & ’ The mean of this distribution is:     72 Probabilistic Logic Networks bn mean= . N How does this bag full of balls relate to our situation? We may say: • Let our bag be the set B, so N=|B|. ! • Let the black balls in the bag correspond to the elements of the set A!B, so that b=|A!B| • The white balls then correspond to elements of B-A. • The n balls we are picking are the elements of the set C!B, so n=|C!B|.", Probabilistic Logic Networks,chapter 5
"This probabilistic “bag full of balls” model embodies the assumption that A and C are totally independent and uncorrelated, so that once B and A are fixed, the chance of a particular subset of size |C!B| lying in A!B is the same as the chance of that element of C lying in a randomly chosen set of size |A!B|. This yields the formula: bn A""B C""B = N B which is an estimate of the size ! (C""B)""(A""B). So if we assume that A and C are independent inside B, we can say ! (C""B)""(A""B) P((C""A)B)= B A""B C""B = =P(AB)P(CB).", Probabilistic Logic Networks,chapter 5
"2 B Similarly, for the second term, by simply replacing B with U-B and then doing some algebra, we find !     Chapter 5: First Order Extensional Inference 73 P((C""A)(U#B)) =P(A(U#B))P(C(U#B)) A""(U#B)C""(U#B) = 2 U#B [P(A)#P(A""B)][P(C)#P(A""B)] = . [1#P(B)]2 So altogether, we find ! P(C""A)=P(B)P((C""A)B)+P(U#B)P((C""A)(U#B)) [1#P(B)][P(A)#P(A""B)][P(C)#P(C""B)] =P(B)P(AB)P(CB)+ [1#P(B)]2 and hence ! P(C""A) P(CA)= P(A) P(AB)P(CB)P", Probabilistic Logic Networks,chapter 5
"(B) [1#P(A""B)][P(C)#P(C""B)] = + P(A) 1#P(B) [1#P(BA)][P(C)#P(C""B)] =P(BA)P(CB)+ 1#P(B) [1#P(BA)][P(C)#P(B)P(CB)] =P(BA)P(CB)+ . 1#P(B) Note that in the above we have used Bayes’ rule to convert ! P(BA)P(A) P(AB)= . P(B) We now have the PLN deduction formula expressed in terms of conditional and absolute probabilities. In terms of our above-introduced notation for term and rela! tionship strengths, we may translate this into:     74 Probabilistic Logic Networks (1""s )(s ""s s ) s =s s + AB C C BC AC AB BC 1""s B which is the formula mentioned above", Probabilistic Logic Networks,chapter 5
". ! 5.2.2.2 PLN Deduction and Second-Order Probability We now give a formal proof of the independence-assumption-based PLN deduction formula. While the proof involves a lot of technical details that aren’t conceptually critical, there is one aspect of the proof that sheds some light on the more philosophical aspects of PLN theory: this is its use of second-order probabilities. We first define what it means for a set of probabilities to be consistent with each other. Note that, given specific values for s = P(A) and s = P(B), not all A B values in the interval [0,1] for s = P(B|A) necessarily make sense. For example, AB if P(A)=0.6=P(B), then the minimum value for P(A!B) = 0.2 so that the minimum value for P(B|A) is 0.2/0.6 = 1/3.", Probabilistic Logic Networks,chapter 5
"Definition: We say that the ordered triple (s ,s ,s ) of probability values A B AB ! s =P(A), s =P(B), and s =P(BA) is consistent if the probabilities A B AB satisfy the following condition: & s +s (1# & s # ! m!ax $$0, A B !!! ’ s ’ min $$1, sB !!. s AB % A "" % A "" Definition: The ordered triple of subsets (A, B, C) for which the ordered triples (s ,s ,s ) and(s ,s ,s ) are both consistent, we shall call fully conA B AB B C BC sistent subset-triples. We will prove: Theorem! 1 (PLN Deduction Formula) Let U denote a set with |U| elements. Let Sub(m) denote the set of subsets of U containing m elements.", Probabilistic Logic Networks,chapter 5
"Let (A, B, C) be a fully consistent subset-triple. Further suppose that each of the values s , s , s ,s , and s divides evenly into |U|. Next, A B C AB BC define f(x)=P[P(CA)=xA""Sub(Us ),B""Sub(Us ),C""Sub(Us ),P(BA)=s ,P(CB)=s ] A B C AB BC Then, where E() denotes the expected value (mean), we have !     Chapter 5: First Order Extensional Inference 75 (1""s )(s ""s s ) E[f(x)]=s =s s + AB C B BC . AC AB BC 1""s B This theorem looks at the space of all finite universal sets U (all “sample spaces”), and with each U it looks at all possible ways of selecting subsets A, B, ! and C out of U.", Probabilistic Logic Networks,chapter 5
"It assumes that this size of U is given, and that certain absolute and conditional probabilities regarding A, B, and C are given. Namely, it assumes that P(A), P(B), P(C), P(B|A) and P(C|B) are given, but not P(C|A). For each U, it then looks at the average over all A, B, and C satisfying the given probabilities, and asks: If we average across all the pertinent (A, B, C) triples, what will P(C|A) come out to, on average? Clearly, P(C|A) may come out differently for different sets A, B, and C satisfying the assumed probabilities. But some values are more likely than others, and we’re looking for the mean of the distribution of P(C|A) values over the space of acceptable (A, B, C) triples.", Probabilistic Logic Networks,chapter 5
"This is a bit different from the usual elementary-probability theorems in that it’s a second-order probability: we’re not looking at the probability of an event, but rather the mean of a probability over a certain set of sample spaces (sample spaces satisfying the initially given probabilistic relationships). In spite of the abstractness induced by the use of second-order probabilities, the proof is not particularly difficult. Essentially, after one sets up the average over pertinent sample spaces and does some algebra, one arrives at the same sort of hypergeometric distribution problem that was used in the heuristic derivation in the main text. The difference, however, is that in this proof there is no ad-hoc independence assumption; rather, the independence comes out of the averaging process automatically because on average, approximate probabilistic independence between terms is the rule, not the exception.", Probabilistic Logic Networks,chapter 5
"Proof of Theorem 1 (PLN Deduction Formula): The way the theorem is stated, we start with a set U of |U| elements, and we look at the set of all subset-triples {A, B, C} fulfilling the given constraints. That is, we are looking at subset-triples (A,B,C) for which the Predicate constr defined by constr(A,B,C)= A"" Sub(Us ) AND B"" Sub(Us ) AND C"" Sub(Us ) A B C AND P(BA)=s AND P(CB)=s AB BC evaluates to True. ! Over this set of subset-triples, we’re computing the average of P(C|A). That is, we’re computing     76 Probabilistic Logic Networks 1 E[f(x)]= "" P(CA) M (A,B,C): constr(A,B,C) where M denotes the number of triples (A,B,C) so that constr(A,B,C).", Probabilistic Logic Networks,chapter 5
"Following the lines of our heuristic derivation of the formula, we may split this ! into two sums as follows: 1 P(C""A) E[f(x)]= # M (A,B,C): constr(A,B,C) P(A) 1 % P((C""A)B)P(B) P((C""A)(U$B))P(U$B)( = ’# +# *. M’ (A,B,C): constr(A,B,C) P(A) (A,B,C): constr(A,B,C) P(A) * & ) After going this far, the heuristic derivation then used probabilistic independence ! to split up P((C!A)|B) and P(C!A)|(U-B) into two simpler terms apiece. Following that, the rest of the heuristic derivation was a series of straightforward algebraic substitutions. Our task here will be to more rigorously justify the use of the independence assumption.", Probabilistic Logic Networks,chapter 5
"Here we will not make an independence assumption; rather, the independence will be implicit in the algebra of the summations that are “summations over all possible sets consistent with the given constraints.” We will use formal methods analogous to the heuristic independence assumption, to reduce these sums into simple formulas consistent with the heuristic derivation. We will discuss only the first sum here; the other one follows similarly by substituting U-B for B.", Probabilistic Logic Networks,chapter 5
"For the first sum we need to justify the following series of steps: P((C""A)B)P(B) 1 # M (A,B,C): constr(A,B,C) P(A) 1 P(C""A""B) = # M (A,B,C): constr(A,B,C) P(A) P((C""B)""(A""B)) 1 = # M (A,B,C): constr(A,B,C) P(A) 1 P(C""B)P(A""B) = # M (A,B,C): constr(A,B,C) P(B)P(A) P(CB)P(AB)P(B) 1 = # M (A,B,C): constr(A,B,C) P(A) The final step is the elegant one. It follows because, over the space of all triples (A,B,C) so that constr(A,B,C) holds, the quantities P(C|B) and P(B|A) are constant !     Chapter", Probabilistic Logic Networks,chapter 5
"5: First Order Extensional Inference 77 by assumption. So they may be taken out of the summation, which has exactly M terms. The difficult step to justify is the third one, where we transform P(C""B)P(A""B) P((C""B)""(A""B)) into . This is where the algebra of the P(B) summations is used to give the effect of an independence assumption. To justify this third transformation, it suffices to show that ! ! P(C""B)P(A""B) P((C""B)""(A""B))# P(B) $ =0. (A,B,C): constr(A,B,C) P(A) We will do this by rewriting the sum as ! $ $ P(C""B)P(A""B)’’ & & P((C""B)""(A""B))# )) P(B) .", Probabilistic Logic Networks,chapter 5
"* &* & )) =0 B+Sub(U sB)& (A,C):constr(A,B,C)& P(A) )) & & )) % % (( Note that the term P(A) is constant for all (A,B,C) satisfying constr(A,B,C), so it may be taken out of the summation and effectively removed from consideration, ! yielding $ $ P(C""B)P(A""B)’’ * &* &P ((C""B)""(A""B))# )) =0. B+Sub(U sB) %& (A,C):constr(A,B,C)% P(B) (() We will show that this is true by showing that the inner summation itself is always zero; i.e., that for a fixed B, ! $ P(C""B)P(A""B)’ * &P ((C""B)""(A""B))# ) =0 (2.", Probabilistic Logic Networks,chapter 5
"2) (A,C):constr(A,B,C)% P(B) ( In order to demonstrate Equation 2.2, we will now recast the indices of summation in a different-looking but equivalent form, changing the constraint to one that ! makes more sense in the context of a fixed B. Given a fixed B, let’s say that a pair of sets (A1, C1) is B-relevant iff it satisfies the relationships A1=A!B C1=C!B     78 Probabilistic Logic Networks for some triple (A,B,C) satisfying constr(A,B,C).", Probabilistic Logic Networks,chapter 5
"We now observe that the pair (A1,C1) is B-relevant if it satisfies the constraint predicate constr1(A1,C1;B)= A1"" B AND C1"" B AND A1= As AND C1= Bs AB BC The constraint for |C1| comes from the term in the former constraint constr stating ! P(CB)=s BC For, we may reason ! P(C""B) P(C1) P(CB)= = =s P(B) P(B) BC P(C1)=P(B)s BC ! C1= Bs BC ! Similarly, to get the constraint for |A1|, we observe that ! P(A""B) P(A1) P(BA)= = =s P(A) P(A) AB so that ! A1= As .", Probabilistic Logic Networks,chapter 5
"AB Given a fixed B, and a specific B-relevant pair (A1,C1), let EQ(A1, C1;B) denote the set of pairs (A,C) for which constr(A,B,C) and ! A1= A""B C1=C""B Now we will recast Equation 2.2 above in terms of A1 and C1. Equation 2.2 is equivalent to ! $ P(A1)P(C1)’ + + &P (A1""C1)# ) =0. (A1,C1):constr1(A1,C1;B) (A,C)*EQ(A1,C1;B)% P(B) ( !     Chapter 5: First Order Extensional Inference 79 Because the inner sum is the same for each pair (A,C)! EQ(A1,C1;B), because K "" EQ(A1,C1) is the same for each B-relevant pair (A1,C", Probabilistic Logic Networks,chapter 5
"1), we can rewrite this as $ $ P(A1)P(C1)’’ ! * &K &P (A1""C1)# )) =0 (A1,C1):constr1(A1,C1;B) %& % P(B) (() or just ! $ P(A1)P(C1)’ * &P (A1""C1)# ) =0. (2.3) (A1,C1):constr1(A1,C1;B)% P(B) ( We now have a somewhat simpler mathematics problem. We have a finite set B, with two subsets A1 and C1 of known sizes. Other than their sizes, nothing ! about A1 and C1 is known. We need to sum a certain quantity P(A1)P(C1) Q""P(A1#C1)$ P(B) over all possibilities for A1 and C1 with the given fixed sizes.", Probabilistic Logic Networks,chapter 5
"We want to show this sum comes out to zero. This is equivalent to showing that the average of Q is ! zero, over all A1 and C1 with the given fixed sizes. Now, the second term of Q is constant with respect to averaging over pairs (A1,C1), because P(A1)P(C1) A1C1 = P(B) BU which is independent of what the sets A1 and C1 are, assuming they have fixed A1C1 sizes. So the average of the second term is simply . We will rewrite Equa! BU tion 2.3 as 1 % 1 ( A1C1 L"" $ $ P(A1#C1) = (2.", Probabilistic Logic Networks,chapter 5
"4) ’ ! * M1 A1+B:A1=As AB& M2 C1:constr1(A1,C1;B) ) BU where !     80 Probabilistic Logic Networks • M1 is the number of A1’s that serve as part of a B-relevant pair (A1,C1) with any C1; i.e., the number of terms in the outer sum; • M2 is the number of C1’s that serve as part of a B-relevant pair (A1,C1) for some specific A1; i.e., the number of terms in the inner sum. Note that M2 is independent of the particular set A1 under consideration; and that M = M1 M2. To show (2.4), it suffices to show that for a fixed A1, 1 A1C1 # P(A1""C1)= (2.", Probabilistic Logic Networks,chapter 5
"5) M2 C1:constr1(A1,C1;B) BU To see why this suffices observe that, by the definition in (2.4), if (2.5) held, we’d have ! # A1C1& 1 L"" * % ( . M1 A1)B:A1=As AB$ BU ’ But the expression inside the sum is constant for all A1 being summed over (because they all have A1= As ), and the number of terms in the sum is M1, AB ! so that on the assumption of (2.4) we obtain the result A1C1 L"" . ! BU which is what (2.3) states. So, our task now is to show (2.5). Toward this end we will use an equivalent ! form of (2.4); namely 1 A1C1 # A1""C1= (2.", Probabilistic Logic Networks,chapter 5
"6) M2 C1:constr1(A1,C1;B) B (the equivalence follows from P(A1!C1) = |A1!C1|/|U|). To show (2.6) we can use some standard probability theory, similar to the independence-assumption! based step in the heuristic derivation. We will model the left-hand side of (2.6) as a “bag full of balls” problem. Consider a bag with I balls in it, I black ones and w white ones. We are going to pick n balls out of it, one after another, and we want to know the chance p(k) that k of them are black. The solution to this problem is known to be the hypergeometric distribution, as given above, with mean bn/N. How does this bag full of balls relate to (2.", Probabilistic Logic Networks,chapter 5
"6)? Simply:     Chapter 5: First Order Extensional Inference 81 • Let our bag be the set B, so N=|B|. • Let the black balls in the bag correspond to the elements of the set A1, so that b=|A1| • The white balls then correspond to B minus the elements in A1. • The n balls we are picking are the elements of the set C1, so n=|C1|. This yields the formula: bn A1C1 = . N B What the mean of the hypergeometric distribution gives us is the average of A1""C1 over all I1 with the given size constraint, for a fixed A1 with the given ! size constraint. A1C1 But what Equation 5 states is precisely that this mean is equal to .", Probabilistic Logic Networks,chapter 5
"So, B ! going back to the start of the proof, we have successfully shown that P((C""A)B) # =P(CB)P(BA) .! (A,B,C):constr(A,B,C) P(B) It follows similarly that P((C""A)(U !B)) ! # = P( C(U !B)) P((U !B)A) . (A,B,C):constr(A,B,C) P(B) The algebraic transformations made in the heuristic derivation then show that 1 E[f(x)]= "" P(CA) M {A,B,C}: constr(A,B,C) P(C(U#B))P((U#B)A) =P(CB)P(BA)+ P(U#B) (1""P(BA))(P(C)""P(B)P(CB)) =P(BA)P(CB)+ 1""P(B) ! which, after a change from P to s", Probabilistic Logic Networks,chapter 5
"notation, is precisely the formula given in the theorem. Thus the proof is complete. QED !     82 Probabilistic Logic Networks Next, to shed some light on the behavior of this formula, we now supply graphical plots for several different input values. These plots were produced in the Maple software package. Each plot will be preceded by the Maple code used to generate it. To make the Maple code clearer we will set Maple inputs in bold text; e.g., diff(dedAC(rB,rC,sAB,sBC),rC); and Maple outputs through displayed text such as Recall that in order to apply the deduction rule, the triple(A,B,C) of subsets of a given universal set must be fully consistent.", Probabilistic Logic Networks,chapter 5
"In Maple, this consistency condition takes the form consistency:= (sA, sB, sC, sAB, sBC) -> (Heaviside(sAB-max(((sA+sB! 1)/sA),0))-Heaviside(sAB-min(1,(sB/sA))))*(Heaviside(sBC-max(((sB+sC1)/sB),0))-Heaviside(sBC-min(1,(sC/sB)))); where Heaviside(x) is the Heaviside unit step function defined by #0, if x <0 Heaviside(x)="" !1, if x $0 The deduction strength formula then becomes dedAC := (sA, sB, sC, sAB, sBC) -> sAB * sBC + (1- sAB)*(sC-sB*sBC)/(1sB)* consistency(sA,sB,sC,sAB,sBC); The result of this treatment of the", Probabilistic Logic Networks,chapter 5
"consistency condition is that when the consistency condition indicates inconsistency, the result of the inference comes out as zero. Graphically, this means that the graphs look flat (0 on the z-axis) in certain regions – these are regions where the premises are inconsistent. We now supply a sampling of graphs for the deduction strength formula for several representative values for (s ,s ,s ,s ,s ). Note here that the disconA B C AB BC tinuities come from enforcing the consistency conditions. !     Chapter 5: First Order Extensional Inference 83 plot3d(dedAC(0.1,0.1,0.4,sAB,sBC),sAB=0..1,sBC=0..1,numpoints=400, resolution = 400, axes=BOXED, labels=[sAB,sBC,dedAC]); plot3d(dedAC(0.1,0.1,0.", Probabilistic Logic Networks,chapter 5
"8,sAB,sBC),sAB=0..1,sBC=0..1,numpoints=400, resolution = 400, axes=BOXED, labels=[sAB,sBC,dedAC]);     84 Probabilistic Logic Networks plot3d(dedAC(0.1,0.1,.95,sAB,sBC),sAB=0..1,sBC=0..1,numpoints=800, resolution = 400, axes=BOXED, labels=[sAB,sBC,dedAC]); plot3d(dedAC(0.1,0.2,0.1,sAB,sBC),sAB=0..1,sBC=0..1,numpoints=800, resolution = 400, axes=BOXED, labels=[sAB,sBC,dedAC]);     Chapter 5: First Order Extensional Inference 85 plot3d(dedAC(0.1,0.", Probabilistic Logic Networks,chapter 5
"6,0.5,sAB,sBC),sAB=0..1,sBC=0..1,numpoints=800,axe s=BOXED, labels=[sAB,sBC,dedAC]); plot3d(dedAC(0.1,0.9,0.4,sAB,sBC),sAB=0..1,sBC=0..1,numpoints=800,lab els=[sAB,sBC,sAC],axes=BOXED);     86 Probabilistic Logic Networks 5.2.2.3 Deduction Accounting for Known Dependencies Next, we present a minor variant of the above deduction strength formula. The heuristic inference formula derivation given earlier relies on an independence assumption, and the more rigorous derivation given just above assumes something similar, namely that all possible worlds consistent with the input strength values are equally likely. This is arguably the best possible assumption to make, in the case where no relevant evidence is available.", Probabilistic Logic Networks,chapter 5
"But what about the case where additional relevant evidence is available? A couple of workarounds are possible here. The concept geometry approach proposed in a later section presents a solution to one aspect of this problem – the fact that in some cases the independence assumption is systematically biased in a certain direction, relative to reality. But that doesn’t address the (quite common) case where, for a particular relationship of interest, some partial dependency information is available. For instance, what if one wishes to reason Subset A B Subset B C |Subset A C and one knows that A!B and C!B are not independent. Perhaps there exist conjunctions corresponding to these two compounds (these will be introduced more rigorously later on, but the basic idea should be clear), and perhaps there is an Equivalence or ExtensionalEquivalence relationship between these two Predicates, such as ExtensionalEquivalence <.6> AND A B AND C B indicating a significant known dependency between the two conjuncts.", Probabilistic Logic Networks,chapter 5
"In this case the deduction formula, as presented above, is not going to give accurate results. But one can easily create alternate PLN formulas that will do the trick. In the heuristic deduction formula derivation given above, the key step was where an independence assumption was used to split up the right-hand side of P((C""B)""(A""B)) P((C""A)B)= . P(B) In the case where ( A AND B AND C) <.6> is known, however, one doesn’t need to use the independence assumption at all. Instead one can simply substitute the ! value .6 for the expression P((C""B)""(A""B)) , obtaining the deduction formula !     Chapter 5: First Order Extensional Inference 87 (1""P(BA))(P(C)""P(B)P(CB)) .6 + .", Probabilistic Logic Networks,chapter 5
"P(B)P(A) 1""P(B) If we also had the knowledge (A AND C AND (NOT B) ) <.3> ! then we could simplify things yet further and use the formula .6 .3 + . P(B)P(A) (1""P(B))P(A) The principle illustrated in these examples may be used more generally. If one is given explicit dependency information, then one can incorporate it in PLN via ! appropriately simplified inference formulae. As will be clear when we discuss alternate inference rules below, the same idea works for induction and abduction, as well as deduction. 5.3 The Optimal Universe Size Formula Now we return to the question of the universe-size parameter U. It turns out that if one has information about a significant number of “triple intersection probabilities” in a context, one can calculate an “optimal universe size” for PLN deduction in that context.", Probabilistic Logic Networks,chapter 5
"The deduction rule estimates P(CA)=P(BA)P(CB)+P(¬BA)P(C¬B) where n = |A|, n = |A !C|, etc., this means A AC ! n n n n n AC = AB BC + A,¬B B,¬C n n n n n A A B A ¬B The second term can be expanded ! n n (n ""n )(n ""n ) A,¬B ¬B,C = A AB C BC n n n (n ""n ) A ¬B A U B where n is the size of the universe. U !     88 Probabilistic Logic Networks On the other hand, with triple intersections, one can calculate n =n +n .", Probabilistic Logic Networks,chapter 5
"AC ABC A,¬B,C It’s interesting to compare this with the formula ! n n (n ""n )(n ""n ) n = AB BC + A AB C BC AC n n ""n B U B derived from multiplying the deduction rule by n . Clearly the k’th term of each A formula matches up to the k’th term of the other formula, conceptually. ! Setting the second terms of the two equations equal to each other yields a formula for n , the universe size.", Probabilistic Logic Networks,chapter 5
"We obtain U (n ""n )(n ""n ) n = A AB C BC A,¬B,C n ""n U B (n ""n )(n ""n ) (n ""n )= A AB C BC AC ABC n ""n U B (n ""n )(n ""n ) n =n + A AB C BC U B (n ""n ) AC ABC For example, suppose n =n =n =100, n = n = n = 20, n = 10 A B C AC AB BC ABC Then, ! 80""80 n =100+ =740 U 10 is the correct universe size. The interesting thing about this formula is that none of the terms on the right ! side demand knowledge of the universe size – they’re just counts, not probabilities.", Probabilistic Logic Networks,chapter 5
"But the formula tells you the universe size that will cause the independence assumption in the second term of the deduction formula to come out exactly correctly! Now, what use is this in practice? Suppose we know triple counts n for ABC many triples (A,B,C), in a given context. Then we can take the correct universe sizes corresponding to these known triples and average them together, obtaining a good estimate of the correct universe size for the context as a whole. We have tested this formula in several contexts. For instance, we analyzed a large corpus of newspaper articles and defined P(W), where W is a word, as the percentage of sentences in the corpus that the word W occurs in at least once.", Probabilistic Logic Networks,chapter 5
"The     Chapter 5: First Order Extensional Inference 89 Subset relationship strength from W to V is then defined as the probability that, if V occurs in a sentence, so does W; and the triple probability P(W & V & X) is defined as the percentage of sentences in which all three words occur. On this sort of data the universe size formula works quite well in practice. For instance, when the correct universe size – the number of sentences in the corpus – was around 650,000, the value predicted by the formula, based on just a few hundred triples, was off by less than 20,000. Naturally the accuracy can be increased by estimating more and more triples. If one uses a concept geometry approach to modify the deduction rule, as will be discussed later, then the universe size calculation needs to be modified slightly, with a result that the optimal universe size increases. 5.", Probabilistic Logic Networks,chapter 5
"4 Inversion, Induction, and Abduction “Induction” and “abduction” are complex concepts that have been given various meanings by various different thinkers. In PLN we assign them meanings very close to their meanings in the NARS inference framework. Induction and abduction thus defined certainly do not encompass all aspects of induction and abduction as discussed in the philosophy and logic literature. However, from the fact that a certain aspect of commonsense induction or abduction is not encompassed completely by PLN induction or abduction, it should not be assumed that PLN itself cannot encompass this aspect. The matter is subtler than that. For instance, consider abductive inference as commonly discussed in the context of scientific discovery: the conception of a theory encompassing a set of observations, or a set of more specialized theories. In a PLN-based reasoning system, we suggest, the process of scientific abduction would have to incorporate PLN abduction along with a lot of other PLN inference processes.", Probabilistic Logic Networks,chapter 5
"Scientific abduction would be achieved by means of a non-inferential (or at least, not entirely inferential) process of hypothesis generation, combined with an inferential approach to hypothesis validation. The hypothesis validation aspect would not use the abduction rule alone: executing any complex real-world inference using PLN always involves the combination of a large number of different rules. Given the above deduction formula, the definition of induction and abduction as defined in PLN is a simple matter. Each definition involves a combination of the deduction rule with a single use of the “inversion formula,” which is simply Bayes’ rule. Inversion consists of the inference problem: Given P(A), P(B), and P(A|B), find P(B|A). The solution to this is:     90 Probabilistic Logic Networks P(AB)P(A) P(BA)= P(B) s s s = BA A AB s B which is a simple case of Bayes rule.", Probabilistic Logic Networks,chapter 5
"Induction consists of the inference problem: Given P(A), P(B), P(C), P(A|B) ! and P(C|B), find P(C|A) . Applying inversion within the deduction formula, we obtain for this case: P(AB)P(CB)P(B) # P(AB)P(B)&# P(C)""P(B)P(CB)& P(CA)= +%1 "" (% ( P(A) $% P(A) ’($% 1""P(B) ’( or ! s s s # s s &# s "" s s & s = BA BC B +%1 "" BA B (% C B BC ( AC s $ s ’$ 1"" s ’ A A B In Maple notation, the induction formula indAC := (sA, sB, sC, sBA, sBC) -> sBA * sBC * sB / sA + (1 - sBA * sB / sA", Probabilistic Logic Networks,chapter 5
") *(sC - sB * sBC)/(1-sB)*(Heaviside(sBA-max(((sA+sB-1)/sB),0))! Heaviside(sBA-min(1,(sA/sB))))*(Heaviside(sBC-max(((sB+sC-1)/sB),0))Heaviside(sBC-min(1,(sC/sB)))); is depicted for selected input values in the following figures:     Chapter 5: First Order Extensional Inference 91 plot3d(indAC(0.1,0.1,0.1,sBA,sBC),sBA=0..1,sBC=0..1,numpoints=800, resolution = 400, labels=[sBA,sBC,indAC],axes=BOXED); plot3d(indAC(0.4,0.1,0.1,sBA,sBC),sBA=0..", Probabilistic Logic Networks,chapter 5
"1,sBC=0..1,numpoints=800, resolution = 400, labels=[sBA,sBC,indAC],axes=BOXED);     92 Probabilistic Logic Networks Next, abduction consists of the inference problem: Given P(A), P(B), P(C), P(B|A) and P(B|C), find P(C|A). In this case we need to turn P(B|C) into P(C|B), using inversion. We then obtain: P(BA)P(BC)P(C) P(C)[1""P(BA)][1""P(BC)] P(CA)= + P(B) 1""P(B) or ! s s s s (1"" s )(1"" s ) s = AB CB C + C AB CB AC s 1"" s B B In Maple notation, the formula for abduction is given by abdAC:=(sA,sB,sC,sAB,sCB)->(s", Probabilistic Logic Networks,chapter 5
"AB*sCB*sC/sB+(1-sAB)*(1-sCB)*sC/(1sB))*(Heaviside(sAB-max(((sA+sB-1)/sA),0))-Heaviside(sAB! min(1,(sB/sA))))*(Heaviside(sCB-max(((sB+sC-1)/sC),0))-Heaviside(sCBmin(1,(sB/sC)))); We display here plots for two sets of representative inputs: plot3d(abdAC(.1, .1, .1, sAB, sCB), sAB = 0 .. 1, sCB = 0 .. 1, numpoints = 800, resolution = 400, labels = [sAB, sCB, sAC], axes = BOXED);     Chapter 5: First Order Extensional Inference 93 plo t3d(abdAC(0.1,0.", Probabilistic Logic Networks,chapter 5
"9,0.4,sAB,sCB),sAB=0..1,sCB=0..1,numpoints=800,labels =[sBA,sBC,sCA],axes=BOXED); 5.5 Similarity Next we show how inference rules involving Similarity and associated symmetric logical relationships can be derived from the PLN rules for asymmetric logical relationships. The main rule here is one called “transitive similarity,” which is stated as follows: Transitive Similarity Let sim , sim , and sim represent the strengths of the Similarity relationAB BC AC ships between A and B, between B and C, and between A and C, respectively.", Probabilistic Logic Networks,chapter 5
"Given sim , sim , s , s , and s , the transitive similarity formula for calculating AB BC A B C sim then takes the form: AC     94 Probabilistic Logic Networks 1 sim = AC 1 1 + ""1 deduction(T,T ) deduction(T ,T ) 1 2 3 4 where ! "" s % $1 + B’ sim # s & AB T = A 1 1+sim AB "" s % $1 + C’ sim # s & BC T = B 2 1+sim BC "" s % $1 + B’ sim # s & BC T = C 3 1+sim BC "" s % $1 + A’ sim # s & AB T = B 4 1+sim AB and deduction(T ,T ) and deduction(T ,T ) are calculated using the independence1 2 3 4 based deduction formula.", Probabilistic Logic Networks,chapter 5
"That is, for example, ! deduction(T,T )=TT +(1""T)(s ""s T )/(1""s ) 1 2 1 2 1 C B 2 B The proof of the transitive similarity formula, to be given just below, contains the information needed to formulate several other rules for manipulating Similar! ity relationships, as well: 2inh2sim Given s and s , estimate simAC: AC CA 1 sim = AC 1 1 + ""1 s s AC CA !     Chapter 5: First Order Extensional Inference 95 inh2sim Given s , s and s , estimate sim : AC A C AC 1 sim = AC "" s % $1 + A’ # s & C (1 s AC sim2inh ! Given sim , s and s , estimate s AB A B AB "" s % $1 + B’ sim # s & AB s = A AB 1+sim AB", Probabilistic Logic Networks,chapter 5
"Proof of the Transitive Similarity Inference Formula ! Assume one is given P(A), P(B), P(C), and A""B P(A""B) sim = = AB A#B P(A#B) C""B P(C""B) sim = = BC C#B P(C#B) and one wishes to derive ! A""C P(A""C) sim = = AC A#C P(A#C) We may write !     96 Probabilistic Logic Networks P(A""C) P(A""C) sim = = AC P(A#C) P(A)+P(C)$P(A""C) 1 P(A)+P(C)$P(A""C) = sim P(A""C) AC P(A) P(C) P(A""C) 1 1 + $ = + $1 P(A""C) P(A""C) P(A""C) P(CA) P(AC) So it suffices to estimate P(C|A) and P(A|C)", Probabilistic Logic Networks,chapter 5
"from the given information. (The two cases are symmetrical, so solving one gives on the solution to the other via a sim! ple substitution.) Let’s work on P(C|A) = P(C!A)/P(A). To derive this, it suffices to estimate P(B|A) and P(C|B) from the given information, and then apply the deduction rule. To estimate P(B|A), look at A""B A#B P(BA)= =sim A AB A We thus need to estimate |A""B|/|A|, which can be done by ! A""B A + B # A$B B = =1+ #P(BA) .", Probabilistic Logic Networks,chapter 5
"A A A We have the equation ! # B & P(BA)=sim %1 + ""P(BA)( AB A $ ’ # B& P(BA)(1+sim )=sim %1 + ( AB AB A $ ’ # B& sim %1 + ( AB A $ ’ s =P(BA)= . AB (1+sim ) AB A similar calculation gives us !     Chapter 5: First Order Extensional Inference 97 "" C% sim $1 + ’ BC B # & s =P(CB)= BC (1+sim ) BC with the deduction rule giving us a final answer via ! s = s s + (1-s ) ( s - s s ) / (1- s ).", Probabilistic Logic Networks,chapter 5
"AC AB BC AB C B BC B Similarly, we may derive P(A|C) from ! s = s s + (1-s ) ( s - s s ) / (1- s ) CA CB BA CB A B BA B where ! "" B% $1 + ’ sim C BC # & s =P(BC)= CB 1+sim BC "" A% $1 + ’ sim B AB ! =P(AB)=# & s . BA 1+sim AB We thus obtain ! 1 sim = . AC 1 1 + ""1 s s AC CA 5.5.1 Graphical Depiction of Similarity Inference Formulas ! Next we give a visual depiction of the transitive similarity inference, and related, formulas.", Probabilistic Logic Networks,chapter 5
"The required formulas are: 2inh2sim := (sAC,sCA) -> 1/( 1/sAC + 1/sCA - 1);     98 Probabilistic Logic Networks sim2inh := (sA,sB,simAB) -> (1 + sB/sA) * simAB / (1 + simAB) *(Heaviside(simAB-max(((sA+sB-1)),0))-Heaviside(simABmin(sA/sB,(sB/sA)))); inh2sim := (sAC,sCA) -> 1/( 1/sAC + 1/sCA - 1); simAC := (sA, sB, sC, simAB, simBC) -> inh2sim( sim2inh(sB,sC,simBC), sim2inh(sA,sB,simAB)); where, as before, multiplication by the term Heaviside(simBC-max(((sB+s", Probabilistic Logic Networks,chapter 5
"C-1)),0))-Heaviside(simBCmin(sB/sC,(sC/sB))) represents a consistency condition on the three inputs s , s , and sim . B C BC The transitive similarity rule then looks like: plot3d(simAC(.2, .2, .2, simAB, simBC), simAB = 0 .. 1, simBC = 0 .. 1, axes = BOXED, labels =[simAB, simBC, simAC],numpoints=400, resolution=400);! !     Chapter 5: First Order Extensional Inference 99 plot3d(simAC(.6, .2, .2, simAB, simBC), simAB = 0 .. 1, simBC = 0 .. 1, axes = BOXED, labels =[simAB, simBC, simAC],numpoints=400, resolution=400); ! plot3d(simAC(.", Probabilistic Logic Networks,chapter 5
"8, .1, .1, simAB, simBC), simAB = 0 .. 1, simBC = 0 .. 1, axes = BOXED, labels =[simAB, simBC, simAC],numpoints=400, resolution=400);!     100 Probabilistic Logic Networks Next, a graphical depiction of the sim2inh rule is as follows: plot3d(sim2inh(.2,sC,simBC),sC=0..1, simBC=0..1,axes=BOXED, numpoints=800, resolution =800); plot3d(sim2inh(.5,sC,simBC),sC=0.. 1,simBC=0..", Probabilistic Logic Networks,chapter 5
"1, axes=BOXED, numpoints=800, resolution=800);     Chapter 5: First Order Extensional Inference 101 A visualization of the inverse, inh2sim rule is: plot3d(inh2sim(sAC,sCA), sAC=0.01 ..1, sCA=0 ..1, axes=boxed); ! 5.6 PLN Deduction via Concept Geometry Now we return to the foundational issues raised in the derivation of the deduction formula above. Recall that the derivation of the deduction formula involved an independence assumption, which was motivated conceptually in terms of the process of “averaging over all possible worlds.", Probabilistic Logic Networks,chapter 5
"The question that naturally arises is: how realistic is this process? The somewhat subtle answer is that the averagingover-possible-worlds process seems the best approach (though certainly not perfectly accurate) in the case of deduction of s with no prior knowledge of s , but AC AC can be modified to good effect in the case where there is incomplete prior knowledge regarding the truth-value s . In the case where there is incomplete prior AC knowledge of s , the deduction formula may usefully be revised to take this AC knowledge into account. The key observation here regards systematic violations of the independence assumption that occur due to the fact that most sets we reason on are “cohesive” – they consist of a collection of entities that are all fairly closely related to one another.     102 Probabilistic Logic Networks average, over all sets {A, C}, one will find P(A!C) = P(A) P(C).", Probabilistic Logic Networks,chapter 5
"However, if one has prior knowledge that • P(A!C) > 0 • A and C are “cohesive” sets then one knows that, almost surely, P(A!C) >P(A) P(C). A cohesive set S, within a universe U, is defined as a set whose elements have an average similarity significantly greater than the average similarity of randomly chosen pairs of elements from U. Furthermore, if A and C are cohesive, then the expected size of P(A!C) - P(A) P(C) increases as one’s estimate of P(A!C) increases, although not linearly. The reason for this is not hard to see. Suppose we have a single entity x which is an element of A, and it’s been found to be an element of C as well. (The existence of this single element is known, because P(A! C)>0.", Probabilistic Logic Networks,chapter 5
"Then, suppose we’re given another entity y, which we’re told is an element of A. Treating y independently of x, we would conclude that the probability of y being an element of C is P(C|A). But in reality y is not independent of x, because A and C are not sets consisting of elements randomly distributed throughout the universe. Rather, A is assumed cohesive, so the elements of A are likely to cluster together in the abstract space of possible entities; and C is assumed cohesive as well. Therefore, P(y ""Cy "" A AND x "" A AND x ""C)>P(y ""C) In the context of the PLN deduction rule proof, our A and C are replaced by A1= A!B and C1 = C! B, but the basic argument remains the same. Note that if A and B are cohesive, then A! B and A!¬B are also cohesive.", Probabilistic Logic Networks,chapter 5
"! It’s interesting to investigate exactly what the assumption of cohesion does to the deduction formula. Let us return to the heuristic derivation, in which we observed P(C""A) P(CA)= P(A) P(C""A)=P((C""B)""(A""B))+P((C""¬B)""(A""¬B)) If A!B, C!B, A!¬B and C !¬B are all cohesive, then we conclude that, on average, if ! P((C""B)""(A""B))>0 then ! P((C""B)""(A""B))>P(C""B)P(A""B) !     Chapter 5: First Order Extensional Inference 103 and, on average, if P((C""¬B)""(A""¬B))>0 then ! P((C""¬B)""(A""¬B))>P(C""¬B)P(A""¬B) If there is prior knowledge that", Probabilistic Logic Networks,chapter 5
"P(A!B!C)>0 and/or P(A!¬B!C)>0, then this can be used to predict that one or both of the terms of the deduction formula will ! be an underestimate. On the other hand, what if there’s prior knowledge about s ? If s >0 and A, AC AC B, and C are cohesive, this means that at least one of P(A!B!C)>0 or P(A!¬B!C)>0 holds. And in most cases, unless B has a very special relationship with A and C or else s is very small, both P(A!B!C)>0 and P(A!¬B!C)>0 AC will hold. So given prior knowledge that s >0, we can generally assume that the AC deduction formula will be an underestimate.", Probabilistic Logic Networks,chapter 5
"In terms of the formal statement of the deduction formula, this implies that if we have prior knowledge that s >0, then most of the time we’ll have AC (1""s )(s ""s s ) E[f(x)]>s s + AB C B BC AB BC 1""s B That is, it implies that the deduction formula, when applied to cohesive sets given prior knowledge that the value to be deduced is nonzero, will systematically un! derestimate strengths. Of course, this argument is just heuristic; to make it rigorous one would have to show that, given A, B, and C cohesive, the intersections used in this proof tend to display the probabilistic dependencies assumed. But these heuristic arguments seem very plausible, and they lead us to the following observation: Observation: The Independence-Assumption-Based Deduction Formula Is Violated by the Assumption of Probabilistic Cohesion Let U denote a set with |U| elements.", Probabilistic Logic Networks,chapter 5
"Let Sub(m) denote the set of subsets of U containing m elements. Let s , s , s , s and s be numbers in [0,1], all of which A B C AB BC divide evenly into |U|. Let S be a set of cohesive subsets of U. Let $ A""S#Sub(Us ),B""S#Sub(Us ),’ & A B ) f(x)=P&P (CA)=xC""S#Sub(Us ),P(BA)=s , ) C AB & ) & P(CB)=s ,P(CA)>0 ) % BC ( !     104 Probabilistic Logic Networks Then, where E() denotes the expected value (mean), (1""s )(s ""s s ) E[f(x)]>s s + AB C B BC . AB BC 1""s B This observation is borne out by our practical experience applying PLN, both to simulated mathematical datasets and to real-world data.", Probabilistic Logic Networks,chapter 5
"! So independence-assumption-based PLN deduction is flawed – what do we do about it? There is no simple and complete solution to the problem, but we have devised a heuristic approach that seems to be effective. We call this approach “concept geometry,” and it’s based on replacing the independence assumption in the above deduction rule with a different assumption regarding the “shapes” of the Nodes used in deduction. For starters we have worked with the heuristic assumption of “spherical” Nodes, which has led to some interesting and valuable corrections to the independence-assumption-based deduction formula. But the concept geometry approach is more general and can work with basically any assumptions about the “shapes” of Nodes, including assumptions according to which these “shapes” vary based on context. 5.6.", Probabilistic Logic Networks,chapter 5
"1 Probabilistic Deduction Based on Concept Geometry The concept-geometry approach begins with a familiar set-up: P(A""C) P(CA)= P(A) P(A""C)=P(A""C""B)+P(A""C""¬B) From this point on we proceed a little differently than in ordinary PLN deduction. If it is already believed that P(A!C!B) > 0, then one uses ! P(A""C""B)= f(A""B,C""B) where f is a function to be defined below.", Probabilistic Logic Networks,chapter 5
"Otherwise, one uses ! P(A""C""B)=P(A""B)P(CB) If it is already believed that P(A!C!¬B) >0, then one uses ! P(A""C""¬B)= f(A""¬B,C""¬B) !     Chapter 5: First Order Extensional Inference 105 Otherwise, one uses P(A""C""¬B)=P(A""¬B)P(C¬B) One then computes P(C|A) from P(A|B), P(C|B), P(A|¬B), P(C|¬B) These may be computed from the premise links using ! P(BA)P(A) P(AB)= P(B) P(A¬B)P(¬B)+P(AB)P(B)=P(A) so ! [P(A)""P(AB)P(B)] P(A)[1""P(BA)] P(A¬B)= = P(¬B)", Probabilistic Logic Networks,chapter 5
"1""P(B) [P(C)""P(CB)P(B)] P(C¬B)= 1""P(B) The function f(x,y) is estimated using a collection of model sets, according to the formula ! f(x,y) = expected (probability) measure of the intersection of a model set with measure x and a model set with measure y, assuming that the two model sets have nonzero intersection. The following theorem allows us to calculate f(x, y) in some cases of interest. Specifically, suppose the model sets are “k-spheres” on the surface of a (k+1)sphere. A special case of this is where the model sets are intervals on the boundary of the unit circle. Let V(d) denote the volume of the k-sphere with diameter d, assuming a measure in which the volume of the whole surface of the (k+1)-sphere is 1.", Probabilistic Logic Networks,chapter 5
"Given a measure x, let diam(x) denote the diameter of the k-sphere with measure x. Theorem 3: For model sets defined as k-spheres on the surface of a (k+1)sphere, we have xy f(x,y)= V(diam(x)+diam(y)) !     106 Probabilistic Logic Networks Proof: Let r = diam(x), q = diam(y); equivalently x = V(r), y = V(q). The idea is that the expected volume f(x,y) can be expressed as P( X ""z #r, Y ""z #q X ""Y #r+q) Here X, Y are the centers of the spheres, z is a point belonging to the intersection of the spheres, and ||V|| is the norm of vector V. We assume X, Y, z are chosen uniformly.", Probabilistic Logic Networks,chapter 5
"The first two inequalities express the condition that z belongs to the in! tersection of spheres, the third inequality expresses the condition that these two spheres have a non-empty intersection. Using the definition of conditional probability, we have P( X""z #r, Y""z #q, X""Y #q+r) P( X""z #r, Y""z #q) V(r)V(q) . = = P( X""Y #q+r) P( X""Y #q+r) V(q+r) QED ! As a special case, for intervals on a circle with circumference 1 we get xy f(x,y)= min(1,x+ y) Note here a reasonably close parallel to the NARS deduction formula ! xy x+ y""xy For circles on the sphere, we obtain ! xy f(x,y)= ( ) min1,x+ y""2xy+2 xy(1""x)(1""y) For the sake of concre", Probabilistic Logic Networks,chapter 5
"teness, in the rest of this section we’ll discuss mostly the interval formula, but in practice it is important to tune the dimensionality parame! ter to the specific domain or data source at hand, in order to minimize inference error. Potentially, if one has a number of different formulas from spheres in different spaces, and has some triple-intersection information from a dataset, then one can survey the various lookup tables and find the one most closely matching the dataset. To perform deduction based on these ideas, we need a way to estimate the odds that P(A!B!C)>0. Most simply we could estimate this assuming knowledge     Chapter 5: First Order Extensional Inference 107 that P(A!C) > 0, P(B!C) > 0, P(A!B) > 0 (and assuming we know that A, B, C are cohesive sets).", Probabilistic Logic Networks,chapter 5
"More sophisticatedly, we could use knowledge of the values of these binary intersection probabilities (and we will do so in the following, in a heuristic way). Let’s use G(A,B,C) to denote our estimate of the odds that P(A!B!C) > 0. In that case, if we model sets as intervals on the perimeter of the unit circle as suggested above, we have $ 1 ’ P(A""B""C)=[1#G(A,B,C)]P(A""B)P(CB)+G(A,B,C)P(A""B)P(CB)max&1 , ) %& P(AB)+P(CB) () * $ $ 1 ’’ =P(A""B)P(CB),1 +G(A,B,C)& #1+max&1 , )) / +, %& %& P(AB)+P(CB) () () .", Probabilistic Logic Networks,chapter 5
"* $ 1 ’=P(A""B)P(CB),1 +G(A,B,C)max& 0,#1+ )/ +, %& P(AB)+P(CB) () ./ * $ 1 ’=P(BA)P(CB)P(A),1 +G(A,B,C)max& 0,#1+ )/ +, %& P(AB)+P(CB) () ./ So for the overall deduction rule we have ! ) # &, 1 P(CA)=P(BA)P(CB)+1 +G(A,B,C)max% 0,""1+ (. $% P(AB)+P(CB) ’( *+ -. ) # &, 1 +P(¬BA)P(C¬B)+1 +G(A,¬B,C)max% 0,""1+ (. $% P(A¬B)+P(C¬B) ’( *+ -. Now, to", Probabilistic Logic Networks,chapter 5
"complete the picture, all we need is a formula for G(A,B,C). One heuristic ! is to set G(A,B,C) = 1 if s , s and s are all positive, and 0 otherwise, but obviAB BC AC ously this is somewhat crude. One way to derive a heuristic for G(A,B,C) is to assume that A, B, and C are 1-D intervals on the perimeter of the unit circle. In this case, as we’ll show momentarily, we arrive at the formula G(A,B,C)=P (A,B)+( 1!P (A,B)) P (A,B,C) sym_subsume sym_subsume int_no_subsume where the terms are defined as P(A)""P(B) P (A,B)= sym_subsume P(A)+P(B) P (A,B,C) int_no_subsume =1""min[1+P(B)""P(A#B)""P(C),P(A)+P", Probabilistic Logic Networks,chapter 5
"(B)""P(A#B)]+max[P(B),1""P(C)] !     108 Probabilistic Logic Networks a fairly crude estimate, in that it uses knowledge of P(A!B) and ignores knowledge of the other two binary intersections. A better estimate could be derived making use of the other two binary intersections as well; this estimate was chosen purely for simplicity of calculation. Also, one could make similar estimates using only P(B!C) and only P(A!C), and average these estimates together. 5.6.2 Calculation of a Heuristic Formula for G(A,B,C) Here we will derive the above-mentioned heuristic formula for G(A,B,C), defined as the odds that P(A!B!C) > 0. We do this by assuming that the sets A, B, and C are intervals on the boundary of the unit circle. Assume a measure that assigns the entire perimeter of the unit circle measure 1.", Probabilistic Logic Networks,chapter 5
"Consider B as an interval extending from the top of the circle to the right. Next, consider A. If A totally subsumes B then obviously from P(B!C) >0 we know P(A!B!C)>0. The probability that A totally subsumes B is 0 if P(A) < P(B). Otherwise, if we assume only the P(A)""P(B) knowledge that P(A!B)>0, the odds of A totally subsuming B is . P(A)+P(B) If we assume knowledge of the value of P(A!B), then a different and more complex formula would ensue.", Probabilistic Logic Networks,chapter 5
"Overall we can estimate the probability that A totally subsumes B is ! # P(A)""P(B)& P (A,B)=min%0 , ( subsume P(A)+P(B) $ ’ Symmetrically, the probability that B totally subsumes A is P (B,A) and subsume we may define ! P(A)""P(B) P (A,B)=P (A,B)+P (B,!A )= sym_subsume subsume subsume P(A)+P(B) Suppose A does not totally subsume B, or vice versa? Without loss of generality we may assume that A overlaps only the right endpoint of B. Then let’s com! pute the probability that C does not intersect A!B. For this to happen, the first (counting clockwise) endpoint of C [call this endpoint x] must occur in the portion of A that comes after B, a region with length P(A)-P(A!B).", Probabilistic Logic Networks,chapter 5
"In order for C to intersect B but not A!B, we must have 1< x+P(C)<1+P(B)""P(A#B) !     Chapter 5: First Order Extensional Inference 109 i.e., 1""P(C)< x <1+P(B)""P(A#B)""P(C) But by construction we have the constraint ! P(B)< x <P(A)+P(B)""P(A#B) So x can be anywhere in the interval ! [max(P(B),1""P(C)),min(1+P(B)""P(A#B)""P(C),P(A)+P(B)""P(A#B))] So, the probability that C intersects A!B, given that neither A nor B totally subsumes each other, is ! P (A,B,C) int_no_subsume =1""min[1+P(B)""P(A#B)""P(C),P(A)+P(B)""", Probabilistic Logic Networks,chapter 5
"P(A#B)]+max[P(B),1""P(C)] Our overall estimate is then ! G(A,B,C)=P (A,B)+(1""P (A,B))P (A,B,C) sym_subsume sym_subsume int_no_subsume 5.7 Modus Ponens and Related Argument Forms ! Next, we describe some simple heuristics via which PLN can handle certain inference forms that are common in classical logic, yet not particularly natural from a probabilistic inference perspective. We begin with the classic “modus ponens” (MP) inference formula, which looks like: If P, then Q. P. Therefore, Q. This is closely related to two other classic inference formulas: “modus tollens” (MT; proof by contrapositive): If P, then Q. Q is false. Therefore, P is false. and “disjunctive syllogism” (DS): Either P or Q.", Probabilistic Logic Networks,chapter 5
"  110 Probabilistic Logic Networks Not P. Therefore, Q. Of course, MT is immediately transformable into MP via If ¬Q, then ¬P ¬Q Therefore, ¬P and DS is immediately transformable into MP via If ¬P then Q. ¬P. Therefore, Q. We will also discuss a symmetrized version of modus ponens, of the form If and only if P, then Q. P. Therefore, Q. All these forms of inference are trivial when uncertainty is ignored, but become somewhat irritating when uncertainty is included. Term logic, in which the basic inference step is deduction from two inheritance relationships, plays much more nicely with uncertainty. However, it is still possible to handle MP and associated inferences within a probabilistic term logic framework, at the cost of introducing a simple but highly error-prone heuristic. 5.7.", Probabilistic Logic Networks,chapter 5
"1 Modus Ponens In PLN terms, what MP looks like is the inference problem Inheritance A B <s > AB A <s > A |B <?> This is naturally approached in a PLN-deduction-ish way via P(B)=P(BA)P(A)+P(B¬A)P(¬A) But given the evidence provided, we can’t estimate all these terms, so we have no choice but to estimate P(B|¬A) in some very crude manner. One approach is to set ! P(B|¬A) equal to some “default term probability” parameter; call it c. Then we obtain the rule     Chapter 5: First Order Extensional Inference 111 s =s s +c(1""s ) B AB A A For instance suppose we know ! Inheritance smelly poisonous <.8> smelly <.7> Then in the proposed heuristic approach, we’re estimating P(poisonous) = .", Probabilistic Logic Networks,chapter 5
"8*.7 + c*.3 where say c = .02 is a default term probability Note that while this is a very bad estimate, it is clearly better than just setting P(poisonous) = .02 or P(poisonous) = .8*.7 which seem to be the only ready alternatives. It’s worth noting that the approach to MP proposed here is conceptually different from the one taken in NARS, wherein (where A and B are statements) Implication A B <s > AB A <s > A is rewritten as Implication A B <s > AB Implication K A <s > A where K is a “universal statement,” and then handled by ordinary NARS deduction. This kind of approach doesn’t seem to work for PLN. The NARS and PLN rules for MP have a similar character, in that they both consist of a correction to the product s s .", Probabilistic Logic Networks,chapter 5
"But in NARS it’s the same sort of correction as is used for ordiA AB nary deduction, whereas in PLN it’s a slightly different sort of correction. 5.7.2 Symmetric Modus Ponens Now we turn to a related inference case, the symmetric modus ponens; i.e., SimilarityRelationship A B <sim > AB A <s > A |B <?>     112 Probabilistic Logic Networks There is an additional subtlety here, as compared to standard MP, because to convert the similarity into an inheritance you have to assume some value for P(B).", Probabilistic Logic Networks,chapter 5
"If we take the above heuristic MP formula for s and combine it with the sim2inh B rule Given sim , s and s , estimate s : AB A B AB s = (1 + s /s ) * sim /( 1 + sim ) AB B A AB AB then we obtain "" s % sim s =s $1 + B’ AB +c(1(s ) B A # s & 1+sim A A AB which (after a small amount of algebra) yields the heuristic formula ! s =s sim +c(1""s )(1+sim ) B A AB A AB For example (to anticipate our later discussion of PLN-HOI), this is the rule we’ll need to use for the higher-order inference ! Equivalence <.8> Evaluation friendOf ($X,$Y) Evaluation friendOf ($Y,$X) Evaluation friendOf(Ben, Saddam) <.", Probabilistic Logic Networks,chapter 5
"6> |Evaluation friendOf(Saddam, Ben) <?> Given these numbers the answer comes out to .48 + .72*c. If we set c = .02 then we have .49 as an overall estimate. 5.8 Converting Between Inheritance and Member Relationship Next we discuss conversion rules between Subset and Member relationships. The heuristic member-to-inheritance inference rule lets you reason, e.g., Member Ben Americans <tv1> |Inheritance {Ben} Americans <tv2> where {Ben} is the term with Member Ben {Ben} and no other members. This conversion maps fuzzy truth-values into probabilistic truth-values. While     Chapter 5: First Order Extensional Inference 113 the precise form of the member-to-inheritance rule depends upon the form truth-value being used, all forms follow the same fundamental idea: they all keep the mean strength constant and decrease the confidence in the result.", Probabilistic Logic Networks,chapter 5
"For indefinite truth-values we accomplish this decrease in confidence by increasing the interval width by a constant multiplicative factor. The heuristic inheritance-to-member inference rule goes in the opposite direction and also keeps the strength constant and decreases confidence in a similar manner. The semantics of these rules is a bit tricky – in essence, they are approximately correct only for terms that represent cohesive concepts; i.e., for sets whose members share a lot of properties. They are badly wrong for Atoms representing random sets. They are also bad for sets like “Things owned by Izabela” which are heterogeneous collections, not sharing so many common properties. This issue is a subtle one and relates to the notion of concept geometry, to be discussed in a later section. 5.9 Term Probability Inference The inference formulas given above all deal with inference on relationships, but they involve term probabilities as well as relationship probabilities. What about inference on term probabilities? Some kinds of Term-based inference require higher-order reasoning rules.", Probabilistic Logic Networks,chapter 5
"For instance, if a PLN based reasoning system knows s = s = .5, and it knows that man woman Inheritance human ( man OR woman) <.9999> then it should be able to infer that s should not be .001. In fact it may well human come to the wrong conclusion, that s = .5, unless it is given additional inforhuman mation such as man AND woman <.00001> in which case after applying some logical transformation rules it can come to the correct conclusion that s = .9999 . This will become clearer when we discuss human higher-order inference in the following chapter. But it is valuable to explicitly encode term probability inference beyond that which comes indirectly via HOI. For instance, suppose the system knows that Inheritance cat animal and it sees evidence of a cat in its perceptual reality, thus increasing its probability estimate s .", Probabilistic Logic Networks,chapter 5
"Then, it should be able to adjust its probability estimate of s indicat animal rectly as well, by inference. The rule here is a very simple one – just a rearrangement of Bayes’ rule in a different order than is used for inversion. From     114 Probabilistic Logic Networks P(BA)P(A) P(B)= P(AB) one derives the formula ! s s s = A AB B s BA or in other words A <s > A ! Inheritance A B <s > AB |B <s > B This brings us to a point to be addressed a few sections down: how to deal with circular inference. We will introduce a mechanism called an “inference trail,” which consists in essence of a series of Atoms, so that the truth-value of each one is considered as partially determined by the truth-values of its predecessors in the series.", Probabilistic Logic Networks,chapter 5
"If one is using trails, then term probability inference must be included in inference trails just as relationship probability inference is, so that, after inferring s from s and s , one does not then turn around and infer s from animal cat cat,animal cat, animal s and s . animal cat In general, various mechanisms to be introduced in the following sections and discussed primarily in the context of relationship strengths may be assumed to apply in the context of term strengths as well. For example, the revision rule discussed in the following section works on term truth-values just as on relationship truth-values, and distributional truth-values as discussed in Chapter 7 carry over naturally as well. 5.10 Revision In this section we introduce another important PLN inference rule: the “revision” rule, used to combine different estimates of the truth-value of the same Atom.", Probabilistic Logic Networks,chapter 5
"In general, truth-value estimates to be merged via revision may come from three different sources: • External sources, which may provide different estimates of the truth-value of the same Atom (e.g., the New York Times says a certain politician is very trustworthy, but the Wall Street Journal says he is only moderately trustworthy) • Various cognitive processes, inside an integrative AI system, provide different estimates of the truth-value of the same Atom     Chapter 5: First Order Extensional Inference 115 • Different pathways of reasoning, taken internally by PLN, which may provide different estimates of the truth-value of the same Atom Generally speaking, it is the latter source which provides the greatest number of truth-value estimates, and which makes the revision formula an extremely critical part of PLN. Pei Wang, in (Wang, 1993), has given an excellent review of prior approaches to belief revision – including probabilistic approaches – and we mostly agree with his assessments of their shortcomings.", Probabilistic Logic Networks,chapter 5
"His own approach to revision involves weighted averaging using a variable called “confidence,” which is quite similar to PLN’s “weight of evidence” – and although we don’t think it’s ideal (otherwise the detailed developments of this section wouldn’t be necessary), we do find his approach basically sensible, unlike most of the approaches in the literature. The key point that Wang saw but most other belief revision theorists did not is that, in order to do revision sensibly, truth-values need at least two components, such as exist in NARS and in PLN but not in most uncertain inference frameworks. 5.10.1 Revision and Time Before proceeding with the mathematics, it’s worth mentioning one conceptual issue that arises in the context of revision: the nonstationary nature of knowledge. If different estimates of the truth-value of an Atom were obtained at different times, they may have different degrees of validity on this account.", Probabilistic Logic Networks,chapter 5
"Generally speaking, more weight needs to be given to the relationship pertaining to more recent evidence – but there will of course be many exceptions to this rule. In an integrative AI context, one approach to this problem is to push it out of the domain of logic and into the domain of general cognition. The treatment of revision in PLN reflects our feeling that this is likely the best approach. We assume, in the PLN revision rule, that all information fed into the reasoning system has already been adapted for current relevance by other mental processes. In this case all information can be used with equal value. In an integrative AI system that incorporates PLN, there should generally be separate, noninferential processes that deal with temporal discounting of information.", Probabilistic Logic Networks,chapter 5
"These processes should work, roughly speaking, by discounting the count of Atoms’ truth-values based on estimated obsoleteness, so that older estimates of the same Atom’s truth-value will, all else being equal, come to have lower counts. According to the revision formulas to be given below, estimates with lower counts will tend to be counted less in the revision process. However, because in any complex AI system, many cognitive processes are highly nondeterministic, it cannot be known for sure that temporal updating has been done successfully on two potential premises for revision. Hence, if the premises are sufficiently important and their truth-values are sufficiently different, it may be appropriate to specifically request a temporal updating of the premises,     116 Probabilistic Logic Networks and carry through with the revision only after this has taken place. This strategy can also be used with other inference rules as well: requesting an update of the premises before deduction, induction, or abduction takes place. 5.10.", Probabilistic Logic Networks,chapter 5
"2 A Heuristic Revision Rule for Simple Truth-values In this section we give a simple, heuristic revision rule for simple truth-values consisting of (strength, weight of evidence) pairs. In the following chapter we present a subtler approach involving indefinite probabilities; and in following subsections of this section we present some subtler approaches involving simple truthvalues. All in all, belief revision is a very deep area of study, and the PLN framework supports multiple approaches with differing strengths and weaknesses. Suppose s is the strength value for premise 1 and s is the strength value for 1 2 premise 2. Suppose further that n is the count for premise 1 and n is the count for 1 2 premise 2. Let w = n /(n +n ) and w = n /(n +n ) and then form the conclusion 1 1 1 2 2 2 1 2 strength value s = w s + w s .", Probabilistic Logic Networks,chapter 5
"1 1 2 2 In other words, a first approximation at a reasonable revision truth-value formula is simply a weighted average. We may also heuristically form a count rule such as n = n + n – c min(n , n ), where the parameter c indicates an assumption 1 2 1 2 about the level of interdependency between the bases of information corresponding to the two premises. The value c=1 denotes the assumption that the two sets of evidence are completely redundant; the value c=0 denotes the assumption that the two sets of evidence are totally distinct. Intermediate values denote intermediate assumptions. 5.10.3 A Revision Rule for Simple Truth-values Assuming Extensive Prior Knowledge In this and the next few subsections, we dig more deeply into the logic and arithmetic of revision with simple truth-values. This material is not currently utilized in our practical PLN implementation, nor has it been fully integrated with the indefinite probabilities approach.", Probabilistic Logic Networks,chapter 5
"However, we feel it is conceptually valuable, and represents an important direction for future research. To illustrate conceptually the nature of the revision process, we consider here the simple case of an extensional inheritance relation L = Subset A B Suppose we have two different estimates of this relationship’s strength, as above. We consider first the case where the weight of evidence is equal for the     Chapter 5: First Order Extensional Inference 117 two different estimates, and then take a look at the role of weight of evidence in revision. Suppose that the two different estimates were obtained by looking at different subsets of A and B. Specifically, suppose that the first estimate was made by looking at elements of the set C, whereas the second estimate was made by looking at elements of the set D.", Probabilistic Logic Networks,chapter 5
"Then we have A""B""C s = 1 A""C A""B""D s = 2 A""D ! By revising the two strengths, we aim to get something like ! A""B""(C#D) s* = 3 A""(C#D) which is the inheritance value computed in the combined domain C!D. Now, there is no way to exactly compute s * from s and s, but we can com3 1 2 ! pute some approximation s ! s *. Let’s write 3 3 x s = 1 1 y 1 x s = 2 2 y ! 2 If we assume that C and D are independent in both A and A""B, then after a bit of algebra we can obtain the heuristic revision rule: ! x x x + x "" 1 2 1 2 u s = 3 y y y + y "" 1 2 1 2 u The derivation goes as follows.", Probabilistic Logic Networks,chapter 5
"Firstly, we have !     118 Probabilistic Logic Networks A""B""(C#D) = A""B""C + A""B""D $ A""B""C""D A""(C#D) = A""C + A""D $ A""C""D ! We thus have ! x + x "" A#B#C#D s* = 1 2 3 y + y "" A#C#D 1 2 And we now have an opportunity to make an independence assumption. If we assume that C and D are independent within A, then we have ! A""C""D # A""C&# A""D& =% (% ( U U U $ ’$ ’ (where U is the universal set) and thus ! A""C A""D y y A""C""D = = 1 2 U u (where u = |U|).", Probabilistic Logic Networks,chapter 5
"Similarly if we assume that C and D are independent within A""B, we have ! A""B""C A""B""D x x A""B""C""D = = 1 2 U u ! which yields the revision rule stated above: ! x x x + x "" 1 2 1 2 u s = 3 y y y + y "" 1 2 1 2 u As with the deduction rule, if the system has knowledge about the intersections A""C""D or A""B""C""D, then it doesn’t need to use the independence ! assumptions; it can substitute its actual knowledge. Now, one apparent problem with this rule is that it doesn’t actually take the form ! !     Chapter 5: First Order Extensional Inference 119 s = f(s,s ) 3 1 2 It requires a little extra information.", Probabilistic Logic Networks,chapter 5
"For instance suppose we know the strength of ! Subset A C <s AC> Then we know y = A""C =s P(A)u 1 AC x =sy 1 1 1 so we can compute the needed quantities (x , y ) from the given strength s plus 1 1 1 the extra relationship strength s . AC ! 5.10.4 A Revision Rule for Simple Truth-values Assuming Extensive Prior Knowledge But – following up the example of the previous subsection – what do we do in the case where we don’t have any extra information, just the premise strengths s 1 and s ? Suppose we assume y =y =q, for simplicity, assuming no knowledge about 2 1 2 either. Then s =x /q, s = x /q , and we can derive a meaningful heuristic revision 1 1 2 2 formula.", Probabilistic Logic Networks,chapter 5
"In this case the x x x + x "" 1 2 1 2 u s = 3 y y y + y "" 1 2 1 2 u given above specializes to ! ss qq ss q sq+s q"" 1 2 s +s "" 1 2 1 2 u 1 2 u s = = 3 qq q q+q"" 2"" u u which can be more simply written (setting c=q/u) ! s +s ""ss c s = 1 2 1 2 3 2""c Here c is a critical parameter, and as the derivation shows, intuitively !     120 Probabilistic Logic Networks • c = 1 means that both premises were based on the whole universe (which is generally implausible, because if this were the case they should have yielded identical strengths) • c = .", Probabilistic Logic Networks,chapter 5
"5 means that the two premises were each based on half of the universe • c = 0 means that the premises were each based on negligibly small percentages of the universe Different c values may be useful in different domains, and it may be possible in some domains to tune the c value to give optimal results. Concretely, if c = 0, then we have s +s s = 1 2 3 2 i.e., the arithmetic average. We have derived the intuitively obvious “averaging” heuristic for revision, based on particular assumptions about the evidence sets un! der analysis. But suppose we decide c=.5, to take another extreme. Then we have 2 s = (s +s "".5ss ) 3 3 1 2 1 2 Suppose s = s = .1, then the average formula gives s =.", Probabilistic Logic Networks,chapter 5
"1, but this alternate for1 2 3 mula gives ! 2 s = (0.1+0.1""0.5#0.1#0.1)=0.13 3 3 which is higher than the average! So what we find is that the averaging revision formula effectively corresponds to the ass!um ption of an infinite universe size. With a smaller universe, one obtains revisions that are higher than the averaging formula would yield. Now, for this to be a significant effect in the course of a single revision one has to assume the universe is very small indeed. But iteratively, over the course of many revisions, it’s possible for the effect of a large but non-infinite universe to have a significant impact. In fact, as it turns out, some tuning of c is nearly always necessary – because, using the above heuristic formula, with c nonzero, one obtains a situation in which revision gradually increases relationship strengths.", Probabilistic Logic Networks,chapter 5
"Applied iteratively for long enough, this may result in relationship strengths slowly drifting up to 1. And this may occur even when not correct – because of the independence assumption em-     Chapter 5: First Order Extensional Inference 121 bodied in the (s s ) term of the rule. So if c is to be set greater than zero, it must 1 2 be dynamically adapted in such a way as to yield “plausible” results, a notion which is highly domain-dependent. 5.10.5 Incorporating Weight of Evidence into Revision of Simple Truth-values Next, one can modify these rules to take weight of evidence into account.", Probabilistic Logic Networks,chapter 5
"If we define 2d v = 1 1 d +d 1 2 v =2""v 2 1 then some algebra yields ! x x v x +v x "" 1 2 1 1 2 2 u s = 3 y y v y +v y "" 1 2 1 1 2 2 u ! which yields the corresponding heuristic formula v s +v s ""ss c s = 1 1 2 2 1 2 3 2""c The algebra underlying this involved going back to the original derivation of the revision rule, above. Where we said ! A""B""(C#D) = A""B""C + A""B""D $ A""B""C""D A""(C#D) = A""C + A""D $ A""C""D ! we now instead have to say ! A""B""(C#D) =v A""B""C +v A""B""D $ A""B""C", Probabilistic Logic Networks,chapter 5
"""D 1 2 !     122 Probabilistic Logic Networks A""(C#D) =v A""C +v A""D $ A""C""D 1 2 where 2d v = 1 ! 1 d +d 1 2 v =2""v 2 1 which is equivalent to the formulas given above. ! 5.10.6 Truth-value Arithmetic As a kind of footnote to the discussion of revision, it is interesting to observe that one may define a kind of arithmetic on PLN two-component truth-values, in which averaging revision plays the role of the addition on strengths, whereas simple summation plays the role of revision on counts. We have not found practical use for this arithmetic yet, but such uses may emerge as PLN theory develops further. The first step toward formulating this sort of truth-value arithmetic is to slightly extend the range of the set of count values.", Probabilistic Logic Networks,chapter 5
"Simple two-component PLN truth-values, defined above, are values lying in [0,1]![0, !]. However, we can think of them as lying in [0,1]![-!, !], if we generalize the interpretation of “evidence” a little bit, and introduce the notion of negative total evidence. Negative total evidence is not so counterintuitive, if one thinks about it the right way. One can find out that a piece of evidence one previously received and accepted was actually fallacious. Suppose that a proposition P in the knowledge base of a system S has the truth-value (1,10). Then, for the system’s truth-value for P to be adjusted to (.5,2) later on, it will have to encounter 9 pieces of negative evidence and 1 piece of positive evidence.", Probabilistic Logic Networks,chapter 5
"That is, all 10 of the pre-July-19 observations must be proved fraudulent, and one new observation must be made (and this new observation must be negative evidence). So, we may say (anticipating the truth-value arithmetic to be introduced below): (1,10)+(1,""9)+(0,1)=(0.5,2) Now, suppose we have two truth-values T and S. We then define ! T+S=W(W""1(T)+W""1(S)) where the function W is a nonlinear mapping from the real plane into the region [0,1] ! [-!, !].", Probabilistic Logic Networks,chapter 5
"!     Chapter 5: First Order Extensional Inference 123 Similarly, where c is a real number, we define the scalar multiple of a truthvalue T by c*T =W(W""1(c)*W""1(T)) It is easy to verify that the standard vector space axioms (Poole, 2006) hold for these addition and scalar multiplication operations. ! A vector space over the field F is a set V together with two binary operations, • vector addition: V (cid:186) V → V denoted v + w, where v, w (cid:15464) V, and • scalar multiplication: F (cid:186) V → V denoted a v, where a (cid:15464) F and v (cid:15464) V, satisfying the axioms below. Four require vector addition to be an Abelian group, and two are distributive laws. 1.", Probabilistic Logic Networks,chapter 5
"Vector addition is associative: For all u, v, w (cid:15464) V, we have u + (v + w) = (u + v) + w. 2. Vector addition is commutative: For all v, w (cid:15464) V, we have v + w = w + v. 3. Vector addition has an identity element: There exists an element 0 (cid:15464) V, called the zero vector, such that v + 0 = v for all v (cid:15464) V. 4. Vector addition has an inverse element: For all v (cid:15464) V, there exists an element w (cid:15464) V, called the additive inverse of v, such that v + w = 0. 5.", Probabilistic Logic Networks,chapter 5
"Distributivity holds for scalar multiplication over vector addition: For all a (cid:15464) F and v, w (cid:15464) V, we have a (v + w) = a v + a w. 6. Distributivity holds for scalar multiplication over field addition: For all a, b (cid:15464) F and v (cid:15464) V, we have (a + b) v = a v + b v. 7. Scalar multiplication is compatible with multiplication in the field of scalars: For all a, b (cid:15464) F and v (cid:15464) V, we have a (b v) = (ab) v. 8. Scalar multiplication has an identity element: For all v (cid:15464) V, we have 1 v = v, where 1 denotes the multiplicative identity in F.", Probabilistic Logic Networks,chapter 5
"The zero of the truth-value + operator is W(0); the unit of the truth-value * operator is W(1). Note that axioms 4 and 6, which deal with closure, hold only because we have decided to admit negative amounts of total evidence. The next question is, what is a natural nonlinear mapping function W to use here? What we would like to have is # E+(g) & W""1(S (g),E*(g))=W""1% f , E+(g)+ E""(g)( f $% E+(g)+ E""(g) f f ’( f f =(E+(g)""E""(g), E+(g)+ E""(g)) f f f f !     124 Probabilistic Logic Networks In other words, we want # a & W(a""b,a+b)=% ,a+b( .", Probabilistic Logic Networks,chapter 5
"$ a+b ’ This is achieved by defining ! "" x % 1+ $ ’ y W(x, y)=$ ,y’ $ 2 ’ $ ’ # & W(1(x,y)=(y(2x(1),y) Now, finally getting back to the main point, the relationship between truthvalue addition as just defined and PLN revision is not hard to see. We now lapse ! into standard PLN notation, with s for strength and N for count. Where s = N+/N, i i i we have (s,N )+(s ,N )=W(W""1(s,N )+W""1(s ,N )) 1 1 2 2 1 1 2 2 =W(N+ ""N"",N )+(N+ ""N"",N +N ) 1 1 1 1 2 1 2 =W((N+ +N+)""(N"" +N""),", Probabilistic Logic Networks,chapter 5
"(N+ +N+)+(N"" +N"")) 1 2 1 2 1 2 1 2 # & N+ +N+ =% 1 2 ,(N+ +N+)+(N"" +N"")( % (N+ +N+)+(N"" +N"") 1 2 1 2 ( $ 1 2 1 2 ’ =(ws +w s ,N +N ) 1 1 2 2 1 2 where ! N w = 1 1 N +N 1 2 N w = 2 2 N +N 1 2 This is just the standard PLN revision rule: weighted averaging based on count. So we see that our truth-value addition rule is nothing but revision! !     Chapter 5: First Order Extensional Inference 125 On the other hand, our scalar multiplication rule boils down to # N+ & c*(", Probabilistic Logic Networks,chapter 5
"*(s,N)=c*% ,N+ +N"" ( N+ +N"" $ ’ =W(c*(N+ ""N""),c*(N+ +N"")) # N+ & =% ,c*(N+ +N"") ( N+ +N"" $ ’ =(s,c*N) In other words, our scalar multiplication leaves strength untouched and multiplies only the total evidence count. This scalar multiplication goes naturally with ! revision-as-addition, and yields a two-dimensional vector space structure on truthvalues. 5.11 Inference Trails One issue that arises immediately when one implements a practical PLN system is the problem of circular inference (and, you may recall that this problem was briefly noted above in the context of term probability inference). For example, suppose one has P(American) = <[0.05, 0.15], 0.9, 10> P(silly) = <[0.3, 0.", Probabilistic Logic Networks,chapter 5
"5], 0.9, 10> P(human) = <[0.4, 0.6], 0.9, 10> Then one can reason Subset American human <[0.7, 0.9], 0.9, 10> Subset human silly <[0.4, 0.45], 0.9, 10> |Subset American silly <[0.370563, 0.455378], 0.9, 10> but then one can reason Subset human silly <[0.4, 0.45], 0.9, 10> |Subset silly human <[0.377984, 0.595297], 0.9, 10> and Subset American silly <[0.370563, 0.455378], 0.9, 10> Subset silly human <[0.377984, 0.", Probabilistic Logic Networks,chapter 5
"595297], 0.9, 10>     126 Probabilistic Logic Networks |Subset American human <[0.408808, 0.578298], 0.9, 10> which upon revision with the prior value for (Subset American human) yields Subset American human <[0.578047, 0.705526], 0.9, 10> Repeating this iteration multiple times, one obtains a sequence Subset American human <[0.42278, 0.550354], 0.9, 10> Subset American human <[0.435878, 0.563317], 0.9, 10> Subset American human <[0.429098, 0.555749], 0.9, 10> .. This is circular inference: one is repeatedly drawing a conclusion from some premises, and then using the conclusion to revise the premises.", Probabilistic Logic Networks,chapter 5
"The situation gets even worse if one includes term probability inference in the picture. Iterated inference involves multiple evidence-counting, and so the resulting truth values become meaningless. Note, however, that due to the nature of indefinite probabilities, the generation of meaningless truth values appears to be bounded, so after a few iterations no additional meaningless values are generated. One way to deal with this problem is to introduce a mechanism called “inference trails,” borrowed from NARS. This is not the only approach, and in a later chapter we will discuss an alternative strategy which we call “trail free inference.” But in the present section we will restrict ourselves to the trail-based approach, which is what is utilized in our current primary PLN software implementation. In the trail-based approach, one attaches to an Atom a nonquantitative object called the “inference trail” (or simply the “trail”), defined as a partial list of the terms and relationships that have been used to influence the Atom’s truth-value.", Probabilistic Logic Networks,chapter 5
"For the trail mechanism to work properly, each Atom whose truth-value is directly influenced by inference must maintain a trail object. Atoms newly formed in an AI system via external input or via noninferential concept creation have empty trails. Atoms formed via inference have nonempty trails. The rule for trail updating is simple: In each inference step, the trail of conclusion is appended with the trails of the premises, as well as the premises themselves. The intersection of the trails of two atoms A and B is then a measure of the extent to which A and B are based on overlapping evidence. If this intersection is zero, then this suggests that A and B are independent. If the intersection is nonzero, then using A and B together as premises in an inference step involves “circular inference.", Probabilistic Logic Networks,chapter 5
"As well as avoiding circular inference, the trail mechanism has the function of preventing a false increase of “weight of evidence” based on revising (merging) atoms whose truth-values were derived based on overlapping evidence. It can also be used for a couple of auxiliary purposes, such as providing information when the system needs to explain how it arrived at a given conclusion, and testing and debugging of inference systems.     Chapter 5: First Order Extensional Inference 127 The use of inference trails means that the choice of which inference to do is quite a serious one: each inference trajectory that is followed potentially rules out a bunch of other trajectories, some of which may be better! This leads to subtle issues relating to inference control, some of which will be addressed in later chapters – but which will not be dealt with fully in these pages because they go too far beyond logic per se.", Probabilistic Logic Networks,chapter 5
"In practice, it is not feasible to maintain infinitely long trails, so a trail length is fixed (say, 10 or 20). This means that in seeking to avoid circular inference and related ills, a reasoning system like PLN can’t use precise independence information, but only rough approximations. Pei Wang has formulated a nice aphorism in this regard: “When circular inference takes place and the circle is large enough, it’s called coherence.” In other words, some large-circle circularity is inevitable in any mind, and does introduce some error, but there is no way to avoid it without introducing impossible computational resource requirements or else imposing overly draconian restrictions on which inferences can occur. Hence, the presence of undocumented interdependencies between different truth-value assessments throughout an intelligent system’s memory is unavoidable.", Probabilistic Logic Networks,chapter 5
"The undocumented nature of these dependencies is a shortcoming that prevents total rationality from occurring, but qualitatively, what it does is to infuse the set of relationships and terms in the memory with a kind of “conceptual coherence” in which each one has been analyzed in terms of the cooperative interactions of a large number of others. 5.12 The Rule of Choice When the revision rule is asked to merge two different truth-value estimates, sometimes its best course may be to refuse. This may occur for two reasons: 1. Conflicting evidence: the two estimates both have reasonably high weight of evidence, but have very different strengths. 2. Circular inference violations: the rule is being asked to merge a truth-value T with another truth-value T that was derived in part based on T. 1 2 1 The name “Rule of Choice” refers to a collection of heuristics used to handle these cases where revision is not the best option.", Probabilistic Logic Networks,chapter 5
"There is not really one particular inference rule here, but rather a collection of heuristics each useful in different cases. In the case of conflicting evidence, there is one recourse not available in the circular inference case: creating a distributional truth-value. This is not always the best approach; its appropriateness depends on the likely reason for the existence of the conflicting evidence. If the reason is that the different sources observed different examples of the relationship, then a distributional truth-value is likely the right solution. If the reason is that the different sources simply have different opinions, but one and only one will ultimately be correct, then a distributional truth-value is likely not the right solution.     128 Probabilistic Logic Networks For instance, suppose we have two human Mars explorers reporting on whether or not Martians are midgets.", Probabilistic Logic Networks,chapter 5
"If one explorer says that Martians are definitely midgets and another explorer says that Martians are definitely not midgets, then the relevant question becomes whether these explorers observed the same Martians or not. If they observed different Martians, then revision may be applied, but to preserve information it may also be useful to create a distributional truth-value indicating that midget-ness among Martians possesses a bimodal distribution (with some total midgets, the ones observed by the first explorer; and with some total non-midgets, the ones observed by the second explorer). On the other hand, if the two explorers saw the same Martians, then we may not want to just revise their estimates into a single number (say, a .5 strength of midget-ness for Martians); and if we do, we want to substantially decrease the weight of evidence associated with this revised strength value.", Probabilistic Logic Networks,chapter 5
"Nor, in this case, do we want to create a bimodal truthvalue: because the most likely case is that the Martians either are or are not midgets, but one of the explorers was mistaken. On the other hand, one option that holds in both the conflicting evidence and circular inference cases is for the inference system to keep several versions of a given Atom around – distinct only in that their truth-values were created via different methods (they came from different sources, or they were inferentially produced according to overlapping and difficult-to-revise inference trails). This solution may be achieved knowledge-representation-wise by marking the different versions of an Atom as Hypothetical (see the following chapter for a treatment of Hypothetical relationships), using a special species of Hypothetical relationship that allows complete pass-through of truth-values into regular PLN inference. Next we describe several approaches that are specifically applicable to the case where the Rule of Choice is used to avoid circular inference (i.", Probabilistic Logic Networks,chapter 5
"., where it is used to mediate trail conflicts). First, an accurate but terribly time-consuming approach is to take all the elements from the trails of the two relationships Rel S P <t > 1 Rel S P <t > 2 merge them together into an atom set S, and then do a number of inference steps intercombining the elements of S, aimed at getting a new truth-value t so that Rel S P <t> This very expensive approach can only be taken when the relationship involved is very, very important. Alternately, suppose that T = (Rel S P ).Trail <t > MINUS (Rel S P).", Probabilistic Logic Networks,chapter 5
"Trail <t > 1 2 Then we can define Rel S P <t > 3 as the result of doing inference from the premises Rel S P <t > 2     Chapter 5: First Order Extensional Inference 129 T and we can merge Rel S P <t > 1 Rel S P <t > 3 using ordinary revision. This of course is also costly and only applicable in cases where the inference in question merits a fairly extreme allocation of computational resources. Finally, a reasonable alternative in any Rule of Choice situation (be it conflicting evidence or circular inference related), in the case where limited resources are available, is to simply do revision as usual, but set the degrees of freedom of the conclusion equal to the maximum of the degrees of freedom of the premises, rather than the sum. This prevents the degrees of freedom from spuriously increasing as a result of false addition of overlapping evidence, but also avoids outright throwing-out of information.", Probabilistic Logic Networks,chapter 5
"A reasonable approach is to use this crude alternative as a default, and revert to the other options on rare occasions when the importance of the inference involved merits such extensive attention allocation.  ", Probabilistic Logic Networks,chapter 5
"  Chapter 6: First-Order Extensional Inference with Indefinite Truth-Values Abstract In this chapter we exemplify the use of indefinite probabilities in a variety of PLN inference rules (including exact and heuristic ones). 6.1 Inference Using Indefinite Probabilities In this chapter we show how the inference rules described above can be applied when truth-values are quantified as indefinite probabilities, rather than as individual probabilities or (probability, weight of evidence) pairs. We outline a general three-step process according to which a PLN inference formula, defined initially in terms of individual probabilities, may be evaluated using indefinite probabilities. We will first present these three steps in an abstract way; and then, in the following section, exemplify them in the context of specific probabilistic inference formulas corresponding to inference rules like term logic and modus ponens deduction, Bayes’ rule, and so forth.", Probabilistic Logic Networks,chapter 6
"In general, the process is the same for any inference formula that can be thought of as taking in premises labeled with probabilities, and outputting a conclusion labeled with a probability. The examples given here will pertain to single inference steps, but the same process may be applied holistically to a series of inference steps, or most generally an “inference tree/DAG” of inference steps, each building on conclusions of prior steps. Step One in the indefinite probabilistic inference process is as follows. Given intervals [L,U] of mean premise probabilities, we first find a distribution from the i i “second-order distribution family” supported on[L1,U1]""[L,U ], so that these i i i i means have [L,U] as (100!b)% credible intervals.", Probabilistic Logic Networks,chapter 6
"The intervals [L1,U1] either i i i i i a! re of the form "" m m+k% , when the “interval-type” parameter associated with , $ ’ # n+k n+k& ! the premise is asymmetric, or are such that both of the intervals [L1,L] and ! i i ! [U,U1] each have probability mass b /2, when interval-type is symmetric. i i i Nex!t, in Step Two, we use Monte Carlo methods on the set of premise truthvalues to generate final distribution(s) of probabilities as follows. For each prem! ise we randomly select n values from the (“second-order”) distribution found in ! 1 Step One. These n values provide the values for the means for our first-order dis1 tributions. For each of our n first-order distributions, we select n values to repre1 2 sent the first-order distribution.", Probabilistic Logic Networks,chapter 6
"We apply the applicable inference rule (or tree of     132 Probabilistic Logic Networks inference rules) to each of the n values for each first-order distribution to generate 2 the final distribution of first-order distributions of probabilities. We calculate the mean of each distribution and then – in Step Three of the overall process – find a (100!b)% credible interval, [L ,U ], for this distribution of means. i f f When desired, we can easily translate our final interval of probabilities, [L ,U ], into a final, (s ,n ,b ) triple of strength, count, and credibility levels, f f f f f as outlined above.", Probabilistic Logic Networks,chapter 6
"! Getting back to the “Bayesianism versus frequentism” issues raised in Chapter 4, if one wished to avoid the heuristic assumption of decomposability regarding ! the first-ord!er plausibilities involved in the premises, one would replace the firstorder distributions assumed in Step Two with Dirac delta functions, meaning that no variation around the mean of each premise plausibility would be incorporated. This would also yield a generally acceptable approach, but would result in overall narrower conclusion probability intervals, and we believe would in most cases represent a step away from realism and robustness. 6.1.1 The Procedure in More Detail We now run through the above three steps in more mathematical detail. In Step One of the above procedure, we assume that the mean strength values follow some given initial probability distributiong (s) with support on the i interval[L1,U1].", Probabilistic Logic Networks,chapter 6
"If the interval-type is specified as asymmetric, we perform a i i U m search until we find values k2 so that "" ig(s)ds=b, where L = i and !L i i i n +k2 i m +k2 ! U = i . If the interval-type is symmetric, we first ensure, via parameters, i n +k2 i that each first-order distribution is symmetric about its mean s, by setting ! ! i L =s ""d, U =s +d and performing a search for d to ensure that i i i i "" LU iig i(s)ds=b. In either case, each of the intervals [L i,U i] will be a (100!b)% ! credible interval for the distributiong (s). i ! W!e note that we may be able to obtain the appropriate credible intervals for the distributionsg(s) only for certain values of b.", Probabilistic Logic Networks,chapter 6
"For this reason, we say that a value ! i ! b is truth-value-consist!en t whenever it is feasible to find (100!b)% credible intervals of the appropriate type. In Step Two of the above procedure, we create a family of distributions, drawn ! from a pre-specified set of distributional forms, and with means in the intervals [L,U]. We next apply Monte Carlo search to form a set of randomly chosen i i “premise probability tuples”. Each tuple is formed via selecting, for each premise of the inference rule, a series of points drawn at random from randomly chosen !     Chapter 6: Extensional Inference with Indefinite Truth Values 133 distributions in the family. For each randomly chosen premise probability tuple, the inference rule is executed. And then, in Step Three, to get a probability value s for the conclusion, we take the mean of this distribution.", Probabilistic Logic Networks,chapter 6
"Also, we take a f credible interval from this final distribution, using a pre-specified credibility level b f, to obtain an interval for the conclusion [L f,U f]. ! ! 6.2 A Few Detailed Examples ! In this section we report some example results obtained from applying the indefinite probabilities approach in the context of simple inference rules, using both symmetric and asymmetric interval-types. Comparisons of results on various inference rules indicated considerably superior results in all cases when using the symmetric intervals. As a result, we will report results for five inference rules using the symmetric rules; while we will report the results using the asymmetric approach for only one example (term logic deduction). 6.2.1 A Detailed Beta-Binominal Example for Bayes’ Rule First we will treat Bayes’ rule, a paradigm example of an uncertain inference rule -- which however is somewhat unrepresentative of inference rules utilized in PLN, due to its non-heuristic, exactly probabilistic nature.", Probabilistic Logic Networks,chapter 6
"The beta-binomial model is commonly used in Bayesian inference, partially due to the conjugacy of the beta and binomial distributions. In the context of Bayes’ rule, Walley develops an imprecise beta-binomial model (IBB) as a special case of an imprecise Dirichlet model (IDM). We illustrate our indefinite probabilities approach as applied to Bayes’ rule, under the same assumptions as these other approaches. We treat here the standard form for Bayes’ rule: P(A)P(BA) P(AB)= . P(B) Consider the following simple example problem. Suppose we have 100 gerbils of unknown color; 10 gerbils of known color, 5 of which are blue; and 100 rats of known color, 10 of which are blue. We wish to estimate the probability of a ran! domly chosen blue rodent being a gerbil.", Probabilistic Logic Networks,chapter 6
"The first step in our approach is to obtain initial probability intervals. We obtain the following sets of initial probabilities shown in Tables 1 and 2, corresponding to credibility levels b of 0.95 and 0.982593, respectively.     134 Probabilistic Logic Networks Table 1. Intervals for credibility level 0.90 EVENT [L,U] [L1, U1] G "" $1 0 ,12% =[.476190, 0.571429] [0.419724, 0.627895] ’ # 21 21& R "" 8 ,12% =[0.380952, 0.571429] [0.26802, 0.684361] $ ’ ! # 21 21& B|G [0.3, 0.7] [0.0628418, 0.937158] ! B|R [0.06, 0.", Probabilistic Logic Networks,chapter 6
"14] [0.0125683, 0.187432] Table 2. Intervals for credibility level 0.95 EVENT [L, U] [L1, U1] G "" $1 0 ,12% =[.476190, 0.571429] ’ # 21 21& [0.434369, 0.61325] R ! "" 8 ,12% =[0.380952, 0.571429] [0.29731, 0.655071] $ ’ # 21 21& B|G [0.3, 0.7] [0.124352, 0.875649] ! B|R [0.06, 0.14] [0.0248703, 0.17513] We begin our Monte Carlo step by generatingn random strength values, cho1 toxks""1(1""x)k(1""s)""1", Probabilistic Logic Networks,chapter 6
"sen from Beta distributions proportional with mean values of s=11 for P(G); s=10 for P(R); s=1 forP(BG); and 1 for P(BR), and s= 21 21 2 10 ! with support on [L1, U1]. Each of these strength values then serves, in turn, as parameters of standard Beta distr! ib utions. We generate a random sample of n 2 ! points f !ro m each of these ! s tanda !rd Beta distributio!n s. ! We next apply Bayes’ theorem to each of the n n quadruples of points, gen1 2 erating n sets of sampled distributions. Averaging across each distribution then 1 gives a distribution of final mean strength values. Finally, we tran!sfo rm our final distribution of mean strength values back to (s, n, b) triples.", Probabilistic Logic Networks,chapter 6
"! !     Chapter 6: Extensional Inference with Indefinite Truth Values 135 6.2.1.1 Experimental Results Results of our Bayes’ rule experiments are summarized in Tables 3 and 4. Table 3. Final probability intervals and strength values for P(G|B) using initial b-values of 0.90 CREDIBILITY LEVEL INTERVAL 0.90 [0.715577, 0.911182] 0.95 [0.686269, 0.924651] Table 4. Final probability intervals and strength values for P(G|B) using initial b-values of 0.95 CREDIBILITY LEVEL INTERVAL 0.90 [0.754499, 0.896276] 0.95 [0.744368, 0.907436] 6.2.1.", Probabilistic Logic Networks,chapter 6
"2 Comparisons to Standard Approaches It is not hard to see, using the above simple test example as a guide, that our indefinite probabilities approach generalizes both classical Bayesian inference and Walley’s IBB model. First note that single distributions can be modeled as envelopes of distributions with parameters chosen from uniform distributions. If we model P(B) as a uniform distribution; P(A) as a single beta distribution and P(BA) as a single binomial distribution, then our method reduces to usual Bayesian inference. If, on the other hand, we model P(B) as a uniform distribution; P(A) as an envelope of beta distributions; and P(BA) as an envelope of binomial distributions, then our envelope approach reduces to Walley’s IBB model. Our ! envelope-based approach thus allows us to model P(B) by any given family of distributions, rather than restricting us to a uniform distribution.", Probabilistic Logic Networks,chapter 6
"This allows for more ! flexibility in accounting for known, as well as unknown, quantities. To get a quantitative comparison of our approach with these others, we modeled the above test example using standard Bayesian inference as well as Walley’s IBB model. To carry out standard Bayesian analysis, we note that given that there 2100 are 100 gerbils whose blueness has not been observed, we are dealing with !     136 Probabilistic Logic Networks “possible worlds” (i.e., possible assignments of blue/non-blue to each gerbil). Each of these possible worlds has 110 gerbils in it, at least 5 of which are blue, and at least 5 of which are non-blue. For each possible world w, we can calculate the probability that drawing 10 gerbils from the population of 110 existing in world W yields an observation of 5 blue gerbils and 5 non-blue gerbils.", Probabilistic Logic Networks,chapter 6
"This probability may be writtenP(DH), where D is the observed data (5 blue and 5 non-blue gerbils) and H is the hypothesis (the possible world W). P(DH)P(H) Applying Bayes’ rule, we have P(HD)= . A !ss uming that P(H) P(D) is constant across all possible worlds, we find thatP(HD) is proportional toP(DH). Given this distribution for the possible values of the n!um ber of blue ! gerbils, one then obtains a distribution of possible values P(gerbilblue) and calculates a credible interval. The results of this Bayesian approach are summa! ! rized in Table 5 ! Table 5. Final Probability Intervals for P(G|B) CREDIBILITY LEVEL INTERVAL 0.90 [0.8245614035, 0.8630136986] 0.95 [0.", Probabilistic Logic Networks,chapter 6
"8214285714, 0.8648648649] We also applied Walley’s IBB model to our example, obtaining (with k=10) the "" 2323 2453% interval , , or approximately [0.43007, 0.88465]. In comparison, the $ ’ # 2886 2886& hybrid method succeeds at maintaining narrower intervals, albeit at some loss of credibility. With a k-value of 1, on the other hand, Walley’s approach yields an interval of ! [0.727811, 0.804734]. This interval may seem surprising because it does not include the average given by Bayes’ theorem. However, it is sensible given the logic of Walley’s approach. In this approach, we assume no prior knowledge of P(G) and we have 10 new data points in support of the proposition that P(GB) is 55/65.", Probabilistic Logic Networks,chapter 6
"So we assume beta distribution priors with s=0 and s=1 for the “endpoints” of P(G) and use n=10 and p=11/13 for the binomial distribution for P(GB). The density function thus has the form ! xks(1""x)k(1""s) xnp(1""x)n(1""p) f(x)= ! #1 xks(1""x)k(1""s) xnp(1""x)n(1""p) dx 0 !     Chapter 6: Extensional Inference with Indefinite Truth Values 137 Now for s=1 and the value of the learning parameter k=1, the system with no prior knowledge starts with the interval [1/3, 2/3].", Probabilistic Logic Networks,chapter 6
"With only 10 data points in support of p=11/13 and prior assumption of no knowledge or prior p=1/2, Walley’s method is (correctly according to its own logic) reluctant to move quickly in support of p=11/13, without making larger intervals via larger k-values. 6.2.2 A Detailed Beta-Binominal Example for Deduction Next we consider another inference rule, term logic deduction, which is more interesting than Bayes’ rule in that it combines probability theory with a heuristic independence assumption.", Probabilistic Logic Networks,chapter 6
"The independence-assumption-based PLN deduction rule, as derived in Chapter 5, has the following form for “consistent” sets A, B, and C: (1""s )(s ""s s ) s =s s + AB C B BC AC AB BC 1""s B where ! P(A""C) s =P(CA)= AC P(A) assuming the given data s =P(BA), s =P(CB), s =P(A), s =P(B), AB BC A B ! and s =P(C). C Our example for the deduction rule will consist of the following premise truthvalues. In Tab !le 6, we provide t!he values for [ !L ,U], and b !, a s well as the values corresponding to the mean s and count n. In the following sets of examples we as! sumed k=10 throughout. Table 6.", Probabilistic Logic Networks,chapter 6
"Premise truth-values used for deduction with symmetric intervals Premise s [L,U] [L1, U1] A 11/23 [10/23, 12/23]![0.434783, 0.521739] [0.383226, 0.573295] B 0.45 [0.44, 0.46] [0.428142, 0.471858] AB 0.413043 [0.313043, 0.513043] [0.194464, 0.631623] BC 8/15 [7/15, 9/15] ! [0.466666, 0.6] [0.387614, 0.679053]     138 Probabilistic Logic Networks We now vary the premise truth-value for variable C, keeping the mean s conC stant, in order to study changes in the conclusion count as the premise width", Probabilistic Logic Networks,chapter 6
"[L,U] varies. In Table 7, b = 0.9 for both premise and conclusion, and s = 0.59. The fiC nal count n is found via the inverse Bernoulli function approach of Chapter 4. AC Table 7. Deduction rule results using symmetric intervals Premise C Conclusion AC [L,U] [L1, U1] sAC [L,U] nAC [0.44, 0.74] [0.26213, 0.91787] 0.575514 [0.434879, 0.68669] 11.6881 [0.49, 0.69] [0.37142, 0.80858] 0.571527 [0.47882, 0.65072] 18.8859 [0.54, 0.64] [0.48071, 0.", Probabilistic Logic Networks,chapter 6
"69929] 0.571989 [0.52381, 0.612715] 42.9506 [0.58, 0.60] [0.56814, 0.61186] 0.571885 [0.4125, 0.6756] 10.9609 For comparison of using symmetric intervals versus asymmetric, we also tried identical premises for the means using the asymmetric interval approach. In so doing, the premise intervals [L, U] and [L1, U1] are different as shown in Tables 8 and 9, using b = 0.9 as before. Table 8. Premise truth-values used for deduction with asymmetric intervals Premise s [L,U] [L1, U1] A 11/23 [0.44, 0.52] [0.403229, 0.560114] B 0.45 [0.", Probabilistic Logic Networks,chapter 6
"44, 0.462222] [0.42818, 0.476669] AB 0.413043 [0.38, 0.46] [0.3412, 0.515136] BC 8/15 [0.48, 0.58] [0.416848, 0.635258] Table 9. Deduction rule results using symmetric intervals Premise C Conclusion AC [L,U] [L1, U1] sAC [L,U] nAC [0.40, 0.722034] [0.177061, 0.876957] 0.576418 [0.448325, 0.670547] 13.683 [0.45, 0.687288] [0.28573, 0.801442] 0.577455 [0.461964, 0.661964] 15.", Probabilistic Logic Networks,chapter 6
"630 [0.50, 0.652543] [0.394397, 0.725928] 0.572711 [0.498333, 0.628203] 26.604 [0.55, 0.617796] [0.526655, 0.600729] 0.568787 [0.4125, 0.6756] 10.961     Chapter 6: Extensional Inference with Indefinite Truth Values 139 6.2.3 Modus Ponens Another important inference rule is modus ponens, which is the form of deduction standard in predicate logic rather than term logic. Term logic deduction as described above is preferable from an uncertain inference perspective, because it generally supports more certain conclusions. However, the indefinite probabilities approach can also handle modus ponens; it simply tends to assign conclusions fairly wide interval truth-values.", Probabilistic Logic Networks,chapter 6
"The general form of modus ponens is A A ! B |B To derive an inference formula for this rule, we reason as follows. Given that we know P(A) and P(B|A), we know nothing about P(B|¬A). Hence P(B) lies in the interval [Q,R]=[P(A and B),1""(P(A)""P(A and B))] =[P(BA)P(A),1""P(A)+P(BA)P(A)]. For the modus ponens experiment reported here we used the following premises: For A we used ""1 0 12% ; and for A ! B we used [L,U],b = $ , ’, 0.9 # 23 23& ! <[L,U],b>=<[0.313043,0.513043],0.9>. We proceed as usual, choosing distributions of distributions for both P(A) and P(B|A).", Probabilistic Logic Networks,chapter 6
"Combining these we find a distribution of distri!bu tions [Q,R] as defined above. Once again, by calculating means, we end up with a distribution of [Q,R] intervals. Finally, we find an interval [L,U] that contains (100!b)% of the final [Q,R] intervals. In our example, our final [L,U] interval at the b = 0.9 level is [0.181154, 0.736029]. 6.2.4 Conjunction Next, the AND rule in PLN uses a very simple heuristic probabilistic logic formula:P(A AND B)=P(A)P(B). To exemplify this, we describe an experiment consisting of assuming a truth-value of <[L, U], b>=<[0.4, 0.5], 0.9> for A and a truth-value of <[L, U], b>=<[0.2, 0.", Probabilistic Logic Networks,chapter 6
"3], 0.9> for B. The conclusion truth-value for P(A AND B) then becomes ! <[L,U],b>=<[0,794882, 0.123946], 0.9>.     140 Probabilistic Logic Networks 6.2.5 Revision Very sophisticated approaches to belief revision are possible within the indefinite probabilities approach; for instance, we are currently exploring the possibility of integrating entropy optimization heuristics as described in (Kern-Isberner, , 2004) into PLN for this purpose (Goertzel, 2008). At present, however, we treat revision with indefinite truth-values in a manner analogous to the way we treat them with simple truth-values. This approach seems to be effective in practice, although it lacks the theoretical motivation of the entropy minimization approach. Suppose D1 is the second-order distribution for premise 1 and D2 is the second-order distribution for D2.", Probabilistic Logic Networks,chapter 6
"Suppose further that n is the count for premise 1 1 and n is the count for premise 2. Let w = n /(n +n ) and w = n /(n +n), and then 2 1 1 1 2 2 2 1 2 form the conclusion distribution D = w D1 + w D2. We then generate our 1 2 <[L,U],b,k> truth-value as usual. As an example, consider the revision of the following two truth-values <[0.1, 0.2], 0.9, 20> and <[0.3, 0.7], 0.9, 20>. Calculating the counts using the inverse function discussed in Chapter 4 gives count values of 56.6558 and 6.48493 respectively. Fusing the two truth-values yields <[0.13614, 0.", Probabilistic Logic Networks,chapter 6
"233208], 0.9, 20> with a resulting count value of 52.6705. The inclusion of indefinite probabilities into the PLN framework allows for the creation of a logical inference system that provides more general results than Bayesian inference, while avoiding the quick degeneration to worst-case bounds inherent with imprecise probabilities. On more heuristic PLN inference rules, the indefinite probability approach gives plausible results for all cases attempted, as exemplified by the handful of examples presented in detail here.", Probabilistic Logic Networks,chapter 6
"What we have done in this chapter is present evidence that the indefinite probabilities approach may be useful for artificial general intelligence systems – first because it rests on a sound conceptual and semantic foundation; and second because when applied in the context of a variety of PLN inference rules (representing modes of inference hypothesized to be central to AGI), it consistently gives intuitively plausible results, rather than giving results that intuitively seem too indefinite (like the intervals obtained from Walley’s approach, which too rapidly approach [0,1] after inference), or giving results that fail to account fully for premise uncertainty (which is the main issue with the standard Bayesian or frequentist first-order-probability approach).", Probabilistic Logic Networks,chapter 6
"  Chapter 7: First-Order Extensional Inference with Distributional Truth-Values Abstract In this chapter we extend some of the ideas of the previous chapters to deal with a different kind of truth-value, the “distributional truth-value,” in which the single strength value is replaced with a whole probability distribution. We show is that if discretized, step-function distributional truth-values are used, then PLN deduction reduces simply to matrix multiplication, and PLN inversion reduces to matrix inversion. 7.1 Introduction The strength formulas discussed above have concerned SimpleTruthValues and IndefiniteTruthValues. However, the PLN framework as a whole is flexible regarding truth-values and supports a variety of different truth-value objects. In this chapter we extend some of the ideas of the previous sections to deal with a different kind of truth-value, the “distributional truth-value,” in which the single strength value is replaced with a whole probability distribution.", Probabilistic Logic Networks,chapter 7
"In brief, what we show is that if discretized, step-function distributional truth-values are used, then PLN deduction reduces simply to matrix multiplication, and PLN inversion reduces to matrix inversion. Other extensions beyond this are of course possible. For instance, one could extend the ideas in this chapter to a SecondOrderDistributionalTruthValue, consisting of a completely-specified second-order probability distribution (as opposed to an indefinite probability, which is a sketchily specified, compacted second-order probability distribution). But these and additional extensions will be left for later work. Before launching into mathematics, we will review the conceptual basis for “distributional truth-values” by elaborating an example. Given a “man” Atom and a “human” Atom, suppose we’ve observed 100 humans and 50 of them are men. Then, according to the SimpleTruthValue approach, we may say Subset man human < 1> Subset human man <.", Probabilistic Logic Networks,chapter 7
"5>     142 Probabilistic Logic Networks What this says is: All observed examples of “man” are also examples of “human”; half of the examples of “human” are also examples of “man.” The limitations of this simple approach to truth-value become clear when one contrasts the latter of the two relationships with the relation Subset human ugly <.5> Suppose that from listening to people talk about various other people, an AI system has gathered data indicating that roughly 50% of humans are ugly (in terms of being called ugly by a majority of humans talking about them).", Probabilistic Logic Networks,chapter 7
"The difference between this observation and the observation that 50% of humans are men is obvious: • In the case of man-ness, nearly 50% of humans totally have it, and nearly 50% of humans totally don’t have it; intermediate cases are very rare • In the case of ugliness, it’s not the case that nearly 50% of humans are not at all ugly whereas nearly 50% are totally ugly. Rather there is a spectrum of cases, ranging from no ugliness to absolute ugliness, with the majority of people close to the middle (just barely uglier than average, or just barely less ugly than average). In the language of statistics, man-ness has a bimodal distribution, whereas ugliness has a unimodal distribution. Taking this difference in underlying distribution into account, one can do more refined inferences.", Probabilistic Logic Networks,chapter 7
"ExampleofUnimodalDistribution ExampleofBimodalDistribution     Chapter 7: Distributional Truth Values 143 From examples like this, one arrives at the idea of “distributional truth-values” – TruthValue objects that contain not only a strength value (reflecting the mean of a truth-value distribution) but a better approximation to an entire probability distribution reflecting the truth-value of an entity. We have experimented with both StepFunctionTruthValue and PolynomialTruthValue objects, two specific implementations of the notion of distributional truth-values. The “distributions” involved are functions f :[0,1]2 ""[0,1], where for example, if Subset A C <f,w> ! then f(x,y)=t means ! P( Member Q C < y > Member Q C < x >)=t, and w represents the weight-of-evidence.", Probabilistic Logic Networks,chapter 7
"In practice, one cannot look at the full space of distributions f, but must choose ! some finite-dimensional subspace; and as noted above, we have explored two possibilities: polynomials and step functions. Polynomials are more elegant and in principle may lead to more efficient computational algorithms, but the inference formulas are easier to work out for step functions, and the implementation complexity is also less for them, so after some preliminary work with polynomials we decided to focus on step function distributional approximations. 7.2 Deduction and Inversion Using Distributional Truth-Values Now we give some mathematics regarding PLN inference with distributional truth-values, assuming discretized, step-function distributional truth-values. The mathematical framework used here is as follows. Consider three Terms, A, B and pA pB pC C, with Term probability distributions denoted by vectors , , where, e.g.", Probabilistic Logic Networks,chapter 7
"pA =(pA,pA, K,pA) 1 2 n ! ! ! pA =P(A) i i The event A is defined as i !     144 Probabilistic Logic Networks <P(A)""t i,A i+1,A where (t =0,t ,K,t ) is a (not necessarily equispaced) partition of the 1,A 2,A n,A=1 unit interval. It may often be useful to set the partition points so that ! ! 1 PA "" i n#1 for all i. Note that, if one follows this methodology, the partition points will generally be unequal for the different Terms in the system. This is not problematic for any of the inference mathematics we will use here. In fact, as we’ll see, deduction works perfectly well even if the number of partition points is different for different ! Terms.", Probabilistic Logic Networks,chapter 7
"However, inversion becomes problematic if different Terms have different numbers of partition points; so it’s best to assume that n is a constant across all Terms. P(CA) PAC Next, define the conditional probability by the matrix , where PAC =P(C A) i,j j i ! ! So long as A and C have the same number of partition points, this will be a square matrix. Given this set-up, the deduction rule is obtained as follows. ! Theorem. Assume A and C are independent in B for all i, j, k. Then i j k PAC =PABPBC . Proof. The theorem follows from the key equation n P(C A i)=""P(C B k)P(B A i) ! j j k k=1 which we formally demonstrate in a moment.", Probabilistic Logic Networks,chapter 7
"This equation, after a change of notation, is n ! PAC =""PABPBC i,j i,k k,j k=1 which according to the definition of matrix multiplication means !     Chapter 7: Distributional Truth Values 145 PAC =PABPBC Now, the key equation is demonstrated as follows. We know n #P(C ""B ""A) ! n j k i P(C A )=#P((C ""B )A) = k=1 j i j k i P(A) k=1 i Assuming C and A are independent in B , we have j i k n P(A ""B ) n P(C A )=#P(C B ) i k =#P(C B )P(B A) ! j i j k P(A) j k k i k=1 i k=1 The critical step in here is the independence assumption P(C ""B ""A)=P(C B )P(A ""B ) j k i j", Probabilistic Logic Networks,chapter 7
"k i k ! To see why this is true, observe that the assumption “A and C are independent in i j B” implies k C ""B ""A =P(C B )A ""B . j k i j k i k ! Dividing both sides by N, the size of the implicit universal set, yields the desired equation, completing the proof. QED ! Next, the inversion rule is obtained from the equation PB =PABPA (which is obvious, without any independence assumptions), which implies =(PAB)""1 PA PB ! Thus, if a relationship (SubsetRelationship A B) has distributional TruthValue PAB, it follows that the relationship (SubsetRelationship B A) should have distributional TruthValue (PAB)-1. In other words, PLN inversion is matrix inversion. ! Note that no term probabilities are used in any of these formulas. However, term probabilities can be derived from the distributional TruthValues.", Probabilistic Logic Networks,chapter 7
"Specifically, the term probability for A is the expected value of the distributional truthvalue for A.     146 Probabilistic Logic Networks Also, note that the inversion formula has error magnification problems due to the fact that the matrix inversion formula involves division by a determinant. And, in the unlikely case where the determinant of PAB is zero, the inversion formula fails, though one may use matrix pseudoinverses (Weisstein, 2008) and probably still obtain meaningful results. Continuous versions of the equations may also be derived, by shrinking the partition in the above formulas. 7.", Probabilistic Logic Networks,chapter 7
"3 Relationship between SimpleTruthValue and DistributionalTruthValue Inference Formulas In the case where n=2, one can obtain something quite similar (but not identical) to the SimpleTruthValue formulas by setting the partition interval t for the 1,A Term A so that mean(P(A))=P(t <P(A)""t =1) 1,A 2,A and doing something similar for the Terms B and C. In this case one has, e.g., ! P(A)=(P(A),1""P(A)) . Regardless of how one sets the t values, one can obtain fairly simple formulas 1 for the 2-partition-interval case. One has, e.g.", Probabilistic Logic Networks,chapter 7
"! ""P (B A) P(B A )% PAB =$ 1 1 1 2 ’ $P (B A) P(B A )’ # 2 1 2 2 & or equivalently ! # P(B A) P(B A ) & PAB =% 1 1 1 2 ( %1 ""P(B A) 1""P(B A )( $ 1 1 1 2 ’ And for the inversion formula, one has ! Det(PAB)=P(B A)""P(B A)P(B A )""P(B A )+P(B A )P(B A) 1 1 1 1 1 2 1 2 1 2 1 1 =P(B A)""P(B A ) 1 1 1 2 !     Chapter 7: Distributional Truth Values 147 P(t <P(B)) so that inversion is only degenerate", Probabilistic Logic Networks,chapter 7
"when is independent of 1,B whether “t <P(A)” is true or not. The inversion formula is then given by 1,A #1 ""P(B A ) ""P(B A )& [P(B A)""P(B A )](PAB)""!1 = % 1 2 1 2 ( 1 1 1 2 $%"" 1+P(B 1A 1) P(B 1A 1) ’( ! Note that here, as in general, ! (PAB)""1 #PBA even though both ! PA =(PAB)""1 PB PA =PBAPB hold. This is not surprising, as many different 2D matrices map a particular 1D subspace of vectors into another particular 1D subspace (the subspace here being ! the set of all 2D vectors whose components sum to 1).", Probabilistic Logic Networks,chapter 7
"The term most analogous to ordinary PLN-FOI inversion is P(B)=(PAB)""1 P(A)+(PAB)""1 P(A ) 1 11 1 12 2 (1""P(B A ))P(A)""P(B A )P(A) 1 2 1 1 2 1 = P(B A)""P(B A ) 1 1 1 2 Note that we are inverting the overall PAB matrix without inverting the individ(PAB)""1 #PBA ual entries; i.e., because we do not get from this an estimate of ! P(A B) . 1 1 The term of the deduction formula that is most analogous to PLN-FOI deduction is given by ! ! PAC =P(P(C)>t P(A)>t ) 11 1,C 1,A =P(C B)P(B A)+P(C B )P(B A) 1 1 1", Probabilistic Logic Networks,chapter 7
"1 1 2 2 1 which is similar, but not equal, to the FOI formula ! s =P(CB)P(BA)+P(C¬B)P(¬BA) . AC !  ", Probabilistic Logic Networks,chapter 7
"  Chapter 8: Error Magnification in Inference Formulas Abstract In this chapter, we mathematically explore the sensitivity of PLN strength formulas to errors in their inputs. 8.1: Introduction One interesting question to explore, regarding the PLN strength formulas, is the sensitivity of the formulas to errors in their inputs. After all, in reality the “true” probabilities s , s , s , s , s are never going to be known exactly. In pracA B C AB BC tical applications, one is nearly always dealing with estimates, not exactly known values. An inference formula that is wonderful when given exact information, but reacts severely to small errors in its inputs, is not going to be very useful in such applications. This issue is dealt with to a large extent via the weight of evidence formulas presented in Chapters 4 and 5.", Probabilistic Logic Networks,chapter 8
"In cases where an inference formula is applied with premises for which it has high error sensitivity, the weight of evidence of the conclusion will generally suffer as a consequence. However, it is interesting from a theoretic standpoint to understand when this is going to occur – when error sensitivity is going to cause inferences to result in low-evidence conclusions. Toward that end, in this chapter we use the standard tools of multivariable calculus to study the sensitivity of the inference formulas with respect to input errors. The main conceptual result of these calculations is that without careful inference control, a series of probabilistic inferences may easily lead to chaotic trajectories, in which the strengths of conclusions reflect magnifications of random errors rather than meaningful information. On the other hand, careful inference control can avoid this chaotic regime and keep the system in a productive mode.", Probabilistic Logic Networks,chapter 8
"The first, obvious observation to make regarding inferential error sensitivity in PLN is that if the errors in the inputs to the formulas are small enough the error in the output will also be small. That is, the inference formulas are continuous based on their premise truth values. In the case of deduction, for example, this conclusion is summarized by the following theorem: Theorem (PLN Deduction Formula Sensitivity) Let U denote a set with |U| elements. Let Sub(m) denote the set of subsets of U containing m elements. Let ! indicate an approximate inequality.", Probabilistic Logic Networks,chapter 8
"Assume s ""1, and s"" #s , s"" #s , s"" #s ,s"" #s , s"" #s , and let B A A B B C C AB AB BC BC ! ! ! ! ! !     150 Probabilisitic Logic Networks * $ A""Sub(Us# ),B""Sub(Us# ),C""Sub(Us# ),’f(x)=P, P(CA)= x & A B C )/ . , & P(BA)=s# ,P(CB)=s# )/ + % AB BC (. Then, where E() denotes the expected value (mean), we have E[ f (x)] "" s = s s +(1#s )(s #s s )/(1#s ) AC AB BC AB C B BC B ! More formally: ""#>0,$%>0 so that P(A)""s <#, P(B)""s <#, A B P(C)""", Probabilistic Logic Networks,chapter 8
"s <#, P(BA)""s <#, and P(CB)""s <# implies C AB BC E[f(x)""s ] <# where s is defined by the above formula. The proof of ! AC AC this th! eo rem is obvious and is om!itte d. ! The situation with abduction is fairly similar to that with deduction, except that ! ! there is an additional restriction: s cannot equal 0 or 1. This is achievable by ap! B propriate adjustment of the count N of the universe. ! On the other hand, a glance back at the induction formula will show that the situation is a little different there. For induction, to avoid sensitive dependence on input values we need s to be bounded away from 1, and we also need s to be B A bounded away from 0.", Probabilistic Logic Networks,chapter 8
"If s is near 1 or s is near 0, induction should not be carB A ried out, because the results can’t be trusted. And, unlike in the deduction case, this problem can’t always be avoided by a careful choice of universe size. The problem is that if one increases the universe size to make s smaller, one also B makes s smaller, which may bring it too close to zero. If the term B is a lot more A frequent than the term A, then there is no way to do inductive inference meaningfully, unless one is sure that all one’s premise truth-values are known very close to exactly. One interesting consequence of this fact is that, in the case of induction, there is no way to set the universe size globally that will make all inductions in the system even minimally reliable. To get reliable inductions, one has to let the universe size vary across different inferences.", Probabilistic Logic Networks,chapter 8
"So, if one wants to use the inductive inference rule (a very valuable heuristic), one has to give up the idea of a single global “space of events” serving as an implicit universe for all inferences in the system. One has to consider inferences as contextual. These observations about the singularities of the inference formulas, however, are only the most simplistic way of studying the formulas’ error-sensitivity. A deeper view is obtained by looking at the partial derivatives of the inference formulas, which are shown below. These calculations – to be discussed in more detail below – reveal that the degree of error-sensitivity is highly dependent on the particular input values. For instance, consider the deduction formula. For the input values s = .5, s =s = .", Probabilistic Logic Networks,chapter 8
"25, BC C B we have     Chapter 8: Error Magnification in Inference Formulas 151 "" s 1 AC = ""s 3 AB which means that the deduction formula decreases rather than increases errors in its input s value. On the other hand, if s = 1, s =.25, s = .5, then we have AB BC C B ! ""s AC =1.5 ""s AB which means that errors in the input s value are increased by a factor of 1.5. AB To really understand error magnification in PLN, one has to look at the norm ! of the gradient vectors of the inference rules, as will be discussed in the following section. These analyses basically confirm what the above partial-derivative examples suggest: depending on the input values, sometimes an inference rule will magnify error, and sometimes it will decrease error (and sometimes it’ll leave the error constant).", Probabilistic Logic Networks,chapter 8
"If errors are magnified in the course of an inference, then we’re essentially losing precision in the course of the inference – we’re losing information. Let’s say we have a set of strength values that are known to 10 bits of precision apiece. If we do a series of inference steps, each one of which magnifies error by a factor of 2, then after 10 steps of inference we’ll be dealing with totally meaningless conclusions. We will have “chaos” in the series of strengths associated with the trail of inferences, in the sense that the strength values observed will draw on later and later bits of the initial probabilities. In a practical computational context there are only a few significant bits and the rest is roundoff error, which means that in some cases, after enough inference steps have passed, the strength values could come to consist of nothing but roundoff error. Fortunately, this potentially very serious problem can be managed via careful attention to weight of evidence.", Probabilistic Logic Networks,chapter 8
"In this chapter we will focus on the SimpleTruthValue case, but the issue of error magnification and inferential chaos arises equally severely in other cases. In the DistributionalTruthValue case, error magnification will often result in probability distribution function (PDF) truth-values that are relatively uniformly distributed across the interval. 8.2: Gradient Norms and Error Magnification Our assessment of the error magnification properties of PLN rules relies on computations of the norms of the gradient vectors of the rules. The concepts of error magnification and chaos in a discrete dynamical systems context are covered well in Devaney’s book (Devaney, 1989.", Probabilistic Logic Networks,chapter 8
"     152 Probabilisitic Logic Networks Basic Mathematics of Gradient Norms All the PLN strength formulas are of the form v = f(w) where • w lies in n-dimensional space (4-dimensional for deduction, 3! dimensional for inversion) • v lies in 1-dimensional space Let’s ask what happens if w is perturbed by a small error e, so that we’re looking at v\'= f(w\')= f(w+e) We can write v\'= f(w)+((grad(f ))(w))•e+O( e2) !f where grad(f )= f + f +L+ f , and f = .", Probabilistic Logic Networks,chapter 8
"1 2 n i !x i Since we have v\'""v !((grad(f )(w)))•e it follows that v\'""v # (grad(f )(w)) w""w\' cos! and on average 2 v\'!v "" (grad(f )(w)) w!w\' . # As a result, if we want the amount of error to decrease from one step to the next, we’d like to have (grad(f )(w)) <1     Chapter 8: Error Magnification in Inference Formulas 153 and if we want the amount of error to decrease on average we definitely need ! (grad(f )(w)) < , 2 (though having only this is chancy, because one could always find that the upper bound of the inequality was realized for a long enough time to cause real trouble). 8.2.", Probabilistic Logic Networks,chapter 8
"2 Inference Formula Gradient Norms The following section presents algebraic calculations and graphical depictions of the gradient norms for the deduction, inversion, sim2inh and inh2sim formulas. As the illustrations given here show, the gradient norms behave rather differently for the different rules. For deduction the norm of the gradient is often, but by no means always, less than 1. For some parameter values it’s usually greater than 1; for others it’s nearly always less. The algebraic formula involved is complex, but one clear fact is that when s is close to 1, “good” gradient norms are relatively rare. A big universe B size seems to push the rule toward stability. We need to be more careful with the inversion rule. As s gets close to zero, A the gradient norms increase toward infinity, due to division by s . As s apA A proaches 1 another problem occurs in that the region of consistent inputs becomes very small.", Probabilistic Logic Networks,chapter 8
"For values of s that are between these two extremes, on the other A hand, sizable regions for s and s exist for which the gradient norm is “good.” B AB For inh2sim, the gradient norm maxes out around 1.4. The problem area occurs when both premises, s and s , are very close to 1; that is, when A and C almost AC CA entirely overlap. It would be a shame to avoid doing inh2sim in these cases, which suggests that a more subtle control strategy than “avoid all error magnification” is called for (a topic we’ll return to below). For sim2inh, the gradient norm can get quite large (20 or so). This tends to occur when s is large, and s and sim are both small. If we take s very small, then B C BC B the gradient norm tends to be bounded above by 1.", Probabilistic Logic Networks,chapter 8
"Here, a large universe size makes this rule dampen rather than magnify error. 8.2.3 Visual Depiction of PLN Formula Derivatives Now we report some Maple work deriving and displaying the gradient norms of the PLN inference formulas. The partial derivatives of the PLN deduction formula are computed as:     154 Probabilisitic Logic Networks diff(dedAC(sA,sB,sC,sAB,sBC),sA); 0 diff(dedAC(sA,sB,sC,sAB,sBC),sB); (1""sAB)sBC (1""sAB)(sC""sBsBC) "" + 1""sB (1""sB)2 diff(dedAC(sA,sB,sC,sAB,sBC),sC); ! 1""sAB 1""sB diff(dedAC(sA,sB,sC,sAB,sBC),sAB); ! sC""sBsBC sBC"" 1""sB diff", Probabilistic Logic Networks,chapter 8
"(dedAC(sA,sB,sC,sAB,sBC),sBC); ! (1""sAB)sB sAB"" 1""sB 8.2.3.1 Depictions of the Gradient Norm of the Deduction Formula ! The norm of the gradient vector of the PLN deduction formula is thus dedAC_grad_norm := (sA,sB,sC,sAB,sBC) -> ( ( -(1-sAB)*sBC/(1-sB)+(1sAB)*(sC-sB*sBC)/(1-sB)^2)^2 + ( (1-sAB)/(1-sB))^2 + ( sBC-(sCsB*sBC)/(1-sB))^2 + (sAB-(1-sAB)*sB/(1-sB) )^ 2 ) ^ (1/2) *(Heaviside(sABmax(((sA+sB-1)/sA),0))-Heaviside(sAB-min(1,(s", Probabilistic Logic Networks,chapter 8
"B/sA))))*(Heaviside(sBCmax(((sB+sC-1)/sB),0))-Heaviside(sBC-min(1,(sC/sB))));     Chapter 8: Error Magnification in Inference Formulas 155 $$ (1#sAB)(sC#sBsBC)’ dedAC_grad_norm:=(sA,sB,sC,sAB,sBC)""&& # ) %& %& (1#sB)2 () 2’1 /2 (1#sAB)2 $ sC#sBsBC’ 2 $ (1#sAB)sB’ + +& sBC# ) +& sAB# ) ) (1#sB)2 % 1#sB ( % 1#sB ( () Finally, we are ready to show graphs of the deduction formula. We display two ! sets of graphs: plot3d(dedAC_grad_norm(.", Probabilistic Logic Networks,chapter 8
"1,.1,.1, sAB, sBC), sAB=0..1, sBC=0..1, labels=[sAB,sBC,ded], numpoints=800, axes=BOXED); is the graph of the norm of the gradient vector, while dedAC_grad_norm_only01 := (sA, sB, sC, sAB, sBC) -> piecewise (dedAC_grad_norm (sA, sB, sC, sAB, sBC) < 0, 0, dedAC_grad_norm(sA, sB, sC, sAB,sBC) < 1, dedAC_grad_norm(sA, sB, sC, sAB, sBC), 0); is a filtered version of the graph, showing only those values of the norm that are less than or equal to 1. plot3d(dedAC_grad_norm(.1,.1,.", Probabilistic Logic Networks,chapter 8
"1, sAB, sBC), sAB=0..1, sBC=0..1, labels=[sAB,sBC,ded], numpoints=800, resolution = 400, axes=BOXED);     156 Probabilisitic Logic Networks plot3d(dedAC_grad_norm_only01(.1,.1,.1,sAB,sBC),sAB=0..1,sBC=0..1,la bels=[sAB,sBC,ded],numpoints=800,axes=BOXED); plot3d( dedAC_grad_norm(.1,.4, .7,sAB, sBC), sAB=0 ..1, sBC=0..1, axes=BOXED);     Chapter 8: Error Magnification in Inference Formulas 157 plot3d(dedAC_grad_norm_only01(.1,.4,.7,sAB,sBC),sAB=0..1,sBC=0..", Probabilistic Logic Networks,chapter 8
"1,la bels=[sAB,sBC,dedAC],numpoints=800,axes=BOXED); plot3d( dedAC_grad_norm(.4,.1,.9,sAB,sBC), sAB=0 ..1, sBC=0..1, axes=BOXED);     158 Probabilisitic Logic Networks plot3d(dedAC_grad_norm_only01(.4,.1,.9,sAB,sBC),sAB=0..1,sBC=0..1,la bels=[sAB,sBC,ded],numpoints=1000,axes=BOXED); 8.2.3.2 Depictions of the Gradient Norm of the Inversion Formula Now we give similar figures for the inversion formula. These are simpler because the inversion formula has fewer input variables.", Probabilistic Logic Networks,chapter 8
"Recall that the inversion formula looks like invAB := (sA,sB, sAB) -> sAB * sB / sA*(Heaviside(sAB-max(((sA+sB1)/sA),0))-Heaviside(sAB-min(1,(sB/sA)))); We may thus compute the partial derivatives and norm-gradient as diff(invAB(sA,sB,sAB),sA); sABsB "" sA2 diff(invAB(sA,sB,sAB),sB); ! sAB sA !     Chapter 8: Error Magnification in Inference Formulas 159 diff(invAB(sA,sB,sAB),sAB); sB sA invAB_nd := (sA,sB, sAB) -> ( ( sAB/sA)^2 + ( -sAB*sB/sA^2)^2 + ( sB/sA) ^2)^(1/2)*(Heaviside(s", Probabilistic Logic Networks,chapter 8
"AB-max(((sA+sB-1)/sA),0))-Heaviside(sABmin(1,(sB/sA)))); ! invAB_nd:=(sA,sB,sAB)"" sAB2 sAB2sB2 sB2$ $ $ sA+sB#1 ’’ $ $ sB’’ ’ + + & Heaviside& sAB#max& ,0)) #Heaviside& sAB-min&1 , )) ) sA2 sA4 sA2% % % sA (( % % sA(( ( The following function returns the inversion derivative norm only in cases where ! it’s in [0,1] (where we have error decrease rather than increase), and returns 0 otherwise: invAB_nd_only01:=(sA,sB,sAB)->piecewise(invAB_nd(sA,sB,sAB)<0, 0,invAB_nd(sA,sB,sAB)<", Probabilistic Logic Networks,chapter 8
"1,invAB_nd(sA,sB,sAB),0); Now we give plots of inversion over all inputs, for various values of the input variable s A.     160 Probabilisitic Logic Networks plot3d( invAB_nd(.01,sB,sAB), sB=0 ..1, sAB=0..1, axes=BOXED, numpoints=1000, resolution = 400, labels=[sB,sAB,invAB_nd]); plot3d( invAB_nd(.1,sB,sAB), sB=0 ..1, sAB=0..1, axes=BOXED, numpoints=800, resolution=400, labels=[sB,sAB,invAB_nd]);     Chapter 8: Error Magnification in Inference Formulas 161 plot3d( invAB_nd_only01(.1,sB,sAB), sB=0.01 ..1, sAB=0..", Probabilistic Logic Networks,chapter 8
"1, axes=BOXED, numpoints=1000, resolution=400, labels=[sB,sAB,invAB_nd]); plot3d( invAB_nd(.5,sB,sAB), sB=0 ..1, sAB=0..1, axes=BOXED, numpoints=800, resolution = 400, labels=[sB,sAB,invAB_nd]);     162 Probabilisitic Logic Networks plot3d( invAB_nd_only01(.5,sB,sAB), sB=0 ..1, sAB=0..1, axes=BOXED, numpoints=1000, resolution = 400, labels=[sB,sAB,invAB_nd]); plot3d( invAB_nd(.9,sB,sAB), sB=0 ..1, sAB=0..", Probabilistic Logic Networks,chapter 8
"1, axes=BOXED, numpoints=1000, resolution = 400, labels=[sB,sAB,invAB_nd]);     Chapter 8: Error Magnification in Inference Formulas 163 plot3d( invAB_nd_only01(.9,sB,sAB), sB=0 ..1, sAB=0..1, axes=BOXED, numpoints=1000, resolution = 400, labels=[sB,sAB,invAB_nd]); 8.2.3.3 Depictions of the Gradient Norms of the Similarity-Inheritance Conversion Formulas Finally, in this subsection we give a similar treatment for the sim2inh conversion formula.", Probabilistic Logic Networks,chapter 8
"Here we have sim2inh := (sA,sB,simAB) -> (1 + sB/sA) * simAB / (1 + simAB) *(Heaviside(simAB-max(((sA+sB-1)),0))-Heaviside(simAB-min(sA/sB, (sB/sA)))); inh2sim := (sAC,sCA) -> 1/( 1/sAC + 1/sCA - 1); from which we calculate the derivatives diff(inh2sim(sAC,sCA),sAC); 1 # 1 1 & 2 % + ""1( sAC2 $ sAC sCA ’ !     164 Probabilisitic Logic Networks diff(inh2sim(sAC,sCA),sCA); 1 # 1 1 & 2 % + ""1( sCA2 $ sAC sCA ’ inh2sim_nd := (sAC,sCA) -> (", Probabilistic Logic Networks,chapter 8
"(1/((1/sAC+1/sCA-1)^2*sAC^2))^2 + (1/((1/sAC+1/sCA-1)^2*sCA^2))^2 )^(1/2); ! 1 1 inh2sim_nd:=(sAC,sCA)"" + $ 1 1 ’ 4 $ 1 1 ’ 4 & + #1) sAC4 & + #1) sCA4 % sAC sCA ( % sAC sCA ( diff( sim2inh(sA,sB,simAB), sA); ! sBsimAB "" sA2(1+simAB) diff( sim2inh(sA,sB,simAB), sB); ! simAB sA(1+simAB) diff( sim2inh(sB,sC,simBC), simBC); ! sC # sC& 1+ %1 + ( simBC", Probabilistic Logic Networks,chapter 8
"sB $ sB’ "" 1+simBC (1+simBC)2 sim2inh_nd := (sB,sC,simBC) -> ( (simAB/(1+simBC))^2 + (sB/sC^2*simBC/(1+simBC))^2 + ((1+sB/sC)/(1+simBC)(1+sB/sC)*sim!B C/(1+simBC)^2)^2 )^(1/2) *(Heaviside(simBC-max(((sB+sC1)),0))-Heaviside(simBC-min(sB/sC,(sC/sB))));     Chapter 8: Error Magnification in Inference Formulas 165 sim2inh_nd:=(sB,sC,simBC)"" $ sB $ sB’ ’ 2 simBC2 sB2 simBC2 +& 1+ #%&1 + sC() simBC) + &", Probabilistic Logic Networks,chapter 8
"sC ) (1+simBC)2 sC4(1+simBC)2 & 1+simBC (1+simBC)2 ) & ) % ( $ $ $ sB sC’’ ’ & Heaviside(simBC-max(sB+sC-1,0))#Heaviside& simBC-min& , )) ) % % % sC sB(( ( We then produce the following diagrams: ! plot3d( inh2sim_nd(sAC, sCA), sAC=0..1, sCA=0..1, axes=BOXED);     166 Probabilisitic Logic Networks plot3d( sim2inh_nd(.1, sB, simAB), sB=0..1, simAB=0..1, resolution=400, axes=BOXED); plot3d(sim2inh_nd(0.01, sB, simAB), sB = 0 ..", Probabilistic Logic Networks,chapter 8
"1, simAB = 0 .. 1, resolution = 800, numpoints = 800, axes = BOXED);     Chapter 8: Error Magnification in Inference Formulas 167 plot3d( sim2inh_nd(.4, sB, simAB), sB=0..1, simAB=0..1, resolution=800, axes=BOXED); plot3d( sim2inh_nd(.9, sB, simAB), sB=0..1, simAB=0..1, axes=BOXED, resolution=800);     168 Probabilisitic Logic Networks Depictions of the Gradient Norms of the Deduction Formulas Incorporating Dependency Information Now we repeat the above exercise for the deduction variants that incorporate dependency information.", Probabilistic Logic Networks,chapter 8
"The deduction formula using dependency information with a known value for intABC= P((C!B)!(A!B)) is: dedAC_dep1 := (sA, sB, sC, sAB, sBC, intABC) -> intABC / sA + (1sAB)*(sC-sB * sBC)/(1-sB); The corresponding formula using dependency information with known intAnBC= P((C!B\')!(A!B\')) becomes: dedAC_dep2 := (sA,sB,sC,sAB,sBC,intAnBC) -> (sAB * sBC + intAnBC / sA); Similarly, the formula using dependency information in both arguments is dedAC_dep12 := (sA, intABC,intAnBC) -> intABC / sA + intAnBC / sA ; For each of these formulas there are consistency constraints similar to those given earlier.", Probabilistic Logic Networks,chapter 8
"Now we need to include an additional constraint on the intABC and/or the intAnBC variables. For dependency with a known value for intABC, for example, consistency implies the following constraint: consistency:= (sA, sB, sC, sAB, sBC,intABC) -> (Heaviside(sABmax(((sA+sB-1)/sA),0))-Heaviside(sAB-min(1,(sB/sA))))*(Heaviside(sBCmax(((sB+sC-1)/sB),0))-Heaviside(sBC-min(1,(sC/sB))))*(Heaviside(intABCmax(sA+sB+sC-2,0))-Heaviside(intABC-min(sA,sB,sC))); We wish to determine the behavior of the gradient norms of these formulas. As before, we first calculate the partial derivatives for each formula.", Probabilistic Logic Networks,chapter 8
"The partial derivatives of the PLN deduction formula with dependency information in the first argument are diff(dedAC_dep1(sA,sB,sC,sAB,sBC,intABC),sA); intABC "" sA2 diff(dedAC_dep1(sA,sB,sC,sAB,sBC,intABC),sB); !     Chapter 8: Error Magnification in Inference Formulas 169 (1""sAB) sBC (1""sAB)(sC""sB sBC) "" + 1""sB (1""sB)2 diff(dedAC_dep1(sA,sB,sC,sAB,sBC,intABC),sC); ! 1""sAB 1""sB diff(dedAC_dep1(sA,sB,sC,sAB,sBC,intABC),sAB); ! sC""sB sBC "" 1""sB diff(dedAC_dep1(sA,sB,sC,sAB,sBC,intABC),s", Probabilistic Logic Networks,chapter 8
"BC); ! (1""sAB)sB "" 1""sB diff(dedAC_dep1(sA,sB,sC,sAB,sBC,intABC),intABC); ! 1 sA The norm of the gradient vector of the PLN deduction formula with dependency in the first argument is then: ! dedAC_nd_dep1 := (sA, sB,sC,sAB,sBC,intABC)->(((1-sAB)/(1sB))^2+(1/sA)^2+(-(sC-sB*sBC)/(1-sB))^2+(-(1-sAB)*sB/(1-sB))^2+(intABC/sA^2)^2+(-(1-sAB)*sBC/(1-sB)+(1-sAB)*(sC-sB*sBC)/(1sB)^2)^2)^(1/2); $ (1""sAB)2 (sC""sBsBC)2 1 dedAC_nd_dep1:""(sA", Probabilistic Logic Networks,chapter 8
",sB,sC,sAB,sBC,intABC)#& + + %& (1""sB)2 sA2 (1""sB)2 +(1""sAB)2sB2 +intABC2 +$ ""(1""sAB)sBC +(1""sAB)(sC""sBsBC)’ 2’ )1 /2 & ) (1""sB)2 sA4 %& 1""sB (1""sB)2 () () Similarly, the partial derivatives of the PLN deduction formulas with depend! ency information in the second argument are:     170 Probabilisitic Logic Networks diff(dedAC_dep2(sA,sB,sC,sAB,sBC,intAnBC),sA); intABC "" sA2 diff(dedAC_dep2(sA,sB,sC,sAB,sBC,intAnBC),sB); ! 0 diff(dedAC_dep2(sA,sB,sC,sAB,sBC,int", Probabilistic Logic Networks,chapter 8
"AnBC),sC); 0 diff(dedAC_dep2(sA,sB,sC,sAB,sBC,intAnBC),sAB); sBC diff(dedAC_dep2(sA,sB,sC,sAB,sBC,intAnBC),sBC); ! sAB diff(dedAC_dep2(sA,sB,sC,sAB,sBC,intAnBC),intAnBC); ! 1 sA The norm of the gradient vector of the PLN deduction formula with dependency in the second argument then becomes: ! dedAC_dep2_nd:=(sA,sB,sC,sAB,sBC,intAnBC) -> ((intAnBC/sA^2)^2 + (sBC)^2 +(sAB)^2+(1/sA)^2)^(1/2); intAnBC2 1 dedAC_dep2_nd:=(sA,sB,sC,sAB,sBC,intAnBC)"" +sBC2+sAB2+", Probabilistic Logic Networks,chapter 8
"sA4 sA2 Last, we calculate the norm of the gradient vector for the PLN deduction for! mula knowing the values for both intABC and intAnBC. The partial derivatives for this case are found using the following calculations. diff(dedAC_dep12(sA,sB,sC, intABC,intAnBC),sA);     Chapter 8: Error Magnification in Inference Formulas 171 intABC intAnBC "" "" sA2 sA2 diff(dedAC_dep12(sA,sB,sC,intABC,intAnBC),sB); ! 0 diff(dedAC_dep12(sA,sB,sC,intABC,intAnBC),sC); 0 diff(dedAC_dep12(sA,sB,sC,intABC,intAnBC),intABC); 1 sA diff(dedAC_dep12(sA,sB,sC,intABC,intAnBC),intAnBC); ! 1 sA", Probabilistic Logic Networks,chapter 8
"Hence the norm of the gradient vector for PLN deduction when both intABC and intAnBC are known is: ! dedAC_dep12_nd:=(sA,sB,sC,intABC,intAnBC)->((-intABC/sA^2intAnBC/sA^2)^2+(1/sA)^2+(1/sA)^2)^(1/2); $ intABC intAnBC’ 2 2 dedAC_dep12_nd:=(sA,sB,sC,intABC,intAnBC)"" & # # ) + % sA2 sA2 ( sA2 We now give graphical depictions of the gradient norms for the inference formulas that incorporate dependency. We first exhibit several representative samples ! when intABC is known.     172 Probabilisitic Logic Networks plot3d(dedAC_nd_dep1(.1,.1,.5,.9,sBC,intABC)*consistency(.1,.1,.5,.", Probabilistic Logic Networks,chapter 8
"9,sBC, intABC), sBC=0.. 1,intABC=0.. 1, axes=BOXED, labels=[sBC,intABC,dedAC_1_nd], numpoints=2000, resolution=800); plot3d(dedAC_nd_dep1(.5,.5,.1,.5,sBC,intABC)*consistency(.5,.5,.1,.5,sBC, intABC), sBC=0.. 1,intABC=0.. 1, axes=BOXED, labels=[sBC,intABC,dedAC_1_nd], numpoints=1000, resolution=800);     Chapter 8: Error Magnification in Inference Formulas 173 plot3d(dedAC_nd_dep1(.9,.5,.9,.5,sBC,intABC)*consistency(.9,.5,.5,.5,sBC, intABC), sBC=0.. 1,intABC=0..", Probabilistic Logic Networks,chapter 8
"1, axes=BOXED, labels=[sBC,intABC,dedAC_1_nd], numpoints=2000, resolution=800); The next few graphs are representative of the deduction rule with a known value for intAnBC. plot3d(dedAC_dep2_nd(.1,.1,.1,.5,sBC,intAnBC)*consistency(.1,.1,.1,.5,sB C,intAnBC), sBC=0.. 1,intAnBC=0.. 1, axes=BOXED, labels=[sBC,intAnBC,dedAC_2_nd], numpoints=800);     174 Probabilisitic Logic Networks plot3d(dedAC_dep2_nd(.5,.5,.9,.5,sBC,intAnBC)*consistency(.5,.5,.9,.5,sB C,intAnBC), sBC=0.. 1,intAnBC=0..", Probabilistic Logic Networks,chapter 8
"1, axes=BOXED, labels=[sBC,intAnBC,dedAC_2_nd], numpoints=2000, resolution=800); plot3d(dedAC_dep2_nd(.9,.1,.9,.1,sBC,intAnBC)*consistency(.9,.1,.9,.1,sB C,intAnBC), sBC=0.. 1,intAnBC=0.. 1, axes=BOXED, labels=[sBC,intAnBC,dedAC_2_nd], numpoints=2000, resolution=800);     Chapter 8: Error Magnification in Inference Formulas 175 Finally we present a small sample of the graphs for deduction when both intABC and intAnBC are known. plot3d( dedAC_dep12_nd(.1,.5,.9,intABC,intAnBC)*consistency(.1,.5,.9, .", Probabilistic Logic Networks,chapter 8
"5,intABC, intAnBC), intABC=0.. 1, intAnBC=0.. 1, axes=BOXED, labels = [intABC, intAnBC, dedAC_12_nd], numpoints=2000, resolution=800); plot3d( dedAC_dep12_nd(.5,.5,.9,intABC,intAnBC) * consistency(.5,.5,.9, .5,intABC, intAnBC), intABC=0.. 1, intAnBC=0.. 1, axes=BOXED, labels = [intABC, intAnBC, dedAC_12_nd], numpoints=2000, resolution=800);     176 Probabilisitic Logic Networks 8.3 Formal Chaos in PLN The above gradient norm results imply, among other things, the possibility of deterministic chaos associated with PLN.", Probabilistic Logic Networks,chapter 8
"To see this, let’s begin by constructing an artificially simple PLN-based reasoning dynamic. Suppose one has a set of J entities, divided into M categories (represented by M Concept terms). Suppose one builds the M(M-1) possible (probabilistically weighted) Subsets between these M ConceptTerms. Assume each of the Subsets maintains an arbitrarily long inference trail, and assume the Subsets are ordered overall (e.g., in an array). This ordering naturally defines an ordering on the set of pairs of Subsets as well. Let V denote the set of ConceptTerms and Subsets thus constructed. Then one can define a deterministic dynamic on this space V, as follows: 1) Choose the first pair of Subsets that can be combined (using deduction, induction, or abduction) without violating the inference trail constraints. 2) Do the inference on the pair of Subsets.", Probabilistic Logic Networks,chapter 8
"3) Use the revision rule to merge the resulting new Subset in with the existing Subset that shares source and target with the newly created Subset. The dynamics, in this scheme, depend on the ordering. But it is clear that there will be some orderings that result in a long series of error-magnifying inferences. For these orderings one will have genuinely chaotic trajectories, in the sense of exponential sensitivity to initial conditions. If the initial conditions are given to B logB bits of precision, it will take roughly steps for the inferred values to lose logz all meaning, where z is the average factor of error magnification. This dynamic does not meet all definitions of mathematical chaos, because eventually it stops: eventually there will be no more pairs of relationships to combine without violating the !tra il constraints. But the time it will run is hyperexponential in the number of initial categories, so for all intents and purposes it behaves chaotically.", Probabilistic Logic Networks,chapter 8
"The big unknown quantity in this example deterministic inferential dynamic, however, is the percentage of chaotic trajectories for a given set of initial categories. How many orderings give you chaos? Most of them? Only a rare few? Half? The mathematics of dynamical systems theory doesn’t give us any easy answers, although the question is certainly mathematically explorable with sufficient time and effort. In the inference control mechanism we use in the current implementation of PLN, we use a different method for picking which pair of relationships to combine     Chapter 8: Error Magnification in Inference Formulas 177 at a given step, but the ultimate effect is the same as in this toy deterministic inferential dynamic. We have a stochastic system, but we may then have stochastic chaos. The question is still how common the chaotic trajectories will be. 8.3.", Probabilistic Logic Networks,chapter 8
"1 Staying on the Correct Side of the Edge of Chaos We can see a very real meaning to the “Edge of Chaos” concept here (Langton 1991; Packard 1988). Doing reasoning in the error-magnification domain gives rise to chaotic dynamics, which is bad in this context because it replaces our strength values with bogus random bits. But we want to get as close to the errormagnification domain as possible – and dip into it as often as possible without destroying the quality of our overall inferences – so that we can do reasoning in as broadly inclusive a manner as possible. Looking at the problem very directly, it would seem there are four basic approaches one might take to controlling error magnification: Safe approach Check, at each inference step, whether it’s an errormagnifying step or not. If it’s an error-magnifying step, don’t do it. Reckless approach Just do all inference steps regardless of their error magnification properties.", Probabilistic Logic Networks,chapter 8
"Cautious approach Check, at each inference step, whether it’s an errormagnifying step or not. If the gradient-norm is greater than some threshold T (say, T=2 or T=1.5), then don’t do it. The threshold may possibly be set differently for the different inference rules (some reasons to do this will be mentioned below). Adaptive approach Check, at each inference step, whether it’s an errormagnifying step or not. If it’s an error-magnifying step, decide to do it based on a calculation.", Probabilistic Logic Networks,chapter 8
"Each logical Atom must keep a number D defined by     178 Probabilisitic Logic Networks Initially, D = 1 At each inference step involving premises P1,…,Pn, set D = D * (norm of the gradient of the pertinent innew ference rule evaluated at (P1 … Pn) ) In the adaptive approach, the decision of whether to do an inference step or not is guided by whether it can be done while keeping D below 1 or not. The reckless approach is not really viable. It might work acceptably in a deduction-only scenario, but it would clearly lead to very bad problems with inversion, where the derivative can be huge. We could lose three or four bits of precision in the truth-value in one inference step. The adaptive approach, on the other hand, is very simple code-wise, and in a way is simpler than the cautious approach, because it involves no parameters.", Probabilistic Logic Networks,chapter 8
"In practice, in our current implementation of PLN we have not adopted any of these approaches because we have subsumed the issue of avoiding error magnification into the larger problem of weight-of-evidence estimation. The adaptive approach to avoiding error magnification winds up occurring as a consequence of an adaptive approach to weight-of-evidence-based inference control. Conceptually speaking, we have reached the conclusion that speculative reasoning, unless carefully controlled, or interwoven with larger amounts of nonspeculative reasoning, can lead to significant error magnification. Sometimes a cognitive system will want to do speculative error-magnifying reasoning in spite of this fact. But in these cases it is critical to use appropriately adaptive inference control. Otherwise, iterated speculative inference may lead to nonsense conclusions after a long enough chain of inferences – and sometimes after very short ones, particularly where inversion is involved.", Probabilistic Logic Networks,chapter 8
"  Chapter 9: Large-Scale Inference Strategies Abstract In chapter 9, we consider special inference approaches useful for inference on large bodies of knowledge. 9.1 Introduction The inference rules presented in the previous chapters follow a common format: given a small number of premises (usually but not always two), derive a certain conclusion. This is the standard way logical reasoning is described, yet it is not the only way for logical reasoning to be carried out in real inference systems. Another equally important application of logical inference is to derive conclusions integrating large bodies of information. In these cases the standard logical rules are still applicable, but thinking about reasoning in terms of the step-by-step incremental application of standard logical rules is not necessarily the best approach. If one considers “large-scale inference” (or alternatively, “holistic inference”) as a domain unto itself, one quickly arrives at the conclusion that there are different control strategies and even in a sense different inference rules appropriate for this situation.", Probabilistic Logic Networks,chapter 9
"This chapter presents four such specialized approaches to large-scale inference: • Batch inference, which is an alternative to the Rule of Choice usable when one has to make a large number of coordinated choices based on the same set of data • Multideduction, which is an alternative to iterated deduction and revision that is appropriate when one has to do a series of deductions and revision based on the same large body of data • Use of Bayesian networks to augment and accelerate PLN inference • Trail-free inference, the most radical alternative approach considered here, in which trails are omitted and inference is considered as a nonlinear multivariable optimization problem No doubt there are other viable approaches besides these ones. In a larger sense, the point of this chapter is to indicate the diversity of approaches to deriv-     180 Probabilistic Logic Networks nclusions from probabilistic premises, beyond traditional incremental logical theorem proving type approaches. 9.2 Batch Inference In this section we return to the theme of handling circular inference.", Probabilistic Logic Networks,chapter 9
"The Rule of Choice options mentioned in Chapter 5 are relatively simple and have served us adequately in experiments with PLN. However, such an approach is clearly not optimal in general – it’s basically a “greedy” strategy that seeks good inference trajectories via local optimizations rather than by globally searching for good inference trajectories. In a large-scale inference context, it is possible to work around this problem by carrying out a large number of inference steps en masse, and coordinating these steps intelligently. This kind of “batch” oriented approach to Rule of Choice type operations is probably not suitable for real-time reasoning or for cognitive processes in which reasoning is intricately interlaced with other cognitive processes. What it’s good for is efficiently carrying out a large amount of reasoning on a relatively static body of information.", Probabilistic Logic Networks,chapter 9
"The underlying motivation is the fact that, in many cases, it is feasible for a reasoning system to operate by extracting a large set of logical relationships from its memory, doing a lot of inferences on them at once, and then saving the results. The method described in this section, “batch reasoning graph inference” (BRGI), tries to generate the maximum amount of information from a set of inferences on a body of data S, by generating an optimal set of inference trails for inferences from S. This process minimizes the use of the Rule of Choice. The BRGI process carries out choices similar to what the Rule of Choice does in searching for the optimal tree, but in a significantly more efficient way due to its more global scope. In a sense, it makes a lot of “Rule of Choice” type choices all at once, explicitly seeking a globally valuable combination of choices. 9.2.", Probabilistic Logic Networks,chapter 9
"1 Batch Reasoning Graphs Suppose we have a collection S = {S , …, S } of terms or logical relationships, 1 n drawn from a reasoning system’s memory at a single point in time. Define a “batch reasoning graph” (brg) over S as a DAG (directed acyclic graph) constructed as follows: • The leaf terms are elements of S. • All non-leaf terms have two children. • Each non-leaf term is labeled with a logical conclusion, which is derived from its children using one of the PLN inference rules.     Chapter 9: Large-Scale Inference Strategies 181 In such a graph, each term represents a relationship that is derived directly from its children, and is derived indirectly from its grandchildren and more distant progeny. When a brg term A is entered into the memory as a relationship, it should be given a trail consisting of its progeny in the tree.", Probabilistic Logic Networks,chapter 9
"If we wish to conserve memory, we may form a smaller trail considering only progeny down to some particular depth. For two terms A and B, if there is no path through the brg from A to B (following the arrows of the graph), then A and B may be considered “roughly independent,” and can be used together in inferences. Otherwise, A and B are based on the same information and shouldn’t be used together in inference without invocation of the Rule of Choice. From any given set S it is possible to construct a huge variety of possible batch reasoning graphs. The batch reasoning process proposed here defines a quality measure, and then runs a search process aimed at finding the highest-quality brg. Many search processes are possible, of course. One option is to use evolutionary programming; crossing over and mutating DAGs is slightly tricky, but not as problematic as crossing over and mutating cyclic graphs.", Probabilistic Logic Networks,chapter 9
"Another option is to use a greedy search algorithm, analogously to what is often done in Bayes net learning (Heckerman 1996). What is the quality measure? The appropriate measure seems to be a simple one. First, we must define the expectation of an inference. Suppose we have an inference whose conclusion is an Atom of type L, and whose truth-value involves (s,d) = (strength, weight of evidence). This expectation of this inference is d* p(s,L) where p(s,L) is the probability that a randomly chosen Atom of type L, drawn from the knowledge-store of the system doing the inference, has strength s. A ! crude approximation to this, which we have frequently used in the past, is ! d* s""0.5 So, suppose one creates a brg involving N inferences. Then one may calculate T, the total expectation of the set of inferences embodied in the brg.", Probabilistic Logic Networks,chapter 9
"The ratio T/N ! is a measure of the average information per relationship in the brg. A decent quality measure, then, is T c""N +(1#c) N !     182 Probabilistic Logic Networks eans that we want to do a lot of inferences, but we want these inferences to be high quality. This concept, as specifically described above, is directly applicable both to FOI and to all parts of HOI except those that involve Predicates. In order to apply it to HOI on Predicates, the definition of a brg has to be extended somewhat. One has to allow special brg terms that include logical operators: AND, OR, or NOT. A NOT term has one input; AND and OR terms may be implemented with two inputs, or with k inputs.", Probabilistic Logic Networks,chapter 9
"In assessing the quality of a brg, and counting the expectations of the terms, one skips over the relationships going into these logical operator terms; the value delivered by the operator terms is implicit in the expectations of the relationships coming out of them (i.e., the inferences that are made from them). 9.2.2 Example of Alternate Batch Reasoning Graphs To make these ideas more concrete, let us consider some simple examples. Suppose the system contains the relations Inheritance A B Inheritance B C Inheritance C A with strengths derived, not through reasoning, but through direct examination of data.", Probabilistic Logic Networks,chapter 9
"These examples may lead to either of the following graphs (trees), where Inh is short for Inheritance:     Chapter 9: Large-Scale Inference Strategies 183 1) Inh A C (deduction) Inh A B Inh B C 2) Inh B C (induction) Inh A B Inh A C (inversion) Inh C A In Graph 1 (Inh B C) is used to derive (Inh A C), whereas in Graph 2 (Inh A C) is used to derive (Inh B C). So once you’ve done the reasoning recorded in Graph 2, you cannot use (Inh B C) to increase your degree of belief in (Inh A C), so you cannot use the reasoning recorded in Graph 1.", Probabilistic Logic Networks,chapter 9
"And once you’ve done the reasoning recorded in Graph 1, you cannot use (Inh A C) to increase your degree of belief in (Inh B C), so you cannot do the reasoning involved in Graph 2. Which of these is a better graph, 1 or 2? This depends on the truth-values of the relationships at the terms of the brg. If (Inh C A) has a much higher empirically derived strength than (Inh B C), then it may make sense to construct the inference given in graph 2; this may produce a higher-total-average-expectation group of relationships.", Probabilistic Logic Networks,chapter 9
"For a slightly more complex example, consider the following two graphs:     184 Probabilistic Logic Networks Inh A D Inh A C Inh C D Inh A B Inh B C 2) Inh A C Inh A D Inh D C Inh A B Inh B D In Case 1, the inference Inh A D Inh A B |Sim B D cannot be drawn, because (Inh A B) is in the trail of (Inh A D). In Case 2, on the other hand, the inference Inh A C Inh A B |Sim B C cannot be drawn because (Inh A B) is in the trail of (Inh A C).", Probabilistic Logic Networks,chapter 9
"So the above trees can be expanded into directed acyclic graphs of the form     Chapter 9: Large-Scale Inference Strategies 185 1) Inh B D Inh A D Inh A C Inh C D Inh A B Inh B C 2) Inh B C Inh A C Inh A D Inh D C Inh A B Inh B D Which is a better path for reasoning to follow? This depends on the truthvalues of the relationships involved. In particular, suppose that (Inheritance B D).TruthValue.expectation = (Inheritance B C).TruthValue.expectation prior to any of these reasoning steps being carried out. Then the quality of 1 versus 2 depends on the relative magnitudes of (Inheritance D C).TruthValue.expectation and (Inheritance C D).TruthValue.", Probabilistic Logic Networks,chapter 9
"ation     186 Probabilistic Logic Networks Discussion In doing inference based on a large body of relationships, the sort of decision seen in the above simple examples comes up over and over again, and multiple decisions are relationshipped together, providing an optimization problem. This sort of optimization problem can be solved dynamically and adaptively inside an inference system by the standard incremental inference control mechanisms and the Rule of Choice, but it can be done more efficiently by considering it as a “batch inference optimization process” as proposed here. What one loses in the batch reasoning approach is the involvement of noninferential dynamics in guiding the inference process, and the ability to carry out a small number of highly important inferences rapidly based on a pool of background knowledge. Thus, this is not suitable as a general purpose inference system’s only inference control strategy, but it is a valuable inference control strategy to have, as one among a suite of inference control strategies.", Probabilistic Logic Networks,chapter 9
"Most particularly, the strength of batch inference lies in drawing a lot of relatively shallow inferences based on a large body of fairly confident knowledge. Where there is primarily highly uncertain knowledge, noninferential methods will be so essential for guiding inference that batch reasoning will be inappropriate. Similarly, in cases like mathematical theorem-proving where the need is for long chains of inference (“deep inferences”), noninferential guidance of the reasoning process is also key, and batch inference is not going to be as useful. Batch inference can still be used in these cases, and it may well generate useful information, but it will definitely not be able to do the whole job. 9.3 Multideduction Next, we present a complementary approach to large-scale inference that is more radical than batch inference in that it proposes not just a special approach to inference control oriented toward large-scale inference, but a special way of calculating truth-values under the assumption that there is a large amount of evidence to be integrated simultaneously.", Probabilistic Logic Networks,chapter 9
"We introduce a kind of amalgamated inference rule called “multideduction,” which carries out deduction and revision all at once across a large body of relationships, and as a consequence avoids a lot of the compounding of errors that occurs as a result of doing repeated deductions and revisions. In brief, what multideduction does is to perform the inference A""{B,B ,B , LB } 1 2 3 k {B,B ,B , LB }""C 1 2 3 k #A""C !     Chapter 9: Large-Scale Inference Strategies 187 in a single step. A B 1 B 2 … B k C Multideduction This is a change from the standard approach, in which one does such inference for each node B separately, using the deduction rule, and then merges these results i using the revision rule.", Probabilistic Logic Networks,chapter 9
"Multideduction, as described here, incorporates the same independence assumptions about each of the nodes, B, used in the standard deduction formula, and i uses some approximation to do its “revision”; but we believe these approximations are less erroneous than the ones used in the standard “repeated pair-wise deduction and revision” approach. Instead, multideduction is equivalent to “repeated pairwise deduction and revision” where revision is done using a more sophisticated revision rule customized for revision of two deductive conclusions (a rule that involves subtracting off for overlap among the nodes of the two deductions). There is also a variant of multideduction that is based on the concept geometry based deduction formula rather than the independence assumption based deduction formula; this variant is not explicitly described here but can be easily obtained as a modification of the independence based multideduction formulation.", Probabilistic Logic Networks,chapter 9
"Multideduction works naturally with an inference control strategy in which one takes the following steps in each “inference cycle”: • Do node probability inference on all sufficiently important nodes. • Do inversion on all sufficiently important relationships. • For all sufficiently important relationships (InheritanceRelationship A C), calculate s from all B-nodes for which s and s are available. AC AB BC The gauging of importance is beyond the scope of this section (and is not purely a PLN matter, but gets into non-inferential issues regarding the framework into which PLN is integrated). 9.3.1 The Inclusion-Exclusion Approximation Factor The inclusion-exclusion formula from standard combinatorics can be written as     188 Probabilistic Logic Networks n % n n P$ US ’ =(P(S )) (P(S *S )+error =term1+term2+error.", Probabilistic Logic Networks,chapter 9
"# i& i i j i=1 i=1 i,j=1 We may estimate the expected error of using only the first two terms via a formula # term1 term2& mean(error)=""% , ( $ n n2 ’ ! where ! is an “inclusion-exclusion approximation factor.” Points on the graph of ! may be computed using a simulation of random sets or random rectangles, or they may be computed in a domain-specific manner (using example sets S of interest). i The values for ! may be pre-computed and stored in a table. More sophisticatedly, ! it seems one might also be able to make a formula mean(error)=""( P(S ),P(S #S )) i i j by doing some complicated summation mathematics; or use a combination of summation mathematics with the simulation-estimation approach.", Probabilistic Logic Networks,chapter 9
"Generally, the idea is that we have no choice but to use the first two terms of the inclusionexclusion formula as an approximation, because we lack the information about ! higher-order intersections that the other terms require and because the other terms are too numerous for pragmatic computation; but we can make a reasonable effort to estimate the error that this assumption typically incurs in a given domain. So we will use the formula "" n % n n P$ US ’ =(P(S )) (P(S *S )++ # i& i i j i=1 i=1 i,j=1 where the ! will be written without arguments, for compactness, but is intended to be interpreted in accordance with the above comments. Note that this is an additive “fudge factor” and that it must be used with a “stupidity check” that avoids n ! assigning US a value greater than 1. i i =1 9.3.", Probabilistic Logic Networks,chapter 9
"2 Multideduction Strength Formula ! We now move on to the actual multideduction formula. We assume we have two nodes A and C, and a set of nodes {B} for which we know s , s s , s , s , i A C Bi ABi BiC and s . We wish to derive the value s . BiBj AC     Chapter 9: Large-Scale Inference Strategies 189 We write n B= UB i i=1 We will calculate the deduction truth-value via the formula P(A""C) P(A""C""B)+P(A""C""¬B) P(CA)= = P(A) P(A) ! P(A""C""B) +P(A""C""¬B) = + P(A) P(A) The key calculation is # n & P(A""C""B)=P% A""C""UB( +)P(A!C! B) ! $ i’ 1 i=1 [now we assume A and C", Probabilistic Logic Networks,chapter 9
"are independent in the B and the B ! B] i i j ! n (A""B)P(C""B) =# i i $ P(B) i=1 i n P(A""B ""B )P(C""B ""B ) # i j i j +% P(B ""B ) 1 i,j=1,n i j i&j [now we introduce the heuristic approximation ! P(A""B ""B )=P(B ""B )( w P(AB)+( 1#w ) P(AB )) i j i j ABB i ABB j i j i j where d w = B iA ! AB iB j d +d BA B A i j (the d’s are “weights of evidence” of the indicated relationships) and a similar approximation for P(A! B !B ).", Probabilistic Logic Networks,chapter 9
"j i !     190 Probabilistic Logic Networks P A""B)P(C""B) # i i P(B) i=1 i & (P (B i""B j)( w ABiBjP(AB i)+(1$w ABiBj)P(AB j))) $#n + +i i= ,1 j’( (% ( w CBiBjP(CB i)+(1$w CBiBj)P(CB j)) *+ + 1 n =#P(B A)P(A)P(CB) i i i=1 & . P(B A)P(A)1 ) (P (B B)P(B)0 w i 3 + n ( j i i /0 ABiBj P(B i) 23 + $#( + +i i, ,j= j1( (+ (1$w ABiBj)P(B Pj (A B)P )(A", Probabilistic Logic Networks,chapter 9
"). 0 w P(C PB (i B)P )(A) +(1$w CBiBj)P(B Pj (C B)P )(A)1 3 3+ 1 0 CBiBj + ’ j / i j 2* Dividing both sides by P(A) and simplifying a little yields P(A""C""B) ! P(A) ,% s s ( / .’ w s +( 1$w ) AB j B i * 1 n n . &’ AB iB j AB i AB iB j s )* 1 B =#s s $ #s . j 1 +3 AB i B iC B iB j. % s s ( 1 1 i=1 i i. 2j= j1 .+ ’ ’ w CB iB js CB +( 1$w CB iB j) C sB j B i * *1 i - &", Probabilistic Logic Networks,chapter 9
"B )0 j which should look immediately intuitively sensible. We have the sum of all the deduction first-terms for all the B, minus the sum of some terms proportional to i ! the intersections between the B. i The second term in the multideduction rule may be calculated similarly; we have # # n && # # n && P(A""C""¬B)=P% A""C""¬% UB(( =P% A""C""% U¬B(( $ $ i=1 i’’ $ $ i=1 i’’ and hence the calculation is the same as for the first term, but with ¬B substituted i for B everywhere. Hence we have i !     Chapter 9: Large-Scale Inference Strategies 191 P(A""C""¬B) P(A) ,% s s ( / .’ w s +(1$w ) A¬Bj ¬Bi * 1 =#n #n . .", Probabilistic Logic Networks,chapter 9
"&’ A¬Bi¬Bj A¬Bi A¬Bi¬Bj s )* 1 ¬Bj s s $ s 1 +3 A¬Bi ¬BiC ¬Bi¬Bj. % s s ( 1 2 i=1 i i. 2j= j1 .+ ’ ’ w C¬Bi¬Bjs C¬Bi +(1$w C¬Bi¬Bj) C¬ sBj ¬Bi * *1 - & ¬Bj )0 to obtain the final multideduction formula: n ! s AC =""(s ABis BiC+s A¬Bis ¬BiC) i=1 n $ s s ’$ s s ’ #""s & w s +(1#w ) ABj Bi)& w s +(1#w ) CBj Bi) BiBj& ABiBj ABi ABiBj s )& CBiBj CBi CBiBj s ) i", Probabilistic Logic Networks,chapter 9
",j=1 % Bj (% Bj ( i*j n $ s s ’$ s s ’ #""s & w s +(1#w ) A¬Bj ¬Bi)& w s +(1#w ) C¬Bj ¬Bi) ¬Bi¬Bj& A¬Bi¬Bj ABi A¬Bi¬Bj s )& C¬Bi¬Bj C¬Bi C¬Bi¬Bj s ) i,j=1 % ¬Bj (% ¬Bj ( i*j ++++ 1 2 which may be evaluated using the originally given probabilities using the substitutions ! s =1""s A¬B AB i i s ""s s = C B iC ¬B iC 1""s B i 1+s ""s s B BB B s = j i j i ¬B i¬B j 1""s B i The weights ! d w = ¬B iA A¬B i¬B j d +d ¬B iA", Probabilistic Logic Networks,chapter 9
"¬B jA also require expansion. We may estimate ! N =N s =(N ""N )s ¬B iA ¬B B iA U B ¬B iA i i !     192 Probabilistic Logic Networks N is the universe size, and calculate the weight of evidence d¬ from the U BiA count N¬ ; and do similarly for d¬ . BiA BjA Looking at this formula as a whole, what we have is the sum of the individual deductive conclusions, minus estimates of the overlap between the terms. Note how different this is from the standard deduction-plus-revision approach, which averages the individual deductive conclusions. Here the (weighted) averaging approximation is done inside the second term where the overlaps of A and C individually inside each B !B and ¬B !¬B are estimated. Multideduction is similar i j i j to deduction-plus-revision where revision is done using an overlap-savvy form of the revision rule.", Probabilistic Logic Networks,chapter 9
"One could extend this formula in a obvious way to make use of tripleintersection information P(B !B !B). This could be useful in cases where a i j k significant amount of such information is available. However, the number of B i nodes, n, must be quite small for this to be tractable because the third term of the &n# inclusion-exclusion formula has $ !terms. $ ! 3 % "" 9.3.3 Example Application of Multideduction In this section we present an example application where multideduction adds significant value. The example is drawn from a software system that utilizes the NCE for controlling virtual animals in the Second Life virtual world. As described in (Goertzel 2008), that system as a whole has numerous aspects and is focused on learning of novel behaviors. The aspect we will discuss here, however, has to do with the generation of spontaneous behaviors and the modification of logical predicates representing animals’ emotions.", Probabilistic Logic Networks,chapter 9
"The full rule-base used to guide spontaneous behaviors and emotions in this application is too large to present here, but we will give a few evocative examples. We begin with a few comments on rule notation. Firstly, the notation ==> in a rule indicates a PredictiveImplicationRelationship. Rules are assumed to have truthvalue strengths drawn from a discrete set of values 0 VERY LOW LOW MIDDLE HIGH VERY HIGH 1 In the following list, all rules should be assumed to have a truth-value of HIGH unless something else is explicitly indicated.     Chapter 9: Large-Scale Inference Strategies 193 For clarity, in the following list of rules we’ve used suffixes to depict c types of entities: P for personality traits, E for emotions, C for contexts, and S for schemata (the latter being the lingo for “executable procedures” within the NCE). In the case of schemata an additional shorthanding is in place; e.g.", Probabilistic Logic Networks,chapter 9
"barkS is used as shorthand for (Execution bark) where bark is a SchemaNode. Also, the notation TEBlah($X) is shorthand for ThereExists $X Evaluation Blah $X i.e., an existential quantification relationship, such as will be discussed in Chapter 11. Example rules from the rule-base are as follows: angerToward($X) ==> angry loveToward($X) ==> love hateToward($X) ==> hate fearToward($X) ==> fear TEgratitudeToward($X) ==> gratitude angerToward($X) ==> ¬friend($X) <LOW> TE(near($X) & novelty($X)) ==> novelty TEloveToward($X) & sleepy ==> gotoS($X) TE(loveToward($X) & near($X)) & sleepy ==> sleepS gratitudeToward($X) ==> lick($X) atHomeC & sleepyB ==> Ex sleepS <.", Probabilistic Logic Networks,chapter 9
7> gotoS($X) ==> near($X) <.6> gotoS($X) ==> near($X) <.6> AggressivenessP & angryE & barkS => happyE AggressivenessP & angryE & barkS ==> proudE AggressivenessP & angerToward($X) ==> barkAtS($X) <VERY HIGH> AggressivenessP & angerToward($X) ==> barkAtS($X) <VERY HIGH> AggressivenessP & angerToward($X) ==> nipS($X) <MIDDLE> AggressivenessP & near($X) & ¬friend($X) ==> angerToward($X) AggressivenessP & near($X) & enemy($X) ==> angerToward($X) <VERY HIGH> AggressivenessP & near_my_food($X) & ¬friend($X) ==> angryToward($X) <VERY LOW> AggressivenessP, Probabilistic Logic Networks,chapter 9
"& near_my_food($X) ==> angryToward($X) AggressivenessP & angerToward($X) & ¬friend($X) ==> hate($X) AggressivenessP & OtherOrientationP & ownerNear($X) & enemy($X) ==> angerToward($X) AggressivenessP & near($X) & enemy($X) & homeC ==> angerToward($X) AggressivenessP & ¬happyE & ¬angryE ==> boredE AggressivenessP & jealousE ==> angryE AggressivenessP & boredE ==> angryE <LOW>     194 Probabilistic Logic Networks Spontaneous activity of a virtual animal, governed by the above equations, is determined based on the modeling of habitual activity, such as the carrying out of actions that the pet has previously carried out in similar contexts.", Probabilistic Logic Networks,chapter 9
"For each schema S, there is a certain number of implications pointing into (Ex S), and each of these implications leads to a certain value for the truth-value of (Ex S). These values may be merged together using (some version of) the revision rule. However, a complication arises here, which is the appearance of emotion values like happyE on the rhs of some implications, and on the lhs of some others. This requires some simple backward chaining inference in order to evaluate some of the (Ex S) relationships (note that the details of the PLN backward chaining methodology will be described in Chapter 13). And this is an example where multideduction may be extremely valuable. Essentially, what we are doing here is carrying out revision of a potentially large set of deductive conclusions. We saw in our original discussion of revision for simple truth-values that in the case of revision of deductive conclusions, the standard revision formula can be improved upon considerably.", Probabilistic Logic Networks,chapter 9
"Multideduction provides an even more sophisticated approach, which can naturally be extended to handle indefinite probabilities. Using multideduction to estimate the truth-value of (Ex S) for each schema S provides significantly reduced error as compared to the multiple-deductions-followed-by-revision methodology. A similar approach applies to the generation of goal-driven activity based on rules such as the above. As an example, suppose we have a goal G that involves a single emotion/mood E, such as excitement. Then there are two steps: 1. Make a list of schemata S whose execution is known to fairly directly affect E. 2. For these schemata, estimate the probability of achievement of G if S were activated in the current context.", Probabilistic Logic Networks,chapter 9
"For Step 1 we can look for • Schemata on the lhs of implications with E on the rhs • One more level: schemata on the lhs of implications with X on the rhs, so that X is on the lhs of some implication with E on the rhs In the case of a large and complex rule-base, Step 2 may be prohibitively slow without some sort of batch-inference based approach. But for moderate-sized rulebases, a simple approach is unproblematic. However, there is the same problem as there is in the case of spontaneous activity: a lot of deductions followed by revisions, which produce considerable error that may be removed by taking a multideduction approach.     Chapter 9: Large-Scale Inference Strategies 195 9.", Probabilistic Logic Networks,chapter 9
"4 Using Bayes Nets to Augment PLN Inference Pursuing further the example of the previous section, we now explain how Bayes nets techniques may be used, much in the manner of batch inference, to accelerate inference based on a knowledge base that is relatively static over time. Keeping the example rule-base from the previous section in mind, suppose we have a target predicate P, and have a set S of implications I(k) of the form A(k, 1) & A(k, 2) & ... & A(k, n(k)) ==> P <s_k> where each A(k, i) is some predicate (e.g., an emotion, personality trait, or context), and obviously A(k, j)=A(m, i) sometimes. Then this set may be used to spawn a Bayes net that may be used to infer the truth-value of A(k+1, 1) & A(k+1, 2) & ...", Probabilistic Logic Networks,chapter 9
"& A(k+1, n(k+1)) ==> P The methodology for learning the Bayes net is as follows: 1. The set of implications I(k) is transformed into a data table D. 2. This data table is used to generate a Bayes net, using standard Bayes net structure and parameter learning methods Once the Bayes net has been learned, then the standard Bayes net belief propagation method can be used to perform the needed inference. The weakness of this approach is that Bayes net structure learning is a slow process that must be re-done whenever the relevant set S of implications changes considerably. However, once the Bayes net is in place, then the inference step may be done very rapidly. The only nonstandard step here is the production of the data table D from the set S of implications. This is to be done as follows: • The first column header of D corresponds to the target predicate P.", Probabilistic Logic Networks,chapter 9
"The subsequent column headers of D correspond to the set of unique predicates found on the left-hand side of the predicates I(k) in S. • The truth-value strengths s are discretized into a small number d of valk ues, say 11 values. So, each s is replaced with a value sd, which is of the k k form r/(d-1) for r = 0,...,(d-1). • Then, each implication I(k) is used to generate d rows of the data table D. ! In sd * d of these rows the first (P) column has a 1; in k the remaining (1- sd)*d rows it has a 0. k ! In the other columns a 0 value is placed except for those columns corresponding to the predicates that appear as A(k, i) on the left-hand side of I(k).", Probabilistic Logic Networks,chapter 9
"     196 Probabilistic Logic Networks for instance, we have 200 implications in S, involving 30 predicates, and a discretization of strength values into 10 values, then we have a binary data table with 31 columns and 2000 rows. This data table may then be fed into a standard Bayes net learning algorithm. Finally, what do we do if we have an implication of the form (A(k, 1) || A(k, 2))& ... & A(k, n(k)) ==> P <s_k> with a disjunction in it? In general, we could have any Boolean combination on the left-hand side. If there’s just a single disjunction as in the above example, it can be tractably handled by merely doubling the number of data rows created for the implication, relative to what would be done if there were no disjunction.", Probabilistic Logic Networks,chapter 9
"But more broadly, if we have a general Boolean formula, we need to take a different sort of approach. It seems we can unproblematically create rows for the data table by, in the creation of each row, making a random choice for each disjunction. It might be better (but is not initially necessary) to put the formulas in a nice normal form such as Holman’s ENF (Holman 1990; Looks 2006) before doing this. 9.5 Trail-Free Inference via Function Optimization or Experience-Driven Self-Organization (Co-authored with Cassio Pennachin) Finally, this section suggests a yet more radical approach to the problem of doing inference on a large number of logical relationships, with minimal error yet reasonably rapidly. The approach suggested is not trivial to implement, and has not yet been experimented with except in very simple cases, but we are convinced of its long-term potential.", Probabilistic Logic Networks,chapter 9
"We will focus here on the simple experiments we’ve carried out, then at the end talk a little about generalization of the ideas. In the experiments we conducted, we dealt only with Inheritance relationships joining ordinary, first-order terms. Also, we considered for simplicity Atoms labeled with simple (strength, weight of evidence) = (s, w) truth-values. The only inference rules we considered were deduction, revision, and inversion. For revision we used an iterated revision rule called “revision-collapse,” which acts on an Atom and collapses the multiple versions of the Atom into a single Atom (using the revision TV formula). Define a selection operator as a function that maps a list of Atoms into a proper subset of that list; e.g., “select all relationships,” “select all relationships that have multiple versions.” Define an inference protocol as a function that maps a list of Atoms into another one, defined as a combination of selection operators and inference rules.", Probabilistic Logic Networks,chapter 9
"An inference protocol is a special case of an inference control strategy     Chapter 9: Large-Scale Inference Strategies 197 – the specialization consisting in the fact that in a protocol there is no adaptiv wherein the conclusion of one inference affects the choice of what inferences are done next. A real-world inference engine needs a sophisticated inference control strategy – a topic that will be raised in Chapter 13. But for the simple experiments described here, a basic and rigid inference protocol was sufficient. There is a “global revision” inference protocol that is defined as: Select all Atoms with multiple versions, and apply revision-collapse to them. The standard (forward chaining) FOI inference protocol without node probability inference (SP) is defined as follows: • Select all InhRelationships, and apply inversion to them. • Do global revision. • Select all InhRelationship pairs of the form (A --> B, B --> C), and apply deduction to them. • Do global revision.", Probabilistic Logic Networks,chapter 9
"The standard protocol with node probability inference (SPN) is defined by adding the two steps at the end: • Do node probability inference. • Do global revision. One may generate a vector of TVs corresponding to a set of Atoms. Using (s, w) truth-values, this becomes an n-dimensional vector of pairs of floats. Taking the protocol SP as an example, given an Atom-set AS(0) we may define AS(n+1) = SP( AS(n) ) as the Atom-set resulting from applying the protocol SP to the Atom-set AS(n). This series of Atom-sets implies a series of TV-vectors V(n) = v(AS(n)) where v is the mapping taking Atom-sets into TV-vectors.", Probabilistic Logic Networks,chapter 9
"Potentially the series of AS’s may converge, yielding a fixed point AS* = SP(AS*) In this case we may say that the corresponding TV-vector V* represents a TV assignment that is consistent according to the inference protocol SP (or, SPconsistent). A problem, however, is that V* need not be the closest SP-consistent TV-vector to the original TV vector V(0). Similarly, we may define an e-consistent (e-SP-consistent) Atom-set as one for which     198 Probabilistic Logic Networks v(AS)""v*(SP(AS)) <e The degree of SP-inconsistency of AS may be defined as the minimum e for which the above inequality holds. If AS’s degree of SP-inconsistency is 0, then ! AS is SP-consistent.", Probabilistic Logic Networks,chapter 9
"If the series AS(n) converges to a fixed point, then for any e there is some N so that for n>N, AS(n) is e-consistent. However, these e-consistent Atom-sets along this series need not be the closest ones to AS(0). What is the most sensible way to compute the distance between two TVvectors (and, by extension, between two Atom-sets)? One approach is as follows.", Probabilistic Logic Networks,chapter 9
"Assume we have V1=((s11,c11),(s12,c12), K,(s1n,c1n)) V2=((s21,c21),(s22,c22), K,(s2n,c2n)) Then we may define ! d(V1,V2)=#[c1i*c2i* s1i""s2i] i This is a 1-norm of the difference between the strength components of V1 and V2, but with each term weighted by the confidences of the components being differ! enced. One approach to inference, then, is to explicitly search for an Atom-set whose SP-consistent (or e-SP-consistent) TV-vector is maximally close to AS(0). This might in a sense seem less natural than doing iterated inference as in the series AS(n) described above.", Probabilistic Logic Networks,chapter 9
"However, the iterated inference series involves a compounding of errors (the errors being innate in the independence assumptions of the deduction and revision TV formulas), which can lead to a final attractor quite distant from the initial condition in some cases. One way to avoid this is to introduce inference trails – a strategy which effectively stops the iteration after a finite number of steps by using a self-halting inference protocol that incorporates trails, and is hence more complex than SP or SPN described above. Another approach is the one suggested here: use an optimization algorithm to find a conclusion TV-vector satisfying specified criteria. There are several possible ways to formalize the optimization problem. Define x to be a semi-minimum of a function F if there is some neighborhood N about x so that for all y in N, F(y)""F(x); or an e-semi-minimum if F(y)""F(x)#e.", Probabilistic Logic Networks,chapter 9
"Then we may, for instance, look for: • The AS that is a semi-minimum of SP-inconsistency that is closest to A!S (0) !     Chapter 9: Large-Scale Inference Strategies 199 • For fixed e, the AS that is an e-semi-minimum of SP-inconsistency t closest to AS(0) • The AS that minimizes a weighted combination of inconsistency and distance from AS(0) • The AS that minimizes a weighted combination of {the smallest e for which AS is an e-semi-minimum of SP-inconsistency} and distance from AS(0) Let us call this approach to the dynamics of inference optimization-based inference (OBI). One obvious weakness of the OBI approach is its computational cost. However, the severity of this problem really depends on the nature of the minimization algorithm used.", Probabilistic Logic Networks,chapter 9
"One possibility is to use a highly scalable optimization approach such as stochastic local search, and apply it to a fairly large Atom-set as a whole. This may be viable if the Atom-set in question is relatively static. However, another interesting question is what happens if one takes a large Atom-set and applies OBI to various subsets of it. These subsets may overlap, with the results of different OBI runs on different overlapping subsets mediated via revision. This approach may be relatively computationally tractable because the cost of carrying out OBI on small Atom-sets may not be so great. For instance, one experiment we carried out using this set-up involved 1000 random “inference triangles” involving 3 relationships, where the nodes were defined to correspond to random subsets of a fixed finite set (so that inheritance probabilities were defined simply in terms of set intersection). Given the specific definition of the random subsets, the mean strength of each of the three inheritance relationships across all the experiments was about .", Probabilistic Logic Networks,chapter 9
"3. The Euclidean distance between the 3-vector of the final (fixed point) relationship strengths and the 3-vector of the initial relationship strengths was roughly .075. So the deviation from the true probabilities caused by iterated inference was not very large. Qualitatively similar results were obtained with larger networks. Furthermore, if one couples together a number of inference triangles as described in the above paragraph, revising together the results that different triangles imply for each shared relationship, then one obtains similar results but with lower correlations – but still correlations significantly above chance. 9.5.1 Approximating OBI via Experience-Driven SelfOrganization The same thing that OBI accomplishes using an optimization algorithm may be achieved in a more “organic” way via iterated trail-free inference in an experiential system coupled to an environment. Harking back to the “experiential semantics” ideas of Chapter 3, one way to think about OBI is as follows.", Probabilistic Logic Networks,chapter 9
"Considering the     200 Probabilistic Logic Networks ase studies above, of simple terms interrelationshiped by Inheritance relationships, we may imagine a set of such terms and relationships dynamically updated according to the following idea: 1. Each term is assumed to denote a certain perceptual category. 2. For simplicity, we assume an environment in which the probability distribution of co-occurrences between items in the different categories is stationary over the time period of the inference under study. 3. We assume the collection of terms and relationships has its probabilistic strengths updated periodically, according to some “inference” process. 4. We assume that the results of the inference process in Step 3 and the results of incorporating new data from the environment (Step 2) are merged together ongoingly via a weighted-averaging belief-revision process. This kind of process will have qualitatively the same outcome as OBI.", Probabilistic Logic Networks,chapter 9
"It will result in the network drifting into an “attractor basin” that is roughly probabilistically consistent, and is also pretty close to the data coming in from the environment. The key thing in this picture is the revision in Step 4: It is assumed that, as iterated inference proceeds, information about the true probabilities is continually merged into the results of inference. If not for this, Step 3 on its own, repeatedly iterated, would lead to noise amplification and increasingly meaningless results. But in a realistic inference context, one would never simply repeat Step 3 on its own. Rather, one would carry out inference on a term or relationship only when there was new information about that term or relationship (directly leading to a strength update), or when some new information about other terms/relationships indirectly led to inference about that term-relationship.", Probabilistic Logic Networks,chapter 9
"With enough new information coming in, an inference system has no time to carry out repeated, useless cycles of inference on the same terms/relationships – there are always more interesting things to assign resources to. And the ongoing mixing-in of new information about the true strengths with the results of iterated inference prevents the pathologies of circular inference, without the need for a trail mechanism. The conclusion pointed to by this line of thinking is that if one uses an inference control mechanism that avoids the repeated conduction of inference steps in the absence of infusion of new data, issues with circular inference are not necessarily going to be severe, and trails may not be necessary to achieve reasonable term and relationship strengths via iterated inference. Potentially, circular inference can occur without great harm so long as one only does it when relevant new data is coming in, or when there is evidence that it is generating information.", Probabilistic Logic Networks,chapter 9
"This is not to say that trail mechanisms are useless in computational systems – they provide an interesting and sometimes important additional layer of protection against circular inference pathologies. But in an inference system that is integrated with an appropriate control mechanism they are not necessarily required. The errors induced by circular inference, in practice, may be smaller than many other errors involved in realistic inference.", Probabilistic Logic Networks,chapter 9
"  Chapter 10: Higher-Order Extensional Inference Abstract In this chapter we significantly extend the scope of PLN by explaining how it applies to what we call “higher-order inference” (HOI) – by which we mean, essentially, inference using variables and/or using relationships among relationships. One aspect of HOI is inference using quantifiers, which is deferred to the following chapter because in the PLN approach it involves its own distinct set of ideas. 10.1 Introduction The term “higher-order” is somewhat overloaded in mathematics and computer science; the sense in which we use it here is similar to how the term is used in NARS and in functional programming, but different from how it is traditionally used in predicate logic. In the theory of functional programming, a “higher order function” refers to a mathematical function whose arguments are themselves functions. Our use of the term here is in this spirit.", Probabilistic Logic Networks,chapter 10
"PLN-HOI involves a diverse assemblage of phenomena, but the essential thing that distinguishes HOI from FOI is a focus on relationships between relationships rather than relationships between simple terms. As an example of relationships between relationships, much of PLN-HOI deals with relationships between logical predicates, where the latter may be most simply understood as functions that take variables as arguments (though there is also an alternative interpretation of predicates using combinatory logic, in which predicates are higher-order functions and there are no variables. Here we will generally adopt the variable-based approach, though the concepts presented also apply in the combinatory-based approach). Logical relationships between predicates (variable-bearing or no) are then an example of relationships among relationships. In the context of PLN-HOI, the boundary between term logic and predicate logic becomes somewhat blurry and the approach taken is best described as an amalgam. Logical predicates are used centrally and extensively, which is obviously reminiscent of predicate logic.", Probabilistic Logic Networks,chapter 10
"On the other hand, the basic term logic inference rules (such as Aristotelian deduction) also play a central role and are extended to handle Implication relationships between predicates rather than Inheritance relationships between simple terms. This synthetic approach has been arrived at primarily for practical reasons; sticking with pure predicate or term logic (or pure combinatory logic, or any other available “pure” approach) appeared to introduce unnatural complexity into some intuitively simple inferences.     202 Probabilistic Logic Networks present approach, inferences that appear intuitively simple tend to come out formally simple (which is one of the qualitative design guidelines underlying PLN as a whole). PLN-HOI, like PLN-FOI but more so, is a set of special cases. There are many HOI inference rules. However, there are not as many new truth-value formulas as in FOI, though there are some.", Probabilistic Logic Networks,chapter 10
"The PLN approach is to reduce higher-order inferences to first-order inferences wherever possible, via mapping predicates into their SatisfyingSets. This allows the theory of probabilistic first-order inference to naturally play the role of a theory of probabilistic higher-order inference as well. We will begin here by giving some basic PLN rules for dealing with relationships between predicates and between first-order relationships. Then as the chapter develops we will broach more advanced topics, such as Boolean operators, variable instantiation, and combinatory logic based higher-order functions. The following chapter continues the story by extending HOI to handle universal, existential, and fuzzy quantifiers. 10.2 PLN-HOI Objects Before getting started with higher-order inference proper, we briefly review some of the knowledge representation that will be used in the HOI rules. Most of this material was presented earlier in the chapter on knowledge representation and is briefly repeated here with a slightly different slant.", Probabilistic Logic Networks,chapter 10
"A Predicate, in PLN, is a special kind of term that embodies a function mapping Atoms into truth-values. The truth-value of a predicate is similar to the term probability of an ordinary “elementary” term. It can be estimated using term probability inference, or evaluated directly. When evaluated directly it is calculated by averaging the truth-value of the predicate over all arguments. In practice, this average may be estimated by using all known Evaluation relationships in the reasoning system’s memory regarding this predicate. In cases where lots of effort is available and there aren’t enough Evaluation relationships, then a random sampling of possible inputs can be done. All the inference rules for predicates work the same way for multi-argument predicates as for single-argument predicates. This is because a multi-argument predicate may be treated as a single-argument predicate whose argument is a list (multi-argument predicates may also be handled via currying, but discussion of this will be deferred till a later subsection).", Probabilistic Logic Networks,chapter 10
"Let, for instance, (Ben, Ken) denote the list alternatively denoted List Ben Ken Then (using the inference rule for combining Evaluation and Implication relationships, to be given below) we may say things like     Chapter 10: Higher-Order Extensional Inference 203 Evaluation kiss (Ben, Ken) Implication kiss marry |Evaluation marry (Ben, Ken) an inference that goes just the same as if the argument of kiss were an elementary term rather than a list. We will also make use of relationships involving variables along with, or instead of, constant terms, for instance Evaluation do (Ben, $X) Such relationships may be implicitly considered as predicates, and are treated symmetrically with other predicates in all PLN rules. The above example may be treated as a predicate with a single variable, represented by the notation $X. This is a predicate that evaluates, for each argument $X, the degree to which it’s true that Ben does X.", Probabilistic Logic Networks,chapter 10
"Next, as well as predicates we will also make some use of objects called Schemata, which are terms that embody functions mapping Atoms into things other than TruthValue objects. Schemata come along with their own relationship types, Execution and ExecutionOutput (ExOut for short), which have the semantics that if a Schema F embodies a function f so that f(x)= y then ! ExOut f x = y Execution f x y In short, ExOut denotes function application, whereas Execution records the output of a function application. We have introduced a few additional relationship types to make HOI simpler – some of these mentioned above. There is SatisfyingSet which relates a predicate to the set of elements that satisfy it; e.g., SatisfyingSet isAnUglyPerson UglyPeople Then there’s Quantifier     204 Probabilistic Logic Networks with subclasses ForAll ThereExists e.g.", Probabilistic Logic Networks,chapter 10
"to express “There is some woman whom every man loves” one can write ThereExists $X: AND Inheritance $X woman ForAll $Y: Implication Inheritance $Y man Evaluation loves ($Y,$X) Finally, there’s BooleanRelationship with subclasses AND, OR and NOT which also has some other more specialized subclasses to be introduced later. We have just used AND, for instance, in the above example. The most straightforward use of the Boolean relationships is to join relationships or predicates, but there are also special variants of the Boolean relationships that exist specifically to join terms. 10.2.1 An NL-like Notation for PLN Relationships To make the text output of our current PLN implementation easier to understand, we have created a simple pseudo-NL syntax for describing PLN relationships. While not syntactically correct English in most cases, we have found this syntax generally much easier to read than formal notation. This notation will be used in some of the following examples.", Probabilistic Logic Networks,chapter 10
"The following table illustrates the NLlike syntax (which we will use in examples for the remainder of this section):     Chapter 10: Higher-Order Extensional Inference 205 NL-like Formal representation representation Inheritance Mushroom is fungus Concept mushroom Concept fungus Implication If isMushroom then isFungus Predicate isMushroom Predicate isFungus ListRelationship (yeast, mushroom, mold) Concept yeast Concept mushroom Concept mold Evaluation FriendOf(kenji,lu cio) Predicate friendOf List Concept kenji Concept lucio And And(yeast, mushroom, mold) Concept yeast Concept mushroom Concept mold Not Not yeast Concept yeast ExOut The result of applying friendOf Schema friendOf to lucio Concept lucio     206 Probabilistic Logic Networks 10.", Probabilistic Logic Networks,chapter 10
"3 Where HOI parallels FOI The simplest kind of HOI is the kind that exactly mirrors FOI, the only difference being a substitution of predicates for simple terms and a substitution of higher-order relationship types for their corresponding first-order relationship types. An example of this kind of inference is the following deduction involving the predicates is_Brazilian and eats_rump_roast: Implication is_Brazilian is_ugly Implication is_ugly eats_rump_roast |Implication is_Brazilan eats_rump_roast Similar things may be done with Equivalence relationships. This kind of HOI follows the FOI rules, where Implication behaves like Inheritance, Equivalence behaves like Similarity, and so forth. Further examples will be omitted here due to their obviousness, but many examples of such inference will occur in the sample inference trajectories given in Chapter 14 when we discuss application of the current PLN implementation to making a virtual agent learn to play fetch,. 10.", Probabilistic Logic Networks,chapter 10
"4 Statements as Arguments Next we discuss a case that lies at the border between first-order and higherorder inference. Some commonsense relationships, such as “know” and “say,” take statements as arguments. Some commonsense concepts, such as “fact” and “sentence,” take statements as instances. Because the embedded statements in these examples are treated just like ordinary terms, the PLN FOI rules treat them as other terms, though from a semantic point of view the inference is higher-order.", Probabilistic Logic Networks,chapter 10
"For example, “Ben believes that the Earth is flat” can be represented as Evaluation believe ( Ben , (Inheritance Earth flat_thing)) “The Earth is flat is a ridiculous idea” then becomes Inheritance <tv1> Inheritance Earth flat_thing ridiculous_idea By induction, from this statement and Ben’s belief in the flatness of the Earth,     Chapter 10: Higher-Order Extensional Inference 207 one may conclude Inheritance <tv2> SatisfyingSet (believe_curry Ben) ridiculous_idea and thus Evaluation <tv3> believe_curry Ben ridiculous_idea which becomes believe Ben ridiculous_idea <tv3> i.e., “Ben believes in ridiculous ideas.” 10.", Probabilistic Logic Networks,chapter 10
"5 Combining Evaluations and Implications Combining an Evaluation and an Implication yields an Evaluation, as in the example Evaluation is_American Ben <tv1> Implication is_American is_idiot <tv2> |Evaluation is_idiot Ben <tv3> The truth-value strength here is obtained via deduction, for reasons to be elucidated below. For simple truth values, the weight of evidence is a discounted version of the value obtained via deduction, where the discount (which we’ll call a “multiplier”) is the product of the M2ICountMultiplier and the I2MCountMultiplier. In the case of indefinite truth values, we replace the count multiplier with I2MIntervalWidthMultiplier and M2IIntervalWidthMultiplier. These width multipliers keep the means of the truth values constant while simply increasing the interval widths to account for losses in confidence.", Probabilistic Logic Networks,chapter 10
"This is a “heuristic” form of inference whose correctness is not guaranteed by probability theory, due to the use of the I2M and M2I heuristic inference rules. These rules are approximatively valid for “cohesive concepts” whose members share a lot of properties; they’re badly invalid for random concepts. We now run through the detailed logic of this inference rule, using the above illustrative example. First, by the definition of SatisfyingSet, we have:     208 Probabilistic Logic Networks Execution SatisfyingSet is_American Americans <1> |Member Ben Americans <tv1> (where “Americans” is the set of all Americans).", Probabilistic Logic Networks,chapter 10
"We may also define the SatisfyingSet of is_idiot: Execution SatisfyingSet is_idiot idiots <1> By the M2I (MemberToInheritance) heuristic we have Member Ben Americans <tv1> |Inheritance {Ben} Americans <tv2> where tv2 has the same strength as tv1 but, depending on truth-value type, a discounted count or wider interval. By deduction we then have Inheritance {Ben} Americans <tv2> Inheritance Americans idiots <tv3> |Inheritance {Ben} idiots <tv4> The I2M heuristic then yields Inheritance {Ben} idiots <tv4> |Member Ben idiots <tv5> where tv5 has the same strength as tv4 but a discounted count (i.e. a wider interval). From the definition of SatisfyingSet we then have Evaluation is_idiot Ben <tv5> 10.", Probabilistic Logic Networks,chapter 10
"6 Inference on Boolean Combinations Extensional Boolean operators may be defined on predicates in a straightforward way. For instance, if P and Q are Boolean predicates, then the degree to which (P AND Q) is true of a set S may be defined as the percentage of members x of S for which both P(x) and Q(x) are true. If P and Q are non-crisp predicates, then the degree to which (P AND Q)(x) is true may be defined in terms of fuzzy     Chapter 10: Higher-Order Extensional Inference 209 intersection, and the degree to which (P AND Q)(S) is true may be defined truth-value of the fuzzy intersection (P AND Q) evaluated over the set S. Extending these notions to terms is also straightforward, because a Concept term A may be considered in terms of a predicate whose SatisfyingSet is A.", Probabilistic Logic Networks,chapter 10
"In this way, Boolean operators on terms may be defined by means of Boolean operators on predicates. This is equivalent to the more direct approach of defining Boolean operation on Concept terms as set operations – according to which, e.g., the intersection of two Concept terms A and B is the Concept whose member-set is the intersection of the member-sets of A and B. 10.6.1 Inference on Boolean Compounds of Predicates First we give some heuristic rules for dealing with Boolean operators. These rules are generally bad approximations, so they are to be applied only when time is short or when there is no data to do direct evaluation. We illustrate them with simple examples: Inference Rule Truth-value Strength Formula is_Ugly isMale.tv.s = AND is_Male is_Ugly (isMale & isUgly).tv.s |- / isUgly.tv.s isMale is_Ugly isMale.tv.", Probabilistic Logic Networks,chapter 10
= (MUOR is_Male is_Ugly U)/(1-U) |- MU = (isMale OR is_Male isUgly).tv.s U = isUgly.tv.s isMale (isMale & isUgly).tv.s isUgly = isMale.tv.s * |- isUgly.tv.s isMale AND isUgly isMale (isMale & isUgly).tv.s is Ugly = isMale.tv.s + |- isUgly.tv.s - isisMale OR isUgly Male.tv.s * isUgly.tv.s isMale |- (NOT isMale).tv.s = 1NOT isMale isMale.tv.s     210 Probabilistic Logic Networks e subtle point about these rules is that a complex Boolean expression can be evaluated by applying these simple rules in many possible orders. Each order of evaluation allows the usage of different knowledge in memory., Probabilistic Logic Networks,chapter 10
"For instance, if we have is_Male AND is_Ugly AND is_Stupid and the memory contains information about is_Male AND is_Ugly and is_Ugly AND is_Stupid then the inference controller must choose whether to do the evaluation as (is_Male AND is_Ugly) AND is_Stupid or as is_Male AND (is_Ugly AND is_Stupid) Or, the system can do the evaluation both ways and then use revision to merge the results! Optimizing the evaluation order for large expressions is a hard problem. The approach taken in the current implementation is to use all evaluation orders for which the memory contains reasonably confident data, and then revise the results. 10.6.", Probabilistic Logic Networks,chapter 10
"2 Boolean Logic Transformations Next, there are logical transformation rules, which are given in ordinary Boolean logic notation as: (P""Q)#(¬P $Q) (P""Q)#¬(P $¬Q) (P ""Q)""((P #Q)$(Q# P)) ! ! and which may be executed in PLN without change of truth-value. ! For example, !     Chapter 10: Higher-Order Extensional Inference 211 isMale OR isUgly <s,w> may be transformed into Implication <s,w> ExOut NOT isMale isUgly or else Implication <s,w> P isUgly where P is the predicate whose internal function is “NOT isMale”; or else, with variables, Implication <s,w> Evaluation (ExOut NOT isMale) X Evaluation isUgly X 10.6.2.", Probabilistic Logic Networks,chapter 10
"1 Boolean Operators for Combining Terms As already noted, the simple Boolean operators discussed above, though presented in the context of combining predicates, may also be used to combine elementary terms, according to the strategy of considering a term as the SatisfyingSet of a predicate and then combining the terms via combining these predicates. Or, equivalently, one may introduce separate operators for acting on terms, defined, e.g., by extensional intersection: (A AND B) Ext Equivalence Member x (A AND B) Ext (Member x A) AND (Member x B) extensional difference: (A MINUS B) Ext Equivalence Member x (A MINUS B) Ext (Member x A) AND NOT (Member x B)     212 Probabilistic Logic Networks extensional union: (A OR B) Ext Equivalence Member x (A OR B) Ext (Member x A) OR (Member x B) A number of conclusions immediately follow from these definitions; e.", Probabilistic Logic Networks,chapter 10
"., Equivalence Subset x (A AND B) Ext (Subset x A) AND (Subset x B) Implication (Subset x A) OR (Subset x B) Subset (A AND B) x Ext To understand the importance of the latter implication, consider a simple example. Assume there is a term for “Bank” and a term for “American Company,” and we want to make a new term that includes the common instances of the two (so intuitively what we get is “American Bank”). Obviously, the extension of the new term should be built by the intersection of the extensions of “Bank” and “American Company” as arguments, because a term is an instance of “American Bank” if and only if it is a “Bank” and it is an “American Company.” This is what the extensional intersection does.", Probabilistic Logic Networks,chapter 10
"On the other hand, as the common subset of “Bank” and “American Company,” the supersets of “American Bank” include both the supersets of “Bank” and those of “American Company,” so the intension of the new term should be the union of the intensions of “Bank” and “American Company.” For example, “American Bank” is a kind of “Financial Institution” (because “Bank” is a kind of “Financial Institution”), though “American Company” is not a kind of “Financial Institution.” If we use the intersection (or union) operator for both the extension and intension we can still get a term, but it has no natural interpretation in terms of its components. Fortunately, elementary set theory behaves in the appropriate way and we have the implication given above, which says that the extensional intersection belongs to categories defined as the disjunction of the sets of categories to which its components belong.", Probabilistic Logic Networks,chapter 10
"Regarding extensional difference, note that Equivalence Subset x (A MINUS B) AsymExt (Subset x A) AND NOT (Subset x B) Equivalence Subset (A MINUS B) x AsymExt Subset A x     Chapter 10: Higher-Order Extensional Inference 213 The asymmetric extensional difference A MINUS B contains things that are in Ext the extension of A but not in the extension of B. Whether these things will belong to the same categories as B or not, we can’t say, which is why there is no Subset B x term in the second implication given above. Regarding union, we have: Equivalence Subset x (A OR B) AsymExt (Subset x A) OR (Subset x B) Equivalence (Subset A x) OR (Subset B x) Subset (A OR B) x AsymExt These term logical operators may be used inside an inference system in two different ways.", Probabilistic Logic Networks,chapter 10
"They may be used as operators inside larger implications, or they may be used as heuristics for “seeding” the creation of new concepts. For instance, given the existence of the concepts of American Company and Bank, implications of the form Implication American_Company AND Bank AsymMix Y may be learned. Or, a new concept C may be formed, initially defined as American_Company AND Bank AsymMix but given the possibility of “drifting” over time via forming new relationships, gaining new members, etc. Both strategies have their value. 10.7 Variable Instantiation As we’ve noted, variables are not necessary to PLN, in the sense that PLN-HOI also works with combinatory logic operators, which avoid the need for variables via use of higher-order functions. We will discuss this approach in Section 10.9 and elsewhere below.", Probabilistic Logic Networks,chapter 10
"However, in our practical implementation of PLN we have found variables quite useful, and so we have used them in several of the previous subsections of this chapter on HOI, and will continue to use them in later sections and chapters; and we will now describe some rules specifically oriented toward manipulating variables.     214 Probabilistic Logic Networks riable instantiation refers to the inference step wherein one takes a relationship involving variables and then inserts specific values for one of the variables. Of course, instantiating multiple variable values in a relationship can be handled by repeated single-variable instantiations. A useful truth-value function for variable instantiation can be derived as a consequence of several truth-value functions previously derived.", Probabilistic Logic Networks,chapter 10
"In general, suppose we have F($X) <tv> where $X is a variable, and we instantiate this to F(A) <tv > 1 The most direct way to do this is to observe that s = P( F($X) | $X = A) 1 The most accurate and conceptually coherent method for estimating this conditional probability is to include both direct and indirect evidence. We can accomplish this by interpreting s = P( F($X) | Similarity $X A) 1 so that we have Implication AND Evaluation F $X Similarity A $X Evaluation F A To turn this interpretation into a concrete formula, it suffices to break down the proposed inference step into a series of substeps that are equivalent to previously discussed inference rules.", Probabilistic Logic Networks,chapter 10
"What we need is to find a truth-value for the conclusion of the inference: Similarity A B Evaluation F B |Evaluation A B But this inference can be transformed, via two applications of the M2I inference rule, into     Chapter 10: Higher-Order Extensional Inference 215 Similarity A B Inheritance B (SatisfyingSet F) |Inheritance A (SatisfyingSet F) which can be transformed into the following via the sim2inh rule: Inheritance A B Inheritance B (SatisfyingSet F) |Inheritance A (SatisfyingSet F) And now we have arrived at an ordinary deductive inference. So according to this line of reasoning, the truth-value of the variableinstantiation inference conclusion should be: F(A).tv = I2M ( deduction( M2I(F.tv), sim2inh( (Similarity A $X).", Probabilistic Logic Networks,chapter 10
"tv) ) Note that in the above derivation, the assumption is made that the SatisfyingSet of F is a “cohesive” set; i.e., that the elements within it tend to be fairly similar to each other. Without this assumption this approach doesn’t work well, and there’s no way to say anything about F(A) with a reasonably high confidence. PLN automatically accounts for this lack of confidence via the count or interval width multipliers in the M2I/I2M rule, which depend on how cohesive the set involved is. More cohesive sets involve smaller multipliers. The right multiplier for each set may be computed empirically if elements of the set are known, or else it may be computed by inference via the assumption that similar sets will tend to have similar cohesions and therefore similar interval width multipliers.", Probabilistic Logic Networks,chapter 10
"As a particular example of the above inference rule, consider the following two inferences, expressed in pseudo-NL notation: 1) If isDead($X) then there exists $Y so that killed($Y,$X) <[0.1, 0.3], 0.9, 10> |If isDead(Osama) then there exists $Y so that killed($Y,Osama) tv 1 2) If isDead($X) & isTerrorist($X), then there exists $Y so that killed($Y,$X) <[0.5, 0.7] , 0.", Probabilistic Logic Networks,chapter 10
"9, 10>     216 Probabilistic Logic Networks If isDead(Osama) & isTerrorist(Osama), then there exists $Y so that killed($Y,Osama) tv 2 These inferences include quantifiers, which won’t be discussed until the following chapter, but that’s not the main point we need to consider here. Assuming [ Similarity $X Osama ].tv = <[0,15, 0.25], 0.95, 10> and assuming M2I/I2M interval width multipliers of 1.5 for both inferences, then we obtain tv = <[0.123024, 0.486104], 0.9, 10> 1 tv = <[0.235028, 0.570565], 0.", Probabilistic Logic Networks,chapter 10
"9, 10> 2 On the other hand, if we use the fact that the premise of the second inference involves a more cohesive set than the premise of the first inference, then that means that the second inference involves a lesser M2I/I2M interval width multiplier than the first one. In fact the set of $X who are dead terrorists is much smaller and much more cohesive than the set of $X who are dead entities. For dead terrorists (inference 2) we may plausibly assume an M2I/I2M interval width multiplier of 1.2. In this case we obtain the revised value tv = <[0.259443, 0.574509], 0.9, 10> 2 10.7.1 Implications Between Non-Variable-Bearing Relationships Now we present an interesting, somewhat conceptually subtle application of the variable-instantiation formula.", Probabilistic Logic Networks,chapter 10
"We show that it is possible to construct higherorder relationships such as implications that relate simple relationships without variables – and without any higher-order functions being involved. The semantics of these relationships, however, is subtle and involves “possible worlds.” The truth-values of these relationships can be derived via a combination of the possible-worlds interpretation with the variable-instantiation truth-value formula given above. For simplicity, we will discuss this formula in the context of simple truthvalues, but extrapolation to indefinite truth-values is not difficult. As an example, consider Implication Ben is ugly Ben eats beef How can we define the truth-value of this?     Chapter 10: Higher-Order Extensional Inference 217 First, we suggest, the most useful way to interpret the expression is: Implication In possible universe C, Ben is ugly In possible universe C, Ben eats beef Note that this reinterpretation has a variable – the possible universe C – and hence is interpretable as a standard Implication relationship.", Probabilistic Logic Networks,chapter 10
"But we may interpret, e.g., In possible universe C, Ben is ugly as meaning Inheritance (Ben AND “possible universe C ”) ugly In other words, one may view Ben as a multiple-universe-spanning entity, and look at the intersections of Ben with the possible universes in the multiverse. Suppose then that we know Implication Inheritance $X ugly Inheritance $X beef_eater Variable instantiation lets us derive from this Implication Inheritance (Ben AND “possible universe C ”) ugly Inheritance (Ben AND “possible universe C”) beef_eater which is equivalent by definition to Implication Inheritance Ben ugly Inheritance Ben beef_eater So, in short, we may quantify inheritance between non-variable-bearing relationships using the variable instantiation formula. 10.", Probabilistic Logic Networks,chapter 10
"8 Execution Output Relationships As noted above, if we have a function that outputs an Atom, then we use an ExecutionOutput (ExOut for short) relationship to refer to the Atom that’s output,     218 Probabilistic Logic Networks Execution relationship to denote the ternary relationship between schema, input, and output. Here we describe a few basic rules for manipulating ExOut and Execution relationships. First, note that we have the tautology Execution $S $X (ExOut $S $X) <1,1> For example, if we want to say “The agent of a killing situation is usually Armenian” then we may say Implication Inheritance $x killing Inheritance ExOut Agent $x Armenian which is equivalent to Implication Inheritance $x killing Implication Execution Agent $x $y Inheritance $y Armenian In this example, the ExOut relationship allows us to dispense with an additional relationship and an additional variable.", Probabilistic Logic Networks,chapter 10
"There is a general rule Equivalence <1,1> Evaluation F (ExOut A B) AND Execution A B $Y <1,1> Evaluation F $Y that may be used to convert expressions with ExOuts into expressions without them. Finally, an alternative way of saying ExOut Agent $X is to construct a predicate P whose internal predicate-function evaluates “Agent $X”. Then we have Implication Inheritance $X killing     Chapter 10: Higher-Order Extensional Inference 219 Inheritance Evaluation P $X Armenian 10.9 Reasoning on Higher-Order Functions PLN-HOI is capable of dealing with functions or relationships of arbitrarily high order, with relationships between relationships ... between relationships, or functions of functions ... of functions.", Probabilistic Logic Networks,chapter 10
"This property implies that a fully capable PLN can be created without any use of variables at all, due to the possibility of using combinatory logic to represent mathematical expressions that would normally be denoted using variables, in an entirely variable-free way. In practice, it seems that the use of variables is often worthwhile because it allows more compact representation of useful higher-order relationships than would be possible without them. However, there are also many cases where curried relationships of the style common in combinatory logic and combinatory logic based programming languages are useful within PLN, due to their own ability to represent certain types of relationship with particular compactness. As a simple example of a case where the curried representation is semantically natural, consider the schema “very” which maps a predicate P into the predicate “very P.” Then we might have Implication ExOut very $X $X from which we can draw the conclusion e.g.", Probabilistic Logic Networks,chapter 10
"Implication ExOut very isMale isMale Note that the original statement is equivalent to Implication ExOut (ExOut very $X) $Y ExOut $X $Y Alternately, we can do the same thing with combinators rather than variables. To make this work nicely we want to use the curried version of Implication, in which, e.g.,     220 Probabilistic Logic Networks (Evaluation (ExOut Implication_curried A) B).tv where the standard Implication relationship might also be called Implication_list, since it is in effect a predicate that takes a list argument. The above example becomes Evaluation ExOut Implication_curried ExOut very $X ExOut I $X using the I combinator.", Probabilistic Logic Networks,chapter 10
"To eliminate the variable we use the definitions S f g x = (fx) (gx), so B f g x = f(gx) which lets us reason as follows, using Imp as a shorthand for Implication_curried: (Imp (very x)) (I x) = ((B Imp very) x) (I x) = S (B imp very) I x In term and relationship notation, this final result would be written ExOut ExOut S A I A := ExOut ExOut B imp very Note the cost of removing the variables; we’ve introduced a few extra terms and relationships. Whether the variable-free or variable-bearing form is better depends on the kind of reasoning you’re doing. Removing variables allows one to use simpler inference control strategies, but generally increases the size of the structures being reasoned on. The variable-free, curried approach is useful only in cases where this size increase is not too severe.", Probabilistic Logic Networks,chapter 10
"  Chapter 10: Higher-Order Extensional Inference 221 10.10 Unification in PLN1 The concept of unification is an important part of theoretical computer science, and also plays a key role in AI; for example, via its central role in the Prolog language (Deransart et al 1996) and its role in computational linguistics (in unification grammars) (Roark and Sproat 2007.) This section explores the manifestation of unification in PLN, which is relatively simple but different from usual treatments of unification due to the lack of focus on variables and the central role played by uncertainty. To illustrate the nature of unification in PLN, let us introduce an example involving a Predicate P relating the expression values of two genes (XYZ and ABC) at different times: P($T) := Implication expression(XYZ,$T)>.5 AND expression(ABC,$T+1)<.", Probabilistic Logic Networks,chapter 10
"5 expression(XYZ,$T)<.2 and a related Predicate Q: Q := Implication expression(XYZ, $T)>.9 expression(XYZ, $T)<.4 Now, it is not hard to compute the truth-value of the relationship Implication P Q by referring to the actual underlying data points, assuming one has access to this data. But what if one wants to compute this truth-value without doing this, just by manipulating P and Q? To do this, one has to “unify” the two expressions. We may write the two expressions as Implication L R 1 1 Implication L R 2 2 It is possible to find L and R, with nontrivial truth-values, so that Implication L L <t > 1 1 Implication L L <t > 2 2 1 This section was coauthored with Guilherme Lamacie.", Probabilistic Logic Networks,chapter 10
"  222 Probabilistic Logic Networks <t > 1 3 Implication R R <t > 2 4 also with nontrivial truth-values. Specifically, L($T) := [ expression(XYZ, $T) > .9 ] R($T) := [ expression(XYZ, $T) < .2 ] will work. Induction then yields Implication L L <t > 1 2 5 Implication R R <t > 1 2 6 which then yields Implication <t > 7 Implication L R 1 1 Implication L R 2 2 by HOI rules. The “unification” step here is the determination of L and R; these terms “unify” the two original expressions. In this case, because of the specialized nature of the expressions, the unifying expressions are not hard to find. In general, finding unifying expressions is not so simple. 10.10.", Probabilistic Logic Networks,chapter 10
"1 PLN Unification versus Prolog Unification It may not be immediately clear how this process we’ve called “unification” relates to unification as conventionally done in mathematical logic, for instance in the Prolog programming language. A few brief comments in this regard may be valuable. We will use Prolog notation freely, and so this section may not be fully comprehensible to the reader who’s not familiar with Prolog; for necessary background the reader is referred to (Spivey 1996.", Probabilistic Logic Networks,chapter 10
"A typical Prolog unification might involve a query such as ? son(A, sam), sister(B, becky), grandpa (A, B) and a database consisting of father(ted, ben) father(victor, ted) father(sam, victor) male(X) ! father(X, Y)     Chapter 10: Higher-Order Extensional Inference 223 grandpa(X, Z) ! father(X, Y), father(Y, Z) sister(ben, becky) son(X, Y) ! father(Y, X), male(X) To process this, son(A, sam) is unified with son(X, Y), with the substitution $s1 = {A/X_1, Y/sam}, yielding a third expression son(X_1,sam). This expression is then replaced with father(sam, X_1), male(X_1).", Probabilistic Logic Networks,chapter 10
"The first Predicate father(sam, X_1) is unified with father(sam, victor), by substitution $s2 = {X_1/victor}. Given this substitution, the next step is to try to solve male(victor), which is done by unifying it with male(X), by $s3 = {X/victor} and then solving the tail Predicate father(victor, Y) (i.e., unifying it with father(victor, ted)). Now, the next Predicate in the query sister(B, becky) is unified with sister(ben, becky), by $s4 = {B/ben}. Note that substitutions $s1,$s2 and $s4 cause the last Predicate in the query to be grandpa(victor, ben).", Probabilistic Logic Networks,chapter 10
"This can be now solved by unification with grandpa(X, Z), by $s5 = {X/victor, Z/ben}, and the third expression produces grandpa(victor, ben), which leads to the tail Predicates father(victor, Y), father(Y, ben), which are further solved analogously, thus yielding a proof of grandpa(Victor,Ben). In this type of unification, one compares two expressions to each other and tries to find variable values that will work in both expressions. The trick is in the order of processing of expressions; for this, Prolog uses a simple depth-first search with backtracking. PLN unification is a bit different, because there are not necessarily any variables, and implication relations are probabilistic rather than crisp. However, the above example can easily be done in PLN in several ways. Here we will show how it can be done by reformulating each expression in a variable-free way.", Probabilistic Logic Networks,chapter 10
"For instance grandpa(X, Z) ! father(X, Y), father(Y, Z) becomes Implication (B father father) grandpa (since B father father x y = father (father x) y). We then have a “database” consisting of Evaluation (father Ted) Ben Evaluation (father Victor) Ted Evaluation (father Sam) Victor Implication (B father father) grandpa Evaluation (sister Ben) Becky Implication son (C father) What is the “query”? Well, seeing that     224 Probabilistic Logic Networks son(a, Sam) =(son a) Sam = C I Sam (son a) = B (C I Sam) son a sister(b, Becky) = B (C I Becky) sister b the query is then just a search for terms a, b so that a ! SatisfyingSet (B (C I Sam) son ) b ! SatisfyingSet (B (C I Becky) sister) b ! S", Probabilistic Logic Networks,chapter 10
"atisfyingSet ( grandpa a) To put this more elegantly, we can introduce an argument list (a,b) and then define listify grandpa (a, b) = grandpa a b We are then looking for an element of SatisfyingSet( listify grandpa ) "" SatisfyingSet( (B (C I Sam) son, B (C I Becky) sister) ) i.e., for an element that makes the Predicate AND listify grandpa ( (B (C I Sam) son, B (C I Becky) sister) output the value True.", Probabilistic Logic Networks,chapter 10
"Instead of doing classical variable unification, what happens to find an element of the SatisfyingSet of this Predicate function? First, the “sister” term in the term B (C I Becky) sister is matched with the “sister” term in the known fact Evaluation (sister Ben) Becky So Ben is tried out as an argument of B (C I Becky) sister and is indeed found to cause this term to evaluate to True. Then, “son” is found not to match to anything concrete, but is found in the formula Implication son (C father) This formula is then used to create information such as Evaluation (son Ben) Ted Evaluation (son Ted) Victor     Chapter 10: Higher-Order Extensional Inference 225 Evaluation (son Victor) Sam The last of these then matches the term B (C I Sam) son.", Probabilistic Logic Networks,chapter 10
"On the other hand, there is no concrete match for grandpa in the given knowledge base, but there is a rule grandpa = B father father, which matches directly with the knowledge Evaluation (father Ted) Ben Evaluation (father Victor) Ted Evaluation (father Sam) Victor In this way the system is eventually led to discover that the pair (Ben, Victor) lies in the appropriate SatisfyingSet. This example illustrates the general principle that, in a variable-free context, logical unification comes down to finding elements in SatisfyingSets that are defined by intersections of the SatisfyingSets corresponding to Predicates of interest. The chief difference between this example and the earlier example involving gene expression relationships is absence of probabilistic truth-value in the present example. In Prolog-like unification, two terms either match or they don’t – i.e., an item is either a member of a given SatisfyingSet or it’s not.", Probabilistic Logic Networks,chapter 10
"In general PLN unification, this is not the case; there can of course be degrees of membership, which makes the unification process yet more computationally difficult. 10.10.2 General Unification The general problem of unification, in a PLN context, is as follows. Given two Predicates R and S, one wants to find a Predicate T so that SatisfyingSet(T)=SatisfyingSet(R)!SatisfyingSet(S) and SatisfyingSet(T) is as large as possible. Of course, one can nominally solve the problem by simply forming the Predicate T = R AND S But of course in doing this one has no real idea of the truth-value of R AND S, unless one makes a possibly fallacious independence assumption. In some cases, after all, R and S may contradict each other, so that SatisfyingSet(T) is empty.", Probabilistic Logic Networks,chapter 10
"The problem of unification, then, is basically the problem of making a dependence-     226 Probabilistic Logic Networks estimate of the truth-value of the compound R AND S. In practice, rather than evaluating T = R AND S explicitly, one may seek T so that Implication T (R AND S) <t> with t<1, but with the property that the truth-value of T can be relatively readily assessed based on the system’s knowledge. There is no known generally effective algorithm for solving this problem. There are only more or less plausible heuristics. For instance, suppose that R and S are both conjunctive compounds of the form R = R AND … AND R 1 n S = S AND … AND S 1 n Then one can seek to unify each R with someS . Of course, this is not all that i j easy. First of all there are n2 pairings to look at.", Probabilistic Logic Networks,chapter 10
"And for each one of these pairings one is stuck with a smaller, but possibly still difficult, unification problem. Here the terms R and S are effectively serving as the analogue of “variables” i j in Prolog-style unification. There are two profound difficulties here that are not seen in Prolog, however. First, there is no normal form for logical expressions here, so we don’t know which R should match with which S. Second, rather than i j just crisply substituting one “variable” for another, we have to evaluate probabilistic implications (truth-values of relationships of the form Implication T (R, AND i S). j The gene expression example given above is actually an easier case than the conjunctive compound case. A brief re-analysis of this example may be instructive at this point.", Probabilistic Logic Networks,chapter 10
"There we had two examples of the form Implication L R 1 1 Implication L R 2 2 Because these were obviously of the same format, it was reasonable to seek to unify L and L , and R and R respectively. No consideration of a large number of 1 2 1 2 permutations was required. Because of the particular form of the terms in this case, the intersections were not hard to find. For instance R AND R := 1 2 expression(XYZ,t)<.2 AND expression(XYZ,t)<.4 This is an expression that can be simplified easily since Implication R R 1 2 is given, and it’s known that     Chapter 10: Higher-Order Extensional Inference 227 AND( A, B) Implication A B |A so that R AND R = expression(XYZ,t)<.", Probabilistic Logic Networks,chapter 10
"2 1 2 The case L AND L 1 2 := AND expression(XYZ, t)>.5 AND expression(ABC, t+1)<.5 expression(XYZ, t)>.9 is not so simple, however. It can immediately be simplified to L AND L 1 2 := expression(XYZ, t)>.9 AND expression(ABC, t+1)<.5 To go further from here one has to either make some wild guesses or revert to actually evaluating this predicate on real data. The same goes for the further, more speculative reduction Implication expression(XYZ, t) > .9 L AND L 1 2 In practice, then, we propose that unification problems can be approached in PLN via a combination of explicit quantitative predicate evaluation, and transformation using the earlier-given predicate transformation rules.", Probabilistic Logic Networks,chapter 10
"The general rule is: keep on transforming until it doesn’t work anymore, and then start evaluating – where “doesn’t work anymore” means, essentially, that a precipitous drop in confidence has occurred as a result of risky transformations. 10.11 Inference on Embedded Relationships Two crucial cases of higher-order knowledge, requiring special treatment, are hypothetical and contextual knowledge. These are different cases, but closely related. Each of them represents knowledge that is stored in an AI system’s mem-     228 Probabilistic Logic Networks ut implicitly understood not to pertain to the universe at large in a straightforward way. PLN can handle these kinds of knowledge easily, so long as the proper knowledge representation structures are put in place. 10.11.1 Hypothetical Knowledge As a simple, archetypal example of hypothetical knowledge, consider the statement “Matt believes the Earth is flat.", Probabilistic Logic Networks,chapter 10
"Before presenting the way this is dealt with in PLN, we will discuss a couple of incorrect ways of presenting it to explain why we have chosen the course we have. First, the statement could facilely be represented as Evaluation believe Matt (Inheritance Earth flat) The problem with this, however, is that it involves entering a bogus piece of knowledge Inheritance Earth flat into the system. One might try to get around this problem by judiciously inserting truth-values, such as Evaluation <1> believe Matt (Inheritance Earth flat <0>) But this is not a good approach; immediately one faces difficulties. This expression has two possible interpretations: “Matt is certain that the Earth is not flat” “Matt is certain that the Earth is flat, but Novamente believes it is not flat” One might deal with this particular case by just accepting the second interpretation.", Probabilistic Logic Networks,chapter 10
"But this isn’t really a general way out, because it doesn’t give one a way to say “Matt is pretty certain that the Earth is almost surely flat.” If one tries to render this latter statement as Evaluation <.7> believe Matt (Inheritance Earth flat <.9>) then one is inserting the bogus knowledge (Inheritance Earth flat <.9>) into the system and there is no easy workaround.     Chapter 10: Higher-Order Extensional Inference 229 The solution we have taken in PLN is to explicitly introduce a representat “hypotheticalness” into the picture. We do this with the Hypothetical Atom, a unary relationship which has the informal semantics that Hypothetical A means “don’t assume A is true, just assume ‘Hypothetical A’ is true.” The interesting thing is, it’s not necessary to give PLN any kind of refined, explicit semantics for dealing with Hypothetical (sometimes Hyp for short).", Probabilistic Logic Networks,chapter 10
"Rather, Hyp is a marker. Inferences such as Evaluation believe Matt (Hyp (Inheritance Earth flat)) Evaluation believe Matt (Hyp (Inheritance Mars Earth)) |Evaluation believe Matt (Hyp (Inheritance Mars flat)) can be made automatically, just as if Hyp were any other relationship type. The special rule required is, quite simply, a rule stating that before drawing a conclusion L1 L2 |L3 it should be determined whether either L1 or L2 is hypothetical (is pointed to by a Hyp) or not. If so, then the inference is not done – though inferences of the form P (Hyp L1) P (Hyp L2) |P (Hyp L3) may be done, for appropriate P, as in the preceding example. This “inference inhibition” rule is the full explicitly encoded semantics of Hypothetical.", Probabilistic Logic Networks,chapter 10
"An interesting question is whether, for example, the entry of the relation Matt believes ( Hyp (Inheritance unicorns cute)) into the system causes the strength of the truth-value of the unicorn Term to be increased. In other words, if the system finds it’s often valuable to posit hypothetical unicorns, should this increase the system’s estimate of the probability of unicorns in “its world”? One may introduce a system parameter determining the amount     230 Probabilistic Logic Networks hypothetical mention of an Atom increases the TruthValue of that Atom, but ultimately this sort of thing must be dealt with differently in different contexts, hence it requires the learning of appropriate cognitive schemata. The combination of hypothetical and nonhypothetical information may also be useful, although it must be handled with care. For instance, suppose we want to reason Matt believes the Earth is flat Flat things are not round |Matt believes the Earth is not round Properly enough, there is no way to do this directly.", Probabilistic Logic Networks,chapter 10
"Rather, we need an additional premise such as Implication A (Matt believes Hyp(A)) or Implication (A AND (Inheritance A C)) (Matt believes Hyp(A)) Implication (Inheritance flat NOT(round)) C where C is some category of knowledge in which the system is fairly sure Matt believes, in spite of his eccentric beliefs in other domains. Human beings are quite adept at forming such categories C, and thus at managing the overlapping but nonidentical belief systems of themselves and other human beings. This is a skill that seems to be learned in middle childhood, along with other aspects of advanced inference and advanced social understanding. 10.11.2 Higher-Order Statements and Judgments The topic of hypothetical knowledge brings to the fore another issue that has more general relevance: the difference between higher-order statements and higher-order judgments. A higher-order statement is a relationship that treats its component relationships as truth-value-free entities.", Probabilistic Logic Networks,chapter 10
"A higher-order judgment is a relationship that treats its component relationships as truth-valued entities. An example of higher-order judgment is: Evaluation believe Matt (Hyp (Inheritance Earth flat <.9>))     Chapter 10: Higher-Order Extensional Inference 231 This statement is saying that Matt believes the Earth is flat with a .9 strength. Similarly, to say that Cassio does not believe the Earth is flat we could say, for instance, Evaluation believe List Cassio Hypothetical Inheritance Earth flat <.01> The presumption here is that the TruthValue of (Inheritance Earth flat) is part of the entity being fed to the believe Term as an argument.", Probabilistic Logic Networks,chapter 10
"On the other hand, what would be an example of a higher-order statement? When we say Implication AND Inheritance Earth flat Inheritance Earth planet Inheritance planet flat we actually don’t mean Implication AND InheritanceEarth flat <t1> Inheritance Earth planet <t2> Inheritance planet flat <t3> In other words, we don’t mean The earth is flat with TruthValue t 1 The earth is a planet with TruthValue t 2 |Planets are flat with TruthValue t 3 where t t t are the actual truth-values of the respective statements in the system. 1 , 2 , 3 Inference could construct statements like this, but then the truth-value t produced 3 by inference from t and t might not be the actual truth-value of Inheritance 1 2 planet flat in the system.", Probabilistic Logic Networks,chapter 10
"In fact it generally will not be, unless this inference step is the only thing that has ever affected the truth-value of this relationship. In our above examples of hypothetical relationships, we were assuming “higher-order judgment” (HOJ) style inference – that is, we were assuming rela-     232 Probabilistic Logic Networks ips were being referred to with truth-values intact. On the other hand, in our reasoning about flat planets, we were assuming “higher-order statement” (HOS) style inference; that is, we were assuming relationships were being referred to without specific truth-values attached. It seems that in hypothetical inference we often do want to assume the HOJ case. Otherwise, the HOS case is the most common case but not the exclusive case. There are, however, some cases where one might want to make nonhypothetical higher-order judgments.", Probabilistic Logic Networks,chapter 10
"For instance, to represent “I am positive that countries that are rich in oil are moderately wealthy,” one might use Implication <.99> AND Inheritance $X country Inheritance $X oil_rich Inheritance $X wealthy <.6> Here the <.99> is for the “positive”; the <.6> is for the “moderately.” Notationally, one may distinguish the two cases implicitly as follows: if a truth-value marker <t> is included in denoting an Atom that is part of another relationship, one may assume that HOJ is implied. Implementationally, to make the distinction one needs to introduce some kind of marker to distinguish the two cases. The marker should attach to logical relationships; e.g., Implication in the above example. We have chosen to make “higher-order statement” the default case, and require a special marker to denote “higher-order judgment.", Probabilistic Logic Networks,chapter 10
"Where extra notational explicitness is required we may denote this as, for instance, Implication_HOJ <.99> AND Inheritance X country Inheritance X oil_rich Inheritance X wealthy <.6> We have been focusing on cases involving relationships because this is the most common kind of higher-order judgment, but something similar can happen with Terms; e.g., one can say Inheritance_HOJ ugly <.1> beautiful <.8> meaning that things that are not very ugly are often very beautiful. However, this example is better said (to within a decent degree of approximation) as     Chapter 10: Higher-Order Extensional Inference 233 Inheritance ugly NOT beautiful Generally speaking, in the situations we have analyzed, cases where inter-term HOJ relationships are the most convenient alternative are even rarer than similar cases with interrelationship HOJ relationships. 10.11.", Probabilistic Logic Networks,chapter 10
"3 Contextual Knowledge Hypothetical knowledge is knowledge that the reasoning system may not believe at all; contextual knowledge, on the other hand, is knowledge that the system believes only in certain circumstances. For instance, suppose we want to say “Ben is competent in the domain of mathematics; Ben is incompetent in the domain of juggling.” What we mean here is something like Implication Ben AND doing_mathematics competent Implication Ben AND doing_juggling NOT competent Here we are saying that instances of Ben who are doing mathematics are competent (at what they are doing), whereas instances of Ben who are doing juggling are not competent (at what they are doing). Of course, this is only one among many possible representations of the posited conceptual relationships in terms of terms and relationships. In reality, there may be no single term for “competent” or “doing_juggling”, the given HOS’s may be represented as collections of HOJ’s, etc.", Probabilistic Logic Networks,chapter 10
"We have just chosen a particularly simple possible representation for expository purposes. The case of contextual knowledge is so common that we believe it is worthwhile to have a special representational mechanism just for dealing with it. For this reason we introduce Context, with the semantics Context C (Hyp (R X Y)) := R (X AND C) (Y AND C) Ext Ext Context C (Hyp (R X Y Z)) := R (X AND C) (Y AND C) (Z AND C) Ext Ext Ext     234 Probabilistic Logic Networks So for example Context doing_mathematics Hyp (Inheritance Ben competent) Context doing_juggling Hyp NOT (Inheritance Ben competent) Using the above definition of Context these translate into Implication Ben AND doing_mathematics Ext competent AND doing_mathematics Ext Implication Ben AND doing_juggling Ext NOT competent AND doing_juggling Ext which are equivalent to the forms given previously.", Probabilistic Logic Networks,chapter 10
"The value of the Context “macro” is that it simplifies inferences such as Context doing_mathematics Hyp (Inheritance Ben competent) Context doing_mathematics Hyp (Inheritance Gui competent) |Context doing_mathematics Hyp (Similarity Ben Gui) This kind of inference could be done without Context, using the explicit AND -heavy representations of the premise relationships. However, the exisExt tence of Context has the effect of pushing the system to make inferences of this type, whereas otherwise such inferences would be carried out only occasionally as part of the generic process of inference on Predicates. Now, an advanced AI system without Context could be expected to learn from experience that inference on Predicates of the “Context-ish” variety is useful, and create cognitive schema biasing the inference process toward combining Predicates of this type.", Probabilistic Logic Networks,chapter 10
"However, we     Chapter 10: Higher-Order Extensional Inference 235 feel that explicitly pushing the system to combine Contexts inferentially wil big help in getting any PLN-based reasoning system to the point where it’s able to do this kind of advanced cognitive schema creation. 10.12 An Application of PLN-HOI to Inference Based on Natural Language Information Extraction In this section we summarize an example of PLN-HOI that was published in (Ikle and Goertzel, 2007), which was previously carried out using PLN inference rules together with alternate inference formulas acting directly on SimpleTruthValues. The analysis given in (Ikle and Goertzel, 2007) was a modification of an earlier treatment given in (Goertzel et al 2006), which utilized heuristic weight of evidence formulas; the newer treatment utilized indefinite probabilities.", Probabilistic Logic Networks,chapter 10
"What we present here is a small and partial “inference trail” that is part of a larger trail described in (Goertzel et al 2006), which was part of an experiment in integrative NLP using an integrated system that • Parsed the sentences in PubMed abstracts using a grammar parser • Transformed the output of the grammar parser into logical relationships using a collection of expert rules • Performed probabilistic logical inference on these logical relationships using PLN The system, called Bioliterate, was created under a contract from the NIH Clinical Center and was specifically tuned to infer relationships between genes, proteins and malignancies. The overall inference of which this example is a part is depicted qualitatively in the following table, which shows the premises in the form of the actual sentences present in PubMed automatically extracted and used as premises. Importantly, bone loss was almost Premise 1 completely prevented by p38 MAPK inhibition.", Probabilistic Logic Networks,chapter 10
"Premise 2 Thus, our results identify DLC as a novel inhibitor of the p38 pathway and provide a molecular mechanism by which cAMP suppresses p38 activation and promotes apoptosis. Conclusion DLC prevents bone loss. cAMP prevents bone loss. The premises depicted in this table were automatically converted into sets of     236 Probabilistic Logic Networks l relationships using a set of hand-coded expert rules embodying linguistic knowledge, and PLN was used to draw the conclusion via forward chaining inference. The following table depicts a fragment of the overall inference trail, which basically gathers together a number of mutually relevant relationships within a single conjunction. Rule Premises Conclusion Inh inhib1, inhib <[.95, 1], .9> Abduction Inh inhib2, inhib <[.95, 1], .9> Inh inhib1, inhib2 <[0, 0.221708], .9> Similarity Eval subj (prev1, inhib1) <[.", Probabilistic Logic Networks,chapter 10
"95, 1], .9> Substitution Inh inhib1, inhib2 Eval subj (prev1, inhib2) <[.05, .44], .9> Inh inhib2, inhib Deduction Inh inhib, causal_event <[.95, 1], .9> Inh inhib2, causal_event <[.856, .998], .9> Inh inhib2, causal_event Inh prev1, causal_event <[.854, .998], .9> Eval subj (prev1, inhib2) AND Eval subj (inhib2, DLC) <[.95, 1], .9> AND <[0.0625, 0.358972], 0.", Probabilistic Logic Networks,chapter 10
"9> Inh inhib2, causal_event Inh prev2, causal_event Eval subj <prev2, inhib2> Eval subj <inhib , DLC> 2     Chapter 10: Higher-Order Extensional Inference 237 In the overall inference trail this conjunction is then used as a premise to a subsequent inference step, which uses unification to conclude that Eval subj (prev1, inhib2); i.e., that the prevention mentioned in Premise 1 is the subject of the inhibition mentioned in Premise 2. We next use reverse truth-value conversion to con vert the full <[L, U], b, k> truth-values into <s, n, b> triples for all of our premises, as well as for our conclusion. The truth-value conversion results are shown in the table: Premise <[L,U],b> <s,n,b> Inh inhib1 inhib <[0.95, 1], 0.", Probabilistic Logic Networks,chapter 10
"9> <.988, 28, .9> inhib <[0.95, 1], 0.9> <.988, 28, .9> 1 inhib <[0, 0.168], 0.8> <.083, 8, .8> inhib <[0, 0.032], 0.8> <.016, 15, .8> 2 Conclusion AND <[0.06, 0.4], .9> <.214, 6, .9> The heuristic approach reported previously produced a truth-value of <1, 0.07> for the final conjunction. The indefinite probabilities approach produced a truthvalue of <[0.0625, 0.358972], 0.9)> , or <.2143, 6> in <s,n> form, and <.2143, .358> in <s,w> form.", Probabilistic Logic Networks,chapter 10
"These two results are not logically contradictory at all, but the indefinite probabilities result is more informative. The prior heuristic formulas told us that there is very little evidence supporting the contention that the strength of the conclusion is near 1. The indefinite probabilities formulas tell us (more usefully) that there is a reasonable though not overwhelming amount of evidence that the strength of the conclusion is near .21. Being more principled, the indefinite probabilities method is guaranteed, if the underlying distributional assumptions are realistic, to focus on the most interesting part of the conclusion truth-value distribution.", Probabilistic Logic Networks,chapter 10
"  Chapter 11: Handling Crisp and Fuzzy Quantifiers with Indefinite Truth-Values Abstract In this chapter, we exemplify the use of indefinite probabilities in the handling of inference involving crisp universal and existential quantifiers and also fuzzy quantifiers. The treatment is novel, involving third-order probabilities, but appears to give intuitively sensible results in specific examples. 11.1 Quantifiers in Indefinite Probabilities We have discussed a variety of rules for inference on predicates, but haven’t yet broached the subtlest aspect, which is inference on quantified expressions. This is a case where the indefinite probabilities approach bears considerable fruit, allowing us to articulate a conceptually clear (though complex) approach that seems to cut through the confusion we perceive to exist in much of the literature on uncertain inference with quantifiers. The approach we outline is a subtle one.", Probabilistic Logic Networks,chapter 11
"The best way we have found to handle quantifiers within the indefinite probabilities framework is to introduce another level of complexity and utilize third-order probabilities. To understand this, we first consider the problem of “direct evaluation” of the indefinite truth-values of universally and existentially quantified expressions. Building on the ideas from the previous chapter, we solve this problem via a semantic approach that is considerably conceptually different from the one standardly taken in formal logic. Normally, in logic, expressions with unbound variables are not assigned truth-values; truth-value assignment comes only with quantification. In our approach, however, as exemplified in the previous chapter, we assign truth-values to expressions with unbound variables, yet without in doing so binding the variables. This is unusual but not contradictory in any way; an expression with unbound variables, as a mathematical entity, may certainly be mapped into a truth-value without introducing any mathematical or conceptual inconsistency.", Probabilistic Logic Networks,chapter 11
"This allows one to define the truth-value of a quantified expression as a mathematical transform of the truth-value of the corresponding expression with unbound variables, a notion that is key to our approach. This unusual semantic approach adds a minor twist to the notion that our approach to uncertain inference on quantified expressions reduces to standard crisp inference on quantified expressions as a special case. The twist is that our approach reduces to the standard crisp approach in terms of truth-value assignation for all expressions for which the standard crisp approach assigns a truth-value.     240 Probabilistic Logic Networks ver, our approach also assigns truth-values to some expressions (formulas with unbound variables) to which the standard crisp approach assigns no truthvalue.", Probabilistic Logic Networks,chapter 11
"Following up on this semantic approach, we will now explain how, if we have an indefinite probability for an expression F(t) with unbound variable t, summarizing an envelope E of probability distributions corresponding to F(t), we may derive from this an indefinite probability for the expression “ForAll x, F(x).” (Having carried out the transform in this direction, it will then be straightforwardly possible to carry out a corresponding transform in reverse.) The approach we take here is to consider the envelope E to be part of a higher-level envelope E1, which is an envelope of envelopes. The question is, then, given that we have observed E, what is the chance (according to E1) that the true envelope describing the world actually is almost entirely supported within [1-e, 1], where the latter interval is interpreted to constitute “essentially 1” (i.e.", Probabilistic Logic Networks,chapter 11
"e is the margin of error accepted in assessing ForAll-ness), and the phrase “almost entirely supported” is defined in terms of a threshold parameter? Similarly, in the case of existential quantification, we want to know the indefinite probability corresponding to “ThereExists x, F(x).” The question is, then, given that we have observed E, what is the chance (according to E1) that the true envelope describing the world actually is not entirely supported within [0, e], where the latter interval is interpreted to constitute “essentially zero” (i.e., e is the margin of error accepted in assessing ThereExists-ness)? The point conceptually is that quantified statements require you to go one level higher than ordinary statements. So if ordinary statements get second-order probabilities, quantified statements must get third-order probabilities. And the same line of reasoning that holds for “crisp” universal and existential quantifiers turns out to hold for fuzzy quantifiers as well.", Probabilistic Logic Networks,chapter 11
"In fact, in the approach presented here, crisp quantifiers are innately considered as an extreme case of fuzzy quantifiers, so that handling fuzzy quantifiers doesn’t really require anything extra, just some parameter-tuning. The following sections elaborate the above points more rigorously. 11.2 Direct Evaluation of Universally and Existentially Quantified Expressions We first consider the case of the direct evaluation of universally quantified expressions, an inference rule for which the idea is as follows: Given an indefinite truth-value for F(t), we want to get an indefinite TV for G = ForAll x, F(x). The roles of the three levels of distributions are roughly as follows. The firstand second-order levels play the role, with some modifications, of standard indefinite probabilities. The third-order distribution then plays the role of “perturbing” the second-order distribution.", Probabilistic Logic Networks,chapter 11
"The idea is that the second-order distribution     Chapter 11: Handling Crisp and Fuzzy Quantifiers 241 represents the mean for the statement F(x). The third-order distribution then various values for x, and the first-order distribution gives the sub-distributions for each of the second-order distributions. The process proceeds as follows: 1. Calculate [lf1,uf1] Interval for the third-order distribution. This step proceeds as usual for indefinite probabilities: see (Iklé and Goertzel 2008; Iklé et al 2007). Given L, U, k, and b, set s = 0.5.", Probabilistic Logic Networks,chapter 11
"We want to find a value for the variable diff so that the probability density function defined by (x""L1)ks(U1""x)k(1""s) f(x)= where L1=L-diff and U1=U+diff is U1 # (x""L1)ks(U1""x)k(1""s) dx L1 L # (x""L1)ks(U1""x)k(1""s) dx 1""b such that L1 = and ! U #1 (x""L1)ks(U1""x)k(1""s) 2 dx L1 U1 # (x""L1)ks(U1""x)k(1""s) dx 1""b U = . Once one of these last two integrals is U #1 (x""L1)ks(! 1""x)k(1""s) 2 U dx L1 satisfied, they both should be.", Probabilistic Logic Networks,chapter 11
"Alternatively, one can find diff for which U # (x""L1)ks(U1""x)k(1""s) dx ! L =b. U1 # (x""L1)ks(U1""x)k(1""s) dx L1 2. At present we are using only beta distributions for the desired “third-order” distribution family. To generate vectors of means for perturbed F(x) values, we first generate a vector of length n1 of random values chosen from a standard ! beta distribution. Next we scale the random means to the interval [lf1,uf1] using a linear transformation. 3. Now we use the same procedure as in Step 1 to generate symmetric intervals [lf2[i],uf2[i]] for each of the means found in Step 2. These intervals are now the desired [L1, U1] intervals for the third-order distributions. 4.", Probabilistic Logic Networks,chapter 11
"For each mean for the third-order distributions, we generate a sub-distribution. These sub-distributions represent the second-order distributions. ! 5. We next generate first-order distributions with means chosen from the secondorder distributions.     242 Probabilistic Logic Networks w we determine the percentage of elements in each first-order distribution that lie within the interval [1-e, 1]. Recall that we are using the interval [1-e, 1] as a “proxy” for the probability 1. The goal here is to determine the fraction of the first-order distributions that are almost entirely contained in the interval [1e, 1]. By “almost entirely contained” we mean that the fraction contained is at least proxy_confidence_level (PCL). 7. Finally, we find the conclusion <[L,U],b> interval.", Probabilistic Logic Networks,chapter 11
"For each of the third-order means, we calculate the average of all of the second-order distributions that are almost entirely contained in [1-e, 1], giving a list of n1 elements, probs, of probabilities. We finally find the elements of probs corresponding to quantiles # n1(1""b)+1% "" n1(.5+b)+1$ using L=# % and U ="" $ $ 2 & # 2 % Given the above, it is simple to obtain the ThereExists rule through the equivalence ! ! ThereExists x, F(x) ! ¬[ForAll x, ¬F(x)] . 11.3 Propagating Indefinite Probabilities through QuantifierBased Inference Rules As well as “directly evaluating” quantifiers in the manner of the previous section, it is also necessary within a logical reasoning system to carry out various quantifier manipulations.", Probabilistic Logic Networks,chapter 11
"We now discuss a variety of transformation rules that work on quantifiers, drawn from standard predicate logic. First, we have already seen that what is called “the rule of existential generalization” holds in the indefinite probabilities framework (this is just a reformulation of what we have called “direct evaluation” of existentially quantified expressions, above): 1) F(c) <[L,U], b, k> |"" $x, F($x) <[L,U], b, k> where c may be any expression not involving $x. Next, consider universal specification: 2) # $x, F($x) <[L,U], b, k> |F(c) <[L,U, b, k> where c is any expression not involving $x.", Probabilistic Logic Networks,chapter 11
"To see that universal specification also holds with indefinite probabilities, given the truth-value above for ForAll $x, F($x), we can obtain an indefinite truth-value     Chapter 11: Handling Crisp and Fuzzy Quantifiers 243 for F(t). We then use the mean of F(t) over all values t, as a heuristic appro tion to F(c) for a given value c. We have already also seen, at least implicitly, that all the standard quantifier exchange formulas hold for indefinite probabilities: 3) ¬(!x)F(x) "" (#x)¬F(x) (!x)¬F(x) "" ¬(#x)F(x) ¬(!x)¬F(x) "" (#x)F(x) (!x)¬F(x) "" ¬(#x)¬F(x) For our last transformation rule, we consider the operation of removing constants from within existential quantifiers.", Probabilistic Logic Networks,chapter 11
"In predicate logic we have that: 4) # x: G AND F(x) = G AND # x: F(x) Unlike the case for crisp predicate logic however, this rule is not, in general, true using indefinite probabilities. For example, consider the following set of premises with parameter settings e=0.5 and PCL=0.7: truth-value for G = <[0.45, 0.46], 0.9, 10> and truth-value for F(x) = <[0.71, 0.72], 0.9, 10>. Then the result for # x: G AND F(x) becomes <[0.0, 0.04913], 0.9, 10>, while that for G AND # x: F(x) is <[0.23046, 0.28926], 0.9, 10>.", Probabilistic Logic Networks,chapter 11
"On the other hand, we note that a different set of premises can yield similar results from the two approaches. Assuming the same parameter values for e and PCL, and truth-values for both F(x) and G of <[0.99, 1.0], 0.9, 10> gives a result of <[0.98331, 0.99626], 0.9, 10> using # x: G AND F(x), and a similar result of <[0.98344, 0.99620], 0.9, 10> using G AND # x: F(x). For insight into what is happening here, we view H(F)(t) = G AND F(t) as a distortion of the distribution of F.", Probabilistic Logic Networks,chapter 11
"In addition, if J(F) = # x J(x), then J(F) is a nonlinear distortion of F, so that even though H(F) is a linear distortion, it need not commute with J. An obvious and interesting question is then: Under what combination of premise values and parameter settings do the operators H and J “almost” commute? Due to space considerations we defer a thorough study of that question to a future paper. It does appear, however, that premise values near 1 lead to better commutativity than do values farther from 1. 11.4 Fuzzy Quantifiers Analyzing the indefinite probabilities approach to the quantifiers ForAll and ThereExists, it should be readily apparent that indefinite probabilities provide a natural method for “fuzzy” quantifiers such as AlmostAll and AFew.", Probabilistic Logic Networks,chapter 11
"In our discussion of the ForAll rule above, for example, the interval [PCL, 1] represents the fraction of bottom-level distributions completely contained in the interval [1-e, 1]. Recall that the interval [1-e,1] represents a proxy for probability 1.     244 Probabilistic Logic Networks analogy with the interval [PCL, 1] representing the ForAll rule, we can introduce the parameters lower_proxy_confidence (LPC) and upper_proxy_confidence (UPC) so that the interval [LPC, UPC] represents an AlmostAll rule or an AFew rule. More explicitly, by setting [LPC, UPC] = [0.9, 0.99], the interval could now naturally represent AlmostAll. Similarly, the same interval could represent AFew by setting LPC to a value such as 0.05 and UPC to, say, 0.1.", Probabilistic Logic Networks,chapter 11
"Through simple adjustments of these two proxy confidence parameters, we can thus introduce a sliding scale for all sorts of fuzzy quantifiers. Moreover, each of these fuzzy quantifiers is now firmly grounded in probability theory through the indefinite probabilities formalism. 11.5 Examples To further elucidate the above formalism, we now consider two examples. For our first example, we consider an example drawn from [15], which is there called the “crooked lottery” and extensively discussed: [¬ThereExists x Winner(x)""false]& [ [ x(Winner(x)||Winner(y))""Winner(y)]] ThereExists y ForAll The first clause is intended to represent the idea that everyone has a nonzero chance to win the lottery; the second clause is intended to represent the idea that there is one guy, y, who has a higher chance of winning than everybody else.", Probabilistic Logic Networks,chapter 11
"In ! [15] Halpern examines various formalisms for quantifying uncertainty in a logical reasoning context and assesses which ones can provide a consistent and sensible truth-value evaluation for this expression. To evaluate the truth-value of this expression using indefinite probabilities, suppose we assume the truth-value for Winner(x) is <[0.05, 0.1], 0.9, 10>. For the second clause we also assume that the truth-value for Winner(y) is <[0.25, 0.5], 0.9, 10> and that the truth-value for the implication (Winner(x)||Winner(y)""Winner(y)) is <[0.8, 0.9], 0.9, 10>. With these assumptions, we then vary the values of the parameters e and PCL for the ThereExists rule to generate the following graphs of the resulting truth-value intervals.", Probabilistic Logic Networks,chapter 11
"Note that the parameter values e and PCL used in the ForAll rule were the com! plements, 1-e and 1-PCL, of the values used in ThereExists.     Chapter 11: Handling Crisp and Fuzzy Quantifiers 245 The intermediate results for each of the two main clauses provide insight into the interaction between these two clauses. First the results for clause 1: [ ¬ThereExists x Winner(x)""false] . !     246 Probabilistic Logic Networks     Chapter 11: Handling Crisp and Fuzzy Quantifiers 247 For clause 2, [ ThereExists y [ ForAll x(Winner(x)||Winner(y))""Winner(y)]] , all the lower limits are 0. The graph of the upper limit is: ! For our second example, we consider a simple probabilistic syllogism, but expressed using natural language quantifiers: Many women are beautiful. Almost all beautiful things bring happiness.", Probabilistic Logic Networks,chapter 11
"|Many women bring happiness In order to use the indefinite probabilities formalism, we first need to determine appropriate values for the parameters LPC and UPC to represent the fuzzy concepts “many” and “almost all.” In practice, in the case where these rules are used within an integrative AGI system such as the NCE, appropriate values for these fuzzy concepts will be determined by the context in which they appear. In one context, for example, the interval [0.8, 0.9] might represent the idea “many,” but in a different situation we may wish for [0.6, 0.95] to represent “many.” For our example we set e=0.1. Let us suppose that “many” is represented by the interval [LPC, UPC]= [0.4, 0.95], and “almost all” by the interval [0.9, 0.99].", Probabilistic Logic Networks,chapter 11
"We will also assume truth-values identical to those in the previous example. The sequence of conclusions is then illustrated in the following tables.     248 Probabilistic Logic Networks Premise Truth-value Women <[0.45, 0.55], 0.9, 10> An individual woman is beautiful <[0.8, 0.95], 0.9, 10> Conclusion Truth-value Many women are beautiful <[0.35451, 0.63574], 0.9, 10> Premise Truth-value Beautiful things <[0.4, 0.8], 0.9, 10> A beautiful thing brings happiness <[0.8, 0.95], 0.9, 10> Conclusion Truth-value Almost all beautiful things bring hap- <[0.03906, 0.37464], 0.9, 10> piness Premise Truth-value Women <[0.", Probabilistic Logic Networks,chapter 11
"45, 0.55], 0.9, 10> Many women are beautiful <[0.35451, 0.63574], 0.9, 10> Beautiful things <[0.4, 0.8], 0.9, 10> Almost all beautiful things bring hap- <[0.03906, 0.37464], 0.9, 10> piness Happiness <[0.4, 0.9], 0.9, 10> Conclusion Truth-value Many women bring happiness <[0.41308, 0.53068], 0.9, 10>  ", Probabilistic Logic Networks,chapter 11
"  Chapter 12: Intensional Inference Abstract In this chapter we make precise the notion of “intensional inheritance,” by which is meant inheritance based on “properties” or “patterns.” We have mentioned intensional or mixed inheritance relationships here and there previously, but have not formally defined them. 12.1 Introduction We have already sketched out the basic notion of intensional inheritance used in PLN: we define intensional inheritance by associating entities with pattern-sets, so that, e.g., fish would be associated with fish , the set of all patterns associated PAT with fish. The intensional inheritance between fish and whale is then defined as the Subset relationship between fish and whale , and the composite InheriPAT PAT tance between two entities is defined as the disjunction of the Subset and IntensionalInheritance between the entities.", Probabilistic Logic Networks,chapter 12
"However, the concept of “pattern” used here was not formally defined previously; that is one of our tasks here. Before getting into formal details however, the conceptual foundations of the intension/extension distinction are worth reviewing in more depth because they become somewhat subtle. Intuitively, for instance, we might say: Subset whale fish <0> IntensionalInheritance whale fish <.7> Yet one might argue that the IntensionalInheritance relation is unnecessary here, because using only SubsetRelationships we could reason using PLN that Subset fish (lives_in_water AND swims) Subset whale (lives_in_water AND swims) |Subset fish whale <tv> where tv.s > 0. But the problem is that this abductive inference would be based on the erroneous assumption that fish and whale are independent in (lives_in_water AND     250 Probabilistic Logic Networks swims).", Probabilistic Logic Networks,chapter 12
"It would be overridden, for example, by textbook knowledge giving a definition of a fish as a complex predicate combining various attributes. What we want with intensional inheritance is some way of saying that fish inherits from whale that is NOT overridden by textbook knowledge. This is provided by the recourse to pattern-sets. Inference on composite InheritanceRelationships, we contend, better captures the essence of human commonsense reasoning than reasoning on SubsetRelationships. When we say something like “Life is a bowl of (half-rotten) cherries” we’re not merely doing Bayesian inference on the members of the sets “life” and “bowl of (half-rotten) cherries” – we’re noting an inheritance on the association or pattern level, which has a strength greater than would be obtained via Bayesian inference applied in the standard extensional way.", Probabilistic Logic Networks,chapter 12
"Philosophically, the issue is not that probability theory is wrong; the issue is that the most straightforward way of applying probability theory is not in accordance with common human practice, and that in this case common human practice has a good reason for being roughly the way it is (the reason being that finding similarities based on common patterns rather than common members is often useful for understanding a world in which many more patterns exist than would be expected at random). 12.2 Probabilistic Pattern Theory Now we introduce the formal definition of the concept of pattern – a notion central to our analysis of intension but also highly significant in its own right. In Goertzel (1993, 1993a, 1997) a mathematical theory of pattern is outlined, based on algorithmic information theory. The conceptual foundation of that theory is the notion of a pattern as a “representation as something simpler.", Probabilistic Logic Networks,chapter 12
"In Goertzel (2006), a modification to pattern theory is presented in which the algorithmic-information notions are replaced with probability-theoretic notions, but the conceptual spirit of the original pattern theory is preserved. Here we give a brief outline of probabilistic pattern theory, with PLN applications in mind. 12.2.1 Association First, we introduce the notion of association. We will say that a predicate F is associated with another predicate G if P(FG)>P(F¬G) !     Chapter 12: Intensional Inference 251 That is, the presence of F is a positive indicator for the presence of G. The degree of association may be quantified as ASSOC(F,G)=[P(FG)""P(F¬G)]+ (where [x]+ denotes the positive part of x, which is x if x>0 and 0 otherwise).", Probabilistic Logic Networks,chapter 12
"The association-structure of G may be defined as the fuzzy set of predicates F ! that are associated with G, where ASSOC is the fuzzy-set membership function. Of course, the same definition can be applied to concepts or individuals rather than predicates. One may also construct the “causal association-structure” of a predicate G, which is defined the same way, except with temporal quantification. One introduces a time-lag T, and then defines ASSOC(F,G; T) = [P(F is true at time S+T | G is true at time T) – P(F is true at time S+T|~G is true at time T)]+ The temporal version may be interpreted in terms of possible-worlds semantics; i.e.", Probabilistic Logic Networks,chapter 12
"P(F at time S+T|G at time T)) = the probability that the entity A will be satisfied in a random universe at time A, given that G is satisfied in that universe at time T-A P(F) = the probability that predicate F will be satisfied in a random universe, generically If A and B are concepts or individuals rather than predicates, one can define ASSOC(A,B;T) by replacing each of A and B with an appropriately defined predicate. For instance, we can replace A with the predicate F defined by A ExtensionalEquivalence Evaluation F U A Subset A U 12.2.2 From Association to Pattern From association to pattern is but a small step. A pattern in a predicate F is something that is associated with F, but is simpler than F. A pattern in an entity A is defined similarly.     252 Probabilistic Logic Networks We must assume there is some measure c() of complexity.", Probabilistic Logic Networks,chapter 12
"Then we may define the fuzzy-set membership function called the “pattern-intensity”, defined as, e.g., IN(F,G) = [c(G) – c(F)]+ [P(F | G) - P(F|~G)]+ There is also a temporal version, of course, defined as IN(F,G;T) = [c(G;T) – c(F;T)]+ * ASSOC(F,G;T) The definition of complexity referenced here can be made in several different ways. Harking back to traditional pattern theory, one can use algorithmic information theory, introducing some reference Turing machine and defining c(X) as the length in bits of X expressed on that reference machine. In this case, complexity is independent of time-lag T.", Probabilistic Logic Networks,chapter 12
"Or one can take a probabilistic approach to defining complexity by introducing some reference process H, and defining c(F;T) = ASSOC(G,H;T) In this case we may write IN[H](F,G;T) = [ASSOC(F,H;T) – ASSOC(F,G;T)]+ * ASSOC(F,G;T) Notation-wise, typically we will suppress the H and T dependencies and just write IN(F,G) as a shorthand for IN[H](F,G;T), but of course this doesn’t mean the parameters H and T are unimportant. Note the dependence upon the underlying computational model is generally suppressed in both traditional pattern theory and algorithmic information theory. It’s worth noting that this approach can essentially replicate the algorithmic information theory approach to complexity.", Probabilistic Logic Networks,chapter 12
"Suppose that M is a system that spits out bit strings of length <=N – choosing each bit at random, one by one, and then stopping when it gets to a bit string that represents a self-delimiting program on a given reference Turing machine. Next, define the predicate H as being True at a given point in time only if the system M is present and operational at that point in time. Next, for each bit string B of length <=N consider the predicate F[B](U,T) that returns True if B is present in possible-universe U at time T and False otherwise. Then it is clear that P(F[B]) is effectively zero (in a universe without a program randomly spitting out bit strings, the chance of a random bit string occurring is low). On the other hand, -log( P(F[B] at time S | H at time S-T) ) is roughly equal to the length of B.", Probabilistic Logic Networks,chapter 12
"So for this special predicate H and the special predicates F[B], the complexity as defined above is basically equal to the negative logarithm of the program length. Algorithmic information theory emerges as a special case of the probabilistic notion of complexity.     Chapter 12: Intensional Inference 253 We can introduce a notion of relative pattern here – which is useful if patterns need to be calculated contextually.", Probabilistic Logic Networks,chapter 12
"If we assume K as background knowledge, then we can define ASSOC(F,G|K) = [P(F | G & K) - P(F|~G & K)]+ and ASSOC(F, G| K ; T) = [P(F at time S+T | G & K at time T) – P(F at time S+T|~G & K at time T)]+ and IN(F,G|K;T) = [c(G|K;T) – c(F|K;T)]+ * ASSOC(F,G|K;T) IN[H](F,G|K;T) = [ASSOC(F,H|K;T) – ASSOC(F,G|K;T)]+ * ASSOC(F,G|K;T) Using these ideas, associated with any concept or predicate, we may construct a set called the “structure” of that entity.", Probabilistic Logic Networks,chapter 12
"We may do this in (at least) two ways: • The associative structure, which consists of the set of all observations and concepts that are associated with the entity • The pattern structure, which is the set of all observations and concepts that are patterns in the entity For example, returning to our friend Stripedog, we may call these two sets stripedog_ASSOC and stripedog_PAT. Note that stripedog_ASSOC and stripedog_PAT are sets of observations or concepts, whereas stripedog, as defined above, is a set of sets of observations. stripedog is a fuzzy set (as well as a fuzzy cat), since the truth-value of the Evaluation relationship Evaluation F $X Stripedog need not be Boolean. And according to the definitions given above, stripedog_ASSOC is also a fuzzy set because it’s not always entirely clear to what extent a given entity is associated with the individual Stripedog. 12.", Probabilistic Logic Networks,chapter 12
"3 Intensional Inheritance Relationships Now we will use the concepts of the previous section to give a novel perspective on the notion of intensional inheritance, as qualitatively introduced above. Suppose we want to compare our orange cat, Stripedog, to an orange (a particular     254 Probabilistic Logic Networks fruit; let’s name it orange_77). The set of entities stripedog_PAT associated with Stripedog includes observations such as ""orange is observed by me"" But this same observation is also included in orange_77_PAT. From this and other overlapping items, we may calculate that the probability associated with the relationship ExtensionalSimilarity stripedog_PAT orange_77_PAT is reasonably high. In this way, we arrive as a similarity between an orange cat and an orange fruit. Note that no individual orange cats are orange fruits. Nor do any sets of observations need to be identifiers for both orange cats and orange fruits.", Probabilistic Logic Networks,chapter 12
"Rather, all that’s observed in the above similarity relationship is that there exists an overlap between the set of observations corresponding to the orange cat and the set of observations or properties corresponding to the orange fruit. So based on the above we cannot say ExtensionalSimilarity stripedog orange_77 In fact this latter relationship may have a probability of zero. As noted earlier, a similar effect could be obtained via use of abductive inference. If one observes that Subset stripedog orange Subset orange_77 orange and then uses Bayes’ rule to calculate Subset orange orange_77 and then uses PLN deduction to calculate Subset stripedog orange Subset orange orange_77 |Subset stripedog orange_77 one will obtain a nonzero truth-value for the conclusion. However, this inference is based upon the erroneous assumption that stripedog and orange_77 are independent inside the set “orange.” While it is nice that erroneous assumptions can sometimes lead to useful results, this is not a generally sound basis for commonsense inference.", Probabilistic Logic Networks,chapter 12
"In practice it will lead to much lower strength values than humans     Chapter 12: Intensional Inference 255 typically obtain, because of the low strength values frequently produced by Bayes’ rule. We feel that looking at pattern and association sets provides a better qualitative model of human commonsense inference in many cases – a perspective to be elaborated below. Based on this conceptual and mathematical line of thinking, there are several ways to structure a pattern-based reasoning system. One approach would be to construct separate terms for C, C_ASSOC and C_PAT, corresponding to each individual. Sets of individuals, such as the set of cats, would also spawn such distinct terms; there would be a cat term defined as the set of individual cats, and cat_ASSOC and cat_PAT terms defined as the set of observations associated with individual cats.", Probabilistic Logic Networks,chapter 12
"Consistently with the above, the membership function of cat_ASSOC would be defined as [ P(cat| E) – P(cat|~E) ]+ where P(cat) means P(some element of the set of cats is observed), which is equivalent to P(some identifier of some cat is observed). The membership function of cat_PAT would be defined similarly as [c(cat) – c(E)]+ * [ P(cat| E) – P(cat|~E) ]+ However, this is not the implementation we have chosen. We believe this duplication of terms would be conceptually awkward – it doesn’t reflect the way human commonsense reasoning operates. Rather, we humans seem to fairly freely mix up reasoning about patterns with reasoning about sets of individuals. PLN is designed to enable this mixing-up, while at the same time allowing a distinction to be made clearly when this is appropriate.", Probabilistic Logic Networks,chapter 12
"In order to do this, we introduce the notion of an IntensionalInheritance relationship. Subset, as defined above, has a truth-value defined simply by conditional probability. IntensionalInheritance, on the other hand, is a special relationship defined by the convention that, e.g., IntensionalInheritance stripedog orange_77 means Subset stripedog_ASSOC orange_77_ASSOC Formally, we may introduce the ASSOC operator, defined as ExtensionalEquivalence Member $X (ExOut ASSOC $C) AND ExtensionalImplication Subset $Y $E     256 Probabilistic Logic Networks Subset $Y $C NOT ExtensionalImplication Subset $Y $E NOT [Subset $Y $C] and then define the associational version of IntensionalInheritance via the relationship ExtensionalEquivalence IntensionalInheritance_ASSOC $X $Y Subset (ExOut ASSOC $X) (ExOut ASSOC $Y) Next, we may define the", Probabilistic Logic Networks,chapter 12
"ASSOC variant of the Inheritance relationship via ExtensionalEquivalence Inheritance_ASSOC $X $Y OR Subset $X $Y IntensionalInheritance_ASSOC $X $Y And, analogously to the above, we may define a symmetric mixed intensional/extensional relationship, Similarity_ASSOC A B <tv> = OR ExtSim A B <tv1> ExtSim A_ASSOC B_ASSOC <tv2> Of course, one can rewrite the above definitions using PAT instead of ASSOC. On the other hand, if we want to use both association-structure and patternstructure, then in fact things get a little more complicated. We have three different kinds of inheritance to be disjunctively combined; e.g.", Probabilistic Logic Networks,chapter 12
"ExtensionalEquivalence Inheritance_ASSOC_PAT $X $Y OR Subset $X $Y IntensionalInheritance_ASSOC $X $Y IntensionalInheritance_PAT $X $Y Typically we will leave off the qualifiers and just refer to Inheritance rather than Inheritance_ASSOC, etc.     Chapter 12: Intensional Inference 257 Note a significant difference from NARS here. In NARS, it is assumed that X inherits from Y if X extensionally inherits from Y but Y intensionally inherits from (inherits properties from) X. We take a different approach here. We say that X inherits from Y if X’s members are members of Y, and the observations associated with X are also associated with Y. The duality of properties and members is taken to be provided via Bayes’ rule, where appropriate.", Probabilistic Logic Networks,chapter 12
"By making frequent use of this kind of Inheritance relationship in PLN we are making the philosophical assertion that commonsense inference habitually mixes up extensional and intensional inheritance, as defined here, in a disjunctive way. Clearly this particular way of combining probabilities is not mathematically “necessary” or natural in the sense that conditional probabilities are. However, we contend that it is cognitively natural. We believe that it is very cognitively valuable to calculate intensional inheritance and similarity values directly based on the above formulas – and use these together with extensional inheritance and similarity values within PLN inference. This is not hard to do in practice – it’s merely a matter of arranging conditional probability calculations in a different way; a way motivated by human psychology and pattern theory rather than probability theory proper. It is a credit to the flexibility of the PLN framework that it supports this kind of variation so easily.", Probabilistic Logic Networks,chapter 12
"We should note that the calculation of Intensional Inheritance A B involves a heuristic approximation because the probabilities inside, for instance, "" C and "" C , where "" represents the fuzzy membership funcASSOC(A) 1 ASSOC(A) 2 tion (and "" $X is simply a different notation for the truth value of ASSOC($C) Member $X (ExOut ASSOC $C))are not, in general, independent for the two properties C and C . ! ! 1 2 ! One way to minimize this sort of independence-assumption-based error is to !ch oose a set of properties that are relatively independent of each other -- a sort of approximately-orthogonal ontology spanning concept-space. This same sort of appro!xi mately!-o rthogonal decomposition may also be useful in default inference which we discuss in chapter 13. 12.3.", Probabilistic Logic Networks,chapter 12
"1 Intensional Term Probabilities Next, we touch on some mathematical issues raised by these constructions. Since we’re asking a term like stripedog to serve as a proxy for two or more terms – stripedog and stripedog_ASSOC and stripedog_PAT – it would seem that the term should have several, separate term probabilities:     258 Probabilistic Logic Networks • An extensional term probability, indicating the extent to which a random observation-set is an identifier for stripedog and counts as an observation of stripedog • Intensional term probabilities, indicating the extent to which a random observation is associated with, or is a pattern in, stripedog We can define a composite term truth-value via P ($X) = OR( P($X), P ($X), P ($X)) Inheritance ASS PAT Obviously these Inheritance term-probabilities go naturally with Inheritance relationships, whereas the purely extensional term probabilities go naturally with Subset relationships. 12.", Probabilistic Logic Networks,chapter 12
"4 Truth-value Conversion Formulas Next, the question arises how to convert between the extensional and mixed versions of inheritance relationships. Taking the ASSOC version of Inheritance for simplicity, the conversion rule Inheritance A B |Subset A B has the form OR(X,Y) |Y and the conversion rule Subset A B |Inheritance A B has the form X |X OR Y     Chapter 12: Intensional Inference 259 Obviously these are badly underdetermined inference rules, and the only solutions are simple heuristic ones. In the first case, we use the heuristic that s = (OR(X,Y).tv.s Y.tv.s = (s-c)/(1-c) where c is a constant that may be set equal to the average strength of all IntensionalInheritance relations in the system’s memory. In the second case we use the heuristic that s = X.tv.s (OR(X,Y)).tv.", Probabilistic Logic Networks,chapter 12
"= c + (1-c)s where c is defined the same way. 12.5 Intensional Boolean Operators Next, we turn to the intensional versions of the standard Boolean operators discussed in the previous chapter. In general, in PLN we may consider Boolean operators to act either purely extensionally, purely intensionally, or in a mixed intensional/extensional manner. For instance, one may consider (A AND B) purely extensionally, as was done above, by taking the intersection of the sets of members of A and B. This is the default and the simplest approach, but not always the best route. Or, one may also consider (A AND B) purely intensionally, by taking the intersection of the sets of patterns associated with A and B. In this way, along with the standard (extensional) AND, OR, and NOT, we may define operators such as AND and OR .", Probabilistic Logic Networks,chapter 12
"Int Int And, as it happens, certain combinations of intensional and extensional Boolean operations are more useful than others. For instance, it generally makes sense to pair extensional intersection with intensional union: the members of the intersection of A and B will generally share the properties of both A and B. Thus we may define (A AND B) as the term containing the intersection of the extenMix sional relationships of A and B, and the union of the intensional relationships of A and B. Similarly, (A OR B) is the term containing the union of the extensional Mix relationships of A and B, and the intersection of the intensional relationships of A and B. These various operators may be useful for generating new concepts to be utilized in future inferences. For instance, (cat AND dog) is a category of hypothetiInt cal creatures sharing the properties of both cats and dogs.", Probabilistic Logic Networks,chapter 12
"On the other hand, both (cat AND pet) and (cat AND pet) are the interpretations of the informal conMix Ext cept “the set of pet cats” but may be different fuzzy sets, because both cat and pet may have intensional relationships that were not derived from analysis of the members of these sets (but were derived, perhaps, from natural language interpre-     260 Probabilistic Logic Networks tation). In (cat AND pet) these properties are carried over to the intersection but Mix in (cat AND pet) they are not. Ext Apart from these logical operators, it may also be interesting to create new terms by simply taking the union or intersection of all relationships belonging to two terms A and B. However, it is important to understand that this kind of simplistic intersection and union is not the most conceptually natural approach in terms of the logic of either intension or extension. 12.", Probabilistic Logic Networks,chapter 12
"6 Doing Inference on Inheritance Relationships Having defined generic Inheritance as a disjunction of extensional and intensional inheritance, it is now natural to ask whether there are special inference rules for dealing with intensional and generic Inheritance relationships. For instance, suppose we have IntensionalInheritance A B IntensionalInheritance B C Can we then conclude IntensionalInheritance A C via “intensional deduction”? In fact, it seems that the best way to deal with this kind of situation is to use the definition of intensional inheritance in terms of probabilities. From IntensionalInheritance A B along with P(A) and P(B), one can figure out P(A|B); and similarly from IntensionalInheritance B C one can figure out P(C|B).", Probabilistic Logic Networks,chapter 12
"Using ordinary (extensional) deduction one can then figure out P(C|A), and then from this plus P(C) and P(A) one can calculate the strength of IntensionalInheritance A C There is thus no need for a special “intensional deduction rule,” and the same basic method holds for other inference rules. What about mixed inference? Suppose we have Inheritance A B     Chapter 12: Intensional Inference 261 Inheritance B C Can Inheritance A C be concluded? Again, the story is a bit more complicated. The best thing is to record information separately about extensional and intensional inheritance.", Probabilistic Logic Networks,chapter 12
"We can then combine them as weighted combinations of the following four types of inference: 1) ExtensionalInheritance A B ExtensionalInheritance B C |ExtensionalInheritance A C 2) IntensionalInheritance A B IntensionalInheriatnce B C |IntensionalInheritance A C 3) ExtensionalInheritance A B IntesionalInheritance B C |Inheritance A C (mixed Extensional/Intensional) 4) IntensionalInheritance A B ExtensionalInheritance B C Inheritance B C (mixed) On the other hand, sometimes one has information only about inheritance rather than about extensional and intensional inheritance particularly. This may happen, for instance, if one is dealing with knowledge parsed from natural language. The sentence “cats are animals” is best interpreted in terms of mixed inheritance.", Probabilistic Logic Networks,chapter 12
"So if we are doing reasoning of the form cats are animals animals are ugly |cats are ugly then the best course is to map each of the premises into mixed inheritance relationships, then interpret each mixed inheritance relationship as a weighted sum of an extensional inheritance relationship and an intensional inheritance relationship. Inference may then be done to determine the truth-values of     262 Probabilistic Logic Networks IntensionalInheritance cat ugly and ExtensionalInheritance cat ugly and these may be combined to get the truth-value of Inheritance cat ugly There is significant arbitrariness here in the determination of the weighting factor. It may be set arbitrarily as a system parameter, or else it may be determined “inferentially” via analogy to cases where the extensional and intensional inheritance values are known. For instance, if it is known that ExtensionalInheritance dog ugly <.41,.3> IntensionalInheritance dog ugly <.38,.", Probabilistic Logic Networks,chapter 12
"5> ExtensionalInheritance hog ugly <.79,.2> IntensionalInheritance hog ugly <.82,.6> then this may be taken as evidence that for cats as well, ExtensionalInheritance cat ugly should get a lower weighting than IntensionalInheritance cat ugly In all, then, intensional inference in PLN emerges as a different way of organizing the same basic probabilistic information that is used in extensional inference. The intension vs. extension distinction is important in terms of philosophy and cognitive science, yet in terms of the mathematics of probabilistic inference is merely a minor variation in the utilization of the familiar extensional inference rules. 12.7 Doing Inference on Inheritance Relationships So far in our discussion of intension we have considered only strength values, and have not looked at weight of evidence or indefinite probabilities at all. Now we remedy this omission and explore how the indefinite probabilities framework may be extended to handle intensional inference.", Probabilistic Logic Networks,chapter 12
"As usual in PLN, the tricky part is getting the semantics right; the mathematics follows fairly directly from that.     Chapter 12: Intensional Inference 263 Consider the link IntensionalInheritance whale fish Suppose we want to assign an indefinite probability to this link (via direct evaluation rather than inference, initially): what’s the semantics? Crudely, the semantics is P( x in PROP(fish) | x in PROP(whale) ) where PROP(A) may be defined as ASSOC(A) or PAT(A), depending on which variant one is using. One way to conceive of PROP(A), in either case, is as a fuzzy set to which B belongs to a degree determined by a specific formula such as ! (B) = floor[(P(A | B) – P(A | ~B) ) (1 - |A| / |B|) A ] in the case PROP=PAT.", Probabilistic Logic Networks,chapter 12
"If conditional probabilities such as P(A|B) are characterized by second-order probability distributions (as in the indefinite probabilities approach), then as a consequence ! (B) will be characterized as a second-order probability distribution A over the space of fuzzy set membership values. For instance, suppose we choose a specific joint distribution for ( P(A|B), P(A|~B) ); then this will give rise to a specific distribution for ! (B). Choosing various joint distributions for A ( P(A|B), P(A|~B) ), according to a second-order probability distribution specified for these conditional probabilities, one gets various distributions for ! (B); so one A can then derive an envelope of distributions for ! (B). (Of course, we can choose A these joint distributions using an independence assumptions if we want to; e.g., if we lack the information to do otherwise.", Probabilistic Logic Networks,chapter 12
"What this means is that the fuzzy membership value ! (B) is not a single value, but is rather an envelope of distributions A describing a value, which may be summarized in many ways; for example, as an indefinite probability. What we need to know to evaluate the truth-value of the above IntensionalInheritance relationship, then, is: Given the knowledge that X is a property of whale in the above sense, what’s the chance that X is also a property of fish? Obviously this can be quantified in a number of ways.", Probabilistic Logic Networks,chapter 12
"One appealing way involves the choice function f (B) = [ E(! (B)) * woe(! (B))] A A A where E(!A(B)) tells you the expected value of the envelope of distributions corresponding to E(! (B)), and woe(! (B)) tells you the weight of evidence calcuA A lated from this envelope of distributions (either approximated as the b-level confidence interval width of the set of means of distributions in the envelope for some     264 Probabilistic Logic Networks fixed b, or else calculated more elaborately using assumptions or knowledge about the underlying distributions). To calculate the truth-value of the above link, then, we can repeatedly carry out the following series of steps: 1. Choose a property R of whale with a probability proportional to f (whale), where f is the default choice function specified above. R 2. Estimate E(! (fish) ).", Probabilistic Logic Networks,chapter 12
"R Doing this repeatedly gives us a distribution of means, which we can summarize using an indefinite probability. All this specifies how to define the indefinite probability governing the degree to which whale inherits from fish according to direct evaluation. If for some reason we want the whole envelope of distributions corresponding to ! (fish), then we simply have to retain this envelope in Step 2 above. R To calculate IntensionalSimilarity, on the other hand, one may use a minor variation on the above methodology. Continuing with the whale/fish case, for example, one can repeatedly carry out the following series of steps: 1. Choose a random pair (R, Z), where Z is either whale or fish and R is a property of one of them, and the probability of choosing the pair is proportional to f (Z), where f is the default choice function specified above. R 2. Estimate E(! (W) ), where W is either whale or fish and is different from R Z.", Probabilistic Logic Networks,chapter 12
"Doing this repeatedly gives us a distribution of means, which we can summarize using an indefinite probability.  ", Probabilistic Logic Networks,chapter 12
"  Chapter 13: Aspects of Inference Control Abstract In this chapter we will describe a few important aspects of the mathematical and software structures via which PLN has been implemented within the Novamente Cognition Engine – an implementation that has been structured with the goal of allowing PLN to serve the broadest possible variety of functions within an integrative AI context. 13.1 Introduction The focus of this book is PLN mathematics, not software engineering, cognitive science, or integrative AI. However, the purpose with which the PLN mathematics has been conceived is closely tied to these other two topics; we have sought to create an uncertain inference framework capable of serving as a pragmatic uncertain inference component within the integrative Novamente AI system. For simplicity we will avoid giving any details of the Novamente architecture, presenting only the PLN implementation architecture itself. The mathematical, software, and conceptual relationships between PLN and the remainder of the NCE are beyond the scope of this book. 13.", Probabilistic Logic Networks,chapter 13
"2 Forward- and Backward-Chaining Inference The PLN inference rules and formulas are local in character; they tell you how to produce a conclusion from a particular set of premises. They don’t address what is in a sense the most difficult aspect of inference: the determination of which inference rules to perform in what order. We call this process “inference control.” Inference control in PLN is handled via mechanisms that are familiar from traditional AI – forward and backward chaining – but with special twists enforced by the omnipresence of uncertainty in PLN inference. Generically, in PLN or elsewhere, backward chaining is defined as an inference process that starts with one or more inference targets and works backward to find inference paths leading to these targets as conclusions. An inference engine using backward chaining starts by searching the available inference rules until it finds     266 Probabilistic Logic Networks at has a conclusion of a form that matches a desired goal.", Probabilistic Logic Networks,chapter 13
"It then seeks to find suitable premises for the rule – premises that will cause the rule to output the goal as a conclusion. In conventional, crisp-logic Boolean backward chaining, if appropriate premises for that inference rule are not known to be true, then they are added to the list of goals, and the process continues until a path is found in which all goals on the path have been achieved. In PLN backward chaining, the probabilistic truth-values necessitate that the appropriate premises for that inference rule are recorded as such but also added to the list of goals. The process continues until the target atom can be produced with truth-value weight higher than a threshold set by the controller. The trick of making this work effectively is “pruning”: intelligently and adaptively choosing which rules to choose and which premises to feed them, iteratively as the process proceeds. Generic forward chaining, on the other hand, starts with the available knowledge and uses inference rules to derive a series of conclusions.", Probabilistic Logic Networks,chapter 13
"An inference engine using forward chaining searches the inference rules until it finds one whose inputs have the same form as the given premises, then uses the rules to produce output. The output is then added to the set of possible premises to be used in future inference steps. The tricky part here is once again pruning; in most cases there are many different rules from which to select, and it’s not clear which are the right ones. The bugaboo of both forward- and backward-chaining inference control is combinatorial explosion. We describe here some heuristics used to palliate this problem in PLN, but these heuristics do not solve the problem fully adequately, except for the case of fairly simple inferences. Of course, the problem of combinatorial explosion is not fully solvable without introducing implausibly massive computational resources, but it must be solved more thoroughly than is done here if PLN is to be useful for powerful general intelligence.", Probabilistic Logic Networks,chapter 13
"In the overall Novamente architecture, the combinatorial explosion is intended to be quelled via the integration of PLN with other AI components such as evolutionary learning, stochastic pattern mining and adaptive attention allocation, topics that will not be discussed here. 13.3 Differences Between Crisp and Probabilistic TheoremProving While the basic structures of PLN inference control (forward and backward chaining) are familiar from crisp-logic-based theorem-proving, significant differences arise due to the prominent role that uncertainty plays in PLN inference. In the automated proof of crisp logical theorems, the goal is to build an inference trail that proceeds, in each step, from premise predicates with crisp truth-values to new predicates with crisp truth-values. The goal in backward chaining, for exam-     Chapter 13: Aspects of Inference Control 267 ple, is to find a single path of this nature leading from some set of known pre to the given target predicate.", Probabilistic Logic Networks,chapter 13
"In probabilistic theorem-proving, on the other hand, finding a single path from known premises to the target is not necessarily very useful, because the truthvalue estimate achieved via this path may have low confidence. Multiple paths may yield multiple truth-value estimates, which must then be revised or chosen between. These complexities make inference control subtler, and among other factors they imply a more prominent role for “effort management.” It is often easy to find some path from premises to the target if we have no requirements for the resulting truth-value. Therefore, in PLN backward chaining we are in practice more interested in how many resources we are willing to allocate for improving the truth-value of the target by alternative inference paths, rather than aiming for the first possible path we can find. 13.3.", Probabilistic Logic Networks,chapter 13
"1 Atom Structure Templates In order to carry out an individual step in backward- or forward-chaining inference, we need to be able to quickly figure out which rules can be applied to a certain Atom (for forward chaining) or which rules can yield a certain Atom as output (for backward chaining). In order to be able to do this conveniently, we need a way to describe Atom properties in a concise way. We will do this with predicates called Atom structure templates. Given an Atom A, we may construct the predicate A^ so that A^(X) is true iff there is some Y so that A(Y)=X. In the case that A is a first-order Atom (a Term or a Relationship between Terms, with no variables or higher-order functions involved), then A^(X) is true iff X=A; but this is not the most interesting case. E.g.", Probabilistic Logic Networks,chapter 13
"for the Atoms P = Inheritance A B and Q = Inheritance A $1 it is easy to see that Q^(P) is true (via the assignment $1=B) but P^(Q) is not. We may also define more complex Atom types such as HasTruthValueStrengthGreaterThan     268 Probabilistic Logic Networks $1 0.5 which can likewise be used as literals like any other Atoms, or as complex Atom structure templates that map Atoms into Boolean truth-values according to a specific property, such as “whether the truth-value.strength of the argument Atom is greater than 0.5.” Atom structure templates can also be used in conjunctions or disjunctions with other Atom structure templates. For example, the Atom structure template corresponding to AND HasTruthValueStrengthGreaterThan $1 0.5 Inheritance A $1 is true of an Atom such that A inherits from it and its truth-value.strength > 0.5.", Probabilistic Logic Networks,chapter 13
"13.3.2 Rules and Filters Recall the distinction between rules and formulas in PLN. A Formula is an object that maps a vector of truth-values into a single truth-value. For example, the deduction formula takes five truth-values and outputs the truth-value of the resulting Inheritance or Implication relationship. On the other hand, a Rule is an object that maps a vector of Atoms into a single new Atom. Along the way it applies a designated formula to calculate the truth-values. For example, the deduction rule takes two Inheritance relationships (A,B) and (B,C) and outputs a single Inheritance relationship (A,C) with a truth-value calculated by the deduction formula. In the PLN software implementation, inference rules are represented by Rule objects. Each Rule object has both an input filter and an output-based-input filter, which will be used by forward chaining and backward chaining, respectively.", Probabilistic Logic Networks,chapter 13
"The input filter is a vector of Atom structure templates that accept only specific kinds of argument Atoms in the respective place. For example, the input filter of DeductionRule is the vector of two Atoms representing Atom structure templates: InheritanceLink $1 $2 InheritanceLink $2     Chapter 13: Aspects of Inference Control 269 $3. The output-based-input filter is more complex. It is used to detemine which Atoms are needed as parameters for the Rule in order to produce Atoms of desired kind. For example, in order to produce the InheritanceLink A C by the DeductionRule you need to provide, as input to the DeductionRule, the Atoms that satisfy the Atom structure templates InheritanceLink A $1 InheritanceLink $1 C Of course a specific target may not be producible by a given rule, in which case the procedure simply returns a null result.", Probabilistic Logic Networks,chapter 13
"In order to produce two Atoms that satisfy each one of the last two Atom structure templates, we need to begin a new proof process for each one separately, and so on. Since this particular proof contains Atom structure template variables, we must at some point unify the variables. Because they occur inside distinct Atoms, it may happen that a specific unification choice in one will prevent the production of the other altogether. We must then backtrack to this unification step, choose differently, and retry. Note that producing InheritanceLink($1, C) is very different from producing an Atom that satisfies the Atom structure template InheritanceLink($1,C)^. Typically, in backward chaining we are interested in the latter. 13.4 Simplification Some computational reasoning systems require all logical formulas to fall into a certain standard “normal form,” but this is not the case in PLN.", Probabilistic Logic Networks,chapter 13
"There is the option for normalization, and we plan to experiment with using Holman’s Elegant Normal Form (Holman 1990) in PLN, as we do in other aspects of the NCE, but normalization of this nature is not required. As a default, the PLN implementation does only some elementary normalizations, which are not profoundly necessary for PLN inference control but do serve to simplify things and reduce the number of Rule objects required.     270 Probabilistic Logic Networks me “explicit” normalization happens in the proxy that the new Atoms pass before entering the knowledge base, and then some “implicit” normalization happens in the structures used to manipulate the Atoms. An example of the former kind of normalization is conversion of EquivalenceLinks into two ImplicationLinks, while an example of the latter kind is the mechanism that considers identical an Atom A and a single-member ListLink whose outgoing set consists of A. 13.", Probabilistic Logic Networks,chapter 13
"5 Backward Chaining We now describe the PLN backward-chaining inference process. Each step of this process is thought of as involving two substeps: expansion and evaluation. In the expansion step, the filters of Rule objects are used to figure out the possible child nodes for the current BIT (BackInferenceTree) node. These nodes are then inserted into an “expansion pool,” which contains all the nodes that can still be expanded to find additional proof paths. In the evaluation step the BIT is traversed from leaves towards the root, where children of a node are presented as input arguments to the Rule object that the parent node represents. Because each child is in general a set of possible solutions for the given Rule argument, we have multiple combinations of solutions to try out.", Probabilistic Logic Networks,chapter 13
"As noted above, a central characteristic of PLN backward chaining is the fact that “topological” inference path-finding is only a subgoal of the main goal of PLN backward inference: maximizing the weight-of-evidence of the truth-value of the target Atom. So, in order to reach the latter goal, we may need to construct several different inference trees, with inference paths that may have considerable “topological” overlap yet assign different truth-values even to the overlapping nodes. This process of multiple-tree generation can conveniently be described in evolutionary terms, where the genotype of a Backward Inference Tree (BIT) consists of the individual Atoms and the Rules that combine them, while the weight of evidence of the result produced by the tree is its fitness. To calculate the fitness of a BIT it is necessary to evaluate all the proof pathways in the tree, and then combine the final query results using the Rule of Choice.", Probabilistic Logic Networks,chapter 13
"On the implementation level there is some subtlety involved in constructing the trees involved in backward-chaining inference, because the validity of a result may depend on other results that come from other branches. A whole new search tree must be created every time the production of a target Atom is attempted. To achieve this, on each attempt to produce a target Atom we save the partial tree produced thus far, and launch a new tree that contains only a pointer to the partial tree. Thus, the task of improving the truth-value of an Atom requires partial or complete reconstruction of the proof tree several times. Therefore, a critical implementation requirement is the efficiency of the “retry mechanism” that allows for this.     Chapter 13: Aspects of Inference Control 271 Managing variable bindings in this tree expansion process is also tricky. I approach, to carry out backward-chaining inference, you expand the inference tree step-by-step, and you may evaluate the fitness of the tree after any step.", Probabilistic Logic Networks,chapter 13
"To produce each subresult several Rules can be invoked, causing bifurcation. At the time of evaluation, some subresults are in fact sets of results obtained via different pathways. Hence, the evaluation step must look at all the consistent conbinations of these; consistency here meaning that bindings of the subresults used in a specific combination must not conflict with each other. Ideally, one might want to check for binding consistency at the expansion stage and simply discard conflicting results. However, in case of conflict we do not in general know which ones of the conflicting bindings are worse ( i.e., will contribute to lower ultimate fitness) than others. Hence, it is best to do the consistency check at the evaluation stage. 13.6 Simple BIT Pruning For pruning of the backward-chaining inference tree, we use initially a technique based on the multi-armed bandit problem from statistics (Robbins 1952; Sutton, Bartow 1998.", Probabilistic Logic Networks,chapter 13
"Consider first a relatively simple case: the backward-chainer wants to expand a given node N of the BIT it is creating, and there is a number of possibilities for expansion, corresponding to a number of different rules that may possibly give rise to the expression contained at node N. Some of these have not yet been tried, but these have a priori probabilities, which may be set based on the general usefulness of the corresponding rule, or in a more sophisticated system based on the empirical usefulness of the rule in related contexts. Some may have already been expanded before, yielding a truth-value estimate with a known strength and weight of evidence. Then, in our current inference control approach, each possibility for expansion (each rule) must be assigned an “expected utility” number indicating the probability distribution governing the degree of fitness increase expected to ensue from exploring this possibility. For unexplored possibilities this utility number must be the a priori probability of the rule in that context.", Probabilistic Logic Networks,chapter 13
"For possibilities already extensively explored, this utility number may be calculated in terms of the weight of evidence of the truth-value estimate obtained via exploring this possibility, and the amount of computational effort expended in calculating this truth-value. Regarding unexplored or lightly-explored possibilities, the two heuristics used to set the “a priori” utilities for BIT nodes are: 1. Solution space size: Prefer nodes with more restrictive proof targets (i.e., ones with fewer free variables and more structure). 2. Rule priority: Prefer e.g. direct atom lookup to a purely hypothetical guess.     272 Probabilistic Logic Networks take into account information gained by prior exploration of a BIT node, one may utilize a simple heuristic formula such as u = d/E = weight_of_evidence / effort or more generally u = = F/E = fitness / effort (where e.g. fitness might be measured as weight_of_evidence*strength).", Probabilistic Logic Networks,chapter 13
"A more sophisticated variant might weight these two factors, yielding e.g. u = Fa/Eb Given these probabilities, an avenue for expanding the node may be selected via any one of many standard algorithms for handling multi-armed bandit problems; for initial experimentation we have chosen the SoftMax algorithm (Sutton, Bartow 1998), which simply chooses an avenue to explore at random, with a probability proportional to eu/T where T is a temperature parameter. High temperatures make all possibilities equiprobable whereas T=0 causes the method to emulate greedy search (via always following the most probable path). One of the strengths of this overall approach to inference control is its capability to incorporate, into each inference step, information gained via other sorts of inference or via non-inferential processes. This is particularly valuable when considering PLN as a component of an integrative AI system.", Probabilistic Logic Networks,chapter 13
"For instance, if encounters the link ExtensionalInheritance A B as a possibility for exploration in a BIT, then a reasonable heuristic is to check for the existence and truth value of the link h IntensionalInheritance A B If A and B share a lot of properties, they may heuristically be guessed to share a lot of members; etc. Or, if there is no existing information directly pertinent to the ExtensionalInheritance relation being considered, and there are sufficient computational resources available, it might be worthwhile to execute some other non-inferential cognitive process aimed at gathering more relevant information; for instance, if many links of the form     Chapter 13: Aspects of Inference Control 273 MemberLink C B i are available, an evolutionary learning or other supervised categorization algorithm could be used to learn a set of logical rules characterizing the members of B, which could then be used in the inference process to gain additional information potentially pertinent to estimating the utility of expanding Extension", Probabilistic Logic Networks,chapter 13
"alInheritance A B 13.7 Pruning BITs in the Context of Rules with Interdependent Arguments The pruning situation becomes more complex when we consider rules such as Deduction, which take arguments that depend on each other. For Deduction to produce Implication(A,C), input arguments are of the form (Implication(A,$1), Implication($1,C)) where $1 is a variable. Naively, one might just want to simplify this by looking up only Atom pairs that simultaneously satisfy both requirements. This fails, however, because we must also be able to produce both arguments by our regular inference process. So for each argument a separate inference tree is created, and the consistency check must be postponed. Expanding both branches independent of each other is wasteful, because once we find a good result for one of the arguments in that branch, e.g., binding $1 to B, we would like to bias the inference in the other branch toward being consistent with this; i.", Probabilistic Logic Networks,chapter 13
"., producing Implication(B,C). One approach would be to launch a new separate proof process for this purpose, but this is difficult to implement because in general there are other things going on in the proof beside this single (e.g., deduction) step, and we would like to have only a single control path. In this hypothetical approach it would be unclear when exactly we should continue with this proof process and when with the other one(s). A simpler approach is to create a clone of the parent of the current inference tree node, with the one difference that in this new node the said variable (B) has been bound. The new node is then added to the execution pool. This may seem counter-intuitive because we do not restrict the possible proof paths, but simply add a new one. For example, suppose that in this example Implication($1,C) is still in the expansion pool, and the Implication(A,$1) is currently under expansion.", Probabilistic Logic Networks,chapter 13
"The expansion produces result Implication(A,B). Then, the execution pool will contain both Implication($1,C) and Implication(B,C). Note that the whole execution pool will typically not be exhausted, but we only expand a part of it until a sufficiently fit result has been found. Further, we have set the default utilities so that Implication(B,C) has a higher a priori utility than Implication($1,C) because these nodes are identical in all other respects, but the     274 Probabilistic Logic Networks r has fewer free variables and therefore a smaller solution space. It follows that the proof for Implication(B,C) will be attempted before the more general Implication($1,C). Of course, if Implication(B,C) does not exist and has to be produced by inference, then the process may end up expanding Implication($1,C) before it has been able to produce Implication(B,C).", Probabilistic Logic Networks,chapter 13
"But this is fine because Implication(B,C) was simply our first guess, and if the Atom does not exist we have no strong reason to believe that the binding $1=>B should be superior to other alternatives. The bandit problem based approach mentioned above may still be used in the more complex case of rules like deduction with interdependent arguments, but the method of calculating u must be modified slightly. In this case, the probability assigned to a given possible expansion might (to cite one sensible formula out of many possible ones, and to use the ”weight of evidence” fitness function purely for sake of exemplification) be set equal to u = (d d )a/Eb 1 2 where d is the weight of evidence of the truth-value output by the expansion no1 de, and d is the weight of evidence of the inference rule output produced using 2 the output of the expansion node as input. For instance, if doing a deduction Inheritance A B <[0.", Probabilistic Logic Networks,chapter 13
"8, 0.9], 0.9, 10> Inheritance B C <[0.7, 0.8], 0.9, 10> |Inheritance A C <[0.618498, 0.744425], 0.9, 10> then a particular way of producing Inheritance A B would be assigned d based on the weight of evidence obtained for the latter link 1 this way, and would be assigned d based on the weight of evidence obtained for 2 Inheritance A C this way. 13.8 Forward-Chaining Inference Finally, we consider the problem of forward-chaining inference. This is very closely comparable to backward-chaining inference, with the exception that the method of calculating utility used inside the bandit problem calculation is different.", Probabilistic Logic Networks,chapter 13
"One is no longer trying to choose a path likely to yield maximum increase of     Chapter 13: Aspects of Inference Control 275 weight of evidence, but rather a path likely to yield this along with maximu terestingness” according to some specified criterion. For example, one definition of interestingness is simply deviation from probabilistic independence. Another, more general definition is the difference in the result obtained by evaluating the truth-value of the Atom with very little effort versus a large amount of effort. So one might have u = Ic da/Eb where I measures interestingness; or more general versions where e.g. the weight of evidence d is replaced with some other fitness function.", Probabilistic Logic Networks,chapter 13
"The measurement of interestingness is a deep topic unto itself, which brings us beyond PLN proper; however, one PLN-related way to gauge the interestingness of an Atom, relative to a knowledge base, is to ask how different the Atom’s actual truth value is, from the truth value that would be inferred for the Atom if it were removed and then had its truth value supplied by inference. This measures how much surprisingness the Atom’s truth value contains, relative to the surrounding knowledge. Of course, forward-chaining inference can also invoke backward-chaining inference as a subordinate process. Once it has happened upon an interesting Atom, it can then invoke backward chaining to more thoroughly evaluate the truth-value of this Atom via a variety of different paths. Very shallow forward-chaining inference may be used as a method of “concept creation.” For instance, there is value in a process that simply searches for interesting Boolean combinations of Atoms.", Probabilistic Logic Networks,chapter 13
"This may be done by randomly selecting combinations of Atoms and using them as premises for a few steps of forwardchaining inference. These combinations will then remain in memory to be used in future forward- or backward-chaining inference processes. 13.9 Default Inference Inference control contains a great number of subtleties, only a small minority of which has been considered here. Before leaving the topic we will consider one additional aspect, lying at the borderline between inference control, PLN inference proper, and cognitive science. This is the notion of “default inheritance,” which plays a role in computational linguistics within the Word Grammar framework (Hudson 1984), and also plays a central role in various species of nonmonotonic “default logic” (Reiter, 1980; Delgrande and Schaub, 2003).", Probabilistic Logic Networks,chapter 13
"The treatment of default inference in PLN exemplifies how, in the PLN framework, judicious inference control and intelligent integration of PLN with other structures may be used to achieve things that, in other logic frameworks, need to be handled via explicit extension of the logic. To exemplify the notion of default inheritance, consider the case of penguins, which do not fly, although they are a subclass of birds, which do fly. When one     276 Probabilistic Logic Networks ers a new type of penguin, say an Emperor penguin, one reasons initially that they do not fly – i.e., one reasons by reference to the new type’s immediate parent in the ontological hierarchy, rather than its grandparent. In some logical inference frameworks, the notion of hierarchy is primary and default inheritance of this nature is wired in at the inference rule level.", Probabilistic Logic Networks,chapter 13
"But this is not the case with PLN – in PLN, correct treatment of default inheritance must come indirectly out of other mechanisms. Fortunately, this can be achieved in a fairly simple and natural way. Consider the two inferences (expressed informally, as we are presenting a conceptual discussion not yet formalized in PLN terms) A) penguin --> fly <0> bird --> penguin <.02> |bird --> fly B) penguin --> bird <1> bird --> fly <.9> |penguin --> fly The correct behavior according to the default inheritance idea is that, in a system that already knows at least a moderate amount about the flight behavior of birds and penguins, inference A should be accepted but inference B should not. That is, evidence about penguins should be included in determining whether birds can fly – even if there is already some general knowledge about the flight behavior of birds in the system.", Probabilistic Logic Networks,chapter 13
"But evidence about birds in general should not be included in estimating whether penguins can fly, if there is already at least a moderate level of knowledge that in fact penguins are atypical birds in regard to flight. But how can the choice of A over B be motivated in terms of PLN theory? The essence of the answer is simple: in case B the independence assumption at the heart of the deduction rule is a bad one. Within the scope of birds, being a penguin and being a flier are not at all independent. On the other hand, looking at A, we see that within the scope of penguins, being a bird and being a flier are independent. So the reason B is ruled out is that if there is even a moderate amount of knowledge about the truth-value of (penguin --> fly), this gives a hint that applying the deduction rule’s independence assumption in this case is badly wrong.", Probabilistic Logic Networks,chapter 13
"On the other hand, what if a mistake is made and the inference B is done anyway? In this case the outcome could be that the system erroneously increases its estimate of the strength of the statement that penguins can fly. On the other hand, the revision rule may come to the rescue here. If the prior strength of (penguin --> fly) is 0, and inference B yields a strength of .9 for the same proposition, then the special case of the revision rule that handles wildly different truth-value estimates     Chapter 13: Aspects of Inference Control 277 may be triggered. If the 0 strength has much more confidence attached to i the .9, then they won’t be merged together, because it will be assumed that the .9 is an observational or inference error. Either the .9 will be thrown out, or it will be provisionally held as an alternate, non-merged, low-confidence hypothesis, awaiting further validation or refutation.", Probabilistic Logic Networks,chapter 13
"What is more interesting, however, is to consider the implications of the default inference notion for inference control. It seems that the following may be a valuable inference control heuristic: 1. Arrange terms in a hierarchy; e.g., by finding a spanning DAG of the terms in a knowledge base, satisfying certain criteria (e.g., maximizing total strength*confidence within a fixed limitation on the number of links). 2. When reasoning about a term, first do deductive reasoning involving the term’s immediate parents in the hierarchy, and then ascend the hierarchy, looking at each hierarchical level only at terms that were not visited at lower hierarchical levels. This is precisely the “default reasoning” idea – but the key point is that in PLN it lives at the level of inference control, not inference rules or formulas. In PLN, default reasoning is a timesaving heuristic, not an elementary aspect of the logic itself.", Probabilistic Logic Networks,chapter 13
"Rather, the practical viability of the default-reasoning inference-control heuristic is a consequence of various other elementary aspects of the logic, such as the ability to detect dependencies rendering the deduction rule inapplicable, and the way the revision rule deals with wildly disparate estimates.  ", Probabilistic Logic Networks,chapter 13
"  Chapter 14: Temporal and Causal Inference Abstract In this chapter we briefly consider the question of doing temporal and causal logic in PLN, and give an example involving the use of PLN to control a virtually embodied agent in a simulated environment. 14.1 Introduction While not as subtle mathematically as HOI, temporal logic is an extremely important topic conceptually, as the vast majority of human commonsense reasoning involves reasoning about events as they exist in, interrelate throughout, and change, over time. We argue that, via elaboration of a probabilistic event calculus and a few related special relationship types, temporal logic can be reduced to standard PLN plus some special bookkeeping regarding time-distributions. Causal inference, in our view, builds on temporal logic but involves other notions as well, thus introducing further subtlety. Here we will merely scratch the surface of the topic, outlining how notions of causality fit into the overall PLN framework.", Probabilistic Logic Networks,chapter 14
"14.2 A Probabilistic Event Calculus The Event Calculus (Kowalski & Sergo, 1986; Miller & Shanahan, 1991), a descendant of Situation Calculus (McCarthy & Hayes, 1969), is perhaps the bestfleshed-out attempt to apply predicate logic to the task of reasoning about commonsense events. A recent book by Erik Mueller (2006) reviews the application of Event Calculus to the solution of a variety of “commonsense inference problems,” defined as simplified abstractions of real-world situations. This section briefly describes a variation of Event Calculus called Probabilistic Event Calculus, in which the strict implications from standard Event Calculus are replaced with probabilistic implications. Other changes are also introduced, such as a repositioning of events and actions in the basic event ontology, and the introduction of a simpler mechanism to avoid the use of circumscription for avoiding the “frame problem.", Probabilistic Logic Networks,chapter 14
"These changes make it much easier to use Event Calculus     280 Probabilistic Logic Networks asoning within PLN. The ideas in this section will be followed up in the following one, which introduces specific PLN relationship types oriented toward temporal reasoning. We suggest that the variant of event calculus presented here, as well as being easily compatible with PLN, also results in a more “cognitively natural” sort of Event Calculus than the usual variants, though of course this sort of claim is hard to substantiate rigorously and we will not pursue this line of argument extensively here, restricting ourselves to a few simple points. Essentially, these points elaborate in the temporal-inference context the general points made in the Introduction regarding the overall importance of probability theory in a logical inference context: • There is growing evidence for probabilistic calculations in the human brain, whereas neural bases for crisp predicate logic and higher-order logic mechanisms like circumscription have never been identified even preliminarily.", Probabilistic Logic Networks,chapter 14
"• The use of probabilistic implications makes clearer how reasoning about events may interoperate with perceptions of events (given that perception is generally uncertain) and data mining of regularities from streams of perceptions of events (which also will generally produce regularities with uncertain truth-values). • As will be discussed in the final subsection, the pragmatic resolution of the “frame problem” seems more straightforward using a probabilistic variant of Event Calculus, in which different events can have different levels of persistence. 14.2.1 A Simple Event Ontology Probabilistic Event Calculus, as we define it here, involves the following categories of entity: • events o fluents • temporal predicates o holding o initiation o termination o persistence • actions • time distributions o time points o time intervals o general time distributions     Chapter 14: Temporal and Causal Inference 281 A “time distribution” refers to a probability density over the time axis; i assigns a probability value to each interval of time.", Probabilistic Logic Networks,chapter 14
"Time points are considered pragmatically as time distributions that are bump-shaped and supported on a small interval around their mean (true instantaneity being an unrealistic notion both psychologically and physically). Time intervals are considered as time distributions corresponding to characteristic functions of intervals. The probabilistic predicates utilized begin with the functions: hold(event) initiate(event) terminate(event) These functions are assumed to map from events into probabilistic predicates whose inputs are time distributions, and whose outputs are probabilistic truthvalues. The class of “events” may be considered pragmatically as the domain of these functions.", Probabilistic Logic Networks,chapter 14
"Based on these three basic functions, we may construct various other probabilistic predicates, such as holdsAt(event, time point) = (hold (event))(time point) initiatedAt(event,time point) = (initiate(event))(time point) terminatedAt(event, time point) = (terminate(event))(time point) holdsThroughout(event, time interval) = (hold (event))(time interval) initiatedThroughout(event, time interval) = (initiate (event))(time interval) terminatedThroughout(event, time interval) = (terminate (event))(time interval) holdsSometimeIn(event, time interval) = “There exists a time point t in time interval T so that holdsAt(E,t)” initiatedSometimeIn(event, time interval) =     282 Probabilistic Logic Networks “There exists a time point t in time interval T so that initiatedAt(E,t)” terminatedSometimeIn(event, time interval) = “There exists a time point t in time interval T so that", Probabilistic Logic Networks,chapter 14
"terminatedAt(E,t)” It may seem at first that the interval-based predicates could all be defined in terms of the time-point-based predicates using universal and existential quantification, but this isn’t quite the case. Initiation and termination may sometimes be considered as processes occupying non-instantaneous stretches of time, so that a process initiating over an interval does not imply that the process initiates at each point within that interval. Using the SatisfyingSet operator, we may also define some useful schemata corresponding to the above predicates. For example, we may define SS_InitiatedAt via Equivalence Member $X (SatSet InitatedAt(event, *)) ExOut SS_InitiatedAt(event) $X which means that, for instance, SS_InitiatedAt(shaving_event_33) denotes the time at which the event shaving_event_33 was initiated. We will use a similar notation for schemata associated with other temporal predicates, below.", Probabilistic Logic Networks,chapter 14
"Next, there are various important properties that may be associated with events, for example persistence and continuity. Persistent(event) Continuous(event) Increasing(event) Decreasing(event) which are (like hold, initiate, and terminate) functions outputting probabilistic predicates mapping time distributions into probabilistic truth-values. Persistence indicates that the truth-value of an event can be expected to remain roughly constant over time from the point at which is initiated until the point at which the event is terminated. A “fluent” is then defined as an event that is persistent throughout its lifetime. Continuous, Increasing, and Decreasing apply to nonpersistent events, and indicate that the truth-value of the event can be expected to {vary continuously, increase, or decrease} over time. For example, to say that the event of clutching (e.g., the agent clutching a ball) is persistent involves the predicate (isPersistent(clutching))([-infinity,infinity]).", Probabilistic Logic Networks,chapter 14
"     Chapter 14: Temporal and Causal Inference 283 Note that this predicate may be persistent throughout all time even if it is no throughout all time; the property of persistence just says that once the event is initiated its truth-value remains roughly constant until it is terminated. Other temporal predicates may be defined in terms of these. For example, an “action” may be defined as an initiation and termination of some event that is associated with some agent (which is different from the standard Event Calculus definition of action). Next, there is also use for further derived constructs such as initiates(action, event) indicating that a certain action initiates a certain event or done(event) which is true at time-point t if the event terminated before t (i.e., before the support of the time distribution representing t). Finally, it is worth noting that in a logic system like PLN the above predicates may be nested within markers indicating them as hypothetical knowledge.", Probabilistic Logic Networks,chapter 14
"This enables Probabilistic Event Calculus to be utilized much like Situation Calculus (McCarthy, 1986), in which hypothetical events play a critical role. 14.2.2 The “Frame Problem” A major problem in practical applications of Event Calculus is the “frame problem” (McCarthy, 1986; Mueller, 2006), which – as usually construed in AI – refers to the problem of giving AI reasoning systems implicit knowledge about which aspects of a real-world situation should be assumed not to change during a certain period of time. More generally, in philosophy the “frame problem” may be construed as the problem of how a rational mind should bound the set of beliefs to change when a certain action is performed. This section contains some brief conceptual comments on the frame problem and its relationship to Probabilistic Event Calculus and PLN in general.", Probabilistic Logic Networks,chapter 14
"For instance, if I tell you that I am in a room with a table in the center of it and four chairs around it, and then one of the chairs falls down, you will naturally assume that the three other chairs did not also fall down – and also that, for instance, the whole house didn’t fall down as well (perhaps because of an earthquake). There are really two points here: 1. The assumption that, unless there is some special reason to believe otherwise, objects will generally stay where they are; this is an aspect of what is sometimes known as the “commonsense law of inertia” (Mueller, 2006).     284 Probabilistic Logic Networks The fact that, even though the above assumption is often violated in reality, it is beneficial to assume it holds for the sake of making inference tractable. The inferential conclusions obtained may then be used, or not, in any particular case depending on whether the underlying assumptions apply there.", Probabilistic Logic Networks,chapter 14
"We can recognize the above as a special case of what we earlier called “default inference,” adopting terminology from the nonmonotonic reasoning community. After discussing some of the particulars of the frame problem, we will return to the issue of its treatment in the default reasoning context. The original strategy John McCarthy proposed for solving the frame problem (at least partially) was to introduce the formal-logical notion of circumscription (McCarthy, 1986). For example, if we know initiatesDuring(chair falls down, T1) regarding some time interval T1, then the circumscription of holdsDuring and T1 in this formula is initiatesDuring(x,T1) <==> x = “chair falls down” Basically this just is a fancy mathematical way of saying that no other events are initiated in this interval except the one event of the chair falling down.", Probabilistic Logic Networks,chapter 14
"If multiple events are initiated during the interval, then one can circumscribe the combination of events, arriving at the assertion that no other events but the ones in the given set occur. This approach has known shortcomings, which have been worked around via various mechanisms including the simple addition of an axiom stating that events are by default persistent in the sense given above (see Reiter, 1991; Sandewall, 1998). Mueller (2006) uses circumscription together with workarounds to avoid the problems classically found with it. However, none of these mechanisms is really satisfactory. In a real-world scenario there are always various things happening; one can’t simply assume that nothing else happens except a few key events one wants to reason about. Rather, a more pragmatic approach is to assume, for the purpose of doing an inference, that nothing important and unexpected happens that is directly relevant to the relationships one is reasoning about.", Probabilistic Logic Networks,chapter 14
"Event persistence must be assumed probabilistically rather than crisply; and just as critically, it need be assumed only for appropriate properties of appropriate events that are known or suspected to be closely related to the events being reasoned about in a given act of reasoning. This latter issue (the constraints on the assumption of persistence) is not adequately handled in most treatments of formal commonsense reasoning, because these treatments handle “toy domains” in which reasoning engines are fed small numbers of axioms and asked to reason upon them. This is quite different from the situation of an embodied agent that receives a massive stream of data from its sensors at nearly all times, and must define its own reasoning problems and its own     Chapter 14: Temporal and Causal Inference 285 relevant contexts. Thus, the real trickiness of the “frame problem” is not e what the logical-AI community has generally made it out to be; they have sidestepped the main problem due to their focus on toy problems.", Probabilistic Logic Networks,chapter 14
"Once a relevant context is identified, it is relatively straightforward for an AI reasoning system to say “Let us, for the sake of drawing a relatively straightforward inference, make the provisional assumption that all events of appropriate category in this context (e.g., perhaps: all events involving spatial location of inanimate household objects) are persistent unless specified otherwise.” Information about persistence doesn’t have to be explicitly articulated about each relevant object in the context, any more than an AI system needs to explicitly record the knowledge that each human has legs – it can derive that Ben has legs from the fact that most humans have legs; and it can derive that Ben’s refrigerator is stationary from the fact that most household objects are stationary. The hard part is actually identifying the relevant context, and understanding the relevant categories (e.g., refrigerators don’t move around much, but people do). This must be done inductively; e.g., by knowledge of what contexts have been useful for similar inferences in the past.", Probabilistic Logic Networks,chapter 14
"This is the crux of the frame problem: • Understanding what sorts of properties of what sorts of objects tend to be persistent in what contexts (i.e., learning specific empirical probabilistic patterns regarding the Persistent predicate mentioned above) • Understanding what is a natural context to use for modeling persistence, in the context of a particular inference (e.g., if reasoning about what happens indoors, one can ignore the out of doors even it’s just a few feet away through the wall, because interactions between the indoors and out of doors occur only infrequently) And this, it seems, is just plain old “AI inference” – not necessarily easy, but without any obvious specialness related to the temporal nature of the content material.", Probabilistic Logic Networks,chapter 14
"As noted earlier, the main challenge with this sort of inference is making it efficient, which may be done, for instance, by use of a domain-appropriate ontology to guide the default inference (allowing rapid estimation of when one is in a case where the default assumption of location-persistence will not apply). 14.3 Temporal Logic Relationships In principle one can do temporal inference in PLN without adding any new constructs, aside from the predicates introduced above and labeled Probabilistic Event Calculus. One can simply use standard higher-order PLN links to interrelate event calculus predicates, and carry out temporal inference in this way. However, it seems this is not the most effective approach. To do temporal inference in PLN efficiently and elegantly, it’s best to introduce some new relationship types: predictive implication (PredictiveImplication and EventualPredictiveImplication) and     286 Probabilistic Logic Networks tial and simultaneous conjunction and disjunction.", Probabilistic Logic Networks,chapter 14
"Here we introduce these concepts, and then describe their usage for inferring appropriate behaviors to control an agent in a simulation world. 14.3.1 Sequential AND Conceptually, the basic idea of sequential AND is that SeqAND (A , ..., A ) 1 n should be “just like AND but with the additional rule that the order of the items in the sequence corresponds to the temporal order in which they occur.” Similarly, SimOR and SimAND (“Sim” for Simultaneous) may be used to define parallelism within an SeqANDlist; e.g., the following pseudocode for holding up a convenience store: SeqAND enter store SimOR kill clerk knock clerk unconscious steal money leave store However, there is some subtlety lurking beneath the surface here. The simplistic interpretations of SeqAND as “AND plus sequence” is not always adequate, as the event calculus notions introduced above reveal. Attention must be paid to the initiations and terminations of events.", Probabilistic Logic Networks,chapter 14
"The basic idea of sequential AND must be decomposed into multiple notions, based on disambiguating properties we call disjointness and eventuality. Furthermore, there is some additional subtlety because the same sequential logical link types need to be applied to both terms and predicates. Applied to terms, the definition of a basic, non-disjoint, binary SeqAND is SeqAND A B <s, T> iff AND AND A B Initiation(B) – Initiation(A) lies in interval T     Chapter 14: Temporal and Causal Inference 287 Basically, what this says is “B starts x seconds after A starts, where x lies in t terval T.” Note that in the above we use <s, T> to denote a truth-value with strength s and time-interval parameter T. For instance, SeqAND <.", Probabilistic Logic Networks,chapter 14
"8,(0s,120s)> shaving_event_33 showering_event_43 means AND AND shaving_event_33 showering_event_43 SS_Initiation(showering_event_43)– SS_Initiation(shaving_event_33) in [0s, 120s] On the other hand, the definition of a basic disjoint binary SeqAND between terms is DisjointSeqAND A B <s, T> iff AND AND A B Initiation(B) – Termination(A) lies in interval T Basically, what this says is “B starts x seconds after A finishes, where x lies in the interval T” – a notion quite different from plain old SeqAND. EventualSeqAND and DisjointEventualSeqAND are defined similarly, but without specifying any particular time interval.", Probabilistic Logic Networks,chapter 14
"For example, EventualSeqAND A B <s> iff AND AND A B Evaluation after List SS_Initiation(B)     288 Probabilistic Logic Networks SS_Initiation(A) Next, there are several natural ways to define (ordinary or disjoint) SeqAND as applied to predicates. The method we have chosen makes use of a simple variant of “situation semantics” (Barwise 1983). Consider P and Q as predicates that apply to some situation; e.g., P(S) = shave(S) = true if shaving occurs in situation S Q(S) = shower(S) = true if showering occurs in situation S Let timeof(P,S) = the set of times at which the predicate P is true in situation S Then SeqAND P Q is also a predicate that applies to a situation; i.e.", Probabilistic Logic Networks,chapter 14
"(SeqAND P Q)(S) <s,T> is defined to be true of situation S iff AND <s> AND P(S) Q(S) timeof(Q,S) – timeof(P,S) intersects interval T In the case of an n-ary sequential AND, the time interval T must be replaced by a series of time intervals; e.g., SeqAND <s,(T ,…,T )> 1 n A 1 ... A n-1 A n is a shorthand for     Chapter 14: Temporal and Causal Inference 289 AND <s> SeqAND A A <T > 1 2 1 … SeqAND A A <T > n-1 n n Simultaneous conjunction and disjunction are somewhat simpler to handle. We can simply say, for instance, SimAND A B <s, T> iff AND HoldsThroughout(B,T) HoldsThroughout(A,T) and make a similar definition for SimOr.", Probabilistic Logic Networks,chapter 14
"Extension from terms to predicates using situation semantics works analogously for simultaneous links as for sequential ones. Related link types ExistentialSimAND and ExistentialSimOR, defined e.g. via SimOR A B <s, T> iff OR HoldsSometimeIn(B,T) HoldsSometimeIn(A,T) may also be useful. 14.3.2 Predictive Implication Next, having introduced the temporal versions of conjunction (and disjunction), we introduce the temporal version of implication: the relationship ExtensionalPredictiveImplication P Q <s,T> which is defined as ExtensionalImplication <s> P(S) [SeqAND( P , Q) <T>](S)     290 Probabilistic Logic Networks is a related notion of DisjointPredictiveImplication defined in terms of DisjointSeqAND. PredictiveImplication may also be meaningfully defined intensionally; i.e.", Probabilistic Logic Networks,chapter 14
"IntensionalPredictiveImplication P Q <s,T> may be defined as IntensionalImplication <s> P(S) [SeqAND( P , Q) <T>](S) and of course there is also mixed PredictiveImplication, which is, in fact, the most commonly useful kind. 14.3.3 Eventual Predictive Implication Predictive implication is an important concept but applying it to certain kinds of practical situations can be awkward. It turns out to be useful to also introduce a specific relationship type with the semantics “If X continues for long enough, then Y will occur.” In PLN this is called EventualPredictiveImplication, so that, e.g.", Probabilistic Logic Networks,chapter 14
"we may say EventualPredictiveImplication starve die EventualPredictiveImplication run sweat Formally, for events X and Y EventualPredictiveImplication P Q may be considered as a shorthand for Implication <s> P(S) [EventualSeqAND( P , Q) <T>](S) There are also purely extensional and intensional versions, and there is a notion of DisjointEventualPredictiveImplication as well.     Chapter 14: Temporal and Causal Inference 291 14.3.4 Predictive Chains Finally, we have found use for the notion of a predictive chain, where PredictiveChain <s,(T ,…,T )> 1 n A1 ... A n-1 A n means PredictiveImplication <s,T > n SeqAND <(T ,…,T )> 1 n-1 A 1 ...", Probabilistic Logic Networks,chapter 14
"A n-1 A n For instance, PredictiveChain Teacher is thirsty I go to the water machine I get a cup I fill the cup I bring the cup to teacher Teacher is happy Disjoint and eventual predictive chains may be introduced in an obvious way. 14.3.5 Inference on Temporal Relationships Inference on temporal-logical relationships must use both the traditional truthvalues and the probability of temporal precedence; e.g., in figuring out whether PredictiveImplication A B PredictiveImplication B C |PredictiveImplication A C one must calculate the truth-value of     292 Probabilistic Logic Networks Implication A C but also the odds that in fact A occurs before C. The key point here, conceptually, is that the probabilistic framework may be applied to time intervals, allowing PLN to serve as a probabilistic temporal logic not just a probabilistic static logic.", Probabilistic Logic Networks,chapter 14
"In the context of indefinite probabilities, the use of time distributions may be viewed as adding an additional level of Monte Carlo calculation. In handling each premise in an inference, one may integrate over all time-points, weighting each one by its probability according to the premise’s time distribution. This means that for each collection of (premise, time point) pairs, one does a whole inference; and then one revises the results, using the weightings of the premises’ time distributions. 14.4 Application to Making a Virtual Agent Learn to Play Fetch As an example of PLN temporal inference, we describe here experiments that were carried out using temporal PLN to control the learning engine of a humanoid virtual agent carrying out simple behaviors in a 3D simulation world. Specifically, we run through in detail how PLN temporal inference was used to enable the agent to learn to play the game of fetch in the AGISim game world.", Probabilistic Logic Networks,chapter 14
"The application described here was not a pure PLN application, but PLN’s temporal inference capability lay at its core. More broadly, the application involved the Novamente Cognition Engine architecture and the application of PLN, in combination with other simpler modules, within this architecture. The NCE is extremely flexible, incorporating a variety of carefully inter-coordinated learning processes, but the experiments described in this section relied primarily on the integration of PLN inference with statistical pattern mining based perception and functional program execution based agent control. From the PLN and Novamente point of view, the experiments reported here are interesting mostly as a “smoke test” for embodied, inference-based reinforcement learning, to indicate that the basic mechanisms required for doing this sort of inferential learning are integrated adequately and working correctly.", Probabilistic Logic Networks,chapter 14
"One final preliminary note: Learning to play fetch in the manner described here requires the assumption that the system already has learned (or been provided intrinsically with the capability for) how to recognize objects: balls, teachers, and the like. Object recognition in AGISim is something the NCE is able to learn, given the relatively low-bandwidth nature of the perceptions coming into it from AGISim, but that was not the focus of the reported work. Instead, we will take this capability for granted here and focus on perception/action/cognition integration specific to the “fetch” task.     Chapter 14: Temporal and Causal Inference 293 14.4.1 The Game of Fetch “Fetch” is a game typically played between a human and a dog.", Probabilistic Logic Networks,chapter 14
"The basic idea is a simple one: the human throws an object and says “fetch,” the dog runs to the object, picks it up, and brings it back to the human, who then rewards the dog for correct behavior. In our learning experiments, the teacher (a humanoid agent in AGISim) plays the role of the human and the Novamente-controlled agent plays the role of the dog. In more complex AGISim experiments, the teacher is actually controlled by a human being, who delivers rewards to Novamente by using the Reward controls on the AGISim user interface. Due to the simplicity of the current task, in the “fetch” experiments reported here the human controller was replaced by automated control code. The critical aspect of this automated teacher is partial reward.", Probabilistic Logic Networks,chapter 14
"That is, you can’t teach a dog (or a baby Novamente) to play fetch simply by rewarding it when it successfully retrieves the object and brings it to you, and rewarding it not at all otherwise – because the odds of it ever carrying out this “correct” behavior by random experimentation in the first place would be very low.. What is needed for instruction to be successful is for there to be a structure of partial rewards in place. We used here a modest approach with only one partial reward preceding the final reward for the target behavior. The partial reward is initially given for the agent when it manages to lift the ball from the ground, and once it has learnt to repeat this behavior we switch to the final stage where we only reward the agent when it proceeds to take the ball to the teacher and drop it there. 14.4.", Probabilistic Logic Networks,chapter 14
"2 Perceptual Pattern Mining Pattern mining, within the NCE, is a process that identifies frequent or otherwise significant patterns in a large body of data. The process is independent of whether the data is perceptual or related to actions, cognition, etc. However, it is often associated with perceptual data and abstractions from perceptual data. In principle, everything obtained via pattern mining could also be obtained via inference, but pattern mining has superior performance in many applications. The pattern mining relevant to the fetch learning consists of simply recognizing frequent sequences of events such as SequentialAND SimultaneousAND I am holding the ball I am near the teacher I get more reward.     294 Probabilistic Logic Networks in full-fledged PLN notation is SequentialAND SimultaneousAND EvaluationLink holding ball EvaluationLink near ListLink (me, teacher) Reward 14.4.", Probabilistic Logic Networks,chapter 14
"3 Particularities of PLN as Applied to Learning to Play Fetch From a PLN perspective, learning to play fetch is a simple instance of backward chaining inference – a standard inference control mechanism whose customization to the PLN context was briefly described in the previous chapter. The goal of the NCE in this context is to maximize reward, and the goal of the PLN backward chainer is to find some way to prove that if some actionable predicates become true, then the truth-value of EvaluationLink(Reward) is maximized. This inference is facilitated by assuming that any action can be tried out; i.e., the trying of actions is considered to be in the axiom set of the inference. (An alternative, also workable approach is to set a PredictiveImplicationLink($1, Reward) as the target of the inference, and launch the inference to fill in the variable slot $1 with a sequence of actions.", Probabilistic Logic Networks,chapter 14
"PredictiveImplicationLink is a Novamente Link type that combines logical (probabilistic) implication with temporal precedence. Basically, the backward chainer is being asked to construct an Atom that implies reward in the future. Each PredictiveImplicationLink contains a time-distribution indicating how long the target is supposed to occur after the source does; in this case the timedistribution must be centered on the rough length of time that a single episode of the “fetch” game occupies. To learn how to play fetch, The NCE must repeatedly invoke PLN backward chaining on a knowledge base consisting of Atoms that are constantly being acted upon by perceptual pattern mining as discussed above. PLN learns logical knowledge about circumstances that imply reward, and then a straightforward process called predicate schematization produces NCE objects called “executable schemata,” which are then executed.", Probabilistic Logic Networks,chapter 14
"This causes the system to carry out actions, which in turn lead to new perceptions, which give PLN more information to     Chapter 14: Temporal and Causal Inference 295 guide its reasoning and lead to the construction of new procedures, etc. In order to carry out very simple inferences about schema execution as required in the fetch example, PLN uses two primitive predicates: • try(X), indicating that the schema X is executed • can(X), indicating that the necessary preconditions of schema X are fulfilled, so that the successful execution of X will be possible Furthermore, the following piece of knowledge is assumed to be known by the system, and is provided to the NCE as an axiom: PredictiveImplication SimultaneousAnd Evaluation try X Evaluation can X Evaluation done X This simply means that if the system can do X, and tries to do X, then at some later point in time, it has done X.", Probabilistic Logic Networks,chapter 14
"Note that this implication may also be used probabilistically in cases where it is not certain whether or not the system can do X. Note that in essentially every case, the truth value of Evaluation can X will be uncertain (even if an action is extremely simple, there’s always some possibility of an error message from the actuator), so that the output of this PredictiveImplication will essentially never be crisp. The proper use of the “can” predicate necessitates that we mine the history of occasions in which a certain action succeeded and occasions in which it did not. This allows us to create PredictiveImplications that embody the knowledge of the preconditions for successfully carrying out the action. In the inference experiments reported here, we use a simpler approach because the basic mining problem is so easy; we just assume that “can” holds for all actions, and push the statistics of success/failure into the respective truth-values of the PredictiveImplications produced by pattern mining.", Probabilistic Logic Networks,chapter 14
"Next, we must explain a few shorthands and peculiarities that we introduced when adapting the PLN rules to carry out the temporal inference required in the fetch example. The ModusPonensRule used here is simply a probabilistic version of the standard Boolean modus ponens, as described earlier. It can also be applied to PredictiveImplications, insofar as the system keeps track of the structure of the proof tree so as to maintain the (temporally) proper order of arguments. By following the convention that the order in which the arguments to modus ponens must be applied is always the same as the related temporal order, we may extract a plan of consecutive actions, in an unambiguous order, directly from the proof tree. Relatedly, the AndRules used are of the form     296 Probabilistic Logic Networks B |A & B and can be supplied with a temporal inference formula so as to make them applicable for creating SequentialANDs.", Probabilistic Logic Networks,chapter 14
"We will also make use of what we call the SimpleANDRule, which embodies a simplistic independence assumption and finds the truth-value of a whole conjunction based only on the truth-values of its individual constituents, without trying to take advantage of the truth-values possibly known to hold for partial conjunctions. We use a “macro rule” called RewritingRule, which is defined as a composition of AndRule and ModusPonensRule. It is used as a shorthand for converting atoms from one form to another when we have a Boolean true implication at our disposal. What we call CrispUnificationRule is a “bookkeeping rule” that serves simply to produce, from a variable-laden expression (Atom defined by a ForAll, ThereExists or VariableScope relationship), a version in which one or more variables have been bound. The truth-value of the resulting atom is the same as that of the quantified expression itself.", Probabilistic Logic Networks,chapter 14
"Finally, we define the specific predicates used as primitives for this learning experiment, which enables us to abstract away from any actual motor learning: • Reward – a built-in sensation corresponding to the Novamente agent getting Reward, either via the AGISim teaching interface or otherwise having its internal Reward indicator stimulated • goto – a persistent event; goto(x) means the agent is going to x • lift – an action; lift(x) means the agent lifts x • drop – an action; drop(x) means the agent drops x if it is currently holding x (and when this happens close to an agent T, we can interpret that informally as “giving” x to T) • TeacherSay – a percept; TeacherSay(x) means that the teacher utters the string x • holding – a persistent event; holding(x) means the agent is holding x 14.4.", Probabilistic Logic Networks,chapter 14
"4 Learning to Play Fetch Via PLN Backward Chaining Next, we show a PLN inference trajectory that results in learning to play fetch once we have proceeded into the final reward stage. This trajectory is one of many produced by PLN in various indeterministic learning runs. When acted upon by the NCE’s predicate schematization process, the conclusion of this trajectory (depicted graphically in Figure 1) produces the simple schema (executable procedure)     Chapter 14: Temporal and Causal Inference 297 try goto Ball try lift Ball try goto Teacher try drop Ball. Figure 1.", Probabilistic Logic Networks,chapter 14
"Graphical depiction of the final logical plan learned for carrying out the fetch task It is quite striking to see how much work PLN and the perception system need to go through to get to this relatively simple plan, resulting in an even simpler logical procedure! Nevertheless, the computational work required to do this sort of inference is quite minimal, and the key point is that the inference is done by a general-purpose inference engine that was not at all tailored for this particular task. The inference target was EvaluationLink <0.80, 0.0099> Reward: PredicateNode <1, 0>. The final truth-value found for the EvaluationLink is of the form <strength, weight of evidence>, meaning that the inference process initially found a way to achieve the Reward with a strength of 0.80, but with a weight of evidence of only .0099 (the rather unforgiving scaling factor of which originates from the internals of the perception pattern miner).", Probabilistic Logic Networks,chapter 14
"Continuing the run makes the strength and weight of evidence increase toward 1.0. The numbers such as [9053948] following nodes and links indicate the “handle” of the entity in the NCE’s knowledge store, and serve as unique identifiers. The target was produced by applying ModusPonensRule to the combination of     298 Probabilistic Logic Networks PredictiveImplicationLink <0.8,0.", Probabilistic Logic Networks,chapter 14
"01> [9053948] SequentialAndLink <1,0> [9053937] EvaluationLink <1,0> [905394208] ""holdingObject"":PredicateNode <0,0> [6560272] ListLink [7890576] ""Ball"":ConceptNode <0,0> [6582640] EvaluationLink <1,0> [905389520] ""done"":PredicateNode <0,0> [6606960] ListLink [6873008] ExecutionLink [6888032] ""goto"":GroundedSchemaNode <0,0> [6553792] ListLink [6932096] ""Teacher"":ConceptNode <0,0> [6554000] EvaluationLink <1,0> [905393440] ""try"":PredicateNode <0,0> [6552272] ListLink [7504864] ExecutionLink [7505792] ""drop"":GroundedSchemaNode <0", Probabilistic Logic Networks,chapter 14
",0> [6564640] ListLink [7506928] ""Ball"":ConceptNode <0,0> [6559856] EvaluationLink <1,0> [905391056] ""Reward"":PredicateNode <1,0> [191] (the plan fed to predicate schematization, and shown in Figure 1) and SequentialAndLink <1,0.01> [840895904] EvaluationLink <1,0.01> [104300720] ""holdingObject"":PredicateNode <0,0> [6560272] ListLink [7890576] ""Ball"":ConceptNode <0,0> [6582640] EvaluationLink <1,1> [72895584] ""done"":PredicateNode <0,0> [6606960] ListLink [6873008] ExecutionLink [6888032] ""goto"":GroundedSchemaNode <0,0> [6553792] ListLink", Probabilistic Logic Networks,chapter 14
"[6932096] ""Teacher"":ConceptNode <0,0> [6554000] EvaluationLink <1,1> [104537344] ""try"":PredicateNode <0,0> [6552272] ListLink [7504864] ExecutionLink [7505792] ""drop"":GroundedSchemaNode <0,0> [6564640] ListLink [7506928] ""Ball"":ConceptNode <0,0> [6559856]     Chapter 14: Temporal and Causal Inference 299 Next, the SequentialANDLink [840895904] was produced by applying Si ANDRule to its three child EvaluationLinks. The EvaluationLink [104300720] was produced by applying ModusPonensRule to PredictiveImplicationLink <1,0.", Probabilistic Logic Networks,chapter 14
"01> [39405248] SequentialANDLink <1,0> [39403472] EvaluationLink <1,0> [39371040] ""done"":PredicateNode <0,0> [6606960] ListLink [7511296] ExecutionLink [7490272] ""goto"":GroundedSchemaNode<0,0> [6553792] ListLink [7554976] ""Ball"":ConceptNode <0,0> [6558784] EvaluationLink <1,0> [39402640] “done"":PredicateNode <0,0> [6606960] ListLink [7851408] ExecutionLink [7865376] ""lift"":GroundedSchemaNode <0,0> [6553472] ListLink [7890576] ""Ball"":ConceptNode <0,0> [6582640] EvaluationLink <1,0> [39404448] ""holdingObject"":PredicateNode <0", Probabilistic Logic Networks,chapter 14
",0> [6560272] ListLink [7890576] ""Ball"":ConceptNode <0,0> [6582640] which was mined from perception data (which includes proprioceptive observation data indicating that the agent has completed an elementary action), and to SequentialANDLink <1,1> [104307776] EvaluationLink <1,1> [72926800] ""done"":PredicateNode <0,0> [6606960] ListLink [7511296] ExecutionLink [7490272] ""goto"":GroundedSchemaNode <0,0> [6553792] ListLink [7554976] ""Ball"":ConceptNode <0,0> [6558784] EvaluationLink <1,1> [72913264] ""done"":PredicateNode <0,0> [6606960] ListLink [7851408] ExecutionLink [7865376] ""lift"":GroundedSchemaNode <0,0", Probabilistic Logic Networks,chapter 14
"> [6553472] ListLink [7890576] ""Ball"":ConceptNode <0,0> [6582640].     300 Probabilistic Logic Networks The SequentialANDLink [104307776] was produced by applying SimpleANDRule to its two child EvaluationLinks. The EvaluationLink [72926800] was produced by applying RewritingRule to EvaluationLink <1,1> [72916304] ""try"":PredicateNode <0,0> [6552272] ListLink [7511296] ExecutionLink [7490272] ""goto"":GroundedSchemaNode <0,0> [6553792] ListLink [7554976] ""Ball"":ConceptNode <0,0> [6558784] and EvaluationLink <1,1> [72923504] ""can"":PredicateNode <1,0> [6566128] ListLink [7511296] ExecutionLink [7490272] ""goto"":", Probabilistic Logic Networks,chapter 14
"GroundedSchemaNode <0,0> [6553792] ListLink [7554976] ""Ball"":ConceptNode <0,0> [6558784]. The EvaluationLink [72916304], as well as all other try statements, were considered axiomatic and technically produced by applying the CrispUnificationRule to ForallLink <1,1> [6579808] ListLink <1,0> [6564144] ""$A"":VariableNode <1,0> [6563968] EvaluationLink <1,0> [6579200] ""try"":PredicateNode <0,0> [6552272] ListLink <1,0> [6564144] ""$A"":VariableNode <1,0> [6563968] The EvaluationLink [72923504], as well as all other can statements, were considered axiomatic and technically produced by applying CrispUnificationRule to: ForallLink <", Probabilistic Logic Networks,chapter 14
"1,1> [6559424] ListLink <1,0> [6564496] ""$B"":VariableNode <1,0> [6564384] EvaluationLink <1,0> [6550720] ""can"":PredicateNode <1,0> [6566128] ListLink <1,0> [6564496] ""$B"":VariableNode <1,0> [6564384]     Chapter 14: Temporal and Causal Inference 301 The EvaluationLink [72913264] was produced by applying RewritingRule EvaluationLink <1,1> [72903504] ""try"":PredicateNode <0,0> [6552272] ListLink [7851408] ExecutionLink [7865376] ""lift"":GroundedSchemaNode <0,0> [6553472] ListLink [7890576] ""Ball"":ConceptNode <0,0> [6582640]", Probabilistic Logic Networks,chapter 14
"and EvaluationLink <1,1> [72909968] ""can"":PredicateNode <1,0> [6566128] ListLink [7851408] ExecutionLink [7865376] ""lift"":GroundedSchemaNode <0,0> [6553472] ListLink [7890576] ""Ball"":ConceptNode <0,0> [6582640] And finally, returning to the first PredictiveImplicationLink’s children, EvaluationLink [72895584] was produced by applying RewritingRule to the axioms EvaluationLink <1,1> [72882160] ""try"":PredicateNode <0,0> [6552272] ListLink [6873008] ExecutionLink [6888032] ""goto"":GroundedSchemaNode <0,0> [6553792] ListLink [6932096] ""Teacher"":ConceptNode <0,0> [6554000] and EvaluationLink <1,", Probabilistic Logic Networks,chapter 14
"1> [72888224] ""can"":PredicateNode <1,0> [6566128] ListLink [6873008] ExecutionLink [6888032] ""goto"":GroundedSchemaNode <0,0> [6553792] ListLink [6932096] ""Teacher"":ConceptNode <0,0> [6554000] In conclusion, we have given a relatively detailed treatment of a simple learning experiment – learning to play fetch – conducted with the NCE integrative AI system in the AGISim simulation world. Our approach was to first build an integrative AI architecture we believe to be capable of highly general learning, and     302 Probabilistic Logic Networks en apply it to the fetch test, while making minimal parameter adjustment to the specifics of the learning problem.", Probabilistic Logic Networks,chapter 14
"This means that in learning to play fetch the system has to deal with perception, action, and cognition modules that are not fetch-specific, but are rather intended to be powerful enough to deal with a wide variety of learning tasks corresponding to the full range of levels of cognitive development. Ultimately, in a problem this simple the general-intelligence infrastructure of the NCE and the broad sophistication of PLN don’t add all that much. There exist much simpler systems with equal fetch-playing prowess. For instance, the PLN system’s capability of powerful analogical reasoning is not being used at all here, and its use in an embodiment context is a topic for another paper. However, this sort of simple integrated learning lays the foundation for more complex embodied learning based on integrated cognition, the focus of much of our ongoing work. 14.5 Causal Inference Temporal inference, as we have seen, is relatively conceptually simple from a probabilistic perspective.", Probabilistic Logic Networks,chapter 14
"It leads to a number of new link types and a fair amount of bookkeeping complication (the node-and-link constructs shown in the context of the fetch example won’t win any prizes for elegance), but is not fundamentally conceptually problematic. The tricky issues that arise, such as the frame problem, are really more basic AI issues than temporal inference issues in particular. Next, what about causality? This turns out to be a much subtler matter. There is much evidence that human causal inference is pragmatic and heterogeneous rather than purely mathematical (see discussion and references in Goertzel 2006). One illustration of this is the huge variance in the concept of causality that exists among various humans and human groups (Smith 2003). Given this, it’s not to be expected that PLN or any other logical framework could, in itself, give a thorough foundation for understanding causality. But even so, there are interesting connections to be drawn between PLN and aspects of causal inference.", Probabilistic Logic Networks,chapter 14
"Predictive implication, as discussed above, allows us to discuss temporal correlation in a pragmatic way. But this brings us to what is perhaps the single most key conceptual point regarding causation: correlation and causation are distinct. To take the classic example, if a rooster regularly crows before dawn, we do not want to infer that he causes the sun to rise. In general, if X appears to cause Y, it may actually be due to Z causing both X and Y, with Y appearing later than X. We can only be sure that this is not the case if we have a way to identify alternative causes and test them in comparison to the causes we think are real. Or, as in the rooster/dawn case, we may have background knowledge that makes the “X causes Y” scenario intrinsically implausible in terms of the existence of potential causal mechanisms.", Probabilistic Logic Networks,chapter 14
"  Chapter 14: Temporal and Causal Inference 303 Let’s consider this example in a little more detail. In the case of rooster dawn, clearly we have both implication and temporal precedence. Hence there will be a PredictiveImplication from “rooster crows” to “sun rises.” But will the reasoning system conclude from this PredictiveImplication that if a rooster happens to crow at 1 AM the sun is going to rise really early that morning – say, at 2 AM? How is this elementary error avoided? There are a couple of answers here. The first has to do with the intension/extension distinction. It says: The strength of this particular PredictiveImplication may be set high by direct observation, but it will be drastically lowered by inference from more general background knowledge.", Probabilistic Logic Networks,chapter 14
"Specifically, much of this inference will be intensional in nature, as opposed to the purely extensional information (direct evidence-counting) that is used to conclude that roosters crowing imply sun rising. We thus conclude that one signifier of bogus causal relationships is when ExtensionalPredictiveImplication A B has a high strength but IntensionalPredictiveImplication A B has a low strength. In the case of A = rooster crows B = sun rises the weight-of-evidence of the intensional relationship is much higher than that of the extensional relationship, so that the overall PredictiveImplication relationship comes out with a fairly low strength. To put it more concretely, if the reasoning system had never seen roosters crow except an hour before sunrise, and had never seen the sun rise except after rooster crowing, the posited causal relation might indeed be created.", Probabilistic Logic Networks,chapter 14
"What would keep it from surviving for long would be some knowledge about the mechanisms underlying sunrise. If the system knows that the sun is very large and rooster crows are physically insignificant forces, then this tells it that there are many possible contexts in which rooster crows would not precede the sun rising. Conjectural reasoning about these possible contexts leads to negative evidence in favor of the implication PredictiveImplication rooster_crows sun_rises which counterbalances – probably overwhelmingly – the positive evidence in favor of this relationship derived from empirical observation. More concretely, one has the following pieces of evidence:     304 Probabilistic Logic Networks PredictiveImplication <.00, .99> small_physical_force movement_of_large_object PredictiveImplication <.99,.99> rooster_crows small_physical_force PredictiveImplication <.99, .99> sun_rises movement_of_large_object PredictiveImplication <.00,.", Probabilistic Logic Networks,chapter 14
"99> rooster_crows sun_rises which must be merged with PredictiveImplication rooster_crows sun_rises <1,c> derived from direct observation. So it all comes down to: How much more confident is the system that a small force can’t move a large object, than that rooster crows always precede the sunrise? How big is the parameter we’ve denoted c compared to the confidence we’ve arbitrarily set at .99? Of course, for this illustrative example we’ve chosen only one of many general world-facts that contradicts the hypothesis that rooster crows cause the sunrise… in reality many, many such facts combine to effect this contradiction. This simple example just illustrates the general point that reasoning can invoke background knowledge to contradict the simplistic “correlation implies causation” conclusions that sometimes arise from direct empirical observation. 14.5.", Probabilistic Logic Networks,chapter 14
"1 Aspects of Causality Missed by a Purely Logical Analysis In this section we will briefly discuss a couple of aspects of causal inference that seem to go beyond pure probabilistic logic – and yet are fairly easily integrable into a PLN-based framework. This sort of discussion highlights what we feel will ultimately be the greatest value of the PLN formalism; it formulates logical inference in a way that fits in naturally with a coherent overall picture of cognitive function. Here we will content ourselves with a very brief sketch of these ideas, as to pursue it further would lead us too far afield. 14.5.1.1 Simplicity of Causal Mechanisms The first idea we propose has to do with the notion of causal mechanism. The basic idea is, given a potential cause-effect pair, to seek a concrete function mapping the cause to the effect, and to consider the causality as more substantial if this function is simpler.", Probabilistic Logic Networks,chapter 14
"In PLN terms, this means that one is not only looking at the     Chapter 14: Temporal and Causal Inference 305 IntensionalPredictiveImplication relationship underlying a posited causal rel ship, but one is weighting the count of this relationship more highly if the Predicates involved in deriving the relationship are simpler. This heuristic for countbiasing means that one is valuing simple causal mechanisms as opposed to complex ones. The subtlety lies in the definition of the “simplicity” of a predicate, which relies on pattern theory (Goertzel 2006) as introduced above in the context of intensional inference. 14.5.1.2 Distal Causes, Enabling Conditions As another indication of the aspects of the human judgment of causality that are omitted by a purely logical analysis, consider the distinction between local and distal causes.", Probabilistic Logic Networks,chapter 14
"For example, does an announcement by Greenspan cause the market to change, or is he just responding to changed economic conditions on interest rates, and they are the ultimate cause? Or, to take another example, suppose a man named Bill drops a stone, breaking a car windshield. Do we want to blame (assign causal status to) Bill for dropping the stone that broke the car windshield, or his act of releasing the stone, or perhaps the anger behind his action, or his childhood mistreatment by the owner of the car, or even the law of gravity pulling the rock down? Most commonly we would cite Bill as the cause because he was a free agent. But different causal ascriptions will be optimal in different contexts: typically, childhood mistreatment would be a mitigating factor in legal proceedings in such a case. Related to this is the distinction between causes and so-called enabling conditions. Enabling conditions predictively imply their “effect,” but they display no significant variation within the context considered pertinent.", Probabilistic Logic Networks,chapter 14
"For, example oxygen is necessary to use a match to start a fire, but because it is normally always present we usually ignore it as a cause, and it would be called an enabling condition. If it really is always present, we can ignore it in practice; the problem occurs when it is very often present but sometimes is not, as for example when new unforeseen conditions occur. We believe it is fairly straightforward to explain phenomena like distal causes and enabling conditions, but only at the cost of introducing some notions that exist in Novamente but not in PLN proper. In Novamente, Atoms are associated with quantitative “importance” values as well as truth-values. The importance value of an Atom has to do with how likely it is estimated to be that this Atom will be useful to the system in the future. There are short- and long-term importance values associated with different future time horizons.", Probabilistic Logic Networks,chapter 14
"Importance may be assessed via PLN inference, but this is PLN inference based regarding propositions about how useful a given Atom has been over a given time interval. It seems that the difference between a cause and an enabling condition often has to do with nonlogical factors. For instance, in Novamente PLN Atoms are associated not only with truth-values but also with other numbers called attention     306 Probabilistic Logic Networks , including for instance “importance” values indicating the expected utility of the system to thinking about the Atom. For instance, the relationship PredictiveImplication oxygen fire may have a high strength and count, but it is going to have a very low importance unless the AI system in question is dealing with some cases where there is insufficient oxygen available to light fires. A similar explanation may help with the distinction between distal and local causes.", Probabilistic Logic Networks,chapter 14
"Local causes are the ones associated with more important predictive implications – where importance needs to be assigned, by a reasoning system, based on inferences regarding which relationships are more likely to be useful in future inferences.  ", Probabilistic Logic Networks,chapter 14
"  Chapter 1 Introduction The general problem addressed in this book is how to effectively carry out reasoning, knowledge discovery and querying based on huge amounts of complex information about real-world situations. Specically we conceive real-world reasoning here mainly as massively scalable reasoning involving uncertainty, space, time, cause and context. Of course there are other important aspects to reasoning about the real world we live in, e.g. the hierarchical structure of much of the human world, and we will briey touch on some of these here as well. But for the purposes of this book, when we mention real-world reasoning or RWR, were mostly talking about uncertainty, spacetime, cause, context and scalability. The RWR problem is critical in at least two respects: as part of the broader pursuit of articial general intelligence (AGI) (Goertzel & Pennachin, 2006; Goertzel et al.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 1
"2006a; Goertzel & Bugaj, 2008; Hart & Goertzel, 2008), and in terms of the practical information processing needs that have arisen in current society. On the AGI side, it is obvious that every human brain ingests a huge amount of knowledge each waking hour, and somehow we manage to query and analyze our huge, dynamic internal data stores. No AGI design can possibly succeed without some way to effectively carry out intelligent judgment and discovery based on these data stores. AGI also has other aspects, e.g. procedure learning and goal renement (to name just two), but RWR is certainly a huge part of the puzzle. On the practical information processing side, anyone who lives in a developed country these days is aware of the tremendous amount of data continually being gathered about all manner of aspects of the human and natural worlds. Much of this data is discarded shortly after its gathered, but much of it is retained in various repositories.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 1
"However, even when the data is retained, it is rarely utilized to anywhere near the full extent possible, because our state-of-the-art technologies for storing, querying, mining and analyzing very 11      12 Real-World Reasoning large data stores are still very primitive and simplistic (not only compared to what is in principle possible, but compared to what we know to be possible based on contemporary mathematics and computer science). In these pages we review a class of approaches to handling these RWR problems using uncertain, spatiotemporal, contextual and causal logic. Uncertain logic is not the only possible approach to the RWR problem, but we believe its one very promising approach, and its our focus here. While the rst RWR-capable logic system has yet to be constructed, we make an argument, via detailed review of the literature and the state of the art and suggestion of some original ideas, that the time is ripe for their construction.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 1
"The book is intended to serve two purposes: to provide a reasonably accessible overview of the RWR problem and the available technologies and concepts for its solution; and to provide a sketch of one possible avenue toward solution. Toward the overview goal, we review a number of concepts and technologies  some recently developed, some more classical  that address aspects of the RWR problem. While our treatment centers on formal logic, we also introduce material from other areas such as graph databases, probability theory, cognitive architecture and so forth as appropriate. After reviewing a variety of other logical approaches, we present our own approach to real-world reasoning, which is based on the Probabilistic Logic Networks (PLN) framework (Goertzel et al., 2008); and give some detailed suggestions regarding how one might address the scalable real-world inference problem effectively via integrating PLN with other ideas and technologies described.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 1
"Our goal in this regard is not to propose a particular highly-specic technical solution, but rather to describe a class of possible solutions that might be described as scalable spatiotemporal uncertain logic systems. In this vein, in the later chapters we give a number of detailed examples showing the kinds of results one might expect to obtain by approaching a large knowledge store containing information about everyday human activities with the Probabilistic Logic Networks inference framework that we have developed in prior publications. 1.1 The Advantages of a Logical Approach There are many advantages to the logic-based approach relative to others, some of which will be alluded to as the text progresses, but perhaps the largest advantage is its relative representational transparency. That is, if the knowledge stored in a knowledge base, and the patterns recognized in this knowledge base, are represented in a logical format, then      Introduction 13 it is reasonably tractable for humans to inspect this knowledge and these patterns.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 1
"This is a major practical advantage in terms of allowing hybridized human/articial intelligence  and, given the comments made above about the interesting but erratic performance of AI algorithms in our domain, this seems a very important point. Given the advantage of logic-based approaches in terms of representational transparency, the only reason to choose an opaque approach over a logic-based approach would be if the opaque approach were dramatically superior in its capabilities. However, this currently seems not to be the case: in fact the evidence so far seems to indicate that logic-based approaches are the most powerful ones in this sort of context. Some theorists have argued against logic-based approaches to real-world data on the grounds that there are problems with grounding logical symbols in real-world data (the so-called symbol grounding problem (Goertzel et al., 2006a)). However, these objections do not hold up to scrutiny.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 1
"It is true that logic-based approaches cannot function adequately for real-world applications unless the logical symbols used are explicitly associated with observed data-patterns, but there are well-understood technologies for making such associations. Historically, many logic-based AI systems have been used in an ungrounded way, not containing components that directly connect the logical terms used with real-world observations  but this is a problem of poor system architecture, not a aw of the logic-based approach in itself. 1.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 1
"2 Main High-Level Conclusions To give a small hint at what is to come, the main conclusions at the end of our investigation are that  the logic-based approach has the in-principle power to solve the problem of querying and analyzing very large scale spatiotemporal knowledge bases, in a manner respecting the contextual and causal knowledge contained therein  there is a signicant amount of scientic and technological knowledge in the literature regarding nearly every aspect of the application of logic-based technology to this problem  the Achilles heel of current relevant logic-based technology is scalability  the keys to achieving scalability in this context are conceptually understood  adaptive inference control and attention allocation  but have not been explored nearly as thoroughly as they need to be      14 Real-World Reasoning  it seems likely that special techniques may be useful for adaptively controlling real-world scalable inference as opposed to inference in other domains (e.g.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 1
"mathematical theorem proving)  one viable way to achieve scalable real-world reasoning may be to use the Probabilistic Logic Networks framework, perhaps within an integrative AGI design like OpenCog which provides exible means for adaptive inference control We thus suggest that a critical focus of research should be on the development of methods for exploiting the specic statistical structure of real spatiotemporal data, to adaptively guide logical inference methods in performing query and analytical processing. 1.3 Summary We now briey review the chapters to follow, summarizing the main themes and ideas to be introduced. 1.3.1 Part I: Representations and Rules for Real-World Reasoning Part I of the book reviews a host of approaches described in the literature for representing and reasoning about real-world knowledge, including temporal, spatial, contextual and causal knowledge.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 1
"Chapter Two reviews many of the varieties of formal logic that have been developed during the last century, with a focus on those approaches that appear most relevant to the large-scale information-management problem. We begin with a basic review of predicate and term logic, and then move on to subtler variations such as modal logic (the logic of possibility) and deontic logic (the logic of obligation). We also discuss the methods that logic systems use to actually draw logical conclusions based on the information provided to them: forward chaining, in which information items are combined exploratorily to come to new conclusions; and backward chaining, in which a question is posed to the system and it then seeks to nd the answer using multiple logical inference steps based on the information at its disposal. Chapter Three considers various methods of handling uncertainty in formal logic, including fuzzy sets and logic, possibility theory, probability theory, and imprecise and indenite probabilities.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 1
"Uncertainty management is critical to our target application, because a great percentage of real-world data is uncertain, and most of the conclusions one can draw based on real-world data are also uncertain. So, logic systems that only deal with      Introduction 15 absolute truth or falsehood are not going to be very useful for our target application. But, the literature contains a huge number of different methods for dealing with uncertainty  and one of our conclusions is that there isnt necessarily a single best approach. Rather, a practical solution may integrate more than one approach, for instance using both fuzzy and probabilistic methods as appropriate. Figures 1.1 and 1.2 from Chapter Three illustrate several of the possible methods for representing time within logic: Fig. 1.1 Chapter Four grapples with the various ways logicians and computer scientists have devised to represent time within logic. This is a core issue for our current pursuit, because a large percentage of real-world knowledge involves time.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 1
"The most standard method for handling time within logic is Allens interval algebra, which treats time-intervals rather than points as the atomic temporal entities, and enumerates a set of rules for combining and reasoning about time-intervals; but it suffers the decit of being crisp rather than explicitly handling uncertainty. So we review several methods of extending interval algebra to deal with uncertainty, including methods involving fuzziness, probability, and combinations of the two. Figure 1.3 from Chapter Four illustrates the logical relationships between time intervals specied by Allens interval algebra:      16 Real-World Reasoning Fig. 1.2 Fig. 1.3 And the Figure 1.4, also from Chapter Four, is a graphical representation of some temporal relationships between events, using a probabilistic variation of Allens interval algebra: Continuing the theme of its predecessor, Chapter Five deals with temporal inference, reviewing the multiple methods presented in the literature for incorporating time into logic.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 1
"These include methods that simply treat time like any other logical information, and also methods that give time a special status, including reied and modal techniques. We conclude that methods giving time a special status are likely to be dramatically more efcient, and express a particular favor for reied techniques compatible with Allens interval algebra (discussed above) and its variations. We give some concrete examples of temporal inference regarding peoples daily activities.      Introduction 17 Fig. 1.4 For instance, one of the example problems we consider involves a query regarding which people were in the same place as Jane last week, and a knowledge base with the following information:  Susie and Jane use the same daycare center, but Jane uses it everyday, whereas Susie only uses it when she has important meetings (otherwise she works at home with her child).  Susie sends a message stating that Tuesday she has a big meeting with a potential funder for her business.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 1
"Given this information, inference is needed to gure out that on Tuesday Susie is likely to put her child in daycare, and hence (depending on the time of the meeting!) potentially to be at the same place as Jane sometime on Tuesday. To further estimate the probability of the two women being in the same place, one has to do inference based on the times Jane usually picks up and drops off her child, and the time Susie is likely to do so based on the time of her meeting. We show in detail how temporal inference methods can be used to carry out this commonsense inference, and other similar examples. Chapter Six builds on the treatment of time and presents an analogous discussion of a more complex subject, space (critical to our core theme as a substantial percentage of real-world knowledge involves spatial as well as temporal information). We review the Region Connection Calculus, which models the logic of space in terms of a xed set of logical relationships between logical terms that correspond to spatial regions.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 1
"As this is a simple but limited technique, we then consider more complex approaches to representing space in logic, including directional calculus, and occupancy grids as utilized in robotics (which are extremely general yet also resource-intensive, and so should only be used when      18 Real-World Reasoning simpler methods fail). The following diagram, drawn from Chapter Six, depicts the relationships between various spatial regions and spatially distributed phenomena (NTPP stands for Non-Tangential Proper Part, and O stands for Overlapping; these are spatialrelationship predicates drawn from the Region Connection Calculus formalism): Fig. 1.5 Next, as well as time and space, another critical aspect of real-world reasoning is context. Nearly all real-world knowledge implicitly or explicitly gains its meaning from the specic context in which it is understood by human knowledge producers and consumers to exist. So if logical methods are to be applied effectively to real-world data, it is important that they explicitly represent contextuality.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 1
"In Chapter Seven, we review a number of approaches to representing contextuality in logic, and give detailed examples of several. We also consider one example of context representation that is particularly acutely relevant to our application area: the use of contextual logic to handle user modeling. If different users of an information system have different biases and interests, then a logic based system can pay attention to this and give them different information via treating each user as a separate context and then doing contextually-biased reasoning. In addition to context representation, Chapter Seven treats contextual inference, reviewing a number of techniques presented in the literature, and again nding favor in those methods that explicitly represent context as a special relationship within the base logic. We give a concrete examples of contextual inference applied to practical problems regarding people and their interrelationships. One example we consider involves the following assumptions:  Alison is an accountant who is also a musician. Alison is emotional in the context of music, but not in the context of accounting.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 1
"She frequently mentions Canadian place      Introduction 19 names in the context of music (maybe shes a Canadian music fan), but not in the context of accounting.  Bob is in a similar situation, but he frequently mentions Canadian related stuff in both the music and accounting contexts.  Clark is also in a similar situation, but he frequently mentions Canadian related stuff only in the accounting context, not the music context.  People who have a lot to do with Canadian people, and a lot to do with money, have a chance of being involved in suspicious log trafcking activities. We then show how contextual inference methods can be used to estimate the probability that Clark may be involved with log trafcking. Chapter Eight turns briey to causal reasoning, reviewing the multiple formalisms used to represent the notion of causality, and connecting causation to probabilistic and inductive reasoning. 1.3.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 1
"2 Part II: Acquiring, Storing and Mining Logical Knowledge Our focus in this book is doing logical reasoning on real-world knowledge, and this is a large and critical topic  but, once one has a large store of real-world knowledge in logical format, reasoning per se is not the only thing that must be done with it. Part II, a brief interlude at the center of the book, consists of three short chapters which lightly touch three other important issues to do with large stores of logical knowledge: acquiring logical knowledge via transforming real-world data, storing and querying large volumes of logical knowledge, and mining patterns from large logical knowledge stores. Each of these topics could be a book in itself, and here we only roughly sketch the main problems involved and give some pointers into the literature.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 1
"Chapter Nine very briey reviews existing relevant literature, discussing the use of natural language processing technology to map text and voice into sets of logical relationships; and the use of image processing and heuristic algorithms to create logical relationships out of tables, graphs and diagrams. For instance, the following diagram drawn from Chapter Six shows some logical relationships that current NLP technology can extract from the simple sentence Gone for dinner with Bob: Another key question that must be answered if logic-based methods are to be applied to massive real-world data stores is: how can a huge amount of logical knowledge be stored and manipulated? This is not a question about logic per se, its a question about modern computer systems, database and database-like technologies, and so forth. In Chapter Ten,      20 Real-World Reasoning Fig. 1.6 we review a number of current technologies, including relational databases, RDF triplestores, object databases, and hypergraph and graph databases.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 1
"Our conclusion is that at present the latter form the best option, and we give some specic examples of how to translate complex logical knowledge into the specic format required for a graph database. The following table, drawn from Chapter Ten, summarizes some of our ndings in more depth: Technology Strengths Weaknesses Relational DBs  Mature, enterprise grade solutions  Ease of integration with other systems  Poor conceptual t for logical information storage  Inadequate model for reasoning  Complex scalability ObjectOriented DBs  Better conceptual t than relational DBs (still not perfect)  Mature solutions  Single data model  Small ecosystem  Not designed for reasoning Continued on next page      Introduction 21 continued from previous page Technology Strengths Weaknesses Graph DBs  Flexible, dynamic data model  Good performance and scalability  Designed with data analysis in mind  Less mature than competing technologies Hypergraph DBs  Best data model t  Designed with reasoning and data analysis in mind  Alpha stage technology","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 1
"RDF Triplestores  Semantic web friendly  Adequate data model for some inferences  Less mature technology  Rigid data model Documentoriented DBs  Flexible data model  Performance and scalability  Rapidly maturing solutions  Not adequate for reasoning and analysis  More work is left for application layer Columnoriented DBs  Very exible, dynamic data model  Performance and scalability  Rapidly maturing solutions  More work is left for application layer  Not designed for reasoning Key-value DBs  Extremely good performance and scalability  Mature and rapidly maturing solutions  No data model, leaving most work for application layer  Not designed for reasoning Chapter Ten turns to one of the most important applications desirable to carry out on large data stores  data mining (also known as information exploitation, pattern discovery, etc.). Most existing datamining techniques are either specialized for relational      22 Real-World Reasoning databases, or dont scale beyond small knowledge stores.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 1
"We review here some specic datamining algorithms in depth. One conclusion drawn is that, for datamining to really be effective in this context, it will need to be hybridized with inference. Datamining technology, in itself, will always nd too many potentially interesting patterns for any human user to want to explore. So logical inference technology is needed to lter the results of datamining, either via interaction with the datamining process, or via postprocessing. 1.3.3 Part III: Real World Reasoning Using Probabilistic Logic Networks The second major of the book provides a detailed exploration of the applicability of one particular logical framework, Probabilistic Logic Networks, to real-world reasoning problems. This part is different from the previous ones, in that it comprises primarily original work, rather than literature survey and summary.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 1
"Chapters Twelve and Thirteen summarize Probabilistic Logic Networks (PLN), the particular uncertain logic system called which several of the authors (Goertzel and Pennnachin and Geisweiller) and their colleagues have developed over the last years (and published extensively on elsewhere). We outline the basic mechanisms via which PLN deals with a variety of aspects of inference, including term and predicate logic, extensional and intensional inference, and contextual, causal, spatial and temporal inference. Chapter Fourteen turns to the specic problem of inference about changes in large knowledge bases. We consider several concrete examples including the following causal inference scenario:  Before March 2007, Bob never had any Canadian friends except those who were also friends of his wife.  After March 2007, Bob started acquiring Canadian friends who were not friends of his wife.  In late 2006, Bob started collecting Pokemon cards.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 1
"Most of the new Canadian friends Bob made between March 2007 and Late 2007 are associated with Pokemon cards  In late 2006, Bob started learning French. Most of the new Canadian friends Bob made between March 2007 and Late 2007 are Quebecois. We show in detail how a PLN inference engine, combining temporal inference with causal inference and numerous other aspects, can attempt to answer the question: What is the probable cause of Bob acquiring new Canadian friends who are not also friends of his wife?      Introduction 23 Chapter Fourteen also considers spatial inference in the context of change analysis, giving particular attention to the incorporation of the Region Connection Calculus (RCC) into PLN. It is shown how a fuzzy/probabilistic version of RCC may be used together with a fuzzy/probabilistic version of Allens interval algebra to carry out commonsense inferences about the causes of peoples activities and relationships, based on knowledge involving time and space.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 1
"To exemplify the practical use of these ideas, we extend the example of Bob and his Pokemon cards, from the previous chapter, to include the case where some of Bobs friends live near Canada but not actually in Canada, and the inference system has to deal with the notion of fuzzy Canadian-ness as related to spatial geometry. The following gure illustrates the fuzzy spatial membership function corresponding to Canada, used in the example inference: Fig. 1.7 Finally (before Chapter Sixteen which is a brief conclusion), Chapter Fifteen confronts the thorny conceptual and algorithmic issue of inference control: determining which inference steps to take, in which order, in order to answer a question, lter a datamining results list, or carry out an analysis.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 1
"Far from being merely an efciency issue, inference control actually hits many of the deepest issues of AI, including the frame problem (briey stated, that AI systems tend to lack tacit background knowledge about what questions not to bother asking because their answers are supposed to be obvious, or are irrelevant). We discuss a number of specic techniques that may be able to achieve effective inference control in the context of inference on large stores of spatiotemporal logical knowledge,      24 Real-World Reasoning including techniques that hybridize logic with other AI methods such as activation spreading. Here the discussion broadens from logic per se to the topic of cognitive architectures and general AI systems, the point being made that the integrative architectures underlying many such systems exist largely in order to provide effective, scalable inference control. As an example, the OpenCog cognitive architecture in which the PLN inference system is embedded is briey considered.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 1
"  Chapter 2 Knowledge Representation Using Formal Logic Now we begin to dig into the nitty-gritty of our subject matter. Before discussing querying and analysis of complex, heterogeneous spatiotemporal and contextual knowledge, we must discuss representation of temporal knowledge (as well as, to a certain extent, spatial knowledge) ... and before that, we must address knowledge representation in general. In the course of our investigation we must work through a number of difcult questions regarding knowledge representation, including:  Which of the many species of uncertain logic to use as the basis for our knowledge representation  How specically to represent temporal knowledge?  How specically to represent spatial knowledge?  What is the best low-level (e.g. graph) representation of logical knowledge for efcient storage and processing? Logic itself is not a monolithic entity; it comes in many different avors.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"At the highest level, there is the dichotomy between predicate logic and term logic (and there are also systems that hybridize the two, e.g. (Goertzel et al., 2008; Wang, 2006a)). There are also many types of logical system within each of these broad categories, some of which will be reviewed later on. The material in this chapter becomes somewhat formal and technical, for which we apologize to the reader who lacks the relevant taste or experience; but which unfortunately seems unavoidable if we are to give a serious treatment of our topic. The reader lacking appropriate expertise may either consult relevant background material (Copi & Cohen, 1998), or less ideally, skim this material and proceed to the later chapters, some of which will be quite clearly comprehensible without grasp of these preliminaries, some less so. 27      28 Real-World Reasoning 2.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"1 Basic Concepts of Term and Predicate Logic Term logic, or traditional logic, was founded by Aristotle and was the dominating logical framework until the late nineteen century. Term logic uses subject-predicate statements of the form S is P (for instance, Socrates is a man). There are singular and universal terms (the former correspond to unique subjects). There are just four forms of propositions in term logic:  Universal and afrmative (e.g. All men are mortal)  Particular and afrmative (e.g. Some men are philosophers)  Universal and negative (e.g. No philosophers are rich)  Particular and negative (e.g. Some men are not philosophers). New conclusions are derived from premises by syllogisms. Aristotle introduced fourteen syllogisms, of which we will give just two here for illustrative purposes:  (Barbara) If every M is L, and if every S is M, then every S is L.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"(for instance, if every man is mortal, and if every philosopher is a man, then every philosopher is mortal)  (Celarent) If no M is L, and if every S is M, then no S is L. (for instance, if no philosopher is rich and if every poet is a philosopher, then no poet is rich). Syllogisms provide a method for deduction  deriving new facts from already proved facts. In addition there are rules for induction and abduction:  (Induction) If every M is L, and if every M is S, then every S is L. (for instance, if every poet is mortal, and if every poet is a philosopher, then every philosopher is mortal)  (Abduction) If every L is M, and if every S is M, then every S is L.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"(for instance, if every poet is mortal, and if every philosopher is mortal, then every philosopher is poet) Notice that the induction and abduction rules do not neccesarily derive true statements. Nevertheless these are important forms of inference in the face of insufcient evidence, in modern AI reasoning systems as well as in classical Aristotelian term logic (Dimopoulos & Kakas, 1996). Induction and abduction are omnipresent in human commonsense inference. Put simply, induction aims at generalization. In the above example (if every poet is mortal, and if every poet is a philosopher, then every philosopher is mortal), the rst premise yields that all philosophers that are also poets are mortal, but then it is generalized to conclude that all philosophers are mortal.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"Yet, it is possible that there are some philoso     Knowledge Representation Using Formal Logic 29 phers that are not poets, so potentially not mortal, so the above generalization rule does not neccesarily lead to true conclusions. Similarly, abduction aims at explanation. In the above example, the explanation for the fact that every philosopher is mortal may be that it is because every philosopher is a poet. In the late nineteenth century, classical term logic was the subject of criticism, for its weak expressive power and the limited forms of reasoning it permitted. For example, in classical term logic from every car is a vehicle one cannot infer every owner of a car is an owner of a vehicle. In that period, predicate logic was designed, and it still serves as a basis for most mathematical and philosophical formal reasoning.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"However, modern theorists have extended classical term logic in various ways (Englebretsen & Sommers, 2000; Wang, 2006b), so that there are now term logics which equal predicate logic in expressive power. There are also systems that hybridize term and predicate logic, such as our own Probabilistic Logic Networks framework (Goertzel et al., 2008), which will be discussed below. Advocates of term logic often argue that it more closely matches the patterns of human commonsense reasoning. In standard predicate, or rst-order logic, statements have arbitrary propositional form (involving conjunctions, disjunctions, negations, ...) and arbitrary use of quantiers (for instance, for every man, there is a woman, such that for every man, ...). Modern variants of term logic provide this same expressive exibility. Pure predicate logic is a framework in which one can describe other theories.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"This framework is dened by the set of axioms and the set of inference rules (such as if P and if P yields Q, then Q). The proofs are sequences of derivation steps based on these axioms and rules. For example, one can represent in predicate logic and derive not every man is a philosopher if and only if there is a man such that it is not a philosopher. For rstorder logic there are also inductive and abductive rules, not used in mathematical theoremproving, but for uncertain reasoning, most often in AI. First order logic is also used as a basis for many specic logics, including modal, deontic, temporal and spatial logics as will be discussed below. 2.2 Review of Propositional Logic In order to explain predicate logic in more depth, we must begin with a simpler variant called propositional logic.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"Propositional logic can express simple facts while rst-order      30 Real-World Reasoning logic (or predicate logic) also involves quantication and more complex statements. In this sense, rst-order logic subsumes propositional logic. Both propositional and rst-order logic have many practical applications beyond the ones considered here, in describing different processes and concepts. Most important perhaps are applications in computer science, ranging from chip design (Aehlig & Beckmann, 2007) to natural language processing (Meulen, 2001). Both propositional logic and rst-order logic have three important aspects:  syntax  describing well-formed formulae and their basic properties;  semantics  describing meaning of well-formed formulae;  deduction  describing systems for syntactically deriving new formulas from other formulas (with no respect to their meaning). We now give a brief mathematical exposition of these three aspects. First, syntax.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"Let P be an innite, but countable set, whose elements will be called propositional letters or atomic propositions. The set of propositional formulas is dened by the following rules:  all elements of P are propositional formulas;   and  are propositional formulas (which as we will see below, are normally taken to semantically represent False and True)  if A is a propositional formula, then so is A (which is normally taken to represent the negation of A)  if A and B are propositional formulas, then so are AB, AB, A B, AB (which are normally taken to represent And, Or, and implication and equivalence)  each propositional formulas is obtained by a nite number of applications of the above rules. Next, semantics. A valuation v is dened as a mapping from P to the set {0,1}. That is, a valuation assigns either 0 or 1 to any propositional letter.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"An interpretation Iv is an extension of a valuation v, mapping propositional formulas to the set {0,1}, dened in the following way:  Iv() = 0;  Iv() = 1;  Iv (p)=v(p), if p belongs to P;  Iv (A)= 1Iv (A);      Knowledge Representation Using Formal Logic 31  Iv (AB)= min(Iv (A), Iv (B));  Iv (AB)= max(Iv (A), Iv (B));  Iv (AB)= max(1-Iv (A), Iv (B));  Iv (AB)=1, If Iv (A)= Iv(B) and Iv (AB)=0 otherwise. The above semantics is referred as to Tarskis semantics (that he introduced in (Tarski, 1994)).","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"Put simply, this allows the interpretation of propositional formulas like A(B(C  A)) as having truth values drawn from the set {0,1} (given the truth values for A, B and C), with 0 usually interpreted as meaning False and 1 as meaning True. A formula A is satisable if there is a valuation v such that Iv (A)=1 (otherwise, it is unsatisable or inconsistent, aka self-contradictory). A formula A is a tautology if for an arbitrary valuation v Iv (A)=1. If a formula A is a tautology, then we denote that by |= A. For example, it holds that Iv(pq)=1 in a valuation v such that v(p)=1, v(q)=0  so the formula pq is satisable. On the other hand, the formula pp is tautology.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"The problem of checking the satisability of a formula made of conjunctions of disjunctions of literals (variables or negations of variables) is decidable (there is an effective algorithm for solving it) and is called SAT. SAT is on of the most important NP-complete problems. Programs for testing satisability are called SAT solvers. There are different methods for checking satisability of a formula: the simplest is based on truth-tables (i.e. tabular enumeration of all possible combinations of values for the formulas variables, and evaluation of the truth value of the formula for each combination). Other include Davis-Putnam-Logmann-Loveland (DPLL) procedure, and modern solvers  DPLLbased, resolution-based solvers, tableaux-based solvers etc. Modern SAT solvers can decide propositional formulas with thousands variables and clauses (Lynce & Marques-Silva, 2002). 2.2.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"1 Deduction in Propositional Logic There is a number of inference systems for propositional logic. Most of them are actually restrictions of inference systems for rst-order logic. Hilbert-style inference system consists of the following axiom schemes (Mendelson, 1997): (A1) A  (B  A) (A2) (A  (B  C))  ((A  B)  (A  C)) (A3) (B  A)  ((B  A)  B)      32 Real-World Reasoning and the inference rule modus ponens: A, AB  B. A proof or a derivation in a Hilbert system is a nite sequence of formulas such that each element is either an axiom or follows from earlier formulas by the rule of inference.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"A proof of a derivation from a set S of formulas is a nite sequence of formulas such that each term is either an axiom, or is a member of S, or follows from earlier formulas by the rule of inference. If there is a proof for A, then A is a theorem and we denote that by  A. For example, it can be proved that A  A is a theorem, as follows: 1. (A  ((A  A)  A))  ((A  (A  A))  (A  A)) (instance of A2) 2. A  ((A  A)  A) (instance of A1) 3. (A  (A  A))  (A  A) (from 1 and 2, by MP) 4. A  (A  A) (instance of A1) 5.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"A  A (from 3 and 4, by MP) This may seem like a lot of work to prove that A implies A, but thats the nature of formal logic systems! Derivations are broken down into extremely small steps that are rigorously mathematically justied. In human commonsense inference we tend to proceed in large leaps instead, at least on the conscious level  but unconsciously, our brains are carrying out multitudes of small steps, though the analogy between these small steps and the small steps in logical proofs is a subject of debate in the AI and cognitive science community. There is a link between the semantics of propositional logic and the above Hilbert-style system stating that the system is sound and complete: every theorem is tautology, and every tautology is theorem, i.e., |= A if and only if  A.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"While the above is a standard and workable, approach, there are also other inference systems for propositional logic, including the one obtained as restrictions of Gentzen natural deduction and sequent calculus (see the section on rst-order logic). There are also variants for classical and for intuitionistic propositional logic: in the former AA is a theorem, and in the latter it is not (the above Hilbert-style system is classical). 2.3 Review of Predicate Logic Standard, rst-order predicate logic builds on propositional logic as dened above. We will review it using the same categories of syntax, semantics and deduction. Firstly, syntax. Let  be a nite or a countable set, its elements will be called function symbols. Let  be a nite or a countable set, its elements will be called predicate symbols. Let arity be a function that maps elements of  and  to natural numbers.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"The triple      Knowledge Representation Using Formal Logic 33 (,,arity) is called a signature. The set of terms over a signature (,,arity) and a countable set of variables V is dened in the following way:  all elements of V are terms;  if f is a function symbol and arity(f)=0, then f is a term;  if f is a function symbol and arity(f)=n, and if t1, ..., tn are terms, then f(t1,...,tn) is a term.  each term is obtained by a nite number of applications of the above rules. The set of atomic formulas over a signature (,,arity) and a countable set of variables V is dened in the following way:   and  are atomic formulas;  if p is a predicate symbol and arity(p)=n, and if t1, ..., tn are terms, then p(t1, ..., tn) is an atomic formula.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"each atomic formula is obtained by a nite number of applications of the above rules. The set of formulas over a signature (,,arity) and a countable set of variables V is dened in the following way:  each atomic formulas is a formula;  if A is a formula, then so is A;  if A and B are formulas, then so are AB , AB, AB, AB;  if A is a formula and v is a variable, then (x)A and (x)A are formulas;  each formula is obtained by a nite number of applications of the above rules. Next, semantics.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"The meaning of a formula is dened with respect to a pair (D,I), where D is a non-empty set, called the domain, and I is a mapping such that:  To each function symbol of arity 0, I associates an element c from D;  To each function symbol of arity n>0, I associates a total function fI from Dn to D;  To each predicate symbol of arity n>0, I associates a total function pI from Dn to {0,1}. A valuation v, in this context, is dened as a mapping from the set of variable V to the set D.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"An interpretation Iv of terms, with respect to a pair (D,I) and a valuation v is dened in the following way:  Iv(t) = v(t), if t is an element V;  Iv(t) = I(t), if t is a function symbol and arity(t) = 0;      34 Real-World Reasoning  Iv( f(t1,...,tn)) = f I(Iv(t1),...,Iv(tn))) where f I = I( f). An interpretation Iv of formulas, with respect to a pair (D,I) and a valuation v is dened in the following way:  Iv()=0 and Iv()=1  Iv(p(t1, ..., tn))= pI (I(t1), ..., I(tn))) where pI=I(p).","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"Iv (A)=1Iv (A);  Iv (AB)=min(Iv (A), Iv (B));  Iv (AB)=max(Iv (A), Iv (B));  Iv (AB)=max(1-Iv (A), Iv (B));  Iv (AB)=1, If Iv (A)= Iv(B) and Iv (AB)=0 otherwise.  Iv((x)A)=1, if for every valuation w that is identical to v, with a possible exception of x, it holds Iw(A)=1. Otherwise, Iv((x)A)=0;  Iv((x)A)=1, if there is a valuation w that is identical to v, with a possible exception of x, such that it holds Iw(A)=1. Otherwise, Iv((x)A)=0.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"If there is a pair (D,I) and a valuation v, such that Iv(A)=1, then A is satisable, otherwise it is unsatisable, or inconsistent. If for a xed pair (D,I) it holds that Iv(A)=1 for arbitrary valuation v, then A is valid with respect to (D,I). If it holds that Iv(A)=1 for arbitrary pair (D,I) and for arbitrary valuation v, then A is valid and we denote that by |= A. For instance, if the domain D is the set of natural numbers and if I maps p to the relation , then Iv((x)p(x,x))=1 in every valuation v, hence the formula (x)p(x,x) is valid with respect to (D,I). 2.3.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"1 Deduction in First-Order Logic Deduction in rst-order logic is similar conceptually to its analogue in propositional logic, but more complex in detail due to the presence of quantied variables. There are several different deductive systems available; one of the rst was developed by Hilbert in the early 20th century and we will describe it now. In Hilberts systems, formulas are built using only the connectives  and , and the quantiers  (for all) and  (there exists).","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"The system consists of the following axiom schemes: (A1) A  (B  A) (A2) (A  (B  C))  ((A  B)  (A  C)) (A3) (B  A)  ((B  A)  B)      Knowledge Representation Using Formal Logic 35 (A4) (x)A  A[x  t], while the term t is free for x in A (A5) (x)(A  B)  (A  (x)B), while A does not involve free occurrences of x and the following inference rules: Modus ponens: A, AB  B Gen: A  (x)A A proof or a derivation in a Hilbert system is a nite sequence of formulas such that each element is either an axiom or follows from earlier formulas by one of the rules of inference.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"A proof of a derivation from a set S of formulas is a nite sequence of formulas such that each formula is either an axiom, or is a member of S, or follows from earlier formulas by one of the rules of inference. If there is a proof for A, then A is a theorem and we denote that by  A. There is a link between the semantics of rst order logic and the above Hilbert-style system stating that the system is sound and complete: every theorem is valid formula, and every valid formula is theorem, i.e., |= A if and only if  A. In Hilbert-style systems, even for trivial statements, proofs can be non-trivial and rather unintuitive. However, although this kind of system is very demanding for practical use, it is very suitable for formal analyses of formal logic (since it has just a few axioms and inference rules).","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"On the other hand, Gentzen constructed (in the 1930s) a natural deduction system, that better reects usual mathematical reasoning. The price for this increased naturalness is a larger set of inference rules  one for eliminating and one for introducing each logical connective (, , , , ) and quantier (, ), and one for eliminating the logical constant  (13 rules altogether). On the other hand, there is just one axiom scheme (AA), which is not even needed if one adopts the intuitionistic version of Gentzens logic. Some of the rules in Gentzens deductive system are: A AB (introducing ) A A  (eliminating ) A A  B B (eliminating ) and so forth.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"Proofs in Gentzens natural deduction are usually represented as trees with the statement to be proved in the root (at the bottom), and with axioms or assumptions in leaves      36 Real-World Reasoning (all these assumptions have to be eliminated along the proof). Somewhat different in nature from usual mathematical proofs, is the Gentzens sequent calculus, suitable for formal analyses and for automation. The elementary object in this system is a sequent, a construct of the form A1, A2, ..., An  B1, B2, ..., Bm.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"The calculus itself consists of the inference rules for introducing logical connectives and quantiers on both sides of sequents, for instance:   ,A A,   (introducing  left) There are also structural rules for dealing with formulas within one side of a sequent, for instance:    D,   (weakening) There are many variations of the above inference systems, both for rst order logic and for other theories. The above systems can also be used as inference systems for propositional logic  it sufces to omit all axioms and rules involving quantiers. Finally, it is worth noting that rst-order logic is semi-decidable, which means that there can be an algorithm that can always conrm a valid formula is indeed valid (i.e, is a theorem), but cannot always detect that a non-valid formula is not valid.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"Things are simpler in propositional logic which is decidable, meaning that there is an algorithm which can always decide whether a given propositional formula is valid or not. 2.3.2 First-order Theories Next, one can extend basic predicate logic by dening theories within it, which extend the basic axioms by adding other specialized axioms. We will be doing a lot of this in the present book, in the context of specialized theories about time and space. Mathematically, we say that each such theory has a certain signature, consisting of specic sets  and  of function and predicate symbol (with certain arities) extending the basic ones used in predicate logic. Beside the new symbols, to create a theory one has to provide a list of axioms (in the described language). These axioms are then used within a selected deductive framework (e.g., Hilberts system, Gentzens natural deduction, etc.) as additional axioms of the system.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"If a theory T is dened by a signature (, ,arity), and within a deductive system, then we often write T  F or T F to denote that the formula F can be derived in the theory T (i.e., F is a theorem of T).      Knowledge Representation Using Formal Logic 37 As an example within mathematics, the branch of math called group theory can be constructed easily as an extension of pure predicate logic.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"In this formalization, the signature of theory of group consists of:  Functional symbol 0 (of arity 0);  Functional symbol + (of arity 2);  Functional symbol  (of arity 1);  Predicate symbol = (of arity 2), And, the axioms of the theory of groups are: (x)(x = x) (x)(y)(x = y  x = y) (x1)(x2)(y1)(y2)(x1 = y1 x2 = y2  x1 +x2 = y1 +y2) (x1)(x2)(y1)(y2)(x1 = y1 x2 = y2  (x1 = x2  y1 = y2)) (x)(y)(z)(x+(y+z) = (x+y)+z) (x)(x+0 = 0+x = x) (x)(x+(x) =","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"(x)+x = 0) These axioms, added on to the regular axioms of predicate logic, tell you how to use the entities {0, +, , =} in the manner required in group theory. For instance, it can be proved that the following formula is theorem of the theory of groups: (x)(y)(z)(x+z = y+z  x = y) What well see later on in this book are similar theories that involve, not {0, +, , =} but rather entities such as time intervals, spatial regions, and relationships between them. Finally, although we wont make use of this here, its worth noting that, apart from the axiomatic approach to dening theories, theories can be also dened semantically. For a given signature L and a corresponding pair (D,I), a theory of a structure is the set of all sentences over L that are true in (D,I). 2.3.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"3 Forward and Backward Chaining There are two basic search strategies for using inference rules in theorem-proving, and other related AI areas:  Forward chaining      38 Real-World Reasoning  Backward chaining Both strategies are applied to tasks of the same sort: given a set of facts (axioms) and a set of inference rules, the task is to check whether some given fact (formula) F can be derived. The difference between the two strategies is the direction of their search. Namely, forward chaining (also known as data-driven search) starts with the available facts and derive all facts that can be derived, until the given fact F is reached. On the other hand, backward chaining (also known as goal-driven search) starts with the given goal F and apply inference rules in opposite direction, producing new subgoals and trying to reach the facts already available. Let us consider the following example. Let there be given facts: 1) Derek will go out for lunch.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"2) If Alison goes out for lunch, then Bob will go out for lunch as well. 3) If Bob goes out for lunch, then Clark will go out for lunch as well. 4) If Derek goes out for lunch, then Ellen will go out for lunch as well. 5) If Ellen goes out for lunch, then Clark will go out for lunch as well. and assume the (only) inference rule is modus ponens: if X and XY, then Y. The goal to be proved is Clark will go out for lunch. Forward chaining will try to match any two facts with X and XY in order to apply modus ponens and to derive new facts. One (and the only at this step) option is to use the facts 1 and 4 and derive the fact Ellen will go out for lunch. Then, in next step, this new fact and the fact 5 yield Clark will go out for lunch, which was required.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"Backward chaining starts from the goal  from the fact Clark will go out for lunch. If it is matched with Y in modus ponens, then the new subgoals will be X and XY, where X can be anything. The subgoal XY matches the fact 3, so X is matched with Bob will go out for lunch. When proving this subgoal, the fact 2 is used and a new subgoal Alison will go for lunch should be proved. However, this leads to failure. Another option is the following: the subgoal XY matches also the fact 5, so X is matched with Ellen will go out for lunch. When proving this subgoal, the fact 4 is used and a new subgoal Derek will go out for lunch should be proved. This is trivially true, by the fact 1. So, it was proved that Clark will go out for lunch, as required. The above description of forward and backward chaining is simplied.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"Typical applications of forward and backward chaining involve different methods of directing the search. Namely, the main problem that both strategies face is search expense, due to typically huge      Knowledge Representation Using Formal Logic 39 numbers of combinatorial options that have to be considered. Neither of these approaches is superior to the other one, and there are domains in which one if more appropriate than another. There are also hybrid approaches that combine forward and backward search. 2.3.4 Decidability and Decision Procedures In this section we briey introduce a distinction that may be important in practical large-scale logic-based systems: the distinction between proof procedures and decision procedures.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"Put simply:  a proof procedure nds a specic series of steps for getting to a certain conclusion from the axioms of a logical theory  a decision procedure checks whether a certain conclusion can be obtained from the axioms of a logical theory, without necessarily directly supplying the proof (the series of steps) In practical cases, given the outcome of a decision procedure, plus knowledge of the algorithm used to carry out the decision procedure, it is in principle possible to construct a proof. But this may be quite laborious. In some situations, if one just needs to know whether something is true or not in a certain formal theory, it may be easier to use a decision procedure than to nd a specic proof. In the formal lingo of logic, one says a theory T is decidable if there is an algorithm that for any sentence F of T it can decide whether F is theorem of T or not. Such an algorithm is called a decision procedure.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"An example of a decision procedure for propositional logic is the DPLL (Davis-Putnam-Logemann-Loveland) procedure, which lies at the core of all modern SAT solvers. A SAT solver is a program that checks if a large propositional-logic formula can possibly be satised by any assignment of values to variables or not; and doing this via a decision procedure rather than a proof procedure is vastly more computationally efcient. SAT solvers are used in a huge variety of practical applications nowadays, including circuit analysis, natural language parsing, and all manner of large-scale discrete optimization problems. If one had to approach these problems using direct theorem proving in propositional logic, then one would run into terrible combinatorial explosions and quite possibly formal logic would need to be set aside in favor of some different analytical approach.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"There are many widely applied decision procedures for rst-order theories, including decision procedures for linear arithmetic over reals (involving only addition, not multipli     40 Real-World Reasoning cation), multiplicative arithmetic over reals (involving only multiplication, not addition), theory of lists, theory of arrays, etc. (Barrett et al., 2009). There is a family of modern decision procedures for rst-order theories, heavily using propositional reasoning and SMT (satisability modulo theory) techniques. Decision procedures and SMT solvers are widely applied in software and hardware verication. Mathematically speaking, there are many important theories that are not decidable, such as arithmetic over natural numbers, the theory of groups, etc. Hence, for these theories there can be no decision procedures, only heuristics that can prove/disprove certain classes of formulas.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"However, undecidability always involves innity in some form; if one restricts attention to a nite set of data items (as is always the case in practical applications), then it is not an issue. In practical applications, the use of decision procedures is sometimes unavoidable. However, developing an efcient decision procedure is not an easy task  it requires specic knowledge about the theory. There is no generic (decidable) method for constructing efcient decision procedures. In the case of temporal and spatial logics such as we will discuss here, scientists have not yet created appropriate specialized decision procedures, but this seems a highly worthy area of investigation. 2.4 Simple Examples of Formal Logical Inference Weve been discussing logic in an extremely abstract and mathematical way  but our main goal here is real-world reasoning, not mathematical reasoning.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"So before progressing further in the direction of elaborating abstract logic systems, we will digress a bit to clarify the relationship between logic and commonsense inference. This is after all where logic started: formal logic originated, not as a branch of math, but from the motivation of formalizing everyday human thinking. Many real world problems can be formulated in terms of propositional or rst order logic. It is only during the last century and a half that logic has become a sophisticated branch of mathematics. 2.4.1 Example of Kings and Queens Consider the following example, from (Hayes, 1997). The king and his family want to make a party for some ambassadors in their kingdom. The king, the queen, and the prince give their orders to the head of protocol and he has to make a nal list of guests.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"The king said that either the ambassador of Peru, or the ambassador of Qatar, or the ambassador of      Knowledge Representation Using Formal Logic 41 Romania should be invited. The queen said that if the ambassadors of Qatar and Romania are invited, then the ambassador of Peru should be invited too. The prince said that if the ambassador of Romania is invited, then the ambassadors of Peru or the ambassador of Qatar should be invited. The question is whether the head of protocol can obey all orders. The problem can be formulated in terms of propositional logic in the following way. Let us denote by p, q, r the fact that the ambassador of Peru, Qatar, Romania will be invited.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"Then the orders can be formulated as follows:  Kings order: pqr  Queens order: (qr)  p  Princes order: r  pq Or, equivalently:  Kings order: pqr  Queens order: qrp  Princes order: rpq To solve a problem, the head of protocol has to check whether the formula (pqr)(qrp)(rpq) is satisable. He can use a SAT solver for that (and he can nd that he can meet all orders by inviting only the ambassador of Peru). Or, as this is a simple case, using any reasonable propositional logic theorem prover would also work ne. 2.4.2 Example of Minesweeper Or consider, as an another example, the popular computer game of Minesweeper, as depicted in Figure 2.1: There is a board of nm places.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"When a player selects one position on the board, the game is over if it contains a mine. Otherwise, the player gets the number of mines in the adjacent positions. Let us denote by p, q, r, s, t, u six positions in the left upper corner of the board: Let us suppose that we open the position p, and there is no mine. Let us also suppose that we got the number 1 for the position p. If we open a position q and we get the answer 1 again, we can safely open the positions r and u. The explanation is as follows. We associate a propositional variable to each position  the variable is true if there is a mine on the position, and the variable is false otherwise. Since there is no mine on the position p, it      42 Real-World Reasoning Fig. 2.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"1 A screenshot of a Minesweeper GUI follows that: p Since the position p has the associated value 1, there is one mine on either q, s, or t. We can encode this as follows: qst q  st s  qt t  qs Since there is no mine on the position q, it follows that: q      Knowledge Representation Using Formal Logic 43 Fig. 2.2 Since the position q has the value 1, it means that there is one mine on either p, s, t, r, or u. We can encode this as follows: pstru p  stru s  ptru t  psru r  pstu u  pstr The question is whether one can safely open the position r, i.e., whether r is consistent with the above formulas (i.e., is it possible that there is a mine on r).","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"A SAT solver (applied to the above formulas) would easily nd that such a set of formulas is not consistent, so there cannot be a mine on the position r (and, similarly, there cannot be a mine on the position u). 2.4.3 Example of Socrates The above examples all involve propositional rather than predicate logic. Concerning rst-order logic, consider the following classical example. One is given the following two facts:  Socrates is a man.      44 Real-World Reasoning  All men are mortal. We can encode this in rst-order logic as follows:  man(Socrates)  (x)(man(x)  mortal(x)) Applying deduction allows us to conclude that, since Socrates is a man, he must be mortal. For instance, a very simple proof procedure can verify that mortal(Socrates) is a consequence of the above facts by verifying that mortal(Socrates) is inconsistent with the above formulas. 2.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"5 Modal logic Propositional logic and rst-order logic are sometimes not expressive enough for some sorts of common-sense reasoning. For instance, one may wish to extend them using socalled modal statements including qualiers like necessary, possibly, future, past, etc. For such cases, specic logics are used, such as modal and temporal logics. These logics are not contradictory to rst-order logic, and in fact could be viewed as specialized theories constructed within rst-order logic; but this is not always the most useful way to look at them. It is often more helpful to think about them as alternative formalizations of logic. Modal logic describes logical relations of modal notions. These notions may include metaphysical modalities (necessities, possibilities, etc.), epistemic modalities (knowledge, belief, etc.), temporal modalities (future, past, etc.), and deontic modalities (obligation, permission, etc.).","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"Modal logics are widely used in philosophy, articial intelligence, database theory, and game theory. Modern work on modal logics started in 1918. with the monograph A Survey of Symbolic Logic by C. I. Lewis. The main metaphysical modal notions are necessity and possibility; and the modal logic that describe logical relations over them is called alethic modal logic. In alethic modal logic, one can express a statement like It is possible that Bob quit his job. The modailities are represented by modal operators. Formulas are built using propositional connectives and these modal operators. The basic unary modal operators are usually written  (or L) for necessarily and  (or M) for possibly. These two operators are linked in the following way: p  p p  p.      Knowledge Representation Using Formal Logic 45 There is a number of modal extensions of some underlying logics, including the extension of rst-order logic, called modal predicate logic.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"The standard semantics of modal logics is Kripke semantics. The concept of the Kripke semantics of propositional modal logic includes:  A non-empty set W  a set of possible worlds;  A two-place relation R on elements from W  the accessibility relation between worlds, which represents the possible worlds that are considered in a given world, i.e., if we consider a world w0, every world v such that it is in relation R with w0 represents a possibility that is considered at a world w0;  A frame  a tuple (W, R); Given a frame (W, R), a model M is a tuple (W, R, V) where V is a map that assigns to a world a valuation on propositional variables, i.e. for a given world w, V(w) is a function from the set of propositional variables to {0, 1}.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"Interpretation of a formula F with respect to M is dened in the following way (the fact that F is true at a world w in a model M) is denoted by M,w |= F: M,w |= p iff V(w)(p) = 1 (where p is a propositional variable) M,w |= F F iff M,w |= F and M,w |= F. M,w |= F iff not M,w |= F M,w |= F iff, for every world w such that it is in relation R with w it holds that, M,w |= F The semantics of other propositional connectives and the operator  are implied by the above denition. A formula is then dened to be valid in a model M if it is true at every possible world in M. A formula is valid if it is valid in all frames (or every model). For the given semantics, there is a sound and complete inference system for propositional modal logic  the system K.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"In addition to underlying propositional axioms and inference rules, there is the axiom (F  F)  (F  F) and the rule if  F, then  F There are various inference systems for propositonal modal logic obtained by adding extra axioms to K. There are also fuzzy versions of modal logics (Thiele, 1993; Ying, 1988).      46 Real-World Reasoning 2.6 Deontic logic An interesting specialization of modal logic, which is potentially useful for doing logical inference about human behaviors and motivations, is deontic logic  which is concerned with the ideal and actual behavior of social agents, and involves notions like permissible, impermissible, obligatory, gratuitous, optional, etc. Deontic logic has many analogies with alethic modal logic.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"In addition to theoretical interest, deontic logics are used for formalizing different real-world concepts and problems such as morality, normative law, legal analysis, social and business organizations and security systems, computer security, electronic commerce, or legal expert systems. A survey of applications of deontic logic in computer science can be found in (Wieringa & Meyer, 1993), which supplies the following systematization of applications of deontic logic in computer science: 1. Fault-tolerant computer systems. 2. Normative user behavior. 3. Normative behavior in or of the organization. (a) Policy specication. (b) Normative organization behavior (e.g. contracting). 4. Normative behavior of the object system. (a) The specication of law. (b) The specication of legal thinking. (c) The specication of normative rules as deontic integrity constraints.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"(d) Other applications, not discussed above. The rst formalization of deontic logic was given by E. Mally in 1926. More details on deontic logic can be found in (McNamara & Prakken, 1999). In the Traditional scheme for deontic logic, there are ve normative statuses considered:  it is obligatory that (OB)  it is permissible that (PE)  it is impermissible that (IM)  it is gratuitous that (GR)  it is optional that (OP)      Knowledge Representation Using Formal Logic 47 The rst one of the above can be used as a basis, while the remaining ones can be dened in the following way: PEp  OBp IMp  OBp GRp  OBp OPp  (OBp  OBp) Standard Deontic Logic (SDL) is the most studied deontic logic.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"It extends propositional logic by the (one-place) OB deontic operator. Formulas are built in the standard modal-logic way. The semantics of SDL is usually given in Kripke-style. The inference system for SDL consists of axioms for (classical) propositional calculus and inference rules, the additional inference rule if  p then OBp and the following axioms: OB(p  q)  (OBp  OBq) OBp  OBp Consider the following simple example. Let us assume that the hypotheses are: It ought to be the case that Alison does the paperwork. If Alison does the paperwork, then Alison leaves the ofce late. Let us denote Alison does the paperwork by p and Alison leaves the ofce late by q: Then, we can prove It ought to be the case that Alison leaves the ofce late (i.e.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"OBq) as follows: (C1) OBp (hypothesis) (C2) pq (hypothesis) (C3) OB(pq) (deontic inference rule, from (C2)) (C4) OB(pq)  (OBp OBq) (deontic axiom) (C5) OBp  OBq (Modus ponens, from (C3) and (C4)) (C6) OBq (Modus ponens, from (C1) and (C5)) There is a number of variants of the SDL inference system and there are interesting logical and philosophical considerations for each of them. 2.6.1 Fuzzy deontic logic While there is a number of approaches to fuzzy modal logic (see the next chapter for a recall of fuzzy logic), there is a very limited literature on fuzzy deontic logic.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"One version      48 Real-World Reasoning of fuzzy deontic logic was introduced and discussed by (Gounder & Esterline, 1998). In their framework, given the statements p = Person p receives a drivers license. q = Person p is 18 or older. r = Person p is an employee of company c. s = Person p is over 80 years old. t = Person p is under 20 years old. u = Company c gives its employees a bonus. v = The employees of company c arrive at work not more than ten minutes late. one can consider the following interesting deontic statements: OB(p  q) OBp  OBq r  OB(st) u  OBv These statements need not be crisp, but can be in a permissible range which is given in the fuzzy truth value in the interval [0, 1] and one obligation may lead to another obligation.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"Concerning reasoning in this theory, as said in (Gounder & Esterline, 1998), since fuzzy logics work with numerical measures, axiomatic systems are not appropriate. Instead, fuzzy versions of the semantic properties exist and can be shown to correspond to some of the axioms for the crisp systems in special ways that support dependency among assertions in a modal domain. 2.7 The frame problem A nal issue that must be discussed, in the context of knowledge representation using formal logic, is the frame problem, as originally recognized and named in (McCarthy & Hayes, 1969). Put most simply, this is the problem of representing the effects of action without having to represent explicitly a large number of intuitively obvious non-effects (i.e., properties that are not affected by the action).","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"The frame problem also has a wider epistemological importance and it considers whether it is possible, in principle, to limit the scope of the reasoning required to derive the consequences of an action. The name frame problem was derived from a common technique used in producing animated cartoons      Knowledge Representation Using Formal Logic 49 where the currently moving parts of the cartoon are superimposed on the frame, which depicts the non changing background of the scene. While the frame problem is a major issue for certain approaches to logic-based AI, we dont see it as an objection to scalably deploying logic-based technology to draw inferences based on large spatiotemporal knowledge bases (nor to other scalable real-world inference applications). Rather, we see it as an objection to embedding logical inference engines in overly simplistic cognitive architectures.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"We believe the frame problem can be bypassed via judicious inference control heuristics, including some that are implicit in the ideas discussed in the previous section, and some others that will be discussed here. 2.7.1 Review of the Frame Problem To elaborate the frame problem a little more fully, suppose we have the following knowledge:  Alison is in her ofce and she wears a blue suit.  If Alison moves from her ofce, then she is in the loby. If we represent the above in classical rst-order logic, using some suitable formalism for representing time and action (e.g., CTL logic, or an appropriate subset of PLN, both discussed below), we can derive that after Alison moves from her ofce she will be in the lobby. However, we will not be able to derive that Alisons suit is still blue.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"Namely, the knowledge given above does not rule out the possibility that the color of Alisons suit changes when she moves out of her ofce. A straightforward solution for this is to add rules that explicitly describe the non-effects of each action (e.g., when Alison moves from her ofce, the color of her suit does not change). Such formulae are called frame axioms. However, this is not a satisfactory solution. Namely, since most actions do not affect most properties of a situation, in a domain comprising M actions and N properties we will, in general, have to write out MN frame axioms which would make any reasoning process impractical. In (Shanahan & Baars, 2005) there is a more detailed account on the frame problem, including a brief description of Dennett memorable example: ... consider the challenge facing the designers of an imaginary robot whose task is to retrieve an object resting on a wagon in a nearby room.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"But the room also contains a bomb, which is timed to explode soon. The rst version of the robot successfully works out that it must pull the wagon out of the room. Unfortunately, the bomb is on the wagon. And although the robot knows the bomb is on the wagon, it fails to notice that pulling the wagon out brings the bomb along too. So the designers produce a second version of the robot. This model works out all the consequences of its actions before doing anything. But the new robot gets blown up too, because      50 Real-World Reasoning it spends too long in the room working out what will happen when it moves the wagon. It had just nished deducing that pulling the wagon out of the room would not change to color of the rooms walls, and was embarking on a proof of the further implication that pulling the wagon out would cause its wheels to turn more revolutions than there were wheels on the wagon when the bomb exploded.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"So the robot builders come up with a third design. This robot is programmed to tell the difference between relevant and irrelevant implications. When working out the consequences of its actions, it considers only the relevant ones. But to the surprise of its designers, this version of the robot fares no better. Like its predecessor, it sits in the room thinking rather than acting. Do something! they yelled at it. I am, it retorted. Im busily ignoring some thousands of implications I have determined to be irrelevant. Just as soon as I nd an irrelevant implication, I put it on the list of those I must ignore, and. the bomb went off. It is obvious that the human brain incorporates a solution to the frame problem and does not suffer from overwhelming information when deriving new conclusions.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"When we say that Alison left her ofce, we dont need to state explicitly that her suit hasnt change its color, that Bobs cat hasnt changed its sex, that the Sun continues to shine, etc. Such information is taken for granted by common sense. In mathematical logic, however, nothing is taken for granted and in classical logic it is necessary to represent explicitly all the things that change and all the things that do not change by some action. 2.7.2 Working around the Frame Problem Perhaps the best-known attempt to work around the frame problem, within the scope of logic-based AI, begins from the observation that the inference process in classical logic is monotonic, meaning that the set of conclusion can only grow when we add new premises (we do not retract some conclusions if we are presented with some new premise).","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"But, it would be nicer if one could infer that Alisons suit is, generally, of the same color when she leaves her ofce and, in addition, it would be suitable, to add some exceptions, stating otherwise (If Alison spilled coffee on her suit, then she has to change her suit before she leave her ofce). In other words, one would like to be able to declare the general rule that an action can be assumed not to change a given property of a situation unless there is evidence to the contrary. Such reasoning is possible within non-monotonic logics. Despite the fact that there is also a number of problems with the frame problem when addressed by non-monotonic logics, it can be considered that they provide a satisfactory solution. In articial intelligence, there are also some other approaches, that handle different incarnations of the frame problem with more or less success.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"The frame problem is still making an inuence on issues in cognitive sciences, philosophy, psychology, etc.      Knowledge Representation Using Formal Logic 51 Propositional and rst-order logic as dened are monotonic. This means that the set of facts that can be derived from S increases when S increases. However, again, this is not appropriate for some sorts of common-sense reasoning. For example, if we are given a fact that Tweety is a bird, we by default derive the fact that Tweety ies. But, if we are given an additional fact that Tweety is a penguin, then we retract our conclusion and derive the new one  that Tweety does not y.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"There is a family of logics following this motivation and trying to model common-sense reasoning, summarized for instance in (Reiter, 1980; Delgrande & Schaub, 2003) However, nonmonotonic logic is not the only route for circumventing the frame problem; for instance in our own work with PLN, we have taken a signicantly different approach, to which we will return in the nal chapter of this book.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
       ,"Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 2
"  Chapter 3 Quantifying and Managing Uncertainty Another major issue in formal logic systems is the quantication and incorporation of uncertainty. It is of course possible to represent uncertainty using theories within rst-order logic, but many have argued that uncertainty is sufciently basic to commonsense inference that it should be introduced into formal logic at a foundational level. With this in mind, there are many different ways of representing uncertainty within logical formalisms. Some of the leading approaches may be summarized as:  Fuzzy  Traditional Probabilistic  Imprecise / Indenite Probabilistic We have touched some of these briey above, but we will now review them slightly more systematically. 3.1 Fuzzy logic Fuzzy logic (Zadeh, 1965) extends the notion of boolean truth (true vs. false), to encompass degrees of truth varying between 0 and 1; and generalizes the standard boolean operators {, , } by fuzzied counterparts.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 3
"These generalized operators are usually (but not always) dened as follows: x = 1x xy = min(x,y) xy = max(x,y) 53      54 Real-World Reasoning It appears that for various instances of commonsense reasoning, fuzzy logic is more immediately appropriate than boolean crisp logic; for instance the fuzzy predicate: (strong(x)healthy(x))intelligent(x) allows one to characterize the degree to which a person is both strong and healthy, or intelligent; while its boolean interpretation can only draw two crude categories for each predicate, which does not t how one would think about the proposition with our common interpretation of the concepts strong, healthy and intelligent. 3.2 Possibility theory In fuzzy logic, values associated to facts or propositions range from 0 to 1 but represent truth values.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 3
"In possibility theory (Zadeh, 1978), on the other hand, a proposition may be boolean but what is considered instead is its degree of belief by an agent. That degree of belief is represented by two values, the necessity denoted nec(p) and the possibility denoted pos(p) representing the extent to which an agent considers p to be necessary and possible. For instance pos(earth at)=0.9 and nec(earth at)=0.2 represent respectively how much an agent believes the earth is possibly and necessarily at. The possibilities pos(p)=1 and pos(p)=1 represent a total state of ignorance about p, or equivalently nec(p)=0 and nec(p)=0. Having pos(p)=pos(p) does not contradict the principle of bivalence because this is the degree of belief of p which is considered not its truth.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 3
"Thus possibility and necessity are not self-dual, that is it is not the case that pos(p)=1-pos(p), however there are mutually-dual as formalized by the following axiom: nec(p)=1-pos(p) Other axioms permit one to determine the possibility and necessity of a formula based on the possibilities and necessities of its components  but not always. For instance nec(p  q) equals to min(nec(p), nec(q)) like in fuzzy logic, but pos(p  q) is not generally equal to min(pos(p), pos(q)); however one can always bound the latter value using necessity and possibility measures combined: min(nec(p),nec(q))  pos(pq)  min(pos(p),pos(q)) Finally, although fuzzy logic and possibility theory are built around two different semantics, if nec=pos then possibility theory amounts axiomatically to fuzzy logic (with the min/max interpretation for /); and for that reason possibility logic is sometimes referred as an extension of","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 3
"fuzzy logic.      Quantifying and Managing Uncertainty 55 3.3 Inference using Traditional Probabilities As probabilistic methods have become very popular in the AI eld lately, there now exists a wide range of methods and theories regarding probabilistic inference and logic; we will only describe a handful of the most relevant ones here. To understand the following sections the reader needs to be familiar with probability theory. 3.3.1 Bayesian Inference Methods to reason about probabilities include Bayesian inference and Bayesian networks, both called so because they mainly rely on Bayes theorem, a formula from elementary probability theory recalled below: P(Y | X) = P(X | Y)P(Y)/P(X) Bayesian inference is a mean to infer or revise the probability of an hypothesis knowing a set of observations (for instance determining the probability of a disease in the presence of symptoms).","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 3
"Bayesian networks are graphical models, formally DAGs (directed acyclic graphs), representing dependencies and independencies between random variables. We will describe Bayesian networks in some detail here, as they will arise in later chapters when we discuss their applicability to mining causal relationships from large knowledge stores. 3.3.2 Bayesian Networks A Bayesian network is a graphical model that represents a probabilistic joint distribution over a set of variables represented as nodes and direct dependences represented by arcs between nodes. More formally a Bayesian network is a DAG (directed acyclic graph), where each node contains a variable and a function representing a conditional probability of that variable knowing its parents, which are all variables that have an arc pointing to that given node. If the node has no parent then the function represents a marginal probability. See Figure 3.1 for an example of a Bayesian network with 4 variables. Usually the probabilities, conditional and marginal, are coded into matrices. Figure 3.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 3
"2 represents possible probabilities coded in matrices of the Bayesian network of Figure 3.1. They also can be coded in any other appropriate structures, like decision trees, decision graphs and such. As mentioned above, direct dependences are represented with arcs between variables; additionally it is possible to assess the conditional dependence and independence of any      56 Real-World Reasoning Fig. 3.1 A small Bayesian network Fig. 3.2 Probability matrices associated with the network nodes group of variables by applying the notion of d-separation introduced by Pearl [Pearl85].","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 3
"When a triplet of variables X, Y, Z are d-separated then we can conclude that X and Y are independent conditionally to Z; and conversely if there are independent then they are d-separated (actually that equivalence between d-separation and independence does not al     Quantifying and Managing Uncertainty 57 ways hold, only when the distribution is DAG-faithful; we wont recall what DAG-faithful is here, however one should note that most of practical real life distributions are DAGfaithful or close to it). We also wont recall the detailed denition of d-separation here, but what is most important to note here is that the d-separation criterion is a graphical one. This means that one can assess the dependences of variables solely based on the topology of the network, which in itself is a useful thing. Figure 3.3 and 3.4 display two examples of d-separated variables. Fig. 3.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 3
"3 d-separation in sets of variables Fig. 3.4 X8 and X9 are d-separated conditionally to {X3,X4,X7} in this example      58 Real-World Reasoning Given a Bayesian network one can compute the joint distribution of a set of variables X1,...,Xn by applying the following formula: P(X1,...,Xn) = n  i=1 P(Xi | parents(Xi)) where parent (Xi) is the set of variables with outgoing arcs pointing to Xi. It is worth noting that any distribution can be represented by a Bayesian network (see Figure 3.5 for an example of how that can be done). And there are usually many possible Bayesian networks to represent a given distribution, see Figure 3.6 for an example. Fig. 3.5 Bayesian network so that parent (Xi) = XI,...,Xi1 Fig. 3.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 3
"6 Two Bayesian networks representing the same distribution (bottom)      Quantifying and Managing Uncertainty 59 3.3.3 Bayesian Causal Networks Next, a Bayesian causal network is a Bayesian network where the arcs represent causal relationships. A probability distribution alone is usually not enough to determine causality, for reasons we reviewed earlier. And one needs additional knowledge either given by an expert (stating for instance that weather may inuence ice cream sales but not the opposite), or coming from additional assumption, for instance the knowledge that if event A occurs before event B then A can be a cause of B but B cannot be a cause of A. Note however that this sort of assumption involving time must be used cautiously because when A occurs before B, both A and B may actually be the result of a common cause C; in this case, C is called a confounding variable.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 3
"Real life is full of confounding variables! Many techniques available for Bayesian networks apply with little or no modication to Bayesian causal networks. For instance Bayesian inference, described in the next section, works exactly the same for a causal or non causal Bayesian network. On the other hand, network learning techniques may diverge more signicantly because they need to take into account additional background knowledge in order to decide whether or not a given causal relationship is authorized. 3.3.4 Bayesian Inference Given a Bayesian network one can use it to perform various probabilistic inferences. In this context, inference means calculating joint marginal and conditional probabilities. For instance, let us consider the Bayesian causal network of Figure 3.7. Let I, P, O and R be binary random variables respectively representing Internet connection working, Pay-check arrived, On-line purchases and Router overloaded.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 3
"One may want for instance to know the probability that a particular individual is going to make on-line purchases knowing that his router is overloaded and he hasnt received his pay-check yet. That is, we want to compute the conditional probability: P(O = 1 | R = 1,P = 0) The basic method to compute the above is called variable elimination. That is, one eliminates by summation the variables which are absent of the conditional probability of interest, in this example the variable I. This permits one to compute any partial joint marginal probability and the conditional probability is obtained by normalization. So, in the example, one needs to compute the joint marginal probabilities: P(O = 1,R = 1,P = 0)      60 Real-World Reasoning Fig. 3.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 3
"7 Bayesian causal network And the conditional probability is obtained by normalization: P(O = 1 | R = 1,P = 0) = P(O = 1,R = 1,P = 0)/P(R = 1,P = 0) Lets rst compute P(O = 1,R = 1,P = 0) =  i=0,1 P(I = i,O = 1,P = 0) =  i=0,1 P(R = 1)P(I = i | R = 1)P(P = 0)P(O = 1 | P = 0,I = i) = P(R = 1)P(P = 0)  i=0,1 P(I = i | R = 1)P(O = 1 | P = 0,I = i) = .2.8(.40+.6.1) = .","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 3
"0096 Then we compute P(R = 1,P = 0), but since R and P are independent, according to the d-separation criteria, we can directly get P(R = 1)P(P = 0) = .2.8 = .16 So the conditional probability of making on-line purchases is: P(O = 1 | R = 1,P = 0) = .0096/.16 = .06 There exists a variety of algorithms to perform these sorts of inferences, which include various optimizations, like caching and reusing intermediate results and using d-separation criteria (as we have done in the example above) to skip unnecessary summations.      Quantifying and Managing Uncertainty 61 3.3.5 Markov Logic Networks Next, Markov Logic Networks (Richardson & Domingos, 2006) involve a combination of First Order Logic (FOL) and Markov Network.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 3
"This constitutes an elegant way to dene probability distributions over valuations (dened earlier in Section 2.2). Markov Networks are very similar to Bayesian Network but are represented by an undirected graph  instead of a DAG like in Bayesian Networks. As a Bayesian Network, a Markov Network can model any joint distribution; but it gives more compact models for certain classes of distributions. The notion of conditional independence has a simpler representation with Markov Networks. However traditional Bayesian Networks can explicitly represent causality while Markov Networks cannot. In probability theory terms, the full joint distribution of a Markov Network is dened by composing together a set of partial joint distributions over a group of variables that are all directly dependent on each other (i.e., a clique in the usual graph-theoretic terminology).","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 3
"Logic-wise, what this means is that one can dene a probability distribution over the set of valuations (or possible worlds) given a set of rst order logic axioms by building its corresponding Markov network. An valuation is a truth table of all atoms dened in the logic (like for instance follows(Jill, Joel)=true, online(Joel, 11pm)=false, etc), and each atom has a corresponding random variable in the Markov network. Each axiom is associated with a clique (containing the atoms of the formula). Or if the axiom contains variables, as many cliques as instantiated formulas (called ground formulas) of that axiom. Which means that the larger the number of constants in the domain of interpretation is, the bigger the resulting Markov network will be. Informally, the probability of a valuation is proportional to the exponential of the sum of the ground formulas that this valuation satises.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 3
"It is also possible to weight the axioms, so the higher an axiom is weighted the stronger is the constraint of its satisfaction. All ground formulas are weighted identically to their corresponding axioms. Formally P(V = v) = 1 Z exp   j w j  f j(v)  where Z is a normalizing factor so that the probabilities over all valuations sum up to 1, w j is the weight of ground formula j and f j is the evaluation of formula j over the valuation v.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 3
"Once the Markov network has been built it can be used to determine the probability of any given valuation, and of course the conditional or marginal probabilities of any combination of atoms, like P(online(Joel,11pm) = false)      62 Real-World Reasoning or P(online(Jill,11:20pm) | online(Joel,11pm), follows(Jill,Joel)) Valuations that do not actually satisfy all the axioms can have a non null probability as well  but theyll usually have a lesser strength, due to the fact that they fulll less axioms than a satisfying valuation (although that actually depends on the weights of the axioms). Interestingly, one can dene any Markov network by listing the right set of axioms in rst order logic.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 3
"And most importantly one can perform probabilistic inferences about formulas using that Markov network, like computing the probability that a formula F2 is satisable knowing that F1 is satisable. Let us conclude this section with a small example based on the following axioms 1) X follows(X,X) < 5 > 2) X,Y (follows(X,Y)online(Y,11pm))  online(X,11:20pm) < 1 > with their respective weights between brackets. What it says is that it is quite true that someone does not follow itself (weight = 5). And it is relatively less true that someone following someone else who is online would be online 20 minutes later (weight = 1). The constants are Jill and Joel.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 3
"11pm and 11:20pm can be seen as constants of a different sort, but since they are not relevant for this example they are ignored by the universal quantier (this would be easily formalized with a typed FOL). The ground formulas of Axiom 1 are 1)  follows(Jill,Jill) < 5 > 2)  follows(Joel,Joel) < 5 > The ground formulas of Axiom 2 are 3) (follows(Jill,Jill)online(Jill,11pm))  online(Jill,11:20pm) < 1 > 4) (follows(Jill,Joel)online(Joel,11pm))  online(Jill,11:20pm) < 1 > 5) (follows(Joel,Jill)online(Jill,11pm))  online(Joel,11:20pm) < 1 >","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 3
"6) (follows(Joel,Joel)online(Joel,11pm))  online(Joel,11:20pm) < 1 > with their respective weights between brackets. The corresponding Markov network is given in Figure 3.8 There are 8 variables and therefore 28=256 valuations. Lets give an example of the calculation of the probability of two valuations, the one where all variables are false and the one where all variables are true. In the case where all variables are false we can see that      Quantifying and Managing Uncertainty 63 follows(Jill, Jill) online(Jill, 11pm) follows(Joel, Jill) online(Jill, (11:20pm) online(Joel, 11:20pm) follows(Jill, Joel) follows(Joel, Joel) online(Joel, 11pm) Fig. 3.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 3
"8 Markov network corresponding to the axioms given above. A link between two atoms are build whenever the two atoms are both present in a ground formula as they then obviously depends on each other with respect to the probability distribution of valuations. all axioms are satised (neither Jill nor Joel follows itself, and the premise of Axiom 2 is never fullled therefore true).","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 3
"The calculation goes as follows P  vfalse  = 1 Z (2exp(5)+4exp(1)) In the case where all variables are true we can see that only the rst axiom is not fullled, so the calculation goes as follows P(vtrue) = 1 Z (4exp(1)) We still need to calculate the normalizing factor Z = exp(w j  f j(v)) There are 72 valuations satisfying ground formulas 1 and 2, and there are 52 valuations that do not satisfy ground formula 3 to 6 (when the conclusion is false but the premise is true), therefore Z is Z = 272 exp(5)+4  25652 exp(1) = 17056 which lets us with P  v false  = 0.018 P(vtrue) = 0.0006      64 Real-World Reasoning 3.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 3
"4 Imprecise and indenite probability Imprecise probability is a generic term referring to a variety of theories that extend probability theory when the probability measure is partially known or inadequate. The most common class of imprecise probabilities uses upper and lower probabilities (Walley, 1991). That is, an event may be characterized by two values, its upper and lower probabilities, instead of one probability; and this interval is interpreted to delimit the means the probability distributions lying in a certain envelope, and constructed according to certain distributional assumptions. Other theories for dealing with imprecise probability use the notion of metaprobabilities (that is probability distributions over probabilities). There are debates amongst statisticians whether or not to allow meta-probabilities because that meta-level often cannot be subject to repetitive experiments. Therefore some prefer to consider a subjective interpretation of probability (as opposed to a frequentist interpretation), which may affect the choice of the assumptions regarding how to model imprecise probability.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 3
"One well developed imprecise probability theory using meta-probabilities is called Indenite Probability (Goertzel et al., 2006c); it uses lower and upper probabilities plus a degree of condence, more formally any event w has an indenite truth value consisting of a quadruplet <[L, U], b, k> which roughly means that after k more observations there is a meta-probability b that the probability of w lies within [L, U]. This method of quantifying probabilities is used in the Probabilistic Logic Networks approach to inference, which is detailed in Chapter 12.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 3
"  Chapter 4 Representing Temporal Knowledge Weve discussed the basics of logical reasoning and logical knowledge representation, in a general way. Its now time to get more specic and talk about one of the applications of logic thats most central to our topic: logical representation of time. The natural way to present this material is to start with temporal representation and then move to temporal reasoning. However, we will rst make a few brief comments about temporal reasoning, with the goal of motivating our choices in temporal representations. This should be expectable, because in large part its the requirements of reasoning that determine what kinds of knowledge representation are appropriate. Temporal reasoning, broadly construed, is the process of inferring new relations between events localized in time based on known facts and relations about those events.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 4
"As such, temporal reasoning can be divided in two main branches, depending on how time is represented:  Quantitative: in this variant, information regarding absolute, numeric temporal labels  or time stamps, using a computational jargon  is important for reaching conclusions about events, and therefore it is used as part of the modeling. Quantitative temporal reasoning will work with events specied in a temporally hard way, such as event A begins at 01:31 and ends at 02:03.  Qualitative: this variant is not concerned at all with absolute time stamps; instead, only relative relations between events such as event A happens before event B, event C happens during event B, and so on are relevant for producing inferences on the known temporal facts. The quantitative approach is mainly applicable (and relevant) in applications where both accurate timing data is available and extreme time precision is necessary, such as reasoning about the functioning and performance of real-time systems.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 4
"On the other hand, the quali65      66 Real-World Reasoning tative approach is more appropriate to the analysis of data arising from human systems, or from noisy sensors as possessed by biological organisms or robots), and therefore we will concentrate here mainly on the qualitative approach. Within the qualitative approach, there are two main issues to be confronted:  how to represent basic units of time (how to quantify time)  how to represent relationships between basic units of time A number of different approaches exists to both of these problems, and we will now review these. 4.1 Approaches to Quantifying Time Figure 4.1 presents a simple ontology of the different approaches that have been taken to the representation of basic time-units. As depicted in Figure 4.1, the essential aspect of time is its ordered nature: for any given two temporal units, we can judge whether one is before or after the other, or whether the two are simultaneous.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 4
"All methods of quantifying time incorporate this ordering aspect. The simplest mean of ordering time, the point, has basically no qualitative aspects other than ordering. Two time-points can be compared as to their ordering, and theres nothing else to do with them, except to get quantitative and measure the distance between two time-points. On the other hand, we can also consider models of the time-unit that have a richer set of relationships amongst them, such as intersection, adjacency and so forth. We call models like these topological, and the most common approach here is to represent time using intervals. There is an argument that this is a more psychologically natural approach than time-points  a single, indivisible, instantaneous point being a kind of mathematical abstraction. The topological relationships between time-intervals are most commonly treated using Allens interval algebra, to be discussed shortly below.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 4
"In addition to simple time intervals, there are other related approaches such as  fuzzy intervals (the areas near the edge of the interval have less membership in the timeunit than the ones near the center, as determined by some fuzzy membership function)  probabilistic intervals (with the meaning that each subinterval either does or does not belong to the event associated with the time-unit, but the subintervals near the edge of the interval have a smaller chance of belonging to it), which may take the form of treat     Representing Temporal Knowledge 67 O r d e r e d O r d e r e d  T o p o l o g i c a l P o i n t Interval Probabilistic Interval F u z z y Interval Distributional Distributional F u z z y Fig. 4.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 4
"1 Qualitative relations between approaches for temporal representation ing an interval as a condence interval, or of utilizing a whole probability distribution instead of an interval  distributional fuzzy intervals, which use a probability distribution of fuzzy sets (meaning that each subinterval has a certain probability distribution of membership degrees in the event associated with the time-unit) These various possibilities are depicted in Figure 4.2 and 4.3.      68 Real-World Reasoning Fig. 4.2 Point-like, interval and fuzzy representations of an event 4.2 Allens Interval Algebra The most traditional and well-developed theoretical framework for systematizing the qualitative relationships between time-intervals is Allens Interval Algebra, or simply Interval Algebra (IA), formalized for the rst time in (Allen, 1983). IA, in its original and simplest form, models temporal events as intervals, that is, processes that have a beginning and an end in time.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 4
"Based on that, thirteen temporal relations may exist between any given pair of events. Figure 4.4 illustrates those relations with a simple bar representation for intervals: There is a calculus that denes possible relations between time intervals and provides a composition table (see the next chapter for an example of composition table) that can be used as a basis for reasoning about temporal descriptions of events. Time intervals are not necessarily represented by precise endpoints, but rather by their relationships. So, in this framework without metric one can express that a time interval is contained by another time interval etc.      Representing Temporal Knowledge 69 Fig. 4.3 Probabilistic and distributional representations of the same event Fig. 4.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 4
"4 Possible logic relations between two intervals according to Allens Algebra The following table shows many of the base relations in a different notation, explicitly typing them in with logical constraints (Xdenotes the left end of the time interval X and X+ denotes the right end of X):      70 Real-World Reasoning Relation Name Constraint < X Before Y X+ < Y > Y After X m X Meets Y X+ = Y mi Y Met by X o X Overlaps Y X < YX+ < Y+Y oi Y Overlaps by X < X+ s X Starts Y X = Y X+ < Y+ si Y Started by X d X During Y Y < X X+ < Y+ di Y Contains X f X Finishes Y X > Y X+ = Y+  Y Finished by X = X Equals Y X = YX+ = Y+ Constraints in Allens algebra are of the form I1(rel1,...","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 4
",reli)I2, where I1 and I2 are time intervals, and rel1,..., reli are some of the above 13 relations, with the meaning that at least one of them holds. Consider the example, John was not in the room when I touched the switch to turn on the light. Let A be the time John was in the room, B be the time I touched the light switch, and C be the time the light was on. Then we can say A{p,m,mi,pi,}B, that is, A precedes, meets, is met by, or is preceded by B; and B{m,o}C, that is, B meets or overlaps C. Similarly, the sentences During lunch, Alison reads newspaper. Afterwards, she goes to her ofce.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 4
". may be formalized in Allens Interval Algebra as follows:      Representing Temporal Knowledge 71 AlisonReadsNewspaper { d, s, f } AlisonIsHavingLunch AlisonIsHavingLunch { <, m } AlisonGoesToOfce For instance, the notation {d,s,f} refers to three relations in the above table, and indicates their disjunction; so it means during or starts or nishes. It is clear that all of the above relations can be dened from the three binary relations <, =, and > applied to the bounds of two intervals to be located w.r.t each other. For instance, the assertion X overlaps Y corresponds to X < Y  X+ < Y+  Y < X+ as shown in the above table. The basic relations describe relations between denite, certainly known intervals. Uncertainly known intervals may be described by a set of all the basic relations that may apply.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 4
"We call such a set of basic relations a general Allen relation, or just an Allen relation. There is a general relation for every combination of the thirteen basic relations: 213 or 8192 of them. Each of the basic relations is a relation, of course, as are all their combinations. The full relation holds between two intervals about whom nothing is known. The empty relation {} has no meaning in terms of relations between actual intervals, but is the result of some operations on interval relations. The satisfaction problem for Allens interval algebra is determining, for a particular collection of relations on indenite intervals, whether there is any set of specic time values for the intervals such that all the relations in the collection are true.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 4
"For example, a collection of relations and intervals that is not satisable is three intervals A, B, and C such that A { p } B, B { p } C, and C { p } A (each precedes the next, and the last precedes the rst). There are no denite intervals for which all these relations can hold. The satisfaction problem is shown to be NP-complete (Vilain et al., 1989). However, there are subclasses of the problem that are tractable and that permit polynomial-time decision procedures. 4.2.1 Allen Algebra in the Twitter Domain In order to better understand the practical uses of IA, we will now explain how to apply logical formalism to a specic domain, consisting of messages in the Twitter microblogging service. We will make use of this same example domain in some other examples in the following, and so take this opportunity to briey summarize the domain before using it to exemplify interval algebra.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 4
"     72 Real-World Reasoning Twitter is a free web service allowing individuals to post brief public or private messages (tweets). Each tweet deals with specic concepts, entities and sentiments, and has a specic author. Further, there is a social network of tweet authors; tweets may be geographically localized via IP address; and information about tweets is publicly available via the Twitter software API. See Figure 4.5 for a screenshot of the Twitter interface. Fig. 4.5 Screenshot of the Twitter.com web service In order to develop some of our future examples using this domain, we will need to introduce some special primitive logical term and relationship types (i.e. a signature for our theory relating Twitter entities). The primitive terms and relationships we introduce are depicted in Figure 4.6 and 4.7.      Representing Temporal Knowledge 73 Time Place Person Message Concept Sentiment Event Specic entity Number Fig. 4.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 4
"6 Primitive terms for a logical formalism targeting The commonsense semantics of these concepts and relationships should be fairly obvious; however, the right way to rigorously formalize some of these relationships (such as the spatial and temporal ones) is a complex issue, and will occupy a considerable percentage of this book! Returning to interval algebra, Figure 4.8 shows schematically an example sequence of Twitter entries and the events that can be inferred from them. Referring to Figure 4.8, note that many relations between the assigned events can be inferred. For instance: Peter watches debate occurs during Jane plays game. Jane plays game overlaps Bill writes report. Bill makes coffee occurs during Bill writes report. Peter watches debate precedes Bill writes report.      74 Real-World Reasoning Fig. 4.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 4
"7 Logical relationships in the Twitter formalism Temporal relations such as those listed above can be represented in the form of a graph, forming a type of graph that we call a Temporal Interval Network. Those are methodologically interesting in the sense that they transform many problems of IA into graph problems. Figure 4.9 shows the equivalent TI network for many of the relations between the events in Figure 4.8:      Representing Temporal Knowledge 75 Fig. 4.8 Tweets as temporal events, associated with time intervals. Note that in the 4.3 Uncertain Interval Algebra As noted above, IA in its original form presumes the existence of hard instants in time where events start and begin.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 4
"However, in the real world, the actual start and end times of many events may be hard to dene; and furthermore, in the specic case of the Twitter interface the imprecision of events in personal lives is taken into account by the use of a time scale of varying discretization (jumping from half an hour to hourly marks, for instance) as well as the use of words denoting imprecision (about). In the fuzzy version of IA, relations between events are dened in terms of degrees of truth. Putting it simply, the truth value of a relation is the degree of existence of that relation, varying from 0 to 1. For instance, one may estimate that event A precedes event B with a degree of truth of 0.7 . From now on, we will denote such assignments with a functional notation for instance A precedes B with a truth value becomes precedes(A,B,0.7).","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 4
"Interestingly, the use of fuzzy logic allows the existence of multiple relations that are mutually exclusive in the original, rigid IA. For instance one can say that at the relations precedes(A,B,0.7) and overlaps(A,B,0.3) are both valid under a fuzzy modeling. (The previous numeric also illustrates that the sum of the truth values of all relations between two events has to be 1.0, full existence so to speak.) Figure 4.10 shows another TI, this time fuzzy (with truth values attached to edges), showing some of those ambiguously modeled      76 Real-World Reasoning Fig. 4.9 Time interval network for the events of Figure 4.8 relations for the events of Figure 4.8. As one can see, now an edge between any two events is dened by a tuple that may be composed of multiple truth values for different relations.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 4
"Finally, in the probabilistic version of IA, a value represents the probability that a given relation between two events is assigned. Multiple probabilistic relations between two events that would have only one relation in the classic version of IA are also possible. At rst, that may sound very similar to the fuzzy version, specially under the numerical viewpoint. However, conceptually they are completely different. Speaking in terms of TI networks, in the fuzzy case a given edge can be multiple types of relations at the same time, in different degrees. In the probabilistic case, a given edge can have the possibility of being multiple relations, with different probabilities, but that is a modelling of uncertainty on the nature of an edge that in the real world ts into just one of the options. One aspect of probabilistic IA that may clarify its conceptual difference relative to the fuzzy version is the problem of determining the most likely subnetwork. An example of that is illustrated in Figure 4.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 4
"11, where a very small subset of events from the example Twitter      Representing Temporal Knowledge 77 Fig. 4.10 Yet another variation of a TI network modeling imprecision ow is used to compose a probabilistic IA network. For sake of clarity, different relations (with assigned probabilities) between two events are represented by different edges. The dotted edges form the less likely subnetwork, while the solid edges represent the most likely network. In the example, that hints at the sequence of events that most likely happened in reality. Fig. 4.11 Finally, a probabilistic TI network modeling the same events      78 Real-World Reasoning While the possibility has not been explored in the literature, one may also conceive fuzzy probabilistic IA, involving intervals interpreted as condence intervals of centers of fuzzy membership functions. Various other related options also exist, offering a great deal of modeling exibility.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 4
"The problem of nding the most likely subnetwork brings to mind computational considerations on dealing with IA, which will be briey examined here. As already mentioned (and as the most likely network example already illustrates) most if not all IA problems can be modelled in terms of graph problems, and nding the optimal solutions for many of them is combinatorially explosive, and not viable for realistically large IA networks. However, the theoretical work on IA has found subsets of it that are computationally tractable, as well as heuristics that signicantly reduce computational time at the cost of nding a solution that is not guaranteed to be optimal (van Beek & Manchak, 1997). And much of that is just applied graph theory, since many of the relevant problems in IA are reducible to classical graph problems.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 4
"That is, IA networks (classic or otherwise) are in principle manageable by an extremely diverse plethora of optimization methods, as in the case of any graph problem.   ","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 4
"  Chapter 5 Temporal Reasoning In principle, logical inference about propositions involving events embedded in time and space could be treated the same as any other kind of logical inference, but experience shows that this is not an optimal approach, mainly because such an approach leads to extreme inefciencies of inference control. Thus, in this section and the following ones we will consider specic logical formalisms recently proposed for reasoning about time and space. A variety of logical formalisms have been proposed for reasoning about time, including  Standard rst-order logic with temporal arguments  Reied temporal logic  Modal temporal logic (all of which we will review in detail below) and others.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"A large percentage of work in the area of temporal logic appears to fall prey to one of the pitfalls of  Unrealistic simplicity  Excessive complexity  Computational intractability The creation of a scalable, reasonably general temporal logic remains a difcult research problem, and one of our tasks here is to characterize this research problem and explain exactly which of its aspects are most pertinent and most worth attacking at this juncture.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"In order to apply temporal logics such as the above to reasoning on large, complex real-world problems, several issues have to be addressed:  the logic should be simple  if a logic is overcomplicated, crafting a workable, scalable inference control strategy is very unlikely to be feasible; and integrating the logic 79      80 Real-World Reasoning with other system components like query languages and knowledge bases will likely be infeasible  the logic should not be too simple  for instance, for most serious real-world applications simple Priors logic is too simple and inadequate for applications such as reasoning about real-time behaviour of software. Also, a logic that achieves simplicity at the expense of needed expressiveness will be impractical because the formulas describing real algorithms will be too long and complicated to understand.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"the corresponding deduction procedures should be efcient It seems feasible to construct many different approaches satisfying these criteria (including perhaps a PLN based approach as will be exemplied in Part III below); but there is not yet any approach that has been convincingly validated by practical applications to satisfy them. 5.1 The Challenge Time Presents to Classical Logic The essential challenge temporality poses to classical logic is that, in the latter, formulas are evaluated within a single xed world and have xed truth value, that does not change over time. However, most real-world systems are dynamic. Temporal statements, statements involving temporal information, may have truth value that changes over time. For instance, the statements it is Monday or I am at a meeting have constant meaning, but their truth value can vary in time (but, of course, neither statement is ever true and false simultaneously; unless one is dealing with paraconsistent logic, which is beyond the scope of this book).","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"Thus, it is Monday may be satised in some contexts, but not in others. Examples of temporal statements are also I am always too busy, I will eventually be at my ofce , or I will be at my ofce until John give me a phone call. In temporal logics, evaluation takes place within a temporal context. Temporal relationships and reasoning deal with issues like change, actions, causality, ontology of time, underlying logic, temporal constraints used, reasoning algorithms employed, etc. A change describes moving from one state or condition to another one. This relation is temporal (either implicit or explicit) and requires an appropriate representation. Representing the temporal aspect of the knowledge adds the time dimension to the truth of the information. A formalism for representing temporal information has to provide a way for establishing a link between an atemporal assertion and a temporal reference.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"There is a number of approaches for representing temporal relationships, each of them requiring      Temporal Reasoning 81 specic forms of reasoning. Most of them are inuenced by studies by Aristotle and the Megarian and Stoic schools in ancient Greece. Temporal systems can be classied according to different aspects (Emerson, 1991; Pani & Bhattacharjee, 2001):  propositional versus rst-order;  global versus compositional;  time points versus time intervals;  discrete versus dense time;  bounded versus unbounded time;  branching versus linear versus paralel versus circular time;  past versus future tense. These aspects determine expressiveness, and also computational demands. In the rest of this section we review three main families of modern approaches to temporal representation (more details can be found, for instance, in surveys (Pani & Bhattacharjee, 2001) and (Emerson, 1991).","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"Approaches within these families may have different properties according to the list of aspect given above. 5.1.1 First order logic: temporal arguments approach The simplest approach to handling time within logic is to represent time by numerical values. This representation can be applied within rst order logic (FOL), with reals or integers as the intended domain for time variables and constants. There is a number of efcient methods for dealing with rst order statements and reasoning with them, so this approach starts from a well established grounds. This approach is often referred as to temporal arguments approach. Within FOL, we can simply express temporal facts like Alison is working on the same project as Bob at time t, for instance, in the following way: work on same project(Alison, Bob, t) Using sorted rst order logic may be more suitable and, in this case, there is a distinguished sort for temporal arguments.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"For instance, the classical example You can fool all the people some time and you can fool some people all the time, but you cannot fool all the      82 Real-World Reasoning people all the time can be represented in sorted rst order logic in the following way: x:H. t:T. youcanfool(x,t)  x:H. t:T. youcanfool(x,t)  x:H. t:T. youcanfool(x,t), with an intended semantics that links the sort H to the set of humans and the sort T to the set of all time values. By this, time is given a special syntactic and semantic status, but can still share much of the treatment of other sorts of values. If the theory is equiped with axioms on ordering, one can also reason (using general rst-order reasoning frameworks) about statemets like the following one: t0:T. t:T. (t>t0  f(t)).","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"In a similar way, one can use arithmetic operators over time values. Despite its simplicity, well-foundness, and developed proof theory and methods, this approach has severe limitations. For instance, without adding a host of specialized additional predicates, one cannot model aspectual distinctions between for example, states, events and processes, and cannot represent notions used in natural language like now, then, since, while, until, nor notions like a bit later and similar. To get around these problems, there are different formalisms for representing temporal information within rst order logic  a line of thinking that eventually leads back towards the more sophisticated approaches to time representation that we considered earlier. For instance, Interval Temporal Logic (ITL) is a temporal logic for both propositional and rst-order reasoning about periods of time found in descriptions of hardware and software systems, but also in articial intelligence and linguistics.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"ITL can handle both sequential and parallel composition and offers powerful and extensible specication and proof techniques for reasoning about properties involving safety, liveness and projected time (Moszkowski, 1994). It is not identical to Allens interval algebra, but has a great deal in common with the latter. 5.1.2 Reied temporal logic Alternately, rather than the encoding of temporal information within FOL, there is a reied approach relating atemporal terms to temporal terms by specic temporal contexts. In the reication approach to temporal reasoning, the truth of temporal assertions is considered while keeping atemporal parts of assertions unchanged within a specic temporal context. A good overview of reied temporal logics can be found in (Ma & Knight,      Temporal Reasoning 83 2001). One of the most inuential, formal approaches to reied logic was presented by Shoham in 1987 (Shoham, 1987).","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"Reifying a logic means moving into a meta-language where an assertion in an initial language (usually FOL, but a modal logic can also be used), becomes a term in a new language. In this new logic, one can reason about truth values of expressions in an object language through use of the truth values of expressions built with the operator like TRUE and with temporal object as arguments. Thus, TRUE(atemporal expression, temporal qualication) represents a statement whose intended meaning is that the rst argument is true during the time denoted by the temporal qualication. TRUE is not a relation in the logic, nor it is a modal operator, rather it is a reifying context. For instance, the sentence Alison is on a meeting with Bob between 11am and 12am can be expressed as TRUE(MEETING(Alison, Bob), (11am, 12am)).","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"The truth predicates are used to express not only the time when an expression is true but also the patterns of its temporal occurrence. So, in general, the pattern of temporal occurrence for the atemporal expression admits many interpretations other than during. And this brings us back, nally to Allenss interval algebra as discussed above, which naturally leads to a form of reied temporal logic. In Allens variant of reied temporal logic (Allen, 1984), temporal incidence is expressed using the operators HOLDS, OCCURS, and OCCURING in order to express distinctions between states, events and processes. Temporal reication has several advantages. On one hand, this logic gives a special status to time. On the other, it allows one to exibly predicate and quantify over propositional terms. Therefore, it gives the expressive power to discuss relationships between propositional terms and temporal aspects with a high level of generality.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"Due to these qualities, the reied approach has enjoyed considerable popularity in AI. However, it has also been a subject of criticism and attacks. A number of authors have argued that reied temporal logics are unnecessarily complicated and imply a philosophically suspect ontology of time. However, it seems to us that these objections really pertain to specic, simple versions of reied temporal logic, rather than to reied temporal logic in general. For instance, a major problem with simple reied logics is that there is no straightforward way of referring to one temporally referenced object within the context of another temporal interval, such as The leader of the Konrad project in 2003 left our company in      84 Real-World Reasoning 2006. Rather, in simple reied logics, all non-temporal terms have to be evaluated with respect to the same temporal terms, i.e., those given in the TRUE context.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"So, in a simple reied logic, the given example could be expressed in the following inelegant way: x (TRUE(project leader(Konrad)=x, (2003, 2004))  TRUE(left job(x), (2006, 2007))) In addition, in Shohams reied temporal logic, there are no temporal predicates, except  and =, which makes expression of some temporal phenomena awkward. But in the BTK approach (named after the initials of its authors), presented by Bacchus et al in 1991 (Bacchus et al., 1991), each predicate and function symbol can take any number of temporal arguments. For instance, the above example can be represented in a much simpler way: left job(project leader(2003, Konrad), 2006) This is also the approach adopted in PLN, as will be discussed a little later.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"Alternatively, Galton proposed a method of unreication based on incorporating tokens (Galton, 1991). Galtons event token is basically the occurrence of some event at some point m in time Thus,Alison is having a lunch at 3 00pm is an event token. Event tokens act on the one hand as additional parameters to predicate and function symbols, while on the other they can be used in the temporal occurrence predicates. 5.1.3 Modal temporal logic Modal logic, as discussed above, is a formal logic system that deals with modalities, like possibility and necessity. In modal logic, one can express a statement like It is possible that Bob quit his job. The modalities are represented by modal operators. The basic unary modal operators are usually written  (or L) for necessarily and  (or M) for possibly. In a classical modal logic, these two operators are linked in the following way: p  p p  p.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"Tense logic is a kind of modal logic-based system, introduced rst by Prior in 1955 following the idea that tense is a sort of modality, to be set alongside the ordinary modes of necessity and possibility. Tense logic has two sets of modal operators, one for the past and one for the future (while ordinary modal logic has only one). In Priors notation,  Pp stands for It has at some time been the case that p      Temporal Reasoning 85  Fp stands for It will at some time be the case that p  Hp stands for It has always been the case that p  Gp stands for It will always be the case that p. For instance, if p stands for Alisons department moves to China and if q stands for Alison and Bob have communication problems, then G(p  Gq) stands for It will be the case that if Alisons department moves to China, then Alison and Bob will always have communication problems.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"Tense operators can express standard modal operators  and : p can be expressed as PppFp p can be expressed as HppGp Tense logic is obtained by adding the tense operators to an underlying logic, for instance propositional logic, or rst-order logic. For instance, in tense logic built over rst-order logic, the statement A woman will be a CTO can be represented as x(woman(x)  F cto(x)). The standard model-theoretic semantics of tense logic is dened with respect to temporal frames. A temporal frame consists of a set of times together with an ordering relation < on it. This denes the ow of time and the meaning of the tense operators. An interpretation of the tense-logical language assigns a truth value to each atomic formula at each time in the temporal frame.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"For instance, Pp is true at t if and only if p is true at some time t such that t < t (the semantics of other tense operators is dened by analogy). A tense logic formula is valid if it is true at all times under all interpretations over all temporal frames. Prior developed several versions of a calculus of tenses in which one can derive, for instance, Gp  FP, and also (p  F p)  p (what neither is true nor will be true is not possible), linking standard modal operators with Prior tense operators in an elegant way. While in typical rst order representation time is absolute, in modal temporal logic time is relative and statements refer to the present or to other events. There are variants of modal temporal logic for dealing with absolute precise times. Priors system does not specify the nature of time (points or intervals), but in this approach understanding temporal elements as points of time is more natural.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"Apart from tense logic, there is a whole family of other modal temporal logic systems. The modal  calculus is a powerful class of temporal logics with a least xpoint operator  (Scott & De Bakker, 1969; Kozen, 1983). It is used for describing and reasoning      86 Real-World Reasoning about temporal properties of labelled transition systems. The modal  calculus subsumes dynamic and temporal logics like CTL and LTL. In linear temporal logic (LTL), time is not branching, and one can encode formulas about the future of paths, such as that a condition will eventually be true, that a condition will be true until another fact becomes true, etc. In linear temporal logic, there are operators for next (X), globally (G), nally (F), until (U), and release (R).","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"For instance, if p stands for There is electric power, and q is I work on my computer, G(pXq) stands for It always hold that if there is no electric power, in the next moment I will not work on my computer. In the simplest formulation of LTL, the U (until) operator is used to derive all the others. On the other hand, computational tree logic (CTL) is a branching-time logic, meaning that its model of time is a tree-like structure in which the future is not determined; there are different paths in the future, any one of which might be an actual path that is realised. Some of the CTL operators are quantiers over paths (for instance, Ap stands for p has to hold on all paths starting from the current state) and path-specic quantiers (for instance, Xp stands for p has to hold at the next state, Gp stands for p has to hold on the entire subsequent path).","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"For example, if p mean I give a presentation, in CTL one encode I will give a presentation, whatever happens by AGp. There are also other variants of branching-time logic, such as Branching Temporal Logic which also generalizes LTL, and differs from CTL in subtle ways (Kontchakov, 2007). Modal temporal logics are, in a sense, more expressive than FOL. They are also suitable for their modularity. They can directly and neatly be combined with other modal qualications like belief, knowledge etc. For certain sorts of modal temporal logics there are efcient automated theorem provers. Concerning applications, Prior used his tense logic to build theories about the structure and metaphysics of time, while now it has numerous other applications. The notational efciency of modal temporal logics makes them very appealing for applications in natural language understanding, where they have been widely used.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"There are various areas of applications of temporal logic, such as, for example, controlling operation of a bank of identical elevators in a building (Wood, 1989). However, the area where modal temporal logics probably found the widest acceptance and success is in computing, including the theory of programming, database management, program verication, and commonsense reasoning in AI. In applications in program verication, following the inuential work of Pnueli (Pnueli, 1977), the main idea is to formally specify a program and to apply deduction      Temporal Reasoning 87 methods for proving properties of the program like correctness, termination, and possibility of a deadlock. In describing properties to be proved, one can use modal operators, for instance, to state that whenever a request is made, access to a resource is eventually granted, but it is never granted to two requestors simultaneously.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"One of the very successful temporal logics used in verication is temporal logic of actions (TLA), developed by Leslie Lamport, which combines temporal logic with a logic of actions (Lamport, 1994). It is a logic for specifying and reasoning about concurrent systems. Systems and their properties are represented in the same logic. Syntax and semantics of TLA are very simple, yet it is very powerful, both in principle and in practice. It has been used to study and verify cache coherence protocols, memory models, network protocols, database protocols, and concurrent algorithms (Batson & Lamport, 2003). 5.1.4 Integration of deontic and temporal logic There are several approaches for combining deontic and temporal logic. Some of them are based on expressing deontic constraints in terms of temporal logic, while in some deontic logic is extended by temporal operators.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"Some of the most important issues in combining deontic and temporal logic are:  The temporal approaches considered are usually based on tree-structures representing branching time with the same past and open to the future.  On top of these tree-structures, temporal deontic logics typically dene one modal necessity operator, expressing some kind of inevitability or historical necessity, plus deontic obligation operators. One family of those logics expresses the modal and deontic operators in temporal terms, while another family introduces temporal operators that can be combined with the modal and deontic operators. In the approach described in (Dignum & Kuiper, 1997), deontic logic is extended by temporal operators. This approach is used for the specication of deadlines. It uses deontic constraints to specify what is the agent obliged to do and temporal constraints since the obligation is usually to be performed before a certain deadline.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"The following example from (Dignum & Kuiper, 1997) requires both deontic and temporal reasoning.  Alison has to pay the mortgage for her house every month.  Alison borrowed some money from Bob, which she has to repay as soon as she is able to do so.      88 Real-World Reasoning  The roof of Alisons house started leaking. It has to be repaired before the October rains start (It is now September).  Alison wants to go on a midweek holiday. She has an offer to rent a cottage for relatively little money which has to be paid within 30 days after the reservation has been made. Formulas in the system introduced in (Dignum & Kuiper, 1997) are propositional formulas built in the standard way and, in addition, formulas involving actions, the deontic operator OB, and a preference relation over actions called PREFER.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"If  is an action and F is a formula, then []F is also a formula, and its informal meaning is that doing  neccessarily leads to a state where F holds. If F is a formula, then OBF is also a formula, and its informal meaning is it has to be the case that F holds. If 1 and 2 are actions, then PREFER(1,2) is also a formula and its informal meaning is that the action 1 is preferable to the action 2. The formulas are built also using temporal operators X (unary, for next), U (binary, for until), P (unary, for held), S (binary, for held since), DO (unary, for an action to be performed next), DONE (unary, for an action that was performed last). The semantics of formulas is given in Kripke style.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"For example, the statement If Alison borrowed some money from Bob, she has to repay as soon as she is able to do so. is modeled by the formula: DONE(borrow(Alison, Bob))   OB(true < repay(Alison, Bob)) < DONE()  PREFER(repay(Alison, Bob), ) where OB(x<y) states that y can not hold true before x is performed In (Dignum & Kuiper, 1997) there is not a corresponding axiomatization and inference system, nor algorithms for automation of reasoning in the described logic. Partly following and extending ideas from (Dignum & Kuiper, 1997), in (Broersen et al., 2004), deontic logic SDL is combined with temporal logic CTL and deadline constraints are expressed in this new logic. Automation of reasoning in this logic is not discussed, but there is an expectation that it can be handled by CTL theorem provers.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"The approach described in (Cheng, 2006) starts with linear temporal logic (LTL), extend it to state/event LTL (SE-LTL) which takes into account both events and propositions, and nally, extends it to the system SED-LTL with deontic modalities. The semantics of formulas is given in Kripke style, while there is no corresponding axiomatization or reasoning procedures. The system can be used in different computing scenarios involving both temporal and deontic notions. For instance, it can be used for expressing an access control policy in which the permissions depend on time or events.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"For instance, the following resource monitoring problem can be represented within the proposed logic:      Temporal Reasoning 89  Useri has the permission to use the resource r for 5 time units continuously, and he must be able to access it 15 time units after asking, at the latest;  Useri has always the permission to use the resource r, and he has to release it after 5 time units of utilization;  If useri is asking for the resource and he has the permission to use it, then the system has the obligation to give it to him before 5 time units;  If useri uses the resource without the permission, he must not ask for it during 10 time units.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"Alternately, in the approach for combining temporal and deontic modalities proposed in ( Aqvist, 2005), the absolute temporal operator Rt (it holds at the time t that) is used, instead of the relative temporal operators X and U, which are more expressive. There is also a variant of the system with the temporal operator Rth (it is realized at time t in history h that). In semantics of deontic operators, frames that are used are considered as (nite) two-dimensional coordinate systems, where it is possible to distinguish between the longitude (i.e., x-value) and the latitude (i.e., y-value) of any point in such a coordinate system. Times are interpreted as longitudes, and worlds, or histories, as latitudes. There is a corresponding, sound and complete, axiomatization and inference system, but automation of reasoning is not considered.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"Despite the fact that all combinations of deontic and temporal logic have been constructed with applications in mind, it seems that there are still no large-scale applications of these techniques for real-world problems. Hopefully this situation will change in the near future. 5.2 Inference systems for temporal logic As noted above, there are three main approaches to temporal representation and reasoning. Here we continue the discussion and give some simple examples of inference in each of them. We will focus here on inference systems for temporal logic, but wish to also note that there are decision procedures for some of the logics discussed here (Emerson, 1995). Recall that for a given formula F, a decision procedure only gives a yes or no answer whether F is valid/theorem, without providing deductive argument for that. Most decision procedures use model-theoretic, semantic arguments for obtaining the nal result. In some cases, these arguments can be turned to deductive arguments, but that can be very expensive.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"     90 Real-World Reasoning 5.2.1 Inference in the Simple First Order Logic Approach In the approach based on rst order logic and expressing temporal information as rstorder formulas, giving each predicate an extra argument place that corresponds to the temporal dimension. For instance, work on same project(Alison, Bob, 2007) If the rst-order signature is extended with a binary inx predicate < denoting the temporal ordering relation earlier than, and a constant now denoting the present moment, then the tense operators can be simulated by means of rst-order logic, in the following way (p(t) denotes the result of introducing an extra temporal argument place to the predicate p): Pp (informally: p held at some point in past): t(t<now  p(t)) Fp (informally: p will hold at some point): t(now<t  p(t)) Hp (informally: p always held): t(t<now  p(t))","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"Gp (informally: p will always hold): t(now<t  p(t)) In this framework, temporal information is completely expressed by means of rst order logic, therefore standard classical rst-order inference systems can be used as a reasoning vehicle. For instance, let us assume the following statements:  Year 2007 was in the past.  Alison and Bob worked together on the same project in 2007.  If Alison and Bob worked together on the same project, they will always be friends.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"One can represent these statements in the following way:  2007<now  work on same project(Alison, Bob, 2007)  Pwork on same project(Alison, Bob)  Gfriends(Alison, Bob) According to the above denitions, this can be rewritten as:  2007<now  work on same project(Alison, Bob, 2007)  t(t<nowwork on same project(Alison,Bob,t)) t(now<tfriends(Alison,Bob,t)) From 2007 <now and work on same project(Alison, Bob, 2007), it follows that 2007<now  work on same project(Alison, Bob, 2007) and further, t (t<now  work on same project(Alison, Bob, t)),      Temporal Reasoning 91 which, together with the implication from above, yields t(now<t","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"friends(Alison, Bob, t)) i.e., Alison and Bob will always be friends (all three inference steps in this derivation can be simply justied in any reasonable inference system for classical rst order logic). 5.2.2 Reied temporal logic In Allens variant of reied temporal logic (Allen, 1984), discussed above, temporal incidence is expressed using relational predicates HOLDS, OCCURS, and OCCURING, as for example: HOLDS(MEETING(Alison, Bob), (11am, 12am)). OCCURS(DRIVES(Alison, Home, Ofce), (8am, 8.45pm)) where terms of the form (t,t) denote time intervals. These two predicates ensure distinctions between states and events. Statements about states have homogeneous temporal incidence, i.e., they must hold over any subintervals of an interval over which they hold (e.g.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"the meaning of HOLDS(MEETING(Alison, Bob), (11am, 12am)) is that Alison and Bob are at a meeting at any time interval between 11am and 12am). On the other hand, statements about events have inhomogeneous temporal incidence, i.e., such a statement is not true at any subinterval of an interval of which it is true (e.g., if Alison drives from her home to her ofce from 8am to 8.45am, then in any subinterval of that interval, she does not drive from her home to her ofce, but between some other points).","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"These features of reied temporal logic (homogeneity of states and inhomogeneity of events) are ensured in inference systems by axioms such as s,i,i(HOLDS(s,i)In(i,i)  HOLDS(s,i)) e,i,i(OCCURS(e,i)In(i,i)  OCCURS(e,i)) where In denotes the proper subinterval relation. Therefore, given HOLDS(MEETING(Alison, Bob), (11am, 12am)), the above axiom (along with the underlying rst-order style inference system) enables deriving HOLDS(MEETING(Alison, Bob), (11.15am, 11.20am)). Indeed: 1. HOLDS(MEETING(Alison, Bob), (11am, 12am)) (assumption)      92 Real-World Reasoning 2. In((11.15am, 11.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"20am), (11am, 12am)) (by means of interval arithmetic) 3. HOLDS(MEETING(Alison, Bob), (11am, 12am))  In((11.15am, 11.20am), (11am, 12am)) (by propositional calculus) 4. HOLDS(MEETING(Alison, Bob), (11am, 12am))  In((11.15am, 11.20am), (11am, 12am))  HOLDS(MEETING(Alison, Bob), (11.15am, 11.20am)) (by homogeneity axiom and by Hilberts axiom A4) 5. HOLDS(MEETING(Alison, Bob), (11.15am, 11.20am)) (from 3 and 4, by modus ponens) 5.2.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"3 Modal temporal logic As discussed earlier, Priors Tense Logic contains, in addition to the usual truthfunctional operators, four modal operators P, F, H, G. P and F are known as the weak tense operators, while H and G are known as the strong tense operators. There is an intended correspondence between pair of these operators: Pp  Hp Fp  Gp If this correspondence is ensured, then one pair of the operators is redundant. Post worked on different variants of the inference systems for Tense Logic, trying to build an elegant system that has as theorems all formulas that are true following his intended semantics.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"Some of those formulas are: GpFp : What will always be, will be FpFFp : If it will be the case that p, it will be  in between  that it will be FpFFp : If it will never be that p then it will be that it will never be that p (pFp)  p : What neither is true nor will be true is not possible Fp Fp : What will be, will necessarily be Of particular signicance is the system of Minimal Tense Logic Kt for classical propositional tense logic which consists of Hillberts axioms, the modus ponens inference rule, the following axioms: (1) pHFp (What is, has always been going to be)      Temporal Reasoning 93 (2) pGPp (What is, will always have been) (3) H(pq)  (HpHq) (Whatever always follows from what always has","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"been, always has been) (4) G(pq)  (GpGq) (Whatever always follows from what always will be, always will be) (5) Gp GGp (What will always be will always will always be) and the following rules of inference: (RH) p  Hp (RG) p  Gp (US) p  p[  ] The system Kt is sound and complete, i.e., its set of theorems is the set of all valid tense logic formulas. In addition, it is decidable whether a tense logic formula is theorem. The theorems of Kt are all the properties of the tense operators not relying on the temporal order. Let us consider the following statements: If our company has small administrative overhead, then it has big prot It will always be the case that our company has small administrative overhead.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"From the above assumptions, one can derive the fact It will always be the case that our company has big prot. Let us denote by p our company has small administrative overhead and by q our company has big prot. The proof of Gq is as follows: 1. pq (assumption) 2. Gp (assumption) 3. G(pq) (from 1, by RG) 4. G(pq)  (GpGq) (instance of the axiom scheme (4)). 5. GpGq (from 3 and 4, by modus ponens) 6. Gq (from 2 and 5, by modus ponens) 5.2.4 Computational Tree Logic Computational tree logic (CTL), briey described earlier, has many applications in describing and verifying computing processes. CTL uses the following temporal operators:  Ap  stands for all futures, i.e.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"p has to hold on all paths starting from the current state      94 Real-World Reasoning  Ep  stands for some future, i.e., p has to hold on some path starting from the current state Temporal operators have to be followed by one of the following linear temporal operators:  G always  F  sometime  X  next time  U  until The semantics of p U q is that p is true in all states preceding the state where q holds and q will eventually hold. The semantics of other operators are intuitive. The syntax of CTL is given as follows. There are state formulas and path formulas dened in the following way:  Each atomic proposition P is a state formula.  If p, q are state formulas, then so are pq, p.  If p is a path formula, then Ep and Ap are state formulas.  Each state formula is also a path formula.  If p, q are path formulas, then so are pq, p.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"If p, q are path formulas, then so are Gp, Fp, Xp, p U q. The following system is a complete and sound deductive system for CTL (Emerson, 1991): Axiom schemes: All propositional tautologies. EFp  E[ true U p] AGp  EFp AFp  A[ true U p] EGp  AFp EX(pq)  EXp  EXq AXp  Ep E(p U q)  q  (p  EXE(p U q)) A(p U q)  q  (p  AXA(p U q)) EXtrue  AXtrue AG(r  (q  (pEXr)))  (r  A(p U q)) AG(r  (q  EXr))  (r  AFq) AG(r  (q  (pAXr)))  (r  E(p U q)) AG(r  (q","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"AXr))  (r  EFq)      Temporal Reasoning 95 AG(p  q)  (EXp  EXq) Inference rules: If  p then  AGp (Generalization) If  p and  pq then  q (Modus ponens) For example, it can be proved that  AGp yields  p, and if F1 and F2 are logically equivalent propositional formulas, then  AG F1  AG F2 (these statements will be used in the example in further text). 5.2.5 Branching Temporal Logic An interesting variant of CTL is Branching Temporal Logic (Kontchakov, 2007), which also extends linear temporal logic.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"In BTL, one denes a tree as a ow of time F = (W, <) containing a root point r, for which W = {v|r < v}  {r}, and such that for every w  W , the set {w|v < w} is wellfounded and (strictly) linearly ordered by <. A history in F is then dened as a maximal linearly <-ordered subset of W . Finally, an -tree is a tree of this sort in which every history is order isomorphic to (N,<). A branching time model is then dened as a structure B = (F,H,pB0 ,pB1,...), where F = (W,<) is an -tree, H is a set of histories in F (conceived as the set of possible ows of time in the model), and pBi W for all i.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"A logical formula is then evaluated relative to a pairs (h, w) consisting of an actual history h  H and a time point w  h  i.e. formulas are evaluated at time-points in the context of their histories. In such a pair (h, w), the temporal operators are interpreted along the actual history h as in the linear time framework, while the operators E and A quantify over the set of all histories coming through w. We say that a BT L-formula  is satisable if there exists a branching time model B such that (B,h,w) implies  for some history h  H and some time point w  h. The practical upshot is similar to CTL, but the formal properties are subtly different. 5.3 Examples of Temporal Inference in the Twitter Domain In this section we give a concrete exampleof temporal inference in our target domain. Specically, we illustrate the deductive system for CTL logic on one common-sense example.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"Let us suppose we have:      96 Real-World Reasoning  Bob and Clark always have lunch together.  If Alison is having lunch with Bob, and Bob is having lunch with Clark, then Alison is having lunch with Clark.  In all circumstances, it is true that: if Alison has lunch with Clark, then he will not forget her name that day and potentially the next day Alison will have lunch with Clark again. And let us assume that we want to derive: If Alison has lunch with Bob, then potentially Clark will never forget her name.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"Let us denote Alison has lunch with Bob today by p Bob has lunch with Clark today by q Alison has lunch with Clark today by r Clark forgets Alisons name today by s Then, the assumptions can be represented as: AGq pq  r AG(r  (s  EXr)) And the conjecture as: p  EGs We will use the following metatheorem of propositional logic: if  a  b and  b  c, then  a  c.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"One proof of p  EGs then looks like the following:      Temporal Reasoning 97 (1) AG(r  (s  EXr)) (hypothesis) (2) AG(r  (s  EXr))  (r  AFs) (axiom 12) (3) r  AFs (1,2,modus ponens) (4) AFs  AFs (3, propositional logic) (5) r  AFs (3,4, propositional logic) (6) AFs  EGs (axiom 5) (7) r  EGs (5,6,propositional logic) (8) AGq (hypothesis) (9) q (8, theorem) (10) p  p  q (9, propositional logic) (11) p  q  r (hypothesis) (12) p  r (10,11,pro","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"positional logic) (13) p  EGs (7,12,propositional logic)        ","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 5
"  Chapter 6 Representing and Reasoning On Spatial Knowledge A large percentage of real-world knowledge pertains to space as well as time. Humans possess a great deal of evolved and learned common sense regarding representing and reasoning about space, but, this is not innate to software programs so we need to explicitly think about how to make our software systems space-friendly. From a logic point of view, space and time can in principle be treated just like any other sorts of relationships  but this turns out not to be an optimal point of view in terms of either human-friendliness or computational efciency of logic systems. Rather, with space as with time, it is worthwhile to invest the effort to develop specialized techniques for handling spatial and spatiotemporal knowledge.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"As in the case of time, we will not be able to entirely disentangle spatial representation from spatial reasoning; but we will endeavor to do so as much as possible, and in this chapter we will focus mainly on representation (introducing reasoning only when really needed for motivational purposes) and defer reasoning for later. Most generally conceived, spatial reasoning (Cohn et al., 2008) is the capacity to infer relations of direction, morphology, topology, distance, etc, about entities existing in a given space (typically 2D or 3D). There are many combinations of space representation/abstraction and inferential mechanics for dealing with spatial reasoning, usually focusing on different aspects of spatial representations: topology, morphology, direction, etc. In this chapter, we will describe in some detail the main representatives among them, and also discuss ways of extending and integrating those techniques. 99      100 Real-World Reasoning 6.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"1 An Example Scenario for Spatial Representation and Reasoning In order to provide a common ground for explaining the different methodologies used for logically representing spatial knowledge, we will articulate an example scenario wherein various sorts of spatial reasoning can be performed. In order to accommodate several types of spatial reasoning tasks, the example presented here is composed of layered data produced by multiple sources. The use of such multilayered data portrays a situation possible and maybe even frequent in a real-world, largescale, heterogeneous store of spatiotemporal knowledge. The rst layer of data is of linguistic nature a stream of Twitter entries, from which semantic interpretation can extract many elements of spatial and temporal information, as we can see in Figure 6.1. One of the spatial inferences that can be made from the Twitter stream is that the three characters involved Pam, Sam and Tim appear to be in the same city or same metro area, which is called simply Metro City by Pam.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"That brings the second layer of data, shown in Figure 6.2, which presents a more evident spatial nature: it is a simplied map of Metro City, showing many city features neighborhoods, bridges, canals, rivers, islands including those mentioned in the Twitter stream. Many of those features are of an imprecise nature for instance often it is difcult to tell where a neighborhood ends and other one begins. The stream also mentions a weather phenomenon rain that may affect the plans of the characters involved in the Twitter stream. That brings the third layer of data: a (admittedly simplistic) weather pattern map for Metro City, shown in Figure 6.3, portraying the conditions of the city at the time of the rst message. Arrows in the picture show direction of the wind and therefore the overall tendency of the dislocation of the weather patterns.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"It is interesting to note that this third layer is intrinsically uncertain, as the arrows point to tendencies and weather forecast is essentially probabilistic. Now that the common scenario is described and illustrated, the following sections will show how several approaches for representation oriented to spatial reasoning see that scenario, and how they may allow the inference of new information based on that scenario. 6.2 Topological Representation The rst mode of spatial representation we will discuss here is topological representation. To make a rather crude and concise denition, topology deals with connection and    [""Representing and Reasoning On Spatial Knowledge 101     Sam The guests are already arriving! Hope that the house does not look as messy as I think it is... About 10 minutes ago from the web.     Pam I had this bright idea of choosing a hairdresser in Bridgetown, and Sam lives in the Island. Maybe Ill arrive unelegantly late at the party... :( About 30 minutes ago from the web.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"Tim I am leaving now. It is a bit early, but I heard that there is a trac jam at the south Bridge. Hope to be at Sams party in an hour or so, right on time. About 1 hour ago from the web. ' & $ % Sam Great, the bad news are that it is already raining down south. Maybe my plans of an outdoor party will be shatered. Still... Theres nothing but blue sky here in Center Island, perhaps the rain will not get this far... About 2 hours ago from the web. Fig. 6.1 Twitter stream containing spatial and temporal assertions inclusion of regions. For instance, telling if something is an island or a lake inside an island is a topological problem. A spatial representation and reasoning technique that deals with both connection and inclusion at the same time is the Region Connection Calculus (RCC) (Cohn et al., 1997).","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"RCC may be interpreted as a possible extension of Allens Interval Algebra into the spatial domain. The basic entities modeled by RCC are regions (as opposed to points), assumed to be embedded in the same space of some dimensionality. The basic relations in RCC are dened in terms of connection or inclusion between two regions of the analyzed space. The number of possible relations varies according to the granularity of the RCC variety being used. The coarser variety, RCC8, is thus called due to the set of just 8 primitive relations that dene it. Those are illustrated in Figure 6.4. A set of RCC relations stating spatial facts about a group of entities can be used to infer new spatial facts. For instance, if A is a non-tangential proper part of B and B is a non-tangential proper part of C, then A is a non-tangential proper part of C. If on the other ""]   102 Real-World Reasoning Fig. 6.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"2 High-level map of Metro City hand A is a tangential proper part of B and B is a tangential proper part of C, then A can be either a non-tangential or a tangential proper part of C. Or, showing the same inferences predicate-wise: nonTangentialProperPart(A, B)  nonTangentialProperPart(B, C)  nonTangentialProperPart(A, C) tangentialProperPart(A, B)  tangentialProperPart(B, C)  tangentialProperPart(A, C) XOR nonTangentialProperPart(A, C) (Please note that an exclusive or is used at the right side of the logical conditional in the last inference, for an entity cannot be at the same time a tangential and non-tangential proper part of another in the crisp formulation of RCC8.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"All such inferential combinations between three entities are summarized in the composition matrix for RCC8, shown in Table 6.1. For sake of space the usual abbreviations for      Representing and Reasoning On Spatial Knowledge 103 Fig. 6.3 Weather patterns over Metro City RCC8 operations are used. A set of operations in a given cell means that if two entities A and B are related by the operation corresponding to the row and B is related to a third entity C by the operation in the column, then A and C are mapped by the operations in the cell. For instance one can nd the composition corresponding to the last inference above by checking the cell at row TPP, column TPP. The symbol  in the table denotes the universal relation, meaning that any RCC8 relation between A and C is possible in the corresponding case. There are ner-grained versions of RCC like RCC15 and RCC23.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"Also, RCC can receive extensions (for instance a primitive detecting concavity) that also deal with morphology to some extent. Those are extensively discussed in (Cohn et al., 1997). With RCC-8, though, it is already possible to see how space can be represented in terms of region connection in our example scenario. (And the relations in RCC15 and RCC23, as well as their numerous compositions, are probably overcomplex for the didactic examples intended here.)      104 Real-World Reasoning Fig. 6.4 Connection-inclusion relations in RCC8 Looking at the map in Figure 6.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"2, one can make the following assertions using the RCC8 predicates: nonTangentialProperPart(Center Island, Kidney Bay) externallyConnected(Clarice Heights, Seaside District) tangentialProperPart(Bridge Town, Seaside District) overlaps(East Side, Clarice Heights) disconnected(West Side, Center Island) nonTangentialProperPart(L Park, Center Island) externallyConnected(East Side, Snail River) externallyConnected(West Side, Snail River) overlaps(Snail River, Kidney Bay) According to the rules of Region Connection Calculus, it is already possible to make some topological inferences about Metro City from the assertions above.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"For instance: nonTangentialProperPart(Center Island, Kidney Bay), nonTangentialProperPart(L Park, Center Island)  nonTangentialProperPart(L Park, Kidney Bay) Inferences like the one above can be seen as the construction of new links in a RCC network. As often happens in reasoning, problems represented through RCC can also be seen as a graph the aforementioned RCC network -, and the reasoning problem is accordingly reduced to a graph problem with a plethora of methodologies and tools for dealing with it. In order to better illustrate that, Figure 6.5 shows the assertions in the list above in the form      Representing and Reasoning On Spatial Knowledge 105 Table 6.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
1 Composition matrix for RCC8 relations o DC EC PO TPP NTPP TPPi NTPPi EQ DC  DC EC PO TPP NTPP DC EC PO TPP NTPP DC EC PO TPP NTPP DC EC PO TPP NTPP DC DC DC EC DC EC PO TPPi NTPPi DC EC PO TPP TPPi EQ DC EC PO TPP NTPP EC PO TPP NTPP PO TPP NTPP DC EC DC EC PO DC EC PO TPPi NTPPi DC EC PO TPPi NTPPi * PO TPP NTPP PO TPP NTPP DC EC PO TPPi NTPPi DC EC PO TPPi NTPPi PO TPP DC DC EC DC EC PO TPP NTPP TPP NTPP NTPP DC EC PO TPP TPPi EQ DC EC PO TPPi NTPPi TPP NTPP DC DC DC EC PO TPP NTPP NTPP NTPP DC EC PO TPP NTPP * NTPP,"Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"TPPi DC EC PO TPPi NTPPi EC PO TPPi NTPPi PO TPPi NTPPi PO TPP TPPi EQ PO TPP NTPP TPPi NTPPi NTPPi TPPi NTPPi DC EC PO TPPi NTPPi PO TPPi NTPPi PO TPPi NTPPi PO TPPi NTPPi PO TPP NTPP TPPi NTPPi EQ NTPPi NTPPi NTPPi EQ DC EC PO TPP NTPP TPPi NTPPi EQ of a RCC network, with the inferred relation nonTangentialProperPart( L Park, Kidney Bay) represented as a dotted edge. (Due to space restrictions relation names are depicted in the gure as acronyms made from their initials, as traditionally done in RCC notation.) Even in the reasonably crisp map of Metro City, it is possible to see that some of the classic RCC8 relations shown in Figure 6.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"5 are in fact ambiguous due to the intrinsically imprecise nature of many geographic concepts. For instance, is Bridge Town a tangential proper part of the Seaside District or are they externally connected or perhaps overlapping?      106 Real-World Reasoning Fig. 6.5 RCC network representing some topological relations among Metro City regions Are the East and West Side really disconnected or should the Old Bridge be considered a connection between them? That imprecision and ambiguity of some geographical features in the RCC point of view can be better modeled by adding fuzziness to the representation (Schockaert et al., 2008). In fuzzy logic, two entities can be related by multiple predicates, even ones that may sound mutually exclusive in classic logic. Those contradictions and ambiguities are conciliated by the use of truth values indicating the degree to which the relation applies between the two entities.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"In the notation used here, we dene a new logic predicate truthValue(R(A,B),v), indicating that the (fuzzy, in this case) truth value of relation R(A,B) between entities A and B is v. With that in mind, many fuzzy relations can be outlined among the entities shown in Metro City map, as exemplied below: truthValue(overlaps(Bridge Town, Sea Side District),0.3) truthValue(externallyConnected(Bridge Town, Sea Side District),0.3) truthValue(tangentialProperPart(Bridge Town, Sea Side District),0.4) truthValue(disconnected(East Side, West Side),0.9) truthValue(externallyConnected(East Side, West Side),0.1) truthValue(disconnected(Bridge Town, Channel Beach),0.9) truthValue(externallyConnected(Bridge Town, Channel Beach),0.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"1) truthValue(overlaps(Channel Beach, West Side),0.3) truthValue(externallyConnected(Channel Beach, West Side),0.3) truthValue(tangentialProperPart(Channel Beach, West Side),0.4)      Representing and Reasoning On Spatial Knowledge 107 As in the case of classic RCC, those relations can also be translated to a graph form. Figure 6.6 shows the fuzzy RCC8 network corresponding to the assertions above. The main difference visible between that graph and the one in Figure 6.5 is the presence of labels in the edges each one is assigned to a tuple indicating the kind of relation and the truth value of it. (Due to space limitations, the unambiguous though highly verbose logic predication used in the list above is avoided.) Another difference is the presence of multiple edges (relations) between two nodes.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"As an example of a possible interesting subgraph that could be derived from that one, the most intense relations (those with higher truth values) between all connected pairs of nodes are shown as solid edges. Fig. 6.6 A fuzzy RCC network representing some topological relations among Metro City regions Finally, some types of spatial information are better modeled neither in crisp nor in fuzzy ways they are better modeled in terms of probability. Probabilistic spatial relations are conceptually different from the fuzzy ones in the sense that they do not say that two entities have a set of relations at the same time in different degrees. Instead, a set of probabilistic relations between two entities assumes that the actual relation between the two entities is unknown/uncertain, but assigns different probabilities to the existing possibilities. Weather forecast based on the map shown in Figure 6.3 is a perfect target for probabilistic modeling, as it represents intrinsically uncertain information.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"Here we will try to model through RCC an extrapolation of the weather situation on Center Island at the time of the last Twitter message, based on the patterns and tendencies shown in Figure 6.3. In order to accommodate the uncertainty inherent to such a weather forecast, we introduce a new predicate probability(R(A, B), p), indicating that a given spatial relation R between entities      108 Real-World Reasoning A and B has a probability p of being real.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"The list below suggests some probabilities for relations extrapolated by the forecast: probability(nonTangentialProperPart(Center Island, Sunny), 30%) probability(overlaps(Center Island, Sunny), 30%) probability(overlaps(Center Island, Rain), 30%) probability(overlaps(Center Island, Hail), 20%) probability(overlaps(Center Island, Fog), 10%) probability(nonTangentialProperPart(L Park, Center Island), 100%) probability(nonTangentialProperPart(L Park, Sunny), 40%) probability(nonTangentialProperPart(L Park, Rain), 30%) probability(nonTangentialProperPart(L Park, Hail), 20%) probability(nonTangentialProperPart(L Park, Fog), 10%) That list of assertions can also be represented as a RCC network, this time of probabilistic nature, as shown in Figure 6.7. Fig. 6.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"7 A probabilistic RCC network representing some relations among Metro City regions and weather pattern zones 6.3 Directional Reasoning Although qualitative topological reasoning such as that provided by RCC can encompass many spatial relations between entities of many kinds, as illustrated in the previous section, it is by no means without limitations. For instance, RCC is inadequate for representing directional assertions. Directional spatial relations assume that the space is divided into a set of directions or angular alignments that are applicable to all entities. Directional relations are often used      Representing and Reasoning On Spatial Knowledge 109 in everyday descriptions such as Canada is north of the United States or the hurricane is heading Northwest or in order to get to the Main Plaza, turn right, follow two blocks and then turn left.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"Qualitative directional reasoning is represented in a broader way by the STAR Calculus (Mitra, 2004), which by its turn can be considered a generalization of Franks Cardinal Calculus (Frank, 1991). Cardinal Calculus used a system of four or eight directions (as in the conventional cardinal system used in geography, although they did not need a direct correspondence to those), while in the STAR calculus that is generalized to n directions forming n sectors of equal angular amplitude across the 360 degrees of the circle. (The mention of the circle bespeaks the fact that these forms of directional reasoning are primarily concerned with 2D spatial reasoning. However, it is not difcult to see that similar concepts can be produced in higher dimensional spaces with some work.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"The two systems (hereinafter referenced collectively as directional calculus) have still an additional special Null direction meaning no direction at all, or in other words meaning that the two entities directionally compared are in the same place, that is, so close that any statement about directions is meaningless. The basic operations of directional calculus are inversion and composition. Inversion allows commonsensical inferences such as direction(A, B, North)  direction(B, A, South) or if B is North of A, then A is South of B, in natural language. Composition on the other hand combines known directional relations between entities in order to infer new ones, such as in direction(A, B, North), direction(B, C, East)  direction(A, C, Northeast) Indeed, similarly to the case of RCC, all possible compositions in directional calculus can also be mapped into a composition matrix. As an example, Table 6.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"2 shows a composition table for the classic subdivision of geographic 2D space into eight cardinal directions (plus the null direction), adapted from (Frank, 1991) (where other variations are also discussed). Indeed, the same case convention of that publication is used: upper case letters for exact directions and lower case for Euclidean approximates. (The issue of exact and approximate directions is further discussed below as a prologue to fuzzy and probabilistic formulations of directional calculus.) Using the directional predicates dened above, we can again describe the example scenario in terms of a symbolic spatial reasoning representation. Here is an example list of predicates taking into account directly information shown in Figure 6.1 and 6.2:      110 Real-World Reasoning Table 6.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"2 Composition matrix for the operations corresponding to the eight cardinal directions N NE E SE S SW W NW Null N N n ne Null Null Null nw n N NE n NE ne e Null Null Null N NE E ne ne E e se Null Null Null E SE Null e e SE se s Null Null SE S Null Null se se S s sw o S SW Null Null Null s s SW sw w SW W nw Null Null Null sw sw W w W NW n n Null Null Null w w NW NW Null N NE E SE S SW W NW Null direction(Sam, Center Island, Null) direction(Pam, Bridgetown, Null) direction(Center Island, Bridgetown, South) direction(Sam, Pam, South) In the case of Tims Twitter entry, a human looking at Figure 6.2 can easily suspect that he is somewhere in the West Side, since he apparently has to cross the South Bridge to reach Sam, who lives in Center Island.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"An integrated spatial reasoning system taking into account both topological as well as directional reasoning could mount a graph of the connection relations in Metro City (like many of those exemplied here) and also reach the conclusion that Tim is likely on the West Side. (The system would of course have to reason on the assumption that people would choose the shortest path from one place to another, or make some related heuristic assumption, in order to rule out awkward possibilities like Tim being in the West Side, crossing Old Bridge, going South and then entering Center Island through South Bridge.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"Therefore, we can also add the relations: direction(Tim, West Side, Null) direction(West Side, Center Island, East) direction(Tim, Sam, East) Using composition on the assertions above, we can make the inference direction(Tim, Sam, East), direction(Sam, Pam, South)  direction(Tim, Pam, Southeast) Again, all those facts can be represented (and reasoned upon) in the form of a graph, as shown in Figure 6.8. Here again the question of imprecision and uncertainty comes into play. In the case of topological reasoning some relations are naturally crisp (for instance, an island is surely a      Representing and Reasoning On Spatial Knowledge 111 Fig. 6.8 A directional network showing spatial relations between the entities shown in Figure 6.1 and 6.2 non-tangential proper part of a body of water, by denition).","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"In the case of directions, on the other hand, arguably there are no crisp relations in almost all practical cases. For instance in the crisp example, by the assertions above it is declared that Center Island is North of Bridgetown, although perhaps one could also say that it is northeast of Bridgetown. The very denition of directions in directional calculus states them in terms of broad sectors of the circle instead of sharp lines, hinting already at a non-crisp nature for directions. Therefore, a fuzzy modeling for directional calculus comes naturally; it is basically a way of telling that entity A is more in this or that direction than others, relatively to entity B. Following the example case, the same geographical entities in the example above can have directional relations re-stated in fuzzy form as suggested below: truthValue(direction(Center Island, Bridgetown, Southwest), 0.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"5)      112 Real-World Reasoning truthValue(direction(Center Island, Bridgetown, South), 0.5) truthValue(direction(Center Island, West Side, West), 0.8) truthValue(direction(Center Island, West Side, Southwest), 0.2) The directional relations above, being between extensive regions, naturally look intrinsically fuzzy. In the case of the characters in the Twitter stream, though, we are talking about directional relations involving point-like entities. In that case, a probabilistic modeling seems to capture better the conceptual setting at play. For instance we can infer that Tim most likely is somewhere in the West Side, but since the West Side is rather extensive relative to Sams location in Center Island Tim may be West, Southwest or Northwest of Sam.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"However, we cannot say that Tim is at the same time West and Southwest of Sam (as in a fuzzy statement), but we can say that Tim is West of Sam or Southwest of Sam with different probabilities for each alternative. The list below suggests a probabilistic reformulation of the directional relations between the Twitter characters. probability(direction(Sam, Tim, West), 70%) probability(direction(Sam, Tim, Southwest), 20%) probability(direction(Sam, Tim, Northwest), 10%) probability(direction(Sam, Pam, South), 60%) probability(direction(Sam, Pam, Southwest), 40%) probability(direction(Pam, Tim, Northwest), 40%) probability(direction(Pam, Tim, North), 60%) As seen in previous approaches, fuzzy and probabilistic relations can also be represented as networks and thus be treated by many techniques for solving graph problems (analogously to the use of graph theory in analyzing biological networks, see e.g.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"(Przulj, 2005), (Grindrod & Kibble, 2004)). The graph in Figure 6.9 shows at the same time part of the crisp, fuzzy and probabilistic directional relations mentioned above. (This breakdown from the traditional approach of showing just homogeneous graphs used so far is in a way a preview of the following section, where we discuss a modeling tool that might allow the fusion of the approaches discussed so far.) 6.4 Occupancy Grids: Putting It All Together The previous sections show that available representation models for spatial reasoning are somewhat compartmentalized. Region Connection Calculus is appropriate for reason     Representing and Reasoning On Spatial Knowledge 113 Fig. 6.9 A mixed network showing probabilistic, fuzzy and crisp directional relations between the example entities ing in topological (and even morphological) terms on entities with non-negligible extension.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"STAR calculus is interesting for dealing with directional relations between entities treated as dimensionless points. Both approaches are qualitative and therefore they do not deal explicitly with distances and measurements (although it seems unproblematic to add these to the reasoning framework at least in the case of the directional models). A natural question is whether there is a third approach able to integrate most or all of the strong points of the previous approaches, perhaps with a few additional advantages? A positive answer to this question is provided by a modeling construct originally developed not with reasoning explicitly in mind, but rather as a probabilistic spatial representation the distribution of entities in space. This is the formalism of so-called Occupancy Grids (Martin & Moravec, 1996). Occupancy grids (also called Evidence Grids in their more complex and generic forms) were developed for Mobile Robotics and to date that remain their main eld of practical application.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"The initial goal was simply to represent space and obstacles on it, in a probabilistic way that could accommodate noisy, incomplete or ambiguous sensor data gathered by a robot while navigating through an unknown terrain, thus providing an adequate data framework for typical tasks such as obstacle avoidance and path planning. However, it has been demonstrated that spatial data represented in the form of occupancy grids can be used to achieve topological, morphological and directional inferences about the work environment of a robot (Szabo, 2004). Moreover, the original purely probabilistic version can also incorporate fuzziness (Bloch & Safotti, 2003). Therefore, occupancy grids (and their variations) look like the most promising candidates for a common framework able to      114 Real-World Reasoning bridge and integrate all the aspects of spatial reasoning, even though (or perhaps because) they are a paradigm shift from the more symbolic, higher-level representations traditionally used in reasoning.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"The original model of occupancy grids divides space into a regular lattice of cells, in which each cell has an associated probability of being occupied or not. Usually the grid is orthogonal and the space is 2D, although versions using other grid topologies (e.g., hexagonal) and higher-dimensional spaces are easy to devise. From the point of view of a conventional wheeled robot, occupancy is the presence of nondescript impassable solid obstacles. In order to accommodate versatile spatial reasoning about multiple entities, some complexity has to be added to that basic model. For instance the occupancy of a grid cell should not be nondescript. Rather, it should contain the probabilities of presence or fuzzy presences of the entities involved in the reasoning problem. In order to better illustrate that, Figure 6.10 and 6.11 show fuzzy and probabilistic occupancy grids representing data related to the example scenario. Figure 6.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"10 shows a spatial discretization of Metro City land area in terms of an occupancy grid. Different regions of Metro City are shown in different shades. The representation is fuzzy in order to capture the imprecision inherent in dividing neighborhoods of a city. That fuzziness is coded in gray levels, and explains why for instance the East Side is light gray and Clarice Heights is darker, but the border zone between them is a gradation, representing the variation of pertinence to the two different neighborhoods as a given cell seems to be more part of Clarice Heights than the East Side, or the other way around. Shade mixtures observing the same convention can be observed along other neighborhood boundaries of the city. Figure 6.11 on the other hand, is a probabilistic representation of the location of Tim, Sam and Pam. In terms of occupancy grids, those point-like individuals actually become clouds of probability.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"That can be an uniform cloud, as in the case of Sam from the Twitter stream it is impossible to tell where exactly he is in Center Island, and so his probability cloud is homogeneous across the whole extension of the landmass. In the case of Tim, on the other hand, the grid cells of the West Side closer to the Old Bridge are less likely to contain him, since if Tim were in that location presumably he could opt to cross Old Bridge and then North Bridge instead of using South Bridge. (An option that would look particularly attractive if Sam lives on the North part of Center Island.) Finally Pams probability cloud coincides with the fuzzy cloud from Figure 6.10 representing Bridgetown.      Representing and Reasoning On Spatial Knowledge 115 Fig. 6.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"10 A fuzzy occupancy grid representing different regions of Metro City Both gures above point in an intuitive way to a set of methods  often borrowed from digital image processing and computer vision  that allow the translation of information in occupancy grids to more symbolic representation frameworks, such as RCC and directional calculus. For instance the shade gradations representing neighborhoods in Figure 6.10 can be segmented by a plethora of techniques in order to determine crisp neighborhoods, which would be suitable for classic RCC. And the probability clouds in Figure 6.11 could have their centroids determined and submitted to a crisp directional analysis or (perhaps more interestingly) directional probabilities could be computed by considering all the possible pairs of cells from two different clouds, supplying information in a very rened way for a probabilistic directional reasoning process. Also, throughout this analysis of spatial reasoning we have tried to show the equivalence between many spatial reasoning representations and graph representations.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"In the case of occupancy grids, that equivalence comes with particular ease, as grids in general are graphs, in the sense that they are sets of cells (which can be viewed as nodes) connected      116 Real-World Reasoning Fig. 6.11 a probabilistic occupancy grid showing the location clouds of Tim, Pam and Sam. by a regular neighborhood topology (which can be represented by edges). Figure 6.12 illustrates that grid-graph equivalence For the purposes of this text, more specically we are talking about grid cells that can be related to spatial entities being reasoned upon by sets of logic predicates, often of probabilistic or fuzzy nature. Here we dene the construct occupies(A, cell(i, j)), indicating that entity A occupies the cell assigned by coordinates i, j.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"(Many variations of an orthogonal coordinate system can be used, but throughout the examples in this section we will assume that i, j refer to row, column, with numbering starting at 1 for both dimensions.) The predicate occupies is different from the RCC predicates in the sense that it does not inform anything about the relative sizes and states of superposition of A and the cell. A may be a large entity and the cell contains just a tiny portion of it, or A may be a point-like entity that is somewhere in the space mapped by the cell. For instance, the following set of predicates could be related to the cell (20, 15), part of the blurred frontier between Bridgetown and the Seaside District in Figure 6.10: truthValue(occupies(Bridgetown, cell(20, 15)), 0.63)      Representing and Reasoning On Spatial Knowledge 117 Fig. 6.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"12 occupancy grids as graphs following a lattice topology. truthValue(occupies(Seaside District, cell(20, 15)), 0.37) probability(occupies(Pam, cell(20, 15)), 63%) Of course, an occupancy grid is in a sense a ne-grained and regular graph, while in the spatial reasoning representations shown here we have coarse-grained graphs (representing higher-level entities such as whole regions and people, instead of individual space cells). However, in principle, topological and directional information about higher-level entities can be extracted from occupancy grids. Indeed, the issue of low-level to high-level conversion raises the awareness that, in order to be used as a universal framework for spatial reasoning, the operation of occupancy grids has to involve translations from and to other forms of spatial representation. The diagram in Figure 6.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"13 outlines some possible input and output relations between several techniques (not restricted to spatial reasoning) that might play a role in an spatial reasoning engine centered on occupancy grids. As one can see, a spatial reasoning engine created along these lines could potentially pass all the data ux through occupancy grids, which could be the main spatial data representation approach. External information sources (which can be anything, from raw sensory data to natural language extraction of spatial information) could write to the occupancy grids through the Visual Creation module. Interestingly, as one may note from the above discussion of mathematical morphology, many techniques for extracting information from occupancy grids can be borrowed from digital image processing and computer vision (grouped under Visual Interpretation), while techniques for adding information to occupancy grids can be borrowed from data visualization and computer graphics (grouped under Visual Creation). Data extracted from Occupancy Grids by Visual Representation may      118 Real-World Reasoning Fig. 6.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"13 A diagram showing inter-relationships between different kinds of processing that may exist in a comprehensive spatial reasoning engine feed both Directional Reasoning and Topological/Morphological Reasoning processes, and those in turn may write their conclusions back to the occupancy grids through Visual Creation as well as output those conclusions to External Information Consumers. 6.5 Handling Change The examples weve given using the grid-centered system proposed above, as well as higher-level graph representations, so far have involved static situations. (With the possible exception of the example involving weather, where uncertainty is implicit in the dynamics of the phenomena involved; but we did not focus on that aspect.) However, in the greater context of this work, the spatial reasoning approach described in the previous section would most likely work in a dynamic environment, where data from multiple sources and represented in many ways would constantly update the world picture maintained by the system. Those updates could be from higher-level entities and cascade down to the low level of occupancy grids.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"Conversely, change occurring on the lower levels could also cascade up and rearrange the relations between higher-level entities. This last aspect is specially interesting for detecting change between entities (usually high-level ones) important for the problem being handled by the system. Such a bottom-up cascading event is illustrated by Figure 6.14 and 6.15. Indeed Figure 6.14 shows an updated weather map, representing the weather patterns over Metro city at the instant of the last message from Sam. (And here an occupancy grid representation is used instead of the weather blobs of Figure 6.3. Different shades are used to assign the prevalent weather pattern in the grid cell, and the lighter the shade the more intense the pat     Representing and Reasoning On Spatial Knowledge 119 tern represented.) We can see from that updated map that it is actually raining heavily over the whole of Center Island. Therefore, the probabilistic RCC network shown in Figure 6.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"7 must be updated with the new data. The result of such update is shown in Figure 6.15. Fig. 6.14 an updated weather map of Metro City. Instance (c) also adds one of the Twitter characters  Sam  to the picture, putting him in Center Island and therefore also under rain. The example network is kept small for didactic purposes, but considering the probabilities of Pam and Tim also being in the island, they could also be similarly related to present patterns. That kind of inference could be further used by a broader reasoning system (of which the proposed spatial reasoning engine would be just a component) to infer that perhaps the Twitter characters will make subsequent observations about rain at Sams place in their tweets. 6.6 Spatial Logic Temporal logic is a larger eld than spatial logic, but the latter also has a long history and a ourishing body of contemporary work.      120 Real-World Reasoning Fig. 6.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"15 An updated RCC network relating weather patterns to regions and individuals. As an early example, consider Tarskis 1929 work in his Geometry of Solids (Tarski, 1956)  this is a second-order logic system, whose variables range not over points, but instead over the regular closed subsets of 3D Euclidean space (those subsets of R3 equal to the closure of their interior). Tarskis language features two non-logical predicates, corresponding to the binary relation of parthood, and the unary property of being spherical. Much more recently, Aiello, Pratt-Hartmann and Benthem have reviewed the eld of spatial logic in a masterful way in their lengthy book (Aiello et al., 2007), which draws together research from a variety of different areas under a common spatial logic framework.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"They conceive spatial logic broadly as the study of the relationship between geometrical structures and the spatial languages which describe them, and more precisely as follows: By a spatial logic, we understand any formal language interpreted over a class of structures featuring geometrical entities and relations, broadly construed. The formal language in question may employ any logical syntax: that of rstorder logic, or some fragment of rst-order logic, or perhaps higher-order logic. The structures over which it is interpreted may inhabit any class of geometrical spaces: topological spaces, afne spaces, metric spaces, or perhaps a single space such as the projective plane or Euclidean 3-space. And the non-logical primitives of the language may be interpreted as any geometrical properties or relations dened over the relevant domains: topological connectedness of regions, parallelism of lines, or perhaps equidistance of two points from a third.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"What all these logics have in common is that the operative notion of validity depends on the underlying geometry of the structures over which their distinctively spatial primitives are interpreted. Spatial logic, then, is simply the study of the family of spatial logics, so conceived. As Aiello et al note, contemporary work in spatial logic emanates mainly from three application domains, the rst two of which are highly relevant to the present work:      Representing and Reasoning On Spatial Knowledge 121  AI, where attempts have recently been made to develop logics of qualitative spatial reasoning.  The theory of spatial databases, related e.g. to geographical information systems  Image processing, where it is convenient to describe objects as sets of vectors that can be added (taking all linear sums) or subtracted (taking all linear differences), and one may proceed to dene logics involving these operations 6.6.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"1 Extending RCC into a Topological Logic The spatial logic most relevant to our own work with PLN is the extension of RCC into a topological logic, an excellent example of which is presented in (Kontchakov, 2010). To see how the RCC8 relations give rise to a spatial logic, let r1, r2 and r3 be regions in R2 that are homeomorphic to the disc, and suppose that r1, r2 stand in the relation TPP, while r1, r3 stand in the relation NTPP. Its easy to see (from the visual natural of these relationships) that r2, r3 must stand in one of the three relations PO, TPP or NTPP.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"In formal terms, what this means is that the RCC-8-formula TPP(r1, r2)  NTPP(r1, r3)  PO(r2, r3)  TPP(r2, r3)  NTPP(r2, r3), is valid over the spatial domain of disc-homeomorphs in the plane: all assignments of such regions to the variables r1, r2 and r3 make it true. Similar experimentation shows that, by contrast, the formula TPP(r1, r2)  NTPP(r1, r3)  EC(r2, r3), is unsatisable: no assignments of disc-homeomorphs to r1, r2 and r3 make this formula true. A similar analysis may be done in Euclidean 3D space, and more generally. Suppose one has a formal language L, whose variables refer to spatial regions that are subsets of some geometric space.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"Given a subset K of regions, the notion of the satisfaction of an L-formula by a tuple of regions, and thus the notions of satisability and validity of an L-formula with respect to K, can be understood in the usual way. According to the approach of (Kontchakov, 2010), the pair (L,K) may then be called a spatial logic. If all the primitives of L are topological in characteras in the case of RCC8 then we have a topological logic. Otherwise one might have, say, a metric logic. For languages featuring negation, the notions of satisability and validity are dual in the usual sense.      122 Real-World Reasoning Considering RCC8 as the foundation for a logic immediately makes the limitations of the RCC8 relationship-set apparent. For instance, constraints featuring RCC-8 predicates give us no means to combine regions into new ones.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"One way to remedy this deciency is to turn the set of RCC regions into a Boolean algebra with binary operations + and  and a unary operation . Intuitively, we are to think of r1 + r2 as the agglomeration of r1 and r2, r1  r2 as the common part of r1 and r2, and r as the complement of r. Adding these operators to RCC8 yields what is known as BRCC8 (Boolean RCC8). For instance, in BRCC8, the formula EC(r1 + r2, r3)  EC(r1, r3)  EC(r2, r3), is valid, whereas the formula EC(r1, r2)  EC(r1, r2), is unsatisable.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"Taking things one step further, [Kontchakov, 2010a] explores the augmentation of BRCC8 with additional relationships representing connectedness: the unary predicates c and c  k (for k  1). Here one may interpret c(r) as region r is connected and c  k(r) as region r has at most k connected components. These directions of research are highly relevant to the PLN approach that will be reviewed in Part III of this book, as PLN combines RCC relationships with Boolean and other operators in a fairly free way. 6.6.2 Combining Spatial and Temporal Logic PLN also combines spatial and temporal logic freely, as we shall see in many of the examples presented in Part III. Further, it does so in a manner embracing rich time representations (such as intervals) and uncertainty. However, the full formalization of the combination of spatial and temporal logic is still a subject of current research.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"One classic paper in this area is titled Spatial Logic + Temporal Logic = ? (Kontchakov, 2007), and discusses several ways of combining the two, focusing on combinations of Branching Temporal Logic with spatial logics based on the RCC. This is a big step in the right direction, but doesnt handle interval representations of time or uncertainty.   ","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 6
"  Chapter 7 Representing and Reasoning on Contextual Knowledge One of the critical issues confronted in doing logical inference on large spatiotemporal knowledge bases is the fact that most real-world logical relationships are contextual in nature. Contextuality must be explicitly taken into account to make real-world inference tractable. This is one of the issues with which formal-logic-based AI has traditionally had the most difculty. In this chapter we briey explore the notion of contextual inference and the key approaches that have been taken. Generically speaking, cognitive processes are usually contextual in the sense that they depend on the environment, or context, inside which they are carried out. The notion of context, in different varieties, plays a crucial role in different disciplines including natural language semantics, linguistics, cognitive psychology, and articial intelligence. Contextual information covers both knowledge representation and reasoning and their interaction.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"A completely general representation of a particular piece of commonsense knowledge is impossible in practice, because common sense by its nature is not general but has to do with the properties of particular classes of situations. In this sense the only practical way to consider knowledge is as contextual. John McCarthy, one of the founders of the AI eld, discussed this topic as follows (referring to axioms pertaining to real-world phenomena, rather than pure mathematics) (McCarthy, 1987): Whenever we write an axiom, a critic can say that the axiom is true only in a certain context. With a little ingenuity the critic can usually devise a more general context in which the precise form of the axiom doesnt hold. Looking at human reasoning as reected in language emphasizes this point. Consider axiomatizing on so as to draw appropriate consequences from the information expressed in the sentence, The book is on the table.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"The critic may propose to haggle about the precise meaning of on, inventing difculties about what can be between the book and the table, or about how much gravity there has to be in a spacecraft in order to use the word on and whether centrifugal force counts. Thus we encounter Socratic puzzles over what the concept means in complete generality and encounter examples that never arise in life. There simply isnt a most general context. Conversely, if we axiomatize at a fairly high level of generality, the axioms are often longer than is convenient in special situations. Thus humans 123      124 Real-World Reasoning nd it useful to say, The book is on the table, omitting reference to time and precise identication of what book and what table. [...] A possible way out involves formalizing the notion of context [...]. Another way to phrase this is to say that, generally, reasoning is local to a subset of all the known facts.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"One never consider all one knows, but rather a very small subset of it. This small subset, the subset that is used for reasoning about a given goal, determines the context of reasoning. A context encodes an individuals subjective view of some portion or aspect of the world. The individuals complete description of the world is given by the set of all the contexts. There can be different contexts for the same phenomenon and they can describe it at different levels of approximation. The same statement may have different truth values in different contexts. For instance, Alison is my boss can be true from Johns point of view, but not from Mikes point of view. As the above quote from McCarthy suggests, the topic of representing contextual information and contextual reasoning has its roots in the early years of articial intelligence. Also, McCarthys was probably the rst proposal for rigorously modeling humans common sense, the key feature for articial intelligence.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"Another important early use of the concept of context in articial intelligence was (Weyhrauch, 1980), who proposed a mechanized formal contextual reasoning system for Advice taker, a hypothetical computer program, proposed by John McCarthy (McCarthy, 1958) in the 1950s, that was the rst proposal to use formal logic to represent information in a computer. In the late 80s, interest in related issues grew, and context became a widely discussed issue, with a number of different formalizations and applications in knowledge representation and reasoning. Since the 90s the concept of context-awareness has increasingly gained importance in the area of computing and distributed systems, due to its promise as a solution for describing mobile computing in ever-changing environments. Most approaches in the literature have focused on addressing the modeling of context with respect to one application or an application class. While some models take the users current situation (e.g.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"in a meeting) into account, others model the physical environment (e.g., in an ofce); and some work in this area has taken steps towards a common understanding of context, with respect to location, identity, and time. The objective of this work overall is to develop uniform context models, representation and query languages, and accompanying reasoning algorithms. In the remainder of this chapter, we give a brief account of some of the major context representation approaches discussed in the literature today, with an emphasis on logicbased approaches. More details and a survey of models of context can be found, for      Representing and Reasoning on Contextual Knowledge 125 instance, in (Akman & Surav, 1996) and in (Strang & Linnhoff-Popien, 2004), while a survey of applications of context can be found in (Brezillon, 1999).","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"A broad review of context modeling and reasoning can be found in (Bettini, 2010). 7.1 Logic-Based Context Models Following (Bouquet et al., 2001), logic based models of context can be divided into two groups: divide-and-conquer and compose-and-conquer. 7.1.1 The Divide-and-conquer approach In this approach a context is a way of partitioning (and giving a more articulated internal structure to) a global theory of the world. This global theory has an internal structure, and this structure is articulated into a collection of contexts.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"Some of the main features of this approach are:  the facts that are true in a given context c can be isolated and treated as a distinct collection of facts;  there are hierarchical relations between contexts that allow reasoning to climb from a context to a more general context in which the dependence of a fact on a context is explicitly stated;  there are non-hierarchical relations between facts of different contexts (for example, one would like to be able to represent the relation between facts of the form in ofce(person;ofce number) and facts of the form in central ofce(person) in a context that specializes the former context to one specic ofce). The propositional logic of context (LoC) is one of the theories following this approach. It was rst described by McCarthy, and then formalized by Buvac and Mason (Buvac & Mason, 1993).","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"Some of the most important features of LoC are:  Any formula can only be asserted in some context (there are no context independent formulae). The fact that a formula p is asserted in a context c is written as c: p.  Context sequences are used to represent nested contexts. Context sequences allow distinctions, say, between the context of meeting in the context of one company team and the context of meeting in the context of a company.  There is no outermost context. This means that one can always transcend a context c and move to a more general context in which facts about c can be asserted.      126 Real-World Reasoning  Statements about a context c are made through the formula ist(c: p), meaning that the formula p is true in the context c.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"some relations between facts belonging to different contexts are stated through the lifting axioms with the general form ist(c: p)  ist(c0: p0) For example, if in the context of Alisons ofce, Alisons computer is close to her chair, then, in the context of their company, Alisons computer is close to her chair. There is a Hilbert style axiomatization of validity for the logic of context (Buvac & Mason, 1993). It ensures that all propositional tautologies are valid in every context c. Next, Dinsmores theory of partitioned representations PR (Dinsmore, 1991) is similar to LoC, but in PR there is a special space, BASE, that cannot be transcended and can be considered as a sort of outermost context. In PR, a statement is always asserted in a space, while a space represents some potential reality. Each space has exactly one primary context.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"A primary context is dened as a function that maps the truth of a statement in one space onto the satisfaction of a (more complex) statement in another space. For example, the sentence Bob is a project manager can be asserted in a space S1 and Alison believes that [S1] can be the primary context of S1. This allows mapping the truth of Bob is a project manager onto the truth of the sentence Alison believes that Bob is a project manager, which, in turn, is asserted in some other space. Of course, the semantics of Bob is a project manager would be very different in a space S2 whose primary context was, for instance, something like John had a dream that [S2]. Dinsmore also introduces a notion of secondary context, which allows for lateral mappings. Intuitively, a mapping is a consequence of the semantics of the primary contexts involved. In other words, a secondary context opens a channel of communication between two spaces. 7.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"1.2 Compose-and-conquer Approaches In these approaches, a context is a local theory, namely a (partial, approximate) representation of the world, in a network of relations with other local, domain specic theories. There is no global theory of the world, but only many local theories. Each local theory represents a viewpoint on the world and they are the building blocks of the individuals knowledge. The totality of his/her knowledge is given by composing such local theories through a collection of rules that connect them into a (still partial) representation of the world. A local theory is not a partition of some global theory, but it represents (partial) knowledge about some portion of the world, from a given perspective. For example, a local theory can be a representation of physical objects in an ofce.      Representing and Reasoning on Contextual Knowledge 127 In the compose-and-conquer approach, there are no a priori relations between contexts.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"This is a major difference from divide-and-conquer theories. Namely, in divide-andconquer systems, global knowledge implies how contexts are related to each other. On the other hand, in compose-and-conquer theories, there is no predened global knowledge and contexts are autonomous theories. This does not mean that there are no relations between contexts, but only that these relations are established on a peer-to-peer basis. One example of a compose-and-conquer approach to contextual knowledge representation and reasoning is Ghidini and Giunchiglias Local Models Semantics (Ghidini & Giunchiglia, 2001), together with its proof-theoretical counterpart (Giunchiglia & Serani, 1994). It is based on the following two very general principles:  principle of locality: reasoning always happens in a local theory (a context);  principle of compatibility: there may be compatibility constraints between the reasoning processes that happen in different contexts. 7.1.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"3 Compatibility constraints Compatibility constraints represent relations between different contexts. These may be formalized in many different ways. For instance, in the logic based approach to contextual knowledge representation described in (Ghidini & Giunchiglia, 2001), one starts with a family of languages L1;..., Ln;..., where Li is the representation language of a context ci. Each language Li has its set of models Mi. Every subset MTi of Mi satises a set of formulae, each corresponding to a different choice of the theory Ti associated with ci. Once the theory Ti associated with ci is xed, a model belonging to MTi is called a local model of ci. And then, relations between two contexts are represented by compatibility constraints, which state that the truth of a formula F in c1 is related to the truth of the formula F in c2.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"This is achieved by imposing that: sets of local models c1 and c2 of the two contexts c1 and c2 are such that, if the set of local models of c1 satises F, then the set of local models of c2 satises F. Pairs <c1; c2> satisfying the above relation are said to belong to a compatibility relation and dene a model for the pair of contexts c1 and c2. Using the notion of compatibility, a wide range of relations between contexts can be formalized. For example, if a context c1 represents Bobs beliefs at day d, and that it contains the statement I have a board meeting and if c2 represents Bobs beliefs at day d+1, then there can be a relationship between the two contexts such that the sentence Yesterday I had a board meeting is true in c2.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"The      128 Real-World Reasoning above relations are based on considering models, while there is also a deductive counterpart to imposing compatibility constraints. 7.2 Other Approaches to Contextual Knowledge Representation Contextual knowledge representation is used in many different elds, including modeling psychological and cognitive processes. The most important real-world application of contextual knowledge representation at present is in distributed and mobile computing, an area that has to deal with computing devices working in changing contexts. More pertinently to the present book, context based frameworks have been also used in knowledge and data integration  for the integration of information (or knowledge) coming from different sources (see, for instance, (Farquhar et al., 1995)). Different information sources integrated in a unique system can be thought of as partial views (or contexts) on a world. Available information sources are often distributed, redundant, partial, and autonomous.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"Consequently, information sources may adopt different conceptual schemata (including domains, relations, naming conventions, etc). The relations between the different domains and between the interpretation domains of the different databases can be established. For instance the different (but related) meanings of the predicate costs(x, y) in database 1 and database m can be represented by using the following formula: 1 : costs(x, y)  m : y (costs(x, y)  y = 1.07*y) The meaning of the above relations is that if the models of database 1 satisfy the formula costs(x, y), then any models of the database m must satisfy the formula y(costs(x, y)y = 1.07y) which means that item x has price y which is obtained by adding taxes to the price y. One application of these ideas is to traditional relational databases.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"For instance, in one research project, a context-based knowledge was applied to the problem of specifying redundancy among different databases while maintaining a high degree of autonomy (Mylopoulos & Motschnig-Pitrik, 1995). In this work, there are mechanisms for change propagation: i.e., mechanisms that establish whether the effects of a change operation performed in one context are visible in other contexts. In another sort of application, a rich contextual knowledge representation is required for the maintainance of a massive knowledge base such as CYC (Lenat & Sierra, 1999).","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"     Representing and Reasoning on Contextual Knowledge 129 The (controversial) philosophy underlying the CYC AI project is that huge, hand-created knowledge bases covering a wide area of (human) knowledge are required in order to create generally intelligent programs (the authors give an example of a logic-based expert system with the rule that amphibians lay eggs in water; however, the system cannot answer the question whether amphibians lay eggs at all, because its rule base is not sufciently rich to encompass such commonsense facts). However, maintaining and using a huge knowledge base, brings different challenges, one of which is the use of contexts, as the Cyc team has observed: ... as the CYC common sense knowledge base grew ever larger, it became increasingly difcult to shoehorn every fact and rule into the same at world.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"Finally, in 1989, as CYC exceeded 100, 000 rules in size, we found it necessary to introduce an explicit context mechanism. That is, we divided the KB up into a lattice of hundreds of contexts, placing each CYC assertion in whichever context(s) it belonged. Contexts in CYC have a ne internal structure with a dozen mostly-independent dimensions along which contexts vary. Each region of this 12-dimensional space implicitly denes a context. The capability of importing an assertion from one context into another is provided by lifting assertions. They have a general form: ist(c: p)  ist(c0 : p0) For instance, the fact that gold is more expensive than silver in the stock market, can be exported to the context of the black market by the assertion: ist(StockMarket: MoreExpensive(gold, silver))  ist(BlackMarket: MoreExpensive(gold, silver)) 7.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"3 Contextual Knowledge in Probabilistic Logic Networks PLN has a special link to handle contextual knowledge called ContextLink (or simply Context). For instance one can express in the context of programming a method that is a class function by Context Programming Similarity ClassFunction Method      130 Real-World Reasoning The semantics of ContextLink is formally dened by the following equivalence: Context C R A B is equal to R (A And C) (B And C) So reasoning with context simply amounts to reasoning with non-contextual standard operators, in certain stereotypical patterns. However since contexts are often used it may be valuable to introduce some inference macro rules handling context, like the following, used in an inference example given later on: Context C A Implication A B  Context C B Informally this rule says that if A is true in the context C and A implies B then B is true in the context C.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"Also, one may note that the denition involving the Context link assumes that the context applies on a relationship (noted R in the denition given above), nevertheless a truth value over a concept A can be seen as the measure of how much the universe inherits A, that is: A <w> is equal to Inheritance <w> U A where U symbolizes the universe. Given this, we can dene contextual versions of concepts as well as relationships.      Representing and Reasoning on Contextual Knowledge 131 7.4 User Models as Contexts One area where contextual logic has not frequently been applied, but perhaps should be, is the modeling of the preferences, biases and other properties of individual users and user groups of information systems.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"(Bry and Jacquenet (Bry & Jacquenet, 2005) have laid out the logic for application of rst-order logic to user modeling in the semantic web in some detail, but they do not consider contextual logic in specic.) 7.4.1 User Modeling in Information Retrieval Systems A great deal of work has been done to date on user modeling in information retrieval (IR) systems. The relevance of the output of an IR system is not absolute, but relative to specic users; and because of this, the lack of user modeling is one of the main sources of weakness of most contemporary such systems. For example, a tourist and a programmer may use the same word java to search for different information, but many retrieval systems would return the same results.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"It was observed, already in late 70s and early 80s, that information retrieval systems can and should be personalized for users by means of proles (Rich, 1979; Myaeng et al. , 1986). Since then, a lot of efforts have been invested in the area of individual user modelling and individual user proles. The main objective of user modeling is to extract and store information about a user, and to adapt the retrieval tool to the users needs and interests in order to improve the relevance of the results. Such, personalized search is still considered as one of the major challenges in modern information retrieval. For estimating the user-specic relevance of documents different computing methodologies are used, ranging from genetic algorithms, fuzzy logics, modal logic, to classical rst-order logic approaches. User proles can model different users preferences.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"For example, for some users it is preferable to have the list of all results to the query quickly, even with a number of bad results. On the other hand, some users prefer to have a short list of results, even if producing it takes more time. Some users prefer results only in some specic le formats, etc. These criteria and also criteria that dene quality or relevance for a specic user dene his/her prole. Generally, a (user) prole consists of a set of preferences with regard to behavior of a search engine as well constraints on the results it presents to the user (Van Gils & Schabell, 2003). For example, the following criteria may dene a particular user-prole: I prefer a maximum of 25 results per page, and by selecting a relevant resource (clicking on the link) will open a new window. I prefer HTML and PDF formats and refuse the Microsoft DOC format.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"Furthermore, the size of the resource should not exceed 25Mb.      132 Real-World Reasoning Proles can be used for post-processing the results to the query (e.g., some resources can be converted into a prefered format), and most interestingly, can also be used for guiding search. An important distinction in a user modeling context is between explicit and implicit preferences. Explicit preferences consist of information given by a user explicitly. On the other hand, implicit preferences refer to any context information available to the information retrieval system during a users session. Relevance feedback (given by the user to the information retrieval system) is one way for providing more context explicitly and can be effective for improving retrieval accuracy. However, it is often unrealistic to motivate user to explicitly rate the results obtained.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"Therefore, implicit context information and implicit feedback is thus more interesting to exploit and it has been attracting more and more attention (see, for instance, (Kelly & Teevan, 2003) for a bibliography of implicit feedback). In addition, both users explicit and implicit preferences may change over time. There are different sources of implicit user preferences. For instance, the user often need to modify his query in a number of iterations until he is satised. In such scenario, the information to be used by the information retrieval system is not just the current query, but also the complete users search history, information about which documents the user has chosen to view, and even for how long the user has read specic documents, etc. For instance, a simple users prole can be based on keywords from his queries performed or documents that he read. Many approaches use users actions and navigation through data as implicit feedback.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"For instance, some web browsers record user actions and navigation, including dwelling times, mouse clicks, mouse movements, scrolling and elapsed times and user explicit ratings of web pages. Some experimental results show that implicit feedback, especially the dwelling time on a page, amount of scrolling on a page, the combination of time and scrolling, how a user exit a result or end a search session have all a strong correlation with explicit relevance ratings (Claypool et al., 2001; Fox et al., 2005). However, there are also some experimental results showing that there is no general direct relationship between display time and relevance (and that the display time depends on the specic tasks and specic users) (Kelly & Belkin, 2004). There is a range of techniques for using users query history in building his/her prole. Some web browsers use browsing history in past n days for personalized search.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"Some approaches store users interests and can distinguish between long term interests and ad      Representing and Reasoning on Contextual Knowledge 133 hoc queries. Thank to this, modication of queries that can not appropriately be supported by the user prole are not applied. User modeling is widely used not only in Internet browsing, but also in recommender systems providing advice to users about items they might wish to purchase or examine. Recommendations made by such systems can help users navigate through large information spaces of product descriptions, news articles, social information, or other items (Burke, 2000). A general survey covering many aspects of user modeling in information retrieval systems can be found in (Kobsa, 2007). Another survey of the eld, including extensive discussion of different forms of communication with the user (querying, navigation through structures, and visualization) can be found in (Nurnberger & Detyniecki, 2005).","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"A survey of the eld of adaptive user interfaces can be found in (Dieterich et al., 1993). 7.4.2 User modeling from the cognitive perspective Beyond the specic domain of information retrieval, there are several types of computer related models considered by cognitive psychology. Apart from the user models  the computers model of the user (as described above in the context of information retrieval systems), there are also  mental models  users model of the system;  conceptual models  models presented to the user by the system designer. User models can be split into two major categories: empirical quantitative models and analytical-cognitive models (Palermiti & Polity, 1995). Empirical quantitative models are based on users performances when using a given system. Empirically collected performance data are used for constructing abstract formalizations and for dening groups (e.g., skill groups such as experts, beginners).","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"These user models do not express beliefs, reasoning or cognitive processes, but rather external performances. Analytical cognitive models attempt to construct a more qualitative behavioral modeling. The models aim to detect the purposes, strategies, plans or beliefs of the user, so that the system may issue predictions and draw conclusions. These models are not static but dynamic, able to evolve according to different tasks or categories of users, and adaptive, by using the cognitive features detected in the user behavior. Their role is to help the system cooperate, to indicate hypotheses, and to single out areas of interest.      134 Real-World Reasoning User models can also be classied according to three main dimensions:  models of a single typical user versus collections of individual models;  explicit models dened by the designer versus models inferred by the computer on the basis of the users behavior;  long term user characteristics such as areas of interest and expertise versus short term characteristics such as the subject of the last sentence typed. 7.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"4.3 Logic-based user modeling Instead of using approximate models and reasoning, logic based user modeling aim at employing formal reasoning for deducing facts about the user. This family of user modeling approaches is relatively small, but has a number of important ideas and techniques. Some of the logic-based representations used in user modeling are (Kobsa, 1993):  PROLOG-based, which offers a comparatively rich representation language with built-in backward reasoning and the possibility of a smooth migration from knowledge representation to programming;  Predicate logic, which offers more expressiveness than PROLOG, as is needed in many application domains;  Languages with second-order predicates and modal logic, which allow representing assumptions about beliefs and goals of different agents in the same representation language;  Connectionist networks, which have been particularly employed for classication tasks. An overview of different reasoning techniques used in logic-based user modeling systems can be also found in (Kobsa, 1993).","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"Typically, most logic-based systems use simple forward or backward reasoning (using the knowledge from the knowledge base), or some their combinations. 7.4.4 Contextual Logic for User Modeling One potentially promising avenue for applying logical methods to user recommendation involves deploying contextual logic, in such a fashion that each user connotes a distinct context for inference. As this approach seems not to have been touched in the literature yet, we explore it here via a thought experiment style example.      Representing and Reasoning on Contextual Knowledge 135 Our examples concerns book recommendation systems which, when a user locates one book, recommend to him/her other books of potential interest. In such systems today, models of individual users are not necessarily built at all; instead most recommendations are generic  the same for all users locating a certain book. Typically, relationships between books, as a basis for recommendations, are inferred from purchases of individual users, i.e., from the database of all purchases.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"For a certain book A, it is calculated how many users that bought the book A also purchased a book B, a book C, etc. Then the books (a xed number of them  say 10) with the highest score are recommended to the user, as of potential interest. This simple and generic model, commonly used in practice, can be rened in many ways, for instance by applying a specic ordering among the 10 recommended books, derived with respect to the specic user. Let us consider one possible approach for this through a simple example. Suppose there are the following books available: (A) Katya Walter The Tao of Chaos (B) Katya Walter: Dream Mail: Secret Letters for Your Soul (C) Martin Schonberger: I Ching & the Genetic Code: The Hidden Key to Life Suppose there is also stored the relevance of these books to two areas: spiritual literature and popular science.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"These relevance data (given by weight factors from 0 to 1) may then be taken to form two contexts: Popular science context: 1. relevance(A, 0.5) 2. relevance(B, 0.2) 3. relevance(C, 0.7) Spiritual literature context: 1. relevance(A, 0.6) 2. relevance(B, 0.8) 3. relevance(C, 0.3) Let there also be models of two users, Alice and Bob, and their interests, again described by weighting factors. Assume these weighting factors are computed based on their past purchases.      136 Real-World Reasoning Alices model: 1. interested in(popular science, 0.2) 2. interested in(spiritual literature, 0.7) Bobs model: 1. interested in(popular science, 0.9) 2.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"interested in(spiritual literature, 0.2) Now, let us assume that Alice searches the book recommendation system and locates the book (A). The engine queries the database and nds that other users that purchased this book most often also purchased books (B) and (C). So, these two books should be recommended to Alice in some suitable ordering and the relevance of these books to Alice should be estimated.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"Information from the contexts of certain areas apply to all specic users; so, in our example, all facts from all area contexts are also used in specic users contexts by, socalled, bridge rules: Inference rule in User Ns model: Context for the area X: relevance(Y, w1) interested in(X, w2) relevance(Y, w3) relevance(Y , max(w3, w1*w2)) The meaning of this rule is as follows: if, in the context of the area X, the relevance of the book Y is w1, and if w2 is the weight of the user Ns interest in X, then the relevance of the book Y for the user N is w1*w2. Such relevance factors can be calculated for all available areas, and the nal relevance factor for the book Y is the maximum over all areas (the initial relevance factor is 0).","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"In our example, we should apply, within Alices model, the above rule twice for each of the books (B) and (C): Popular science context: relevance(B, 0.2) interested in(popular science, 0.2) relevance(B, 0) relevance(B, 0.04) Spiritual literature context: relevance(B, 0.8)      Representing and Reasoning on Contextual Knowledge 137 interested in(spiritual literature, 0.7) relevance(B, 0.04) relevance(B, 0.56) Popular science context: relevance(C, 0.7) interested in(popular science, 0.2) relevance(C, 0) relevance(C, 0.14) Spiritual literature context: relevance(C, 0.3) interested in(spiritual literature, 0.7) relevance(C, 0.14) relevance(C, 0.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"21) Hence, the estimated relevance of the book (B) for Alice is 0.56, and the relevance for the book (C) is 0.21 and the ordering in recommendation is (B), (C). We can also apply, within Bobs model, the same rules: Popular science context: relevance(B, 0.2) interested in(popular science, 0.9) relevance(B, 0) relevance(B, 0.18) Spiritual literature context: relevance(B, 0.8) interested in(spiritual literature, 0.2) relevance(B, 0.16) relevance(B, 0.18) Popular science context: relevance(C, 0.7) interested in(popular science, 0.9) relevance(C, 0) relevance(C, 0.63) Spiritual literature context: relevance(C, 0.3) interested in(spiritual literature, 0.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"2) relevance(C, 0.06) relevance(C, 0.63) Hence, the estimated relevance of the book (B) for Bob is 0.18, and the relevance for the book (C) is 0.63 and the ordering in recommendation is (C), (B).      138 Real-World Reasoning It is not hard to see how the same form of contextualization could play a role in other sorts of user modeling besides product purchase investigations. For instance, suppose Bill and Betty are two criminal investigators, investigating a crime involving an individual named Giulio (A). Suppose Giulio has connections to (B) Elias, a known drug dealer who is suspected to have sold drugs to Giulios uncle (C) Jonas, a banker convicted of fraud, who works at the bank where Giulio used to work Suppose that Bills expertise is in drug crimes, whereas Bettys expertise is in bank fraud.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"Then, in the context of Bills investigation, the association between A and B should be more prominent; whereas in the context of Bettys investigation, the association between A and C should be more prominent. The structure is exactly the same as in the book purchasing example. (And of course there are many subtler examples of how contextual reasoning could be used to aid in user modeling; we have just presented a simple sort of example to make the conceptual connection clear. 7.5 General Considerations Regarding Contextual Inference Now we turn to issues involving the combination of contextual representation with logical inference systems. The explicit incorporation of context into inference is a subtle matter; a number of approaches have been posited in the research literature, yet none have yet been battle-tested in real-world applications. Here we will review some general considerations regarding contextual inference, then go into more detail regarding PLN-based contextual inference, and give a fairly complex, realistic example of the latter.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"Generally speaking, any deductive system (say, Hilbert style rst-order logic, or temporal logic) can be used contextually, by restricting its rules to certain domains. But this is different than explicitly contextual reasoning, in which a notion of context is incorporated in both the knowledge representation and the inference rules. In practice, the way explicitly contextual reasoning systems tend to work is that inference rules are supplied for specic contexts, and then special rules are also applied for bridging pairs of contexts. For example, the relationship to be superior can be described by the following (implicitly) universally quantied axioms: team leader(x, z)  superior(x, z) superior(x, y)  superior(y, z)  superior(x, z)      Representing and Reasoning on Contextual Knowledge 139 The axioms can be associated by an additional parameter describing a context  for instance, a company or a sport club.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"They can then be exported, by lifting rules, to specic contexts: team leader(x, z, context)  superior(x, z, context) superior(x, y, context)  superior(y, z, context)  superior(x, z, context) or, to the following notational form: context: team leader(x, z)  superior(x, z) context: superior(x, y)  superior(y, z)  superior(x, z) The basic idea here, due originally to McCarthy, is to tackle the problem of generality by using more general axioms if the context is irrelevant, and less general axioms otherwise. In addition to inference rules local to specic contexts, contextual inference systems also utilize rules that link different contexts. For instance, in the inference system described in (Ghidini & Giunchiglia, 2001), there are bridge rules. Bridge rules are rules whose premises and conclusion belong to different contexts.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"For instance, the bridge rule corresponding to the compatibility constraint if the set of local models of c1 satises F, then the set of local models of c2 satises F would be the following: c1 : F c2 : F where c1:F is the premise of the rule and c2:F is the conclusion. Obviously, bridge rules are conceptually different from local rules, rules used in individual contexts. Bridge rules can have different forms and can involve more than two contexts. A deduction is a tree of local deductions, obtained by applying only local rules, concatenated with one or more applications of bridge rules. In a special case when there is no relation between the two contexts, there are no constraints on what is true in the two contexts. Using the machinery of compatibility, a wide range of relations between contexts can be formalized. For example, let c1 represent the prices on the American market at the present moment and assume Gold gets cheaper is true in c1.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"If c2 represents the prices on the Asian market at the present moment, then the sentence In America, gold gets cheaper must be true in c2. This conclusion is based on the inference rule of the form: c1 : F c2 : F is true in c1      140 Real-World Reasoning 7.5.1 Uncertain Contextual Inference Although there is very little literature in this area, there is no reason contextual reasoning cant be applied in the context of uncertain logics. For instance, to uncertainize the example given above, the fact Gold gets cheaper could have a degree of certainty of 0.9 in the context c1of the American market. Furthermore, the relevant bridge rules could also have degrees of certainty associated with them.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"So, for instance, there could be a bridge rule c1: Gold gets cheaper c2: Gold gets cheaper where c1 is the context of the American market, while c2 is the context of the Asian market. This rule could have a degree of certainty, say, 0.8, indicating that it is likely that if gold gets cheaper on the American market, that it also gets cheaper on the Asian market. So, nally, the degree of certainty that gold gets cheaper on the Asian market could be calculated as the product of the degree of certainty that gold gets cheaper on the American market (0.9) and the degree of certainty of the bridge rule (0.8), giving 0.72. On the other hand, there could be a bridge rule c1: local currency gets stronger c2: local currency gets stronger where c1 is the context of USA, while c2 is the context of Japan.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"This rule could have a degree of certainty, say, 0.2, indicating that it is unlikely that if US dollar gets stronger, then yen also gets stronger. If the degree of certainty that US dollar gets stronger is, say, 0.95, then the degree of certainty that yen gets stronger is 0.950.2=0.19. 7.6 A Detailed Example Requiring Contextual Inference In this section we present a simple, specic example of contextual inference, and explain how the ideas discussed above may be applied in this case. In working this example, we will use the approach close to (Ghidini & Giunchiglia, 2001) and rst order logic as the underlying logic, but, for the sake of simplicity, instead of rst-order notation, we will formulate the statements in natural language form. In a following section, we will run through this same example using PLN inference.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"The basic assumptions of the inference, expressed in common informal English, are as follows:      Representing and Reasoning on Contextual Knowledge 141 Alison is an accountant who is also a musician. Alison is emotional in the context of music, but not in the context of accounting. She frequently mentions Canadian place names in the context of music (maybe shes a Canadian music fan), but not in the context of accounting. Bob is in a similar situation, but he frequently mentions Canadian related stuff in both the music and accounting contexts. Clark is also in a similar situation, but he frequently mentions Canadian related stuff only in the accounting context, not the music context. Trivially, Canadian places are associated with Canadian people. People who have a lot to do with Canadian people, and a lot to do with money, have a chance of being involved in suspicious log trafcking activities. Trivially, accounting has to do with money.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"To formalize the above using contextual inference, we will consider the following contexts: music, accounting, to-be-involved, and Canada. We can encode all the above information as axioms in these contexts, while we also have to add some bridge rules (linking different contexts and corresponding to compatibility constraints). Each inference step is made in a specic context. If a person is an accountant, we can encode this information as being that the person is knowledgeable in the context of accounting. It is similar for the context of music. More explicitly, the contexts under consideration are as follows: Music context: (1) Alison is knowledgeable. (2) Alison is emotional. (3) Alison frequently mentions Canadian place names. (4) Bob is knowledgeable. (5) Bob is emotional. (6) Bob frequently mentions Canadian place names. (7) Clark is knowledgeable. (8) Clark is emotional. (9) Clark does not frequently mention Canadian place names.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"     142 Real-World Reasoning Accounting context: (10) Alison is knowledgeable. (11) Alison is not emotional. (12) Alison does not frequently mention Canadian place names. (13) Bob is knowledgeable. (14) Bob is not emotional. (15) Bob frequently mention Canadian place names. (16) Clark is knowledgeable. (17) Clark is not emotional. (18) Clark frequently mention Canadian place names. (19) Money is important. To-be-involved context: (20) If someone is knowledgeable and X is important, then he/she is highly involved with X. (21) If someone frequently mentions place names from X, he/she is highly involved with these places. (22) If someone is highly involved with places from X, he/she is highly involved with anything that is associated with these places.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"Canada context: (23) Canadian places are associated with Canadian people (24) If someone is highly involved with Canadian people and with money, then he/she has a chance of being involved in log trafcking. To keep things relatively comprehensible and informal, we wont explicitly specify the languages of the contexts, and well assume that all relevant information can be expressed in any of the contexts (for instance, we assume that in the music context it can be expressed that someone is involved with Canadian people). Also, we use the following bridge rules:      Representing and Reasoning on Contextual Knowledge 143 To be involved: F Music: F Canada: F Music: F To be involved: F Accounting: F Canada: F Accounting: F In the described system, for example, one can infer that Clark has a chance of being involved in log trafcking (in the context of accounting).","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"The inference in the accounting context goes as follows: Step Inferred information Justication (C1) If someone is knowledgeable and X is important, then he/she is highly involved with X. bridge from (20) (C2) If someone is knowledgeable and money is important, then he/she is highly involved with money. instance of (C1) (C3) Clark is highly involved with money. from (16), (19), and (C2) (C4) If someone frequently mentions place names from X, he/she is highly involved with these places. bridge from (21) (C5) If someone frequently mentions place names from Canada, he/she is highly involved with these places. instance of (C4) (C6) Clark is highly involved with places from Canada. from (18) and (C5) (C7) If someone is highly involved with places from X, he/she is highly involved with anything that is associated with these places.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"bridge from (22) (C8) If someone is highly involved with places from Canada, he/she is highly involved with anything that is associated with these places. instance of (C7) (C9) Canadian places are associated with Canadian people. bridge from (23) (C10) Clark is highly involved with Canadian people. from (C6), (C9), and (C8) (C11) If someone is highly involved with Canadian people and with money, then he/she has a chance of being involved in log trafcking. bridge from (24) (C12) Clark has a chance of being involved in log trafcking. from (C3), (C10), and (C11)      144 Real-World Reasoning Notice that one can prove that Alice is highly involved with Canadian people in the context of music, but cannot prove that Alice is highly involved with Canadian people in the context of accounting.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"Hence, it cannot be proved that Alison has a chance of being involved in log trafcking. If one use a bridge rule connecting the two contexts: music: F accounting: F for all sentences F that can be expressed in the two contexts, then one can infer that Alison is highly involved with Canadian people in the context of accounting, too (if all relevant properties can be expressed in that context). However, such a bridge rule would be unlikely used. Notice also that, in the same manner as for Clark, one can prove that Bob is highly involved with Canadian people in the context of accounting, and further that Bob has a chance of being involved in log trafcking. One may wonder if there can be made a distinction between Bob and Clark. Namely, Bob is highly involved with Canadian people in both contexts music and accounting, while Clark is highly involved with Canadian people only in the context of accounting. In addressing this there are several issues.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"First, one might eliminate the axiom (24), and instead add the following bridge rule: Accounting: X is involved with Canadian people and with with money Music: X is not involved with Canadian people Accounting: has a chance of being involved in log trafcking. Intuitively, this would eliminate the conclusion that Bob has a chance of being involved in log trafcking and keep the conclusion for Clark. However, it is not necessarily the case. Namely, in the context of music, one cannot prove that Clark is involved with Canadian people, but it still does not mean that one can prove that Clark is not involved with Canadian people. Different default logics address this issue and hence can be used for reasoning of the above sort. Using the variants of the systems described above one can infer crisp conclusions as to whether Alison, Bob, or Clark have a chance of being involved in log trafcking or other information.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"However, one cannot infer information on how likely it is that Alison/Bob/Clark is involved in log trafcking. For such information, one might use some fuzzy/probabilistic logic as the underlying logic and keep the rest of the inference system. In such a modied system, one could end up with a conclusion that Clark is the most likely to be involved in log trafcking, Bob is second most likely, and Alison is third most likely. Also, if one      Representing and Reasoning on Contextual Knowledge 145 would replace accounting with marketing in the above examples, then these degrees of suspicion should decrease, while if accounting is replaced with corruption, then the degrees of suspicion should increase. However, these specics will depend on the inference system in question.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
       ,"Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 7
"  Chapter 8 Causal Reasoning Many of the inferences one wants to draw about real-world, spatiotemporal, contextual knowledge involve cause and effect. But the relation between causation and logical inference is subtle and storied. As outlined above, deductive reasoning aims at deriving consequences (or effects, outcomes) from premises (or causes). Abductive reasoning aims at deriving possible causes from effects. Finally, inductive reasoning aims at deriving relationships between causes and effects, rules that lead from one to another. Causal reasoning is generally considered a form of inductive reasoning. More concretely, causal reasoning aims at an epistemological problem of establishing precise causal relationships between causes and effects, with focusing on detecting genuine, real causes for some effects, and genuine, real effects of some causes. Although they share a lot in their background and inferential process, causal inference and statistical inference are different.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 8
"Statistical inference is concerned with associational inference and used for nding associations between exposures (causes) and outcomes, rather then for inferring causation relationships from observations. Causal relationship implies correlation between two events, but the opposite does not hold. The rst attempts at dealing with causality date back to the old Greek philosophers, including Aristotle, but systematic approaches had to wait centuries to come [Danks2006]. In his Novum Organum (1620), Francis Bacon introduced the notions of:  The table of presence (tabula praesentiae)  The table of absence (tabula absentiae)  The table of degrees (tabula graduum) According to Bacon, the cause of a phenomenon is the set of properties that explains every case in each of the three tables. In the mid-nineteenth century, John Stuart Mill proposed an improved form of Bacons method.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 8
"But already by then, the limitations of such approaches 147      148 Real-World Reasoning were understood  since in the eighteenth century, David Hume noticed that causal inference, often intuitive and natural, cannot be formally justied using deduction. This fact is actually a general property of inductive reasoning. It is not possible to justify the pattern the future will continue the pattern of the past via deductive reasoning based on observations, without falling into circular reasoning. One can reason the sun will rise tomorrow morning because it rose for the last 100 mornings, but this relies on the assumption the future will continue the pattern of the past, and its circular to say the future will continue to obey the pattern the future will continue the pattern of the past because it obeyed it in the past. From the twentieth century till the present, philosophers, statisticians, and computer scientists have developed, often building on statistics, various approaches and methods for representing causal structures and solving problems of causal inference.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 8
"Some of them follow the view of the philosopher Karl Popper in which falsication of a hypothesis is more informative than corroboration of a hypothesis. There could be a number of cases that are consistent with a false hypothesis, but a single counterexample requires modifying the hypothesis. A hypothesis that has survived many attempts to refute it is more likely to be true than one that has been corroborated many times. Today as in the past, the problem of causal inference is a central challenge for most of the empirical sciences. Causal effect may be the effect of a given drug or therapy for a specic disease, the effect of education on employment and earnings, the effect of training courses on the labor market, etc. If a causal relationship is discovered and established, it may be possible to control future events to some extent by producing (or preventing) occurrence of certain outcomes.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 8
"There are many real-world applications of causal reasoning, including in economics, law, social sciences, and human-computer interactions, but most important are perhaps those in various branches of medicine and health research. In the following text, we will briey discuss some of the central problems in causal reasoning and some of the most signicant approaches for modeling it. For a more detailed survey, see, for instance, (Kluve, 2001). 8.1 Correlation does not imply causation The assumption that correlation and causation are equivalent often leads to signicant aws in reasoning. The phrase correlation does not imply causation stresses this fact, meaning that if there is a correlation between two events, it still does not necesserily mean      Causal Reasoning 149 that one causes the other (although it is possible that it is the case).","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 8
"A prototypical example of such awed reasoning, employed in medicine, is discussed in (Lawlor, 2004). A number of studies showed that women who were taking combined hormone replacement therapy also had a lower-than-average incidence of coronary heart disease. This correlation led to a widespread idea that hormone therapy was protective against coronary diseases. However, carefully designed experiments demonstrated that this was not true. The explanation was that women taking hormone therapy were mostly from higher socio-economic groups, with better nutritive regimes and, hence, with lower incidence of heart disease. Thus, lower incidence of heart disease was not a consequence of taking hormone therapy, but both were effects of a common cause. Coming to the conclusion that an event X is caused by an event Y if there is a correlation between the two is generally wrong, although this causation can be indeed present. Other potential explanations for correlation between X and Y are the following:  Y is caused by X.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 8
"It is both the case that X is caused by Y and Y is caused by X.  There is a common cause for both X and Y.  Correlation is present due to a pure chance, or due to reasons that are so complex and deep that they cannot be considered as causation between X and Y. 8.2 Other Challenges in Causal Reasoning Causal relationships can be very complex and difcult to deal with. For instance, there are situations when there are several hypotheses about a causal relationship, and it is difcult to select the one that is most likely true. Also, one of the problems is recognizing irrelevant events or causes with small impact on the observed outcome. Let us consider the example, given in (Modern Epidemiology, 2008), that illustrates the process of understanding causality with a description of a child learning that moving a light switch causes the light to turn on.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 8
"However, in wider contexts, the turning-on of the light was sometimes caused also by other subjects:  The mother who replaced the burned-out light bulb.  The electrician who replaced a defective circuit breaker.  The lineman who repaired the transformer that was disabled by lightning.  The social service agency that arranged to pay the electricity bill.      150 Real-World Reasoning Table 8.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 8
"1 Illustration of Simpsons Paradox A B %A smokers 5 5 50% non-smokers 2 3 40% a: Proportion of students smokers and non-smokers who received A A B %A smokers 5 4 56% non-smokers 1 0 100% b: Low-income students A B %A smokers 0 1 0% non-smokers 1 3 25% c: High-income students  The power company, the political authority awarding the franchise, the investment bankers who raised the nancing, the Federal Reserve that eased interest rates, the politician who cut taxes, and the health care providers who contributed to the childs safe birth and healthy development. And there are other deep issues with causal reasoning. Consider an example from (Pearl, 2000), illustrating the problem known as Simpsons Paradox.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 8
"Suppose that a teacher discover that a higher proportion of his students who smoke received a nal grade of A than students who do not smoke, as shown in Table 8.1a. Confused by the hint that smoking has a positive impact on grades, the teacher tries to nd a rational explanation and partition the same data differently, looking at students with low parental income (Table 8.1b) separately from those with high parental income (Table 8.1c). Then, surprisingly, he nds that the situation has been completely reversed: smoking has negative impact on grades in both groups.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 8
"According to Pearl, Simpsons Paradox comes from trying to understand causality solely through probability and statistics: It is an embarrassing yet inescapable fact that probability theory, the ofcial language of many empirical sciences, does not permit us to express sentences such as Mud does not cause rain; all we can say is that the two events are mutually correlated, or dependentmeaning that if we nd one, we can expect to encounter the other (Pearl, 2000). 8.3 Mills Methods In his 1843 book A System of Logic: Ratiocinative and Inductive, John Stuart Mill described ve methods for drawing conclusions about causal relationships. The account of      Causal Reasoning 151 the Mills methods given here and the running example are based on (Kemerling, 2006). The running example is based on the following scenario: in a college, one day an unusual number of students are suffering from severe indigestion.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 8
"The college nurse naturally suspects that this symptom results from something the students ate for lunch, and she wants to nd evidence that will support a conclusion that eating a certain food caused indigestion. The Mills methods are the following.  Method of Agreement: this method applies to cases in which the effect that occurred reveals only one prior circumstance that all of them shared. It is based on the expectation that similar effects are likely to arise from a similar cause. As an example, suppose that out of the four students with indigestion, one had pizza, coleslaw, orange juice, and a cookie; the second had a hot dog and french fries, coleslaw, and iced tea; the third ate pizza and coleslaw and drank iced tea; and the fourth ate only french fries, coleslaw, and chocolate cake. The nurse, by the method of agreement can conclude that eating coleslaw caused the indigestion.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 8
"Method of Difference: by this method, a comparison of a case in which the effect occurred and a case in which the effect did not occur, reveals that only one prior circumstance was present in the rst case but not it the second. In such situations, it is supposed that, other things being equal, different effects are likely to arise from different causes. As an example, suppose that the two students with indigestion ate together, but one became ill while the other did not. The rst had eaten a hot dog, french fries, coleslaw, chocolate cake, and iced tea, while the other had eaten a hot dog, french fries, chocolate cake, and iced tea. Again, the nurse concludes that the coleslaw is what made the rst student ill.  Joint Method of Agreement and Difference: This method is a combination of the rst two methods, and it assumes that genuine causes are necessary and sufcient conditions for their effects.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 8
"Consider the following example: eight students come to the nurse: four of them suffered from indigestion, and with each of these four there is another who did not. Each pair of students had exactly the same lunch, except that everyone in the rst group ate coleslaw and no one in the second group did. The nurse arrives at the same conclusion as above. Method of Concomitant Variation: this method applies when evidences appear to show that there is a correlation between the degree to which the cause occurred and the degree to which the effect occurred. This conforms to our exprectations that effects are typically proportional to their causes. This method does not only notice occurrence      152 Real-World Reasoning or non-occurrence of the causal terms, but also the extent to which each of them takes place.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 8
"As an example, suppose that there are ve students with indigestion: the rst ate no coleslaw and feels ne; the second had one bite of coleslaw and felt a little queasy; the third had half a dish of coleslaw and is fairly ill; the fourth ate a whole dish of coleslaw and is violently ill; and the fth ate two servings of coleslaw and had to be taken to the hospital. The conclusion is again that coleslaw caused the indigestion.  Method of Residues: many elements of a complex effect are shown to result, by reliable causal beliefs, from several elements of a complex cause; whatever remains of the effect must then have been produced by whatever remains of the cause. As an example, suppose that the nurse, during prior investigations of student illness, has already established that pizza tends to produce a rash and iced tea tends to cause headaches.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 8
"Today, a student arrives at the nurses ofce complaining of headache, indigestion, and a rash; this student reports having eaten pizza, coleslaw, and iced tea for lunch. Since she can recognize most of the students symptoms as the effects of known causes, the nurse concludes that the additional effect of indigestion must be caused by the additional circumstance of eating coleslaw. Notice that in all of the above methods, the issue of relevance is crucial. The nurse began with the assumption that what students had eaten for lunch was relevant to their digestive health in the afternoon. That is a reasonable assumption, but the real cause could have been something completely different, something about which the nurse never thought to ask. Therefore, application of Mills methods succeeds only if every relevant suspected cause is taken into account, but that is impossible to guarantee in advance.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 8
"Indeed, the most difcult cases are those in which the real cause was excluded from the analysis as being unobserved or considered irrelevant. Thus, Mills methods cant help to discover causes unless the list of all potential causes is already known. Problems with using Mills methods for proving that one event is the cause of another, are even bigger (Kemerling, 2006). Because of their limitations, Mills methods should rather be considered as a tool for conrming (and not for discovering) hypotheses. If there are several hypotheses about a causal relationship, then Mills methods can be helpful, since they will often enable eliminating most of the considered causes and the last remaining hypotheses will likely be valid.      Causal Reasoning 153 8.4 Hills Criteria Causal reasoning in different forms had been widely used in medicine in twentieth century. In 1965.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 8
"Austin Bradford Hill made a seminal summary of criteria to be used in causal inference in epidemiology (Hill, 1965). The basic underlying questions in Hills criteria for causal inference are: is the association real or artifactual? and is the association secondary to a real cause?. Hills criteria are widely recognized as a basis for inferring causality in epidemiology, but not only in epidemiology: 1. Strength of the association  the stronger an association, the less it could reect the inuence of some other factor(s). This criterion includes consideration of the statistical precision and methodological rigor of the existing studies with respect to bias. 2. Consistency  replication of analysis by different investigators, at different times, in different places, in different populations, with different methods leads to identical or similar ndings. Namely, identical or similar ndings are not likely to be all due to error or artifact.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 8
"In addition, there should be reasonable and convincing explanation for different ndings. 3. Specicity of the association  the more accurately dened the disease and exposure, the stronger the observed relationship should be. But the fact that one agent contributes to multiple diseases is not evidence against its role in other diseases. 4. Temporality  the ability to establish that the putative cause preceded in time the presumed effect. 5. Biological gradient (dose-response)  strength of disease changes with changes in exposure. Still, there could be a threshold effect. 6. Plausibility (credibility) general knowledge and beliefs should be able to explain the observed causal relationship. Still, the observed relationship could be beyond the current knowledge. 7. Coherence  causal interpretation should not conict with observations and with known facts about the natural history of the disease. 8. Experiment  not a guideline, but a method for testing a specic causal hypothesis.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 8
"If available, well designed and conducted experimental studies (e.g., with controlled conditions and changing the exposure) provide strong evidence for or against causation. 9. Analogy  use analogies or similarities between the observed causalities and other causalities.      154 Real-World Reasoning Hill himself did not intend his criteria to be used as a self-contained framework, but rather as guidelines: Here there are nine different viewpoints from all of which we should study association before we cry causation ... None of my nine viewpoints can bring indisputable evidence for or against the case-and-effect hypothesis and none can be required as a sine qua non. What they can do, with greater or lesser strength, is to help us make up our minds on the fundamental question  is there any other way of explaining the set of facts before us, is there any other answer equally, or more, likely than cause and effect. 8.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 8
"5 Graphical models Next, in graphical models of causality, causal relationships are represented by causal graphs (Greenland, 1999; Robins, 2001). A directed acyclical graph (DAG) is causal if every directed edge represents the presence of an effect of the parent (causal) variable on the child (affected) variable. In a causal graph, a directed path represents a causal pathway, and an X-to-Y directed edge represents a direct effect of X on Y. Absence of a directed path from X to Y in the graph corresponds to the causal null hypothesis that no change of the distribution of X can change the distribution of Y. Causal graphs provide simple visual and graph theory methods to check for confounding factors (common cause for two event analyzed for causal relationship) and other relevant properties. 8.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 8
"6 Potential-outcomes (counterfactual) models In counterfactual (or potential-outcomes) approaches to causal reasoning, statements about causality are considered in forms of counterfactual statements (Lewis, 2000). Reasoning focuses on what would have happened if, contrary to fact, the exposure had been something other than what it really was. For instance, the statement that a coleslaw ate by a student caused his indigestion is equivalent to the statement that had not the student eaten the colesaw, he would not have suffered from indigestion. This approach is justied by the fact that there are sistematic ways for dealing with counterfactual statements. For instance, Pearl gave an axiomatic system with clear semantics and effective algorithms for computing counterfactuals (Pearl, 2000).","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 8
"In his framework, for instance, one can calculate a probability that the student would not have indigestion had he not eaten colesaw if it is the case that the student has eaten colesaw and is suffering from indigestion. The potential outcomes framework has applications in epidemiology and medical research, economics,      Causal Reasoning 155 education, psychology; and social science (Gong, 2008). Because of Donald Rubins contributions this is sometimes referred to as the Rubin Model. In potential outcomes models, all possible outcomes, both observable and unobservable, are considered simultaneously, forming outcome vectors. The framework can briey be describes as follows (Greenland, 2002). Suppose we have a population of individual units under study (e.g. mice, people, counties) indexed by i = 1,...,N, a treatment or exposure with M + 1 levels (or actions) 0, 1, ...","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 8
"M, and an outcome variable of interest Y. The standard potential-outcome model assumes that:  Each individual could have received any of the treatment levels.  For each individual i and treatment level j, the outcome for the individual i if the individual gets treatment level j is considered even if the individual does not in fact get j; this value is called the potential outcome. The variable Y represents a generic variable for the actual outcome under the treatment actually given. Then, Yi(j) will be an indicator for the outcome for individual i if that individual is given treatment level j. The vector [Yi(0), Yi(1), ..., Yi(M)] is the potential outcome vector for the individual i. Notice that, in practice, for each individual only one of the potential outcomes Yi(j) is observable, since an individual receives only one possible treatment (and the other treatment states and associated potential outcomes are counterfactuals). Outcomes that are not observable can only be estimated.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 8
"This problem, called missing data problem, is one of the fundamental problem of causal inference. The causal effect is dened as a quantity that contrasts the components of the potential outcomes vector. The choice of treatment is said to have had no effect on Y for individual i if Yi(j) = Yi(k) for every possible pair of treatment levels j and k; otherwise, treatment choice could have had an effect. Treatment choice is said to have had no effect on the population if it had no effect on any individual in the population. As an illustration, let us consider the simplest case when the treatment is binary, i.e., M = 1 (corresponding, for instance, to situations when there is and there is no treatment). Let Ti be a level of the actual treatment for the individual i. Then, the vector [Yi(0), Yi(1)] is the potential outcome vector for the individual i.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 8
"The outcome Yi is equal Yi(0) if Ti = 0 and Yi(1) if Ti = 1. This can be written as: Yi = Yi(0)+Ti(Yi(1)Yi(0)) The difference of the outcomes with and without treatment is characterized by Yi(1)-Yi(0), the benet of treatment. The average treatment effect is equal to (where E denotes expected      156 Real-World Reasoning value): ATE = E[Y(1)Y(0)] The average treatment on the treated individuals is equal to: ATT = E[Y(1)Y(0)|T = 1] These quantities cannot be computer because of unobserved potential outcomes. For instance, let the available data are given in the table given below. The observed values Yi(j) are printed in bold.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 8
"The quantities ATE and ATT can be computed only using estimated values for Yi(j) and this gives ATE=2, ATT=1. Different approaches for estimating these and other unobservable quantities by observable quantities are discussed in (Angrist, 1996). Individual Treatment Yi(0) Yi(1) Yi(1)  Yi(0) 1 0 3 5 2 2 1 2 5 3 3 1 5 4 1 4 0 2 7 5 5 1 1 2 1 One of the practical results of the potential-outcomes models is the identication of a sufcient set of variables could yield the correct causal effect between variables of interest. That characterization of variables, called backdoor criterion, helps in identifying sets of variables worthy of observing. 8.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 8
"7 Structural-equation models In structural equation approach, a network of causation is modeled by a system of equations and independence assumptions (see, for instance, (Greenland, 2002). Each equation shows how an individual response (outcome) variable changes as its direct (parent) causal variables change. The individual may be any unit of interest, such as a person or aggregate. In the system, a variable may appear in no more than one equation as a response variable, but may appear in any other equation as a causal variable. A variable appearing as a response in the system is said to be endogenous (within the system); otherwise it is exogenous. Relationships between variables can be linear but can also be much more complex. Structural equations can be viewed as formulas for computing potential outcomes under various actions. For instance, equations can assert that one variable will not vary      Causal Reasoning 157 with another variable, if some other variables remain constant.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 8
"Structural equations differ from ordinary regression equations (that represent only associations of actual outcomes with actual values of the covariates as one moves across individuals). Structural equations with unknown parameters specify the functional form of effects, but do not provide the exact values of effects; thus, they dont not fully quantify causal relations. 8.8 Probabilistic causation If a causal relationship A causes B is interprered deterministically then it states that A must be always followed by B. Drinking alchohol causes headache is often true, but still not always, so this causal relationship could be considered invalid. Probabilistic causation tends to overcome simple yes or no causation and in this approach, cause only raises the probability of the effect (rather then implies effect), all else being equal. In other words, A probabilistically causes B if occurrence of A increases the probability of B (Hitchcock, 2002).","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 8
"In this approach, causal relationships are explored by using the apparatus of the probability theory. We will not elaborate on this approach here, but it will rise to signicance in later chapters of the book. Standard Bayesian approaches to causal inference will be discussed in Chapter 11 in the context of pattern mining; and then the PLN approach to RWR, discussed in Part III of the book, will include a different variant of probabilistic causal inference in an essential role, tightly integrated with probabilistic approaches to other aspects of inference such as deduction, induction and abduction.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 8
       ,"Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 8
"  PART II Acquiring, Storing and Mining Logical Knowledge        ","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 9
"  Chapter 9 Extracting Logical Knowledge from Raw Data Logical methods are extremely powerful when supplied with appropriate data, but this begs the question of where the data comes from. How does data from the real world get into logical format in the rst place? So far we have discussed simple toy examples, involving a small number of relationships; or else relationships that come from an already-formalized domain such as the Minesweeper game. But the real world is large, messy and not pre-formalized. Our overall goal here is to discuss the application of logic-oriented methods to large, heterogeneous knowledge stores; so we cannot entirely bypass the critical question of how one would construct large, heterogeneous stores of logical information. Essentially the question is how to usefully translate raw data observed in the real world, into sets of logical expressions in appropriate formal languages. This is not an easy question and there is no single answer: the answer is roughly as heterogeneous as the data involved.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 10
"This is not our main focus here, so in this brief chapter we will merely overview some of the issues involved, giving a few references into some relevant literatures where appropriate. In some cases, the transformation process is completely obvious and straightforward. For instance, Figure 9.1 shows how Twitter metadata regarding a message can simply and immediately be transformed into formal relationships (which could easily be mapped from diagrammatic into logical form): For instance, if one has a logical term T1 corresponding to the entity bob dobbs, and a logical term M1 corresponding to the message Where r u?, then one may create from this diagram a logical predicate-argument relationship such as SentTo(M1, T1) and interpret this within many of the formal-logic systems discussed in Part I above.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 10
"To take a different sort of example, in the project described in (Goertzel & de Garis, 2008; Goertzel, 2008) an architecture is described for controlling physical or virtual robots 161      162 Real-World Reasoning Fig. 9.1 Formal relationships extracted from Twitter metadata using a logic-based cognitive engine. In this case the logical relationships come directly from the outputs of sensors, and from the commands needed to be issued to actuators. Here there are no major difculties in representation, though there are signicant difculties in reasoning! For instance, we may represent a certain perceptual relationship by stating that there is a logical relationship of the form tangentialProperPart(P1, P2) between polygons P1 and P2, whose coordinates are then indexed in a special data structure (tangentialProperPart is a spatial logical relationship to be discussed a little later on).","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 10
"And we might represent a relationship regarding the movement of an actuator by moveJoint(7, 1.3, 1.4, 2) indicating the movement of joint 7 at speed 2 in the direction with  = 1.3 radians and  = 1.4 radians (referring to the standard spherical coordinates). Here, as in the Twitter metadata example, the translation of life into logic is relatively straightforward. However, in other cases  including many cases relevant to the topic of this book  the transformation process involves much more difcult choices. In general, transforming raw data into logic is a highly nontrivial matter, which requires the best of current technologies; but it is certainly within the scope of the feasible rather than the impossible. In practice subtle decisions must be made about how much intelligence to put into the transformation process, versus how much to leave to the logical-inference processes acting on the logical knowledge base.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 10
"For example, if a software system is given the text Dogs eat bones, one simple approach at logicizing this input would be to simply turn it into a sequence of propositions about      Extracting Logical Knowledge from Raw Data 163 the individual characters of the text: essentially, propositions of the form At time so-andsuch, I received a text message with g as the third character and so forth. This kind of proposition can be represented easily in formal logic, but this is not necessarily the most useful thing to do. In the remaining sections of this chapter, we will very briey consider two cases of the extraction of logical information from nonlogical sources: tabular and relational data, and linguistic data. Other cases also exist, of course: for instance audio, video and so forth; but reviewing the literatures in all these areas would take us too far aeld. 9.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 10
"1 Extracting Logical Knowledge from Tabular and Relational Data Conceptually, it seems straightforward enough to map tabular or relational data into logical format. Figure 9.2 shows a simple example of a spreadsheet mapped into formal semantic relations: In real life, however, this sort of mapping is extremely difcult, because of the problem of guring out the semantics of the rows and columns of spreadsheets and databases. The eld of table recognition confronts this issue, and is summarized in (Zanibbi et al., 2003). 9.2 Extracting Logical Knowledge from Graphs, Drawings, Maps and Tables A yet more difcult issue is the extraction of formal relationships from graphs, engineering drawings, maps and tables that are encoded as bitmap images or vector drawings. The set of techniques in charge of extracting semantics out of graphical information is grouped under the term Diagram Recognition and has been given some attention for the past two decades.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 10
"Several algorithms, techniques and toolkits have been developed and work well in many cases. However in general it remains a hard problem, probably much harder than one who is unfamiliar with the domain may realize at rst. This is due to the variety of manners one can choose to convey information graphically. Sometimes to interpret correctly a diagram one even needs contextual information possibly scattered in the rest of the document or relying on common sense knowledge. Various domain dependent algorithms have been formulated and applied. However recent work has focused on unifying these techniques into a single framework using formal grammars comparable in a way a compiler generates machine code out of a program written in some programming language [Blostein02]. Except that here the program is a 2D image and the machine code is a diagram model. A diagram model encodes the semantics      164 Real-World Reasoning Fig. 9.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 10
"2 Extracting formal semantic relations from tabular data of the image, for instance if the diagram is a graph, the diagram model may be a list of relationships, and the compiler, or rather called diagram recognizer, would produce an XML le containing the list of nodes and relationships represented by the graph. Following that approach a diagram recognizer may carry out several grammar parsing and data production passes, like: 1) a layout pass, that captures the spatial structure of the image and encodes it into a tree, like inside(circle, left to right(B, o, b))      Extracting Logical Knowledge from Raw Data 165 2) a lexical pass, that tries to group symbols into lexical tokens, for instance B, o, b becomes Bob 3) syntactic and semantic passes to nally generate the diagram model, for instance node(Bob) that expresses that the diagram is a graph with a single node called Bob.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 10
"Similarly regarding table recognition there exists several techniques and recent work has been focused on unifying them (Zanibbi et al., 2004, 2006; Blostein et al., 2000).]. Again the process is divided in several passes, where each pass analyze a certain layer to produce more abstract knowledge for the upper layer and so on until the semantic model is built. 9.3 Extracting Logical Knowledge from Natural Language Text Extracting logical relations from text requires multiple intermediate stages of Natural Language Processing, each of which is subtle and complex in itself. For instance, using state-of-the-art natural language technology, one can transform a sentence such as The dog ate the bone into relationships such as eat(dog, bone) which is an abstract relationship embodying the semantic meaning of the sentence.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 10
"Or, further, one can transform it into eat 2(dog 1, bone 1) which indicates that the sense of eat used in the sentence is the second one in the systems reference dictionary, whereas the senses of dog and bone used are the rst ones in the systems reference dictionary. But performing similar operations on more complex sentences pushes the boundaries of what todays technology can do. One can frame this problem of natural language information extraction (Cowie & Wilks, 2000) in terms of three stages:  mapping text into a syntactic representation  mapping the syntactic representation into a semantic representation  mapping the semantic representation into a more abstract logical representation Figure 9.3 shows a relevant example of these stages, produced using the open-source RelEx software system created by Novamente LLC (Goertzel et al., 2006), which incorporates as a major subsystem the link parser (Grinberg et al.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 10
"1995) created at Carnegie-Mellon University. In the gure, the input sentence is rst transformed into a set of low-level syntactic      166 Real-World Reasoning relations between words. Then these relations are translated into dependency relations such as subj and obj (representing subject and object relations). Finally these dependency relations are translated into formal relationships that can easily be given logical interpretation. The presence of these multiple stages illustrates the complexity of the natural language information extraction process. Fig. 9.3 Successive transformations of text into syntactic, grammatical and nally formal relationships (that are easily transformable to logical relationships) The deepest problem in natural language information extraction has to do with the various sorts of ambiguity that exist in natural language.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 10
"Words may have multiple meanings;      Extracting Logical Knowledge from Raw Data 167 sentences may have multiple parses that all seem syntactically plausible but have varying semantic and pragmatic sensibleness; words may refer back to other words, and so forth. Computational linguistics provides only heuristic and approximative techniques for handling these methods (e.g. (Jurafsky & Martin, 2008); (Manning & Schuetze, 1999)), so, although one may currently make software systems that map natural language text into sets of logical relationships, such systems cannot be expected to work perfectly even for simple sentences, and can provide highly erratic results for complex sentences.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 10
       ,"Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 10
"  Chapter 10 Scalable Spatiotemporal Logical Knowledge Storage Having dealt with the representation of logical knowledge of various sorts, and briey discussed the problem of translating nonlogical knowledge into logical knowledge, we now turn to the question of how large amounts of logical knowledge can pragmatically be stored. This chapter presents a brief and relatively nonmathematical interlude before we plunge into the more technical topics of mining patterns in logical knowledge stores, and carrying out inferences regarding changes and other patterns in logical knowledge stores. Suppose that we represent temporal and spatiotemporal knowledge, appropriately contextualized, in one of the multiple logical formalisms briey discussed above. If we apply these formalisms to real-world situations we are going to obtain incredibly huge numbers of logical propositions, which presents various potential practical difculties.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 11
"The mathematics is the same whether one has a dozen logical propositions or a trillion, but the pragmatics of information-management differs signicantly! In this chapter we review the various available technologies for managing massive amounts of logical terms and relationships. 10.1 Comparison of Available Storage Technologies The following table summarizes the strengths and weaknesses of available data storage technologies from the perspective of storing and managing large amounts of logical information.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 11
"169      170 Real-World Reasoning Technology Strengths Weaknesses Relational DBs  Mature, enterprise grade solutions  Ease of integration with other systems  Poor conceptual t for logical information storage  Inadequate model for reasoning  Complex scalability ObjectOriented DBs  Better conceptual t than relational DBs (still not perfect)  Mature solutions  Single data model  Small ecosystem  Not designed for reasoning Graph DBs  Flexible, dynamic data model  Good performance and scalability  Designed with data analysis in mind  Less mature than competing technologies Hypergraph DBs  Best data model t  Designed with reasoning and data analysis in mind  Alpha stage technology RDF Triplestores  Semantic web friendly  Adequate data model for some inferences  Less mature technology  Rigid data model Documentoriented DBs  Flexible data model  Performance and scalability  Rapidly maturing solutions  Not adequate for reasoning and analysis  More work is left for application layer Continued on next page      Scal","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 11
"able Spatiotemporal Logical Knowledge Storage 171 continued from previous page Technology Strengths Weaknesses Columnoriented DBs  Very exible, dynamic data model  Performance and scalability  Rapidly maturing solutions  More work is left for application layer  Not designed for reasoning Key-value DBs  Extremely good performance and scalability  Mature and rapidly maturing solutions  No data model, leaving most work for application layer  Not designed for reasoning Representing and querying large graph data stores using traditional relational databases is certainly possible, but it would lead to profound scalability and performance problems. Despite efforts from industry leaders in the RDBMS arena, relational databases and graph data are a poor conceptual and implementation t. Graph data typically has a exible structure where connections among similar objects (representing people, entities, time points, spatial locations, and so forth) are numerous. And these connections are the whole point of graph data stores.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 11
"The natural way to map these connections among similar objects to relational databases requires self-joins, since the connected objects are typically stored in the same table. Contemporary RDBMS technology is not optimized for these kinds of self-joins, which creates large bottlenecks both for querying at scale and writing to the database (Lightstone et al., 2007). Object-oriented databases (OODBMS) are mature technologies that provide a better t for graph data, since object instances are naturally persisted as graphs. Distributed, highly scalable commercial products exist, although the whole OODBMS category remains a niche after decades of development efforts. Despite those benets, there is a major drawback for OODBMS in a graph dataset context, which is the implicit assumption of a single object model. While this is adequate for large-scale object-oriented applications, graph data mining and analysis sometimes requires that the stored data be interpreted in different ways, especially when spatial and temporal dimensions are added.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 11
"Theres always a 1-to-1 mapping between a OODBMS representation and some OO design  but for analyzing graph data we often want to switch the design without changing the stored data format (which is expensive for a large database), because      172 Real-World Reasoning we care more about the answers we can get than about retrieving the original data in a way thats fully consistent with how it was stored. Technology thats explicitly oriented towards graph data (as opposed to using graphs to persist object instances) has been developed over the past few years. These tools emphasize the ability to traverse relations and discover new connections. This explicit focus makes such tools a better t for data analysis and mining projects, especially ones involving probabilistic logic. Graph databases arent as mature as their OOBDMS counterparts, however. Among graph DBs, the Hypergraph DB (HGDB) open source project (kobrix.com/hgdb.jsp) was conceived with AI and data mining applications in mind.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 11
"In fact HGDB goes beyond the graph database paradigm and constitutes a hypergraph DB, involving a basic representation that allows n-ary links and links pointing to links as well as nodes. This is convenient because sets of logical predicates are more naturally represented as hypergraphs than graphs. Traditional graph DBs may be used as hypergraph DBs via transforming hypergraphs to and from graphs; but this introduces a performance penalty for the translation, and also has the drawback that, after the translation, some simple hypergraph queries become signicantly more complex graph queries. But HGDB is still at the alpha stage of the development, and the potential advantages of hypergraph DBs over standard graph DBs are still relatively unexplored. The growing interest in the semantic web has led to a number of commercial and open source products for storing knowledge encoded in the RDF data model. These storage solutions, known as Triplestores, can scale up to billions of RDF triples.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 11
"The triple-based data model in RDF, however, is more limited than a free-form graph, which impacts scalability (as a more verbose knowledge representation is needed) and analysis (as algorithms have to be tailored to the more rigid RDF format, sometimes with a signicant performance penalty). Finally, the recent years have seen an explosion in alternative data storage technologies, which are often collectively referred to as NoSQL, emphasizing their rejection of the dominant relational data model. However, the NoSQL umbrella actually contains a number of very diverse technologies, with different design goals and motivations. At the simplest level, we have key-value data storage solutions, some of which are decades old, while others have been developed recently in order to answer the growing need for very fast, distributed (sometimes with complex dynamics for delayed consistency)      Scalable Spatiotemporal Logical Knowledge Storage 173 datastores to support leading web sites, with a large number of concurrent read and write requests.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 11
"These DBs have essentially no built-in data model, allowing for complete exibility and excellent performance, at the cost of increased application level complexity. More complex solutions exist in the form of column-oriented DBs, which store a exible, dynamic data model that is adequate for representation of logical data. While these DBs are typically very fast and easily distributed, graph-based traversals and queries remain expensive unless great care is taken at the application level to organize data, which reduces analytical exibility. The third major kind of storage solution in the NoSQL umbrella is the document store, in which documents have a very exible set of properties, although no structural consistency is necessary. This leads to excellent performance and scalability, along with an evolvable data model that is more adequate for logical information than the extreme simplicity of key-value stores. However, graph-specic analytical operations remain problematic.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 11
"Overall, our conclusion is that graph databases, whether commercial or open-source, are the current best alternative for storing and analyzing very large graph datasets, striking a good balance between the extreme exibility of key and column oriented DBs and the convenience of built-in traversal and search operations. 10.2 Transforming Logical Relationship-Sets into Graphs The discussion in the above section bypasses one issue: the effective representation of sets of logical relationships as graphs. This is not a problematic issue, but bears brief comment because, most literally interpreted, sets of logical relationships would better be represented as mathematical structures called generalized hypergraphs than as graphs per se. So one encounters the problem of translating generalized hypergraphs into traditional graphs, using appropriate, hopefully not too complex transformation rules. Recall that a graph, mathematically, is a set of nodes together with a set of links, where each link is construed as an ordered or unordered pair of nodes.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 11
"Links and nodes may be labeled and may have various numerical weights attached to them (such as fuzzy or probabilistic truth values). A hypergraph extends this model, in that links may join more than two targets. This is useful for representing logical relationships such as give(Jim, Bob, ball) which naturally relate three rather than two entities.      174 Real-World Reasoning Of course, one can work around the need for hypergraph links via using labeled binary links, for example subj(give, Jim) obj(give, Bob) obj2(give, ball) which is how the RelEx NLP system (mentioned earlier) analyzes the sentence Jim gives the ball to Bob (and other dependency parsers would do it similarly).","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 11
"Similarly, the relation eat(Ben, steak) could be represented using a ternary link, or a labeled binary link; or, as RelEx would have it, as a set of labeled binary links subj(eat, Ben) obj(eat, steak) However, attaching a broad variety of semantic labels to links is not always the desired strategy. In general it is desirable to support a broad variety of representational mechanisms, as different approaches to logical formalization of commonsense information are going to choose different ways to set up relationships. In any case, it is straightforward to eliminate hypergraph links via introducing a phantom node corresponding to each hypergraph link, and having the phantom node link binarily to the targets of the hypergraph link. Whats required is that the links emanating from the phantom node be indexed with numbers or some other distinct markers, if the targets of the hypergraph links were so marked. Figure 10.1 illustrates some examples of this, for the above examples.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 11
"Next, what we call a generalized hypergraph extends the hypergraph model further, via allowing links to point to links, which is the most natural way to represent statements like Ben believes Bob likes Jim, e.g. believes(Ben, likes(Bob, Jim)) Alternately, a RelEx-style representation of the above would be subj(believes, Ben) obj(believes, likes)      Scalable Spatiotemporal Logical Knowledge Storage 175 Fig. 10.1 Some hypergraph representations for the Ben eats steak example subj(likes, Bob) obj(likes, Jim) Mapping generalized hypergraphs into graphs is also simply accomplished using phantom nodes, as illustrated in Figure 10.2 Just to make sure the point is clear, we next give some examples involving more complex logical constructs such as actually arise in using PLN for carrying out inferences involving changes in complex knowledge bases. EvaluationLink believes Ben Inheritance Bob busy and Context < .","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 11
"9 >      176 Real-World Reasoning Fig. 10.2 Hypergraph to graph conversions for the Ben believes Bob likes Jim example Accounting Evaluation Mention List Bob CanadianPlaceNames and nally one that is more complex and involves variables AverageLink < .9 > $X, $Y Implication Evaluation Mention List $X $Y IntensionalInheritance      Scalable Spatiotemporal Logical Knowledge Storage 177 $X $Y In summary, using this sort of mapping based on phantom nodes, one can straightforwardly store logical relationships, interpreted as generalized hypergraphs, in graph databases. The transformation required is fairly simple and does not require the same sort of inefcient manipulations as mapping logical propositions into tabular structures as required to store them in standard relational databases. However, the subtle question in mapping hypergraphs into graphs is: which graph operations will have the expected results when mapped back into hypergraph operations.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 11
"For instance, if we map a hypergraph into a graph and then nd the shortest path P between two nodes N and M in the graph ... is the hypergraph path corresponding to P the shortest path between the hypergraph nodes or links corresponding to N and M? Similarly, does a minimum-cost spanning tree in the graph derived from a hypergraph, correspond to a minimum-cost spanning hypertree in the original hypergraph? Is the set of nodes within radius R of graph node N, closely related to the set of hypergraph nodes/links within radius R of the hypergraph node/link corresponding to N? These issues go beyond the scope of the present book, and are in most cases not extremely difcult to resolve, but do require real care.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 11
       ,"Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 11
"  Chapter 11 Mining Patterns from Large Spatiotemporal Logical Knowledge Stores Once one has stored a large knowledge base of logical relationships, then what? One can query the knowledge base  if one knows what one wants to ask for. One can carry out reasoning toward various goals. And another important question is how to nd unknown unknowns  patterns in the knowledge base that are surprising and interesting yet unexpected. This quest goes by multiple names  data mining, pattern mining, information exploitation, and so forth. Whatever you call it, its a difcult challenge because in any large dataset, the number of possible patterns to search through is mind-boggling. Many different pattern mining algorithms exist, and a large subset of these are applicable to the case of mining patterns among logical relationships. Here we will review only two classes of algorithms: frequent subgraph mining, and causal network inference. These are important approaches, but are by no means the only approaches of interest.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 12
"Furthermore, as will be emphasized in later chapters, pattern mining algorithms in themselves are unlikely to be sufcient for the task of nding relevant and interesting relationships in large logical knowledge bases. The problem is that without signicant background knowledge, and the capability to deploy this background knowledge intelligently for analogical inference, its very hard to tell interesting patterns from uninteresting ones. So, in order to really do a good job of spotting interesting patterns in large logical knowledge bases, its likely to be necessary to combine pattern mining algorithms with uncertain and causal inference algorithms. That is, one will need to use pattern mining to produce a moderate-sized pool of potentially interesting patterns, and then use inference to lter this down into a smaller set of probably-interesting patterns.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 12
"As many pattern mining algorithms (including the ones considered here) an be instructed to look for new patterns in the neighborhood of a set of target patterns, the patterns identied as interesting by inference may then be used to seed further pattern mining. This kind of hybridized approach has not been explored much if 179      180 Real-World Reasoning at all in the research literature, but there is little doubt it will be necessary as the sorts of applications envisioned in this book become realities. To add to the challenges, pattern mining in extremely large bodies of knowledge poses particular difculties in terms of scalability. For instance, algorithms must cope with the inability to store the whole knowledge base in the memory of any one machine. This is an area computer science is just beginning to explore. For instance, in the following section we will discuss algorithms for identifying surprisingly frequent subgraphs in large graph knowledge bases, following (Hsu et al., 2008).","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 12
"The unique aspect of their approach is a clever mechanism for recursively decomposing a large graph into a large number of smaller subgraphs, recognizing patterns in the subgraphs, and then assembling overall graph patterns from the patterns recognized in the subgraphs. This general sort of idea likely has much more general applicability. Furthermore, even within the scope of what can be stored within a single machine, there can be sufcient data to render standard pattern mining algorithms inapplicable. So as well as crafting distributed algorithms, one must devise special algorithms capable of handling large bodies of knowledge efciently within a single machine. We will consider one example of this below: algorithms for nding partial causal networks in large bodies of knowledge, which essentially are simplications and scalings-up of better-known algorithms for nding full causal networks in smaller datasets. 11.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 12
"1 Mining Frequent Subgraphs of Very Large Graphs The staple of standard data mining in relational databases is a technique called frequent itemset mining or FIM (Goethals & Zaki, 2004), which seeks to nd the most frequent combinations of data items. There are variants of FIM which seek the most surprising combinations of data items; these are essentially algorithmically identical to FIM, with slightly different underlying mathematics (Chakrabarti et al., 1998; Gallo et al., 2007). In the graph domain, the analogue of FIM is frequent subgraph mining, an area in which there are numerous publications and a handful of open-source software toolkits. An overview of the eld is given in (Ivancsy & Vajk, 2005). These algorithms are directly relevant to our problem of mining patterns in large stores of logical knowledge, because logical predicates may be mapped into graph structures as we discussed in the previous chapter.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 12
"     Mining Patterns from Large Spatiotemporal Logical Knowledge Stores 181 Two simple examples of frequent subgraphs that might be found in large graphs in the Twitter domain are as follows:  Women in East Anglia often send each other private messages about the band Coldplay  Young Chinese in London often express positive sentiment about the Chinese government on the day after the Chinese soccer team wins a game Fig. 11.1 Stages of distributed pattern mining in large graphs, in the HLW algoritm. Top, left: original graph. Top, right: partition into subgraphs tting RAM of individual machines. Bottom, left: identication of frequent subgraphs. Bottom, right: merge of subgraphs embodying repeated patterns. Datamining large graph bases is a challenging problem, because most of the highly scalable datamining algorithms available were designed to operate on tabular data, and perform poorly when adapted to graphs.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 12
"These adaptations often require a xed graph structure, which isnt practical. Spatiotemporal databases (Yeung & Hall, 2007) make the problem even harder due to their continuously changing nature. A datamining algorithm for a large spatiotemporal graph database must fulll at least the following requirements:  Ability to handle data too voluminous to t in RAM without severe performance degradation.  Ability to incrementally mine the database, including the ability to consider only new information      182 Real-World Reasoning  Ability to nd patterns that are frequent in space (occur often across different locations), time (occur frequently over time) and both. These patterns can be static or dynamic with regards to time and/or space. One such algorithm, that weve explored in detail, is due to Hsu, Lee and Wang (hence we nickname it HLW) and it has three phases, loosely illustrated in Figure 11.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 12
"1:  Partition the graph database into units that t into RAM.  Apply a standard graph datamining algorithm to each unit, generating a set of patterns.  Merge the obtained patterns from each unit, obtaining database-wide patterns. Alternative algorithms exist and others can be developed. We dont think this particular algorithm is necessarily ideal, but we believe that any software system designed to identify patterns in a large spatiotemporal logic database needs to include an algorithm that fullls the above requirements. 11.2 Learning Partial Causal Networks Another important example of data analysis that must be performed on large spatiotemporal logical knowledge bases is the search for causal patterns. Note the key distinction between correlation and causation, as depicted in Figures 11.211.3: put roughly, causation may be characterized as the combination of correlation with the presence of a plausible causal mechanism ... where the assessment of the plausibility of a causal mechanism always depends upon contextual understanding. Fig.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 12
"11.2 Correlation is not causation Techniques for inferring networks of causal relationships from databases of events are well known, and are mainly based on the interpretation of a causal network as a Bayesian belief network with causal links [Pearl94].      Mining Patterns from Large Spatiotemporal Logical Knowledge Stores 183 Fig. 11.3 The basics of causation Fig. 11.4 A causal network Figure 11.5 shows some causal relationships in the Twitter context that may be represented this way: a causal relation between message contents of a given person, and a causal relation between message content and follower subscription. None of the standard Bayes net based methods scale up at all well. On the other hand, there are some modern variations of these methods that do deal with reasonably large datasets, via scaling down their ambition and searching for partial causal networks rather than complete ones.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 12
"Building a partial causal network (still capturing most of the causal relations) is relatively tractable even for processes involving tens of thousands of variables. To apply these methods to really huge datasets, one would then combine them with the same sort of graph-partitioning scheme described in the previous section in the context of frequent itemset mining. However, articulating the details of this combination would go beyond the scope of this book.      184 Real-World Reasoning Fig. 11.5 Examples of a causal relation between message contents of a given person and a causal relation between message content and follower subscriptio. 11.3 Scalable Techniques for Causal Network Discovery The leading algorithm for scalably discovering partial causal networks in massive datasets is Local Causal Discovery (LCD), a straightforward technique which has many specialized variants [Silverstein98, Mani01]. The basic idea underlying LCD is simple : testing Conditional Independence for two variables assuming one cause, instead of assuming a conjunction of causes.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 12
"Recall that the notation X//Y|Z stands for X and Y are independent knowing Z, or more formally P(X|Y,Z)=P(X|Z) and P(Y|X,Z)=P(Y|Z). In these terms, the basic idea of LCD is assuming X//Y|Z instead of X//Y|C where C is a set of variables. This core principle underlies all variations of the algorithm. This sort of algorithm can be reasonably efcient; but has the serious limitation that can only discover causal relations involving one cause at a time.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 12
"There are also global approaches that can handle events with multiple causes, via approaches such as  making special causal assumptions [Cheng97]  pre-processing dependence over the graph (that is computing a dependency measure for each pair of variables) [Cheng97]      Mining Patterns from Large Spatiotemporal Logical Knowledge Stores 185  pre-processing the Markov Boundary (which is the minimal Markov Blanket) [Margaritis99] The pre-processing approach allows one to reduce the number of conditional independence tests because many congurations of causes are ruled out after the pre-processing (or/and by causal assumptions). A good example of a more scalable global method is [Margaritis99]. This approach operates by rst estimating the Markov boundary (i.e. the minimal set of variables that isolates, that is makes independent, a given variable from the rest of the network) of each variables to limit the conditional independence search over the Markov boundary of a given variable.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 12
"This works excellently when for instance a variable has a small number of direct causes. Another, related approach is [Nielsen08], which uses the algorithm of the previous paragraph but in an incremental way, based on the assumption that the joint probability distribution is changing over time.        ","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 12
  PART III Probabilistic Logic Networks for Real-World Reasoning        ,"Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 13
"  Chapter 12 Probabilistic Logic Networks The preceding portion of this book has largely constituted literature review; this nal part is a little different and presents original material, designed to cover important areas that seem omitted by the approaches reviewed above. Above we have reviewed aspects of the representation of uncertain spatiotemporal knowledge, and also systems for reasoning on real-world knowledge; but there are major gaps between those two sets of ideas as we have presented them so far. Our discussion of representation focused heavily on fuzzy and probabilistic spatiotemporal knowledge, but the logical reasoning systems discussed dont handle these sorts of uncertainty in a sophisticated or integral way. We suggest that one prerequisite for effective, scalable RWR using a logic-based approach, is to have a logic system that incorporates fuzziness and probability into spatiotemporal, contextual and causal inference in a fundamental way.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 14
"In the chapters in this Part of the book, we aim to show how to do this via creating and manipulating special logical relationship types within the Probabilistic Logic Networks (PLN) formalism that we have introduced in prior publications (Goertzel et al., 2008), and developed in the context of our work on the Novamente Cognition Engine (Goertzel et al., 2004) and OpenCog (Hart & Goertzel, 2008) integrative AI architectures, and will use in some of the detailed examples in later chapters. A complete exposition of PLN would be out of place here; our goal will be to explain enough of the elements and the notation to make the examples given in later chapters comprehensible. 12.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 14
"1 Motivations Underlying PLN The guiding motivation behind the design of PLN was the desire to create an uncertain inference framework capable of encompassing all the sorts of inference that may confront a general intelligence operating in the everyday human world  including reasoning based 189      190 Real-World Reasoning on uncertain knowledge, and/or reasoning leading to uncertain conclusions (whether from certain or uncertain knowledge).","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 14
"Among the general high-level requirements underlying the development of PLN were the following:  To enable uncertainty-savvy versions of all known varieties of logical reasoning, including for instance higher-order reasoning involving quantiers, higher-order functions, and so forth  To reduce to crisp theorem prover style behavior in the limiting case where uncertainty tends to zero  To encompass inductive and abductive as well as deductive reasoning  To agree with probability theory in those reasoning cases where probability theory, in its current state of development, provides solutions within reasonable calculational effort based on assumptions that are plausible in the context of real-world embodied software systems  To gracefully incorporate heuristics not explicitly based on probability theory, in cases where probability theory, at its current state of development, does not provide adequate pragmatic solutions  To provide scalable reasoning, in the sense of being able to carry out inferences involving billions of premises.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 14
"Of course, when the number of premises is fewer, more intensive and accurate reasoning may be carried out.  To easily accept input from, and send input to, natural language processing software systems The practical application of PLN is still at an early stage. Based on our evidence so far, however, we have found PLN to fulll the above requirements adequately well, and our intuition remains that it will be found to do so in general. The overall structure of PLN theory may be described as follows. First, PLN involves some important choices regarding knowledge representation, which lead to specic schematic forms for logical inference rules. The knowledge representation may be thought of as a denition of a set of logical term types and logical relationship types (some of which we will elaborate below), leading to a novel way of graphically modeling bodies of knowledge. It is this graphical interpretation of PLN knowledge representation that led to the network part of the name Probabilistic Logic Networks.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 14
"It is worth noting that the networks used to represent knowledge in PLN are generalized weighted directed hypergraphs (Bollobas, 1998), much more general for example than the binary directed acyclic graphs used in Bayesian network theory. Later on we will review some      Probabilistic Logic Networks 191 methods for translating generalized hypergraphs into ordinary graphs, which can be useful for purposes of visualization, analysis and storage. Next, PLN involves specic mathematical formulas for calculating the probability value of the conclusion of an inference rule based on the probability values of the premises plus (in some cases) appropriate background assumptions. It also involves a particular approach to estimating the condence values with which these probability values are held (weight of evidence, or second-order uncertainty). Finally, the implementation of PLN in software requires important choices regarding the structural representation of inference rules, and also regarding inference control  the strategies required to decide what inferences to do in what order, in each particular practical situation.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 14
"Here we will not be concerned at all with PLNs probability formulas  they are absolutely critical for performing practical inferences and getting useful answers, but here we will only be concerned with exploring the forms of various inferences, and so we will refer the reader to the Probabilistic Logic Networks book (Goertzel et al., 2008) for discussion of quantitative formulas. In our examples here, we will omit quantitative truth values so as to focus on the forms of inferences. In fact, the quantitative truth value associated with an inference may come out differently depending on the particular parameters of the truth value formulas, as claried in the PLN book. 12.2 Term and Predicate Logic in PLN One of the distinguishing features of PLN is the way its inference rules combine predicate logic and term logic. As briey reviewed above, predicate logic and term logic are two different but related forms of logic, each of which can be used both for crisp and uncertain logic.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 14
"Predicate logic is the most-familiar kind, where the basic entity under consideration is the predicate, a function that maps argument variables (which are quantied universally or existentially) into truth values. On the other hand, in term logic, which dates back at least to Aristotle and his notion of the syllogism, the basic element is a subject-predicate statement, denotable in many ways, for instance A  B where  denotes a notion of inheritance or specialization. Logical inferences take the form of syllogistic rules, which give patterns for combining statements with matching      192 Real-World Reasoning terms. (We dont use the  notation much in PLN, because its not sufciently precise for PLN purposes since PLN introduces many varieties of inheritance; but we will use the  notation in this section since here we are speaking about inheritance in term logic in general rather than about PLN in particular).","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 14
"Example term logic inference rules are the deduction, induction, and abduction rules: A  B B  C  A  C A  B A  C  B  C A  C B  C  A  B Fig. 12.1 Deduction Fig. 12.2 Induction Fig. 12.3 Abduction These rules are simple schematically but subtler when one matches them with uncertain truth value formulas. For instance, when one does so, one nds that deduction is infallible, in the case of absolutely certain premises, but uncertain in the case of probabilistic premises; while abduction and induction are always fallible, even given certain premises. In fact, in PLN one derives abduction and induction from the combination of deduction with a simple rule called inversion A  B  B  A whose truth value formula derives from Bayes rule.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 14
"Predicate logic is generally felt to deal more naturally with deduction than with induction, abduction and other uncertain, fallible inference rules. On the other hand, term logic can deal quite elegantly and simply with all forms of inference. Furthermore, as argued in (Goertzel et al., 2008) the predicate logic formulation of deduction proves less amenable to probabilization than the term logic formulation. It is for these reasons, among others, that the foundation of PLN is drawn from term logic rather than from predicate logic. PLN      Probabilistic Logic Networks 193 begins with a term logic foundation, then adds on elements of probabilistic and combinatory logic, as well as some aspects of predicate logic, to form a complete inference system, tailored for easy integration with software components embodying other (not explicitly logical) aspects of intelligence.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 14
"Sommers and Engelbretsen (Englebretsen & Sommers, 2000) have given an excellent defense of the value of term logic for crisp logical inference, demonstrating that many pragmatic inferences are far simpler in term logic formalism than they are in predicate logic formalism. On the other hand, the pioneer in the domain of uncertain term logic is Pei Wang (Wang, 1996), to whose NARS uncertain term logic based reasoning system PLN owes a considerable debt.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 14
"To frame the issue in terms of our above discussion of PLNs relation to traditional probabilistic logic approaches, we may say we have found that many things are signicantly easier in a term logic rather than predicate logic context, including: 1) the formulation of appropriate heuristics to guide probabilistic inference in cases where adequate dependency information is not available, 2) and the creation of appropriate methods to extend rst-order extensional inference rules and formulas to handle other sorts of inference. In these respects, the use of term logic in PLN is roughly a probabilization of the use of term logic in NARS; but of course, there are many deep conceptual and mathematical differences between PLN and NARS, so that the correspondence between the two theories in the end is more historical and theory-structural, rather than being a precise correspondence on the level of content. 12.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 14
"3 Knowledge Representation in PLN PLN knowledge representation is conveniently understood according to two dichotomies: extensional vs. intensional, and rst-order vs. higher-order. The former is a conceptual (philosophical/cognitive) distinction, between logical relationships that treat concepts according to their members versus those that treat concepts according to their properties. In PLN extensional knowledge is treated as more basic, and intensional knowledge is dened in terms of extensional knowledge via the addition of a specic mathematics of intension (somewhat related to information theory). This is different from the standard probabilistic approach which contains no specic methods for handling intension, and also different from Wangs approach in which intension and extension are treated as completely symmetric with neither of them being more basic or derivable from the other.      194 Real-World Reasoning The rst-order versus higher-order distinction, on the other hand, is essentially a mathematical one.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 14
"First-order, extensional PLN is a variant of standard term logic, as originally introduced by Aristotle in his Logic and more recently elaborated by theorists such as Wang (Wang, 1996) and Sommers and Engelbretsen (Englebretsen, 2000). First-order PLN involves logical relationships between terms representing concepts, such as Inheritance cat animal ExtensionalInheritance Pixel 444 Contour 7565 (where the notation is used that R A B denotes a logical relationship of type R between arguments A and B).","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 14
"A typical rst-order PLN inference rule is the standard term-logic deduction rule A  B B  C  A  C which in PLN looks like ExtensionalInheritance A B ExtensionalInheritance B C  ExtensionalInheritance A C As well as purely logical relationships, rst-order PLN also includes a fuzzy set membership relationship, and specically addresses the relationship between fuzzy set membership and logical inheritance, which is closely tied to the PLN concept of intension. Higher-order PLN, on the other hand, has to do with functions and their arguments. Much of higher-order PLN is structurally parallel to rst-order PLN: for instance, implication between statements is largely parallel to inheritance between terms. However, a key difference is that most of higher-order PLN involves either variables or higher-order functions (functions taking functions as their arguments).","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 14
"So for instance one might have ExtensionalImplication Inheritance $X cat      Probabilistic Logic Networks 195 Evaluation eat ($X, mice) (using the notation that R A B denotes the logical relationship R applied to the arguments A and B). Here Evaluation is a relationship that holds between a predicate and its argument-list; so that e.g. Evaluation eat (Sylvester, mice) means that the list (Sylvester, mice) is within the set of ordered pairs characterizing the eat relationship. The parallel of the rst-order extensional deduction rule given above would be a rule ExtensionalImplication A B ExtensionalImplication B C  ExtensionalImplication A C where the difference is that in the higher-order inference case, the tokens A, B and C denote either variable-bearing expressions or higher-order functions. Some higher-order inference rules involve universal or existential quantiers as well.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 14
"While rst-order PLN adheres closely to the term logic framework, higher-order PLN is better described as a mix of term logic, predicate logic and combinatory logic (though the latter aspect will not be emphasized here). The knowledge representation is kept exible as this seems to lead to the simplest and most straightforward set of inference rules. 12.4 PLN Truth Values and Formulas Next, one of the less conventional aspects of PLN  which will not play a major role in this book, but still merits brief mention  is the quantication of uncertainty using impre     196 Real-World Reasoning cise truth values that contain at least two components, and usually more (in distinction from the typical truth value used in probability theory, which is a single number: a probability).","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 14
"PLNs indenite probability approach is related to earlier multi-component truth-value approaches due to Wang and Walley [Wang 2006b; Walley 1991] and others, but is unique in its particulars. The simplest kind of PLN truth value, called a SimpleTruthValue, consists of a pair of numbers <s,w> called a strength and a condence. The strength value is a probability; the condence value is a measure of the amount of uncertainty attached to the strength value. Condence values are normalized into [0,1]. For instance <.6,1> means a probability of .6 known with absolute certainty. <.6,.2> means a probability of .6 known with a very low degree of certainty. <.6,0> means a probability of .6 known with a zero degree of certainty, which is equivalent to <x,0> for any other probability value x.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 14
"Another type of truth value, more commonly used as the default within PLN, is the IndeniteTruthValue. We introduce the mathematical and philosophical foundations of IndeniteTruthValues in Chapter 3. Essentially a hybridization of Walleys imprecise probabilities and Bayesian credible intervals, indenite probabilities quantify truth values in terms of four numbers <L,U,b,k>: an interval [L,U], a credibility level b, and an integer k called the lookahead. IndeniteTruthValues provide a natural and general method for calculating the weight-of-evidence underlying the conclusions of uncertain inferences. Beyond the SimpleTruthValues and IndeniteTruthValues mentioned above, more advanced types of PLN truth value also exist, principally distributional truth values in which the strength value is replaced by a matrix approximation to an entire probability. Note that this then provides for three different granularities of approximations to an entire probability distribution.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 14
"A distribution can be most simply approximated by a single number, a somewhat better approximation being provided by a probability interval, and an even better approximation given by an entire matrix. (Goertzel et al., 2008) denes the various inference rules of PLN, and also associates with each of them a strength value formula with each of them (a formula determining the strength of the conclusion based on the strengths of the premises). For example, the deduction rule mentioned above is associated with two strength formulas, one based on an independence assumption and the other based on a different concept geometry based assumption.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 14
"The independence-assumption-based deduction strength formula looks like B < s B >      Probabilistic Logic Networks 197 C < s C > ExtensionalInheritance A B < s AB > ExtensionalInheritance B C < s BC >  ExtensionalInheritance A C < s AC > where sAC = sAB sBC +(1sAB)(sC sB sBC)/(1sB) This particular rule is a straightforward consequence of elementary probability theory. Some of the other formulas are equally straightforward; but some are subtler and (as explained in detail in (Goertzel et al., 2008)) require heuristic reasoning beyond standard probabilistic tools like independence assumptions. Since simple truth values are the simplest and least informative of our truth value types, they provide quick, but less accurate, assessments of the resulting strength and condence values. A valuable enterprise is extending the simple truth value formulas to IndefiniteTruthValues.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 14
"A careful consideration of this matter shows that indenite truth values provide a natural approach to measuring weight-of-evidence. IndeniteTruthValues can be thought of as approximations to entire distributions, and so provide an intermediate level of accuracy regarding strength and condence. Finally, PLN inference formulas may also be modied to handle entire distributional truth values. Distributional truth values provide more information than the other truth value types. As a result, they may also be used to yield even more accurate assessments of strength and condence. 12.5 Some Relevant PLN Relationship Types and Inference Rules In this section we briey review the specic PLN relationship types and inference rules that will be used in the inference examples given in later chapters. Contextual, spatial and temporal relationships will not be covered here, as these will be described later on in the appropriately specialized chapters. As seen above the PLN formalism allows one to express relationships over predicates and relationships.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 14
"For this purpose it uses higher order operators (comparable in some ways to rst order logic connectives and quantiers), such as Implication, Equivalence, Average and ThereExists (there is also a ForAll operator but as it is not used in the inference exam     198 Real-World Reasoning ples we will not elaborate further on it). The semantics of Equivalence and Implication are easily denable using the SatisfyingSet operator that we dene below. 12.5.1 SatisfyingSet and Member The SatisfyingSet operator allows us to express the concept of a set whose members are all elements that satisfy the predicate. We also recall the Member relationship type that expresses how much an element belongs to a concept (with a truth value that is fuzzy rather than probabilistic). Lets for instance consider the predicate FriendOfBob, dened by the three elements Jack, John and Jill as follows: Evaluation < .7 > FriendOfBob Jack Evaluation < .","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 14
"6 > FriendOfBob John Evaluation < .8 > FriendOfBob Jill According to the denition of the SatisfyingSet operator, we would then have: Member < .7 > Jack SatisfyingSet(FriendOfBob) Member < .6 > John SatisfyingSet(FriendOfBob) Member < .8 > Jill SatisfyingSet(FriendOfBob)      Probabilistic Logic Networks 199 12.5.2 Equivalence and Implication Now we can dene Equivalence and Implication as follows: Equivalence A B is equal to Similarity SatisfyingSet(A) SatisfyingSet(B) and Implication A B is equal to Inheritance SatisfyingSet(A) SatisfyingSet(B) We have dened the mixed versions of Equivalence and Implication.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 14
"The extensional and intensional versions are analogously dened, we give only the denition of ExtensionalEquivalence: ExtensionalEquivalence A B is equal to      200 Real-World Reasoning ExtensionalSimilarity SatisfyingSet(A) SatisfyingSet(B) 12.5.3 Quantiers, Average and ThereExists Quantication in uncertain logic is a somewhat subtle matter. PLN handles it largely via the Average construct, which is a kind of average quantier: the truth value of AverageLink $X F($X) can be dened as the weighted average of the truth value of F($X), i.e. as x w(x)F(x) x w(x) (Note that this approach has been elaborated in detail only for nite domains, as the intended application is to the set of knowledge contained explicitly within an AI system. Extension to innite domains is thought to be possible but would require additional theoretical elaboration.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 14
"There may be other ways to dene the truth value of Average, possibly more advantageous in various respects, but the one outlined above has the advantage of being rather tractable, and it is the approach taken in the current PLN software system. ThereExists is an existential quantication; it is the dual of the quantier ForAll and we will not recall it in detail here (see the PLN book for more information (Goertzel et al., 2008)). Informally lets just say that the truth value of ThereExists $XF($X) quanties how much it exists an $X such that F($X) is essentially not zero, i.e. lying within [e,1], where e is a margin of error. From the detailed treatment of Average and ThereExists given in (Goertzel et al.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 14
"2008), it follows that the truth value of ThereExists $XF($X) must be equal to or greater than the truth value of Average $XF($X), at least assuming that e is equal or smaller than the truth value of Average $XF($X). This will be useful in the inference examples given later on.      Probabilistic Logic Networks 201 12.5.4 Some Inheritance rules The following rule is also useful in examples given later on; it basically says that if all elements that inherit F, also inherit G, then as a consequence F inherits G: Average $X ExtensionImplication Inheritance $X F Inheritance $X G Inheritance F G 12.5.5 Intensional Inheritance As stated earlier in the section, intensional inheritance quanties how much the patterns of a concept inherits from the patterns of another. There is not a unique way to dene the notion of pattern.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 14
"Here our working denition is that the set of patterns of A is the set of concepts which contribute to dene A and which are simpler than A itself. Formally P is a pattern of A if 1) ExtensionaInheritance A P 2) NotLink ExtensionalInheritance NotLink A P 3) c(P) < c(A), where c is a measure of complexity (for instance some variant of Kolmogorov complexity). The reason we require both 1 and 2 is to avoid patterns that are too general to be useful, for instance a concept like BeingPartOfTheUniverse will not be retained as an interesting pattern because it does not help to dene A, since both A and Not A are part of the universe.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 14
"Let us denote PAT(A) the set of patterns of A, then      202 Real-World Reasoning IntensionalInheritance A B is just equivalent to ExtensionalInheritance PAT(A) PAT(B) Not that this denition works just as well if we have an intensional implication instead of inheritance due to equivalence between implication and inheritance (implication is used for predicates and inheritance is used for concepts). 12.6 Applying PLN To sum up: the goal underlying the theoretical development of PLN has been the creation of practical software systems carrying out complex, useful inferences based on uncertain knowledge and drawing uncertain conclusions.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 14
"Toward that end we have implemented most of the PLN theory described in the PLN book, in a PLN module incorporated in the Novamente Cognition Engine (NCE), an integrative articial intelligence software framework (Goertzel, 2006), and the OpenCog engine [Goertzel, 2008; Hart & Goertzel, 2008], an open-source offshoot of the NCE. By far the most difcult aspect of designing a PLN implementation is inference control  which is really a foundational conceptual issue rather than an implementational matter per se. The PLN framework just tells you what inferences can be drawn, it doesnt tell you what order to draw them in, in which contexts. The current PLN implementation utilizes the standard modalities of forward-chaining and backward-chaining inference control. However, the vivid presence of uncertainty throughout the PLN system makes these algorithms more challenging to use than in a standard crisp inference context.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 14
"Put simply, the search trees expand unacceptably fast, so one is almost immediately faced with the need to use clever, experience-based heuristics to perform pruning. The issue of inference control leads into deep issues related to automated reasoning and cognitive science; we briey mention some of these issues in these pages, but do not      Probabilistic Logic Networks 203 fully explore, because that would lead too far aeld from the focus of the book. In the nal chapter we visit this theme in the specic context of exploring exactly how commonsense knowledge about spatial and temporal events may help guide PLN inference to perform scalably on large stores of real-world knowledge. The practical application of PLN is still at an early stage; but so far, we have applied PLN to several areas.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 14
"We have applied it to process the output of a natural language processing subsystem, using it to combine together premises extracted from different biomedical research abstracts to form conclusions embodying medical knowledge not contained in any of the component abstracts (Goertzel et al., 2006). We have also used PLN to learn rules controlling the behavior of a humanoid agent in a 3D simulation world: for instance, PLN learns to play fetch based on simple reinforcement learning stimuli (Goertzel et al., 2007). Our current research involves extending PLNs performance in both of these areas, and bringing the two areas together by using PLN to help the NCE and OpenCog carry out complex simulation-world tasks involving a combination of physical activity and linguistic communication; and, additionally, pursuing the sorts of inferences described in this book, applying PLN to scalable inference on real-world spatiotemporal knowledge stores. 12.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 14
"7 Deploying PLN in the OpenCog System With the above comments in mind, we here briey describe how PLN has been integrated with OpenCog. OpenCog is a complex framework with a complex underlying theory, and here we will only hint at some of its key aspects. OpenCog is an open-source software framework designed to support the construction of multiple AI systems; and the current main thrust of work within OpenCog is the implementation of a specic AGI design called OpenCogPrime (OCP), which is presented in the online wikibook (Hart & Goertzel, 2008). Much of the OpenCog software code, and many of the ideas in the OCP design, have derived from the open-sourcing of aspects of the proprietary Novamente Cognition Engine, which has been described extensively in previous publications (Goertzel et al., 2004).","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 14
"The rst key entity in the OpenCog software architecture is the AtomTable, which is a repository for weighted, labeled hypergraph nodes and hyperedges. In the OpenCog implementation of PLN, the nodes and links involved in PLN are stored here. OpenCog also contains an object called the CogServer, which wraps up an AtomTable as well as (among other objects) a Scheduler that schedules a set of MindAgent objects that each      204 Real-World Reasoning (when allocated processor time by the Scheduler) carry out cognitive operations involving the AtomTable.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 14
"The essence of the OCP design consists of a specic set of MindAgents (including some carrying out various PLN inference operations) designed to work together in a collaborative way in order to create a system that carries out actions oriented toward achieving goals (where goals are represented as specic nodes in the AtomTable, and actions are represented as Procedure objects indexed by Atoms in the AtomTable, and the utility of a procedure for achieving a goal is represented by a certain set of probabilistic logical links in the AtomTable, etc.). OpenCog is still at an experimental stage but has been used for such projects as statistical language analysis, probabilistic inference, and the control of virtual agents in online virtual worlds (see opencog.org). We believe it could also be of signicant value in the many other RWR applications.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 14
"  Chapter 13 Temporal and Contextual Reasoning in PLN In this chapter we review the temporal and causal PLN relationship types and rules that are used to guide these sorts of inference in PLN, and give some simple examples to illustrate how they are used. In the following chapters we will present more elaborate examples of spatiotemporal reasoning, using these constructs and ideas. 13.1 Temporal relationship types First, the notation AtTimeLink < TV > T E means that the event E holds during T, where T is a time interval. So for example: AtTimeLink < .9,.8 > [10:March:2007, 14:March:2007] Evaluation Sick Bob means that Bob is sick with degree 0.9 at condence 0.8 from the 10th of March 2007 to the 14th of March 2007.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 15
"205      206 Real-World Reasoning The time format in the examples is arbitrary and matters little, in practice it is an integer corresponding to the number of time units  a time unit could be 10ms for instance  that have passed since a referential beginning date, the zero time. The relationships intiatedAt and terminatedAt represent respectively when an event starts and stops. So for instance the example above can be similarly expressed by: And initiatedAt < .9,.8 > 10:March:2007 Evaluation Sick Bob terminatedAt < .9,.8 > 14:March:2007 Evaluation Sick Bob Sometimes, that notation is not enough to characterize the temporal aspect of an event. For instance one may want to express that an event has started within an interval, or similarly ended within an interval. For that the temporal relationships initiatedTroughout and terminatedThroughout are used: For instance if Bob has gotten progressively sick and healed progressively too: And initiatedThroughout < .","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 15
"9,.8 > [10:March:2007, 11:March:2007] Evaluation Sick Bob terminatedThroughout < .9,.8 > [13:March:2007, 14:March:2007] Evaluation Sick Bob means that the 10th of March Bob was sick with degree 0 and then that degree progressively increased till the 11th of March. Given these primitives it is possible to express other temporal relationships like OverlapTime which represents how much 2 time intervals overlap: OverlapTime < .8 > [Monday, Wednesday]      Temporal and Contextual Reasoning in PLN 207 [Tuesday, Friday] Or During which represents how much an interval is included in another one: During < 1 > [Tuesday, Wednesday] [Monday, Friday] These two relationships can be considered as shorthands as they can be expressed using initiatedAt and terminatedAt. 13.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 15
"2 PLN Temporal Inference in Action Next we give a concrete example of PLN doing temporal inference, according to the representational mechanisms described above. Suppose a user has submitted to a logical knowledge based system a query regarding which people were in the same place as Jane last week. Suppose Susie and Jane use the same daycare center, but Jane uses it everyday, whereas Susie only uses it when she has important meetings (otherwise she works at home with her child). Suppose Susie sends a message stating that Tuesday she has a big meeting with a potential funder for her business. Inference is needed to gure out that on Tuesday shes likely to put her child in daycare, and hence (depending on the time of the meeting!) potentially to be at the same place as Jane sometime on Tuesday.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 15
"To further estimate the probability of the two women being in the same place, one has to do inference based on the times Jane usually picks up and drops off her child, and the time Susie is likely to do so based on the time of her meeting. So: how do we use PLN to infer the truth value of the proposition that Susie was at the same Place as Jane last week? Formally, in PLN notation our target theorem looks like: ThereExists $Place, $TimeInterval1, $TimeInterval2 And AtTime($TimeInterval1, AtPlace(Susie, $Place)) AtTime($TimeInterval2, AtPlace(Jane, $Place)) OverlapTime($TimeInterval1, $TimeInterval2)      208 Real-World Reasoning During($TimeInterval1, LastWeek) During($TimeInterval2, LastWeek) where atPlace is a predicate that indicates if a given person is at a given place.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 15
"Note that since temporal reasoning has not been fully implemented yet we will not include the numerical values in that example. We make the following assumptions for the purpose of the example inference: Axioms Axioms related to Jane: 1) Jane is at the daycare center everyday of the week between 7am and 7:30am and between 16pm and 16:30pm (when she brings and fetch her child). 1.a) Average $Day And IsWeekDay($Day) AtTime([$Day:7am, $Day:7:30am], AtPlace(Jane, daycare)) 1.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 15
") Average $Day And IsWeekDay($Day) AtTime([$Day:16am, $Day:16:30am], AtPlace(Jane, daycare)) Axioms related to Susie: 2) When Susie has an important meeting at time interval T, she will be in the daycare center during 30 minutes an hour before the beginning of T and after the end of T Implication AtTime(T, ImportantMeeting(Susie)) And AtTime [beginning(T)-1h, beginning(T)-1:30h] AtPlace(Susie, daycare)      Temporal and Contextual Reasoning in PLN 209 AtTime [end(T)+1h, end(T)+1:30h] AtPlace(Susie, daycare) 3) Susie had an important meeting last Tuesday between 1:30pm and 3:15pm AtTime [LastTuesday:1:30pm, LastTuesday:3:15pm]","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 15
"ImportantMeeting(Susie) Inference chain: 1) Susie was at the daycare center Tuesday between 4:15pm and 4:45pm. Using axioms 2 and 3: And AtTime [LastTuesday:12:30pm, LastTuesday:1pm] AtPlace(Susie, daycare) AtTime [LastTuesday:4:15pm, LastTuesday:4:45pm] AtPlace(Susie, daycare) Then using PLN inference rules to deal with And AtTime [LastTuesday:4:15pm, LastTuesday:4:45pm] AtPlace(Susie, daycare) 2) Jane was at the daycare center Tuesday between 4:pm and 4:30pm. Using axioms 1.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 15
"And isWeekDay(Tuesday) AtTime [LastTuesday:4pm, LastTuesday:4:45pm]      210 Real-World Reasoning AtPlace(Jane, daycare) Then using PLN inference rules to deal with And AtTime [LastTuesday:4pm, LastTuesday:4:45pm] AtPlace(Jane, daycare) 3) Then we can infer an instance of the target theorem using the conclusion of inference step 1 and 3+ other axioms related to temporal relationships And AtTime [LastTuesday:4:15pm, LastTuesday:4:45pm] AtPlace(Susie, daycare) AtTime [LastTuesday:4pm, LastTuesday:4:45pm] AtPlace(Jane, daycare) OverlapTime [LastTuesday:4:15pm, LastTuesday:4:45pm] [LastTuesday:4pm, LastTuesday:4:45pm] During [LastTuesday:4:15","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 15
"pm, LastTuesday:4:45pm] LastWeek During [LastTuesday:4pm, LastTuesday:4:45pm] LastWeek 4) And the target theorem is reached using step 3 and PLN existential quantier axioms, by setting $Place=daycare $TimeInterval1=[LastTuesday:4:15pm, LastTuesday:4:45pm], $TimeInterval2=[LastTuesday:4pm, LastTuesday:4:45pm]      Temporal and Contextual Reasoning in PLN 211 and thus concluding ThereExists $Place, $TimeInterval1, $TimeInterval2 And AtTime($TimeInterval1, AtPlace(Susie, $Place)) AtTime($TimeInterval2, AtPlace(Jane, $Place)) OverlapTime($TimeInterval1, $TimeInterval2) During($TimeInterval1, LastWeek) 13.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 15
"3 PLN Causal Relationship Types PLN represents the notion of causality with the PredictiveImplication relationship and some variants thereof. Formally PredictiveImplication is dened as follows: PredictiveImplication < TV > T A B is equal to IntensionalImplication < TV > A SequentialAnd T A B where T is a time (or a time interval) representing the delay between A and B. Which more formally can be expressed as Average < TV > t And      212 Real-World Reasoning AtTime t A AtTime t+T B 13.4 PLN Contextual Inference in Action Finally, in this section, we run the contextual inference example given in Chapter 7 above using PLN rather than the more traditional contextual inference approach explored earlier.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 15
"First we enumerate the assumed axioms, describing each one in natural language and then formally in PLN terms: Axioms for Music Context 1) In the context of music Alice frequently mentions Canadian place names Context < .5,.9 > Music Evaluation Mention List Alice CanadianPlaceNames 2) In the context of music Bob frequently mentions Canadian place names Context < .5,.9 > Music Evaluation Mention List      Temporal and Contextual Reasoning in PLN 213 Bob CanadianPlaceNames 3) In the context of music Clark does not frequently mention Canadian place names Context < .01,.9 > Music Evaluation Mention List Clark CandianPlaceNames Axioms for Accounting Context 4) In the context of accounting Alice does not frequently mention Canadian place names Context < .01,.9 > Accounting Evaluation Mention List Alice CanadianPlaceNames 5) In the context of accounting Bob frequently mentions Canadian place names Context < .5,.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 15
"9 > Accounting Evaluation Mention List Bob CanadianPlaceNames      214 Real-World Reasoning 6) In the context of accounting Clark frequently mentions Canadian place names Context < .6,.9 > Accounting Evaluation Mention List Clark CanadianPlaceNames Non-Context-Specic Axioms 7) Accounting is associated with Money ExtensionalInheritance < .7,.9 > Accounting Money 8) CanadianPlaces is associated with Canada Inheritance < .8,.9 > CanadianPeople CanadianPlacesNames 9) If X frequently mentions Y then he/she is highly involved with Y Average < .9,.8 > List $X, $Y Implication Evaluation Mention List $X $Y      Temporal and Contextual Reasoning in PLN 215 Evaluation Involved $X $Y 10) Non Canadian People involved with Canadian people in the context of Money have a chance of being associated with log trafcking activities AverageLink < .6,.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 15
"8 > ListLink $X ImplicationLink AndLink SubsetLink $X NotLink CanadianPeople ContextLink Money EvaluationLink Involved ListLink $X CanadianPeople InheritanceLink X LogTrafficking 11) Clark is not Canadian ExtensionalInheritance < .9,.9 > Clark Not CanadianPeople      216 Real-World Reasoning 12) It is also necessary to dene the truth value of the following concepts Accounting < .2,.9 > Music < .3,.9 > CanadianPlaceNames < .1,.8 > Money < .2,.9 > CanadianPeople < .25,.8 > Question to answer : What is the chance of Clark being involved with log trafcking? IntensionalInheritance <? > Clark LogTrafficking Next we show one possible inference trail via which PLNs inference rules may estimate the truth value of the target logical relationship, based on the assumption of the above axioms.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 15
"Of course, there exist many other inference trails as well, and in reality an automated PLN inference system (such as the ones implemented in the NCE or OCP AI systems) will nd many of these and produce an overall truth value formed by revising their various conclusions. However, for expositional purposes, it seems sufcient to recount a single inference trail in detail, just show how such inferences go. The problem of inference control  i.e. of how an inference engine may be guided to create inferences like this in a reasonable amount of time  will be discussed in a later chapter. This inference is a very detailed one as it actually has been directly extracted from a real run of PLN (with manually inference control). Inference trail 1) Instantiation of axiom 9, with $X=Clark and $Y=CanadianPlaceNames Implication < .9,.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 15
"9 > Evaluation Mention      Temporal and Contextual Reasoning in PLN 217 List Clark CanadianPlaceNames Evaluation Involved Clark CanadianPlaceNames 2) This step is necessary so that all TVs are correctly contextualized in the Accounting context, using rule to contextualize a context knowledge ContextLink < .9,.54 > Accounting ImplicationLink EvaluationLink Mention ListLink Clark CanadianPlaceNames EvaluationLink Involved ListLink Clark CanadianPlaceNames 3) Using Modus Ponens in the context of Accounting with return of step 1 as the implication and axiom 6 as the antecedent. ContextLink < .45,.48 > Accounting EvaluationLink Involved ListLink Clark      218 Real-World Reasoning CanadianPlaceNames 4) Decontextualize the result of step 3 ExtensionalInheritance < .45,.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 15
"48 > Accounting SatisfyingSetLink EvaluationLink Involved ListLink Clark CanadianPlaceNames 5) Apply PLN deduction rule on axiom 7 and the result of step 4 ExtensionalInheritance < .45,.48 > Money SatisfyingSetLink EvaluationLink Involved ListLink Clark CanadianPlaceNames 6) Contextualize step 5 ContextLink < .45,.48 > Money EvaluationLink Involved ListLink Clark CanadianPlaceNames      Temporal and Contextual Reasoning in PLN 219 7) Using a PLN rule substitute terms given their inheritnace relation and axiom 8 to infer how much Clark in involved with CanadianPeople in the context of Money ContextLink < .45,.3 > Money EvaluationLink Involved ListLink Clark CanadianPeople 8) Infer the conjunction of axiom 11 and the previous step AndLink < .4,.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 15
"27 > InheritanceLink Clark NotLink CanadianPeople ContextLink Money EvaluationLink Involved ListLink Clark CanadianPeople 9) Instantiate axiom 10 with $X = Clark ImplicationLink < .6,.8 > AndLink InheritanceLink Clark NotLink CanadianPeople ContextLink      220 Real-World Reasoning Money EvaluationLink Involved ListLink Clark CanadianPeople InheritanceLink Clark LogTrafficking 10) Apply modus ponens with step 9 as implication and step 8 as antecedent InheritanceLink < .24,.27 > Clark LogTrafficking So we can conclude that with probability 0.24 and condence 0.27 Clark is involved with log trafcking. One could run similar inferences for Bob and Alice and see that they have less change to be related to log trafcking.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 15
"  Chapter 14 Inferring the Causes of Observed Changes In this chapter we consider the specic question of how the ideas of the previous chapters contribute to carrying out reasoning regarding the potential causes of salient changes in large knowledge stores. The following would be three sorts of examples of change-related inference, in the Twitter domain:  Looking for changes in a particular persons patterns of social interaction (a signicant new contact, a number of casual acquaintances with similar proles, etc.), and potential causes of these changes  Looking for groups with changes in sentiment toward a certain person or organization (say, the Tory party), and potential causes of these changes  Looking for places with a signicant change in their relationship to some specic place (say, East Anglia), and potential causes of these changes Corresponding examples on other application areas, such as robotics, are obvious to formulate. In this section we give a detailed exposition of an inference regarding the rst of these example cases.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 16
"The others would be handled similarly. We will make use of the PLN logic framework here, although others could have been utilized as well. In fact no existing logic framework has been eshed out in great detail in the context of precisely this sort of application, so whatever logical formalism one chooses, in order to approach examples like this, one is going to be carrying out a certain amount of creative improvisation. Due to our prior experience with PLN we felt most comfortable carrying out this invention in this context. Specically, we consider the following scenario :  Before March 2007, Bob never had any Canadian friends except those who were also friends of his wife. 221      222 Real-World Reasoning  After March 2007, Bob started acquiring Canadian friends who were not friends of his wife.  In late 2006, Bob started collecting Pokemon cards.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 16
"Most of the new Canadian friends Bob made between March 2007 and Late 2007 are associated with Pokemon cards  In late 2006, Bob started learning French. Most of the new Canadian friends Bob made between March 2007 and Late 2007 are Quebecois. These are the sorts of patterns that might be identied via the pattern mining algorithms discussed above, for nding surprising relationships in large logical knowledge bases. The question we consider here is: suppose such a pattern has been identied, then how do we gure out what its cause might be? We will consider two cases of the above scenario, one involving temporal reasoning only, and one involving both spatial and temporal reasoning. The importance of this sort of question should be clear: it is not a matter of doing obscure analytical detective work, its a matter of guring out whether a pattern that arises from a pattern-mining algorithm is actually interesting enough to merit anyone paying attention to it.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 16
"Pattern mining algorithms tend to nd a lot of patterns, and most of them are pretty uninteresting. When a pattern arises from such an algorithm, it is worthwhile to know whether there is an obvious cause for the pattern  and if so, whether the cause is the kind of cause that is interesting to the humans who are receiving the output of the pattern mining algorithm. Thus, causal inference may actually be viewed as an integral part of the pattern mining process.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 16
"We may in fact posit a repeated process such as: (1) Mine patterns from the knowledge base, biased by a set of concepts and patterns called the focus (2) Perform causal inference to nd a set P of patterns that are signicant but have no known cause, or have causes believed to be interesting to the human users (3) If any of the patterns in P are estimated to be sufciently interesting to any of the human users, report them to these human users (4) Add these interesting patterns to the focus and return to Step 1 We have already discussed the pattern-mining portion of this process; now we turn to the causal inference aspect.      Inferring the Causes of Observed Changes 223 14.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 16
"1 The Case of Bob and His New Friends, with Temporal Inference Only In this section, we will formalize the above example in PLN, and then use PLN inference to assess the strength of a few possible causal relationships that can explain why Bob has gotten new friends from Canada apart from his wifes (after March 2007). 14.1.1 Axioms Here we will use probabilities without condences. As temporal reasoning has not been fully implemented yet we are not going to compute the actual truth values so these probabilities are just indicative. Before starting enumerating the axioms let us dene the following predicates, this will make the axioms much more readable.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 16
"1) Bobs Canadian friends Def CanadianFriendBob = AndLink EvaluationLink FriendOf ListLink Bob $X ExtensionalInheritance $X CanadianPeople 2) Bobs wife friends Def BobWifeFriend = EvaluationLink FriendOf ListLink BobWife $X      224 Real-World Reasoning 3) Canadian Friend of Bob but Not His Wife Def CanadianFriendBobNotWife = AndLink CanadianFriendBob NotLink BobWifeFriend 4) Bob collecting Pokemon cards Def BobCollectingPokemonCards = EvaluationLink Collecting ListLink Bob PokemonCards 5) Bob learning French Def BobLearningFrench = EvaluationLink Learning ListLink Bob FrenchLanguage Now we are ready to enumerate the axioms 1) Before March 2007, Bob never had any Canadian friends except those who were also friends of his wife.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 16
"AtTime < 0 >      Inferring the Causes of Observed Changes 225 Before March 2007 AverageAll $X CanadianFriendBobNotWife 2) After March 2007, Bob started acquiring Canadian friends who were not friends of his wife. InitiatedThroughout < .3 > Between March 2007 And Late 2007 AverageLink $X CanadianFriendBobNotWife 3) In late 2006, Bob started collecting Pokemon cards. InitiatedAt < .9 > Late 2006 BobCollectingPokemonCards 4) The process of collecting $Y shares associations with $Y AverageLink < .7 > $X, $Y ImplicationLink EvaluationLink Collecting ListLink $X $Y Inheritance $X $Y      226 Real-World Reasoning 5) Most of the new Canadian friends Bob made after March 2007 (who are not friends of his wife) are associated with Pokemon cards. AtTime < .","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 16
"6 > March2007 till late2007 AverageAll X ImplicationLink CanadianFriendBobNotWife InheritanceLink X PokemonCards 6) In late 2006, Bob started learning French. So we can now dene the axiom: initiatedAt < 1 > Late 2006 BobLearningFrench 7) If $X learns $Y then $X shares associations with $Y AverageLink < .7 > $X, $Y ImplicationLink Evaluation Learning List $X $Y Inheritance $X $Y      Inferring the Causes of Observed Changes 227 8) Most of the new Canadian friends Bob made after March 2007 (who are not friends of his wife) are Quebecois. AtTime < .7 > March 2007 Till Late 2007 AverageLink $X ImplicationLink CanadianFriendBobNotWife SubsetLink $X Quebecois 9) Quebecois are associated with French language InheritanceLink < .","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 16
"7 > Quebecois FrenchLanguage 14.1.2 Inference Trails We now describe three PLN inference trails, aimed at evaluating the validity of the following inference targets: Theorem 14.1. Bobs Pokemon cards interest is the cause of his new Canadian friendships: PredictiveImplicationLink <? > 3 Months To 1 Year BobCollectingPokemonCards AverageLink X CanadianFriendBobNotWife      228 Real-World Reasoning Theorem 14.2. Bob starting learning French is the cause of his new Canadian friendships: PredictiveImplicationLink <? > 3 Months To 1 Year BobLearningFrench AverageLink X CanadianFriendBobNotWife Theorem 14.3.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 16
"The conjunction of Bob starting learning French and collecting Pokemon cards is the cause of his new Canadian friendships PredictiveImplicationLink <? > 3 Months To 1 Year AndLink BobCollectingPockemonCards BobLearningFrench AverageLink X CanadianFriendBobNotWife 14.1.2.1 Target Theorem 14.1 Note that as of the time the writing, temporal reasoning is not yet fully implemented in OpenCogs PLN inference engine. So we have omitted the truth values in the inference and instead explains how such inferences would be constructed. To illustrate one path for evaluating the truth value of Theorem 14.1 using PLN, we will begin by presenting four steps that go backward from the target theorem to the axioms.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 16
"1) Target 1: Bob starting collecting Pokemon cards is the cause of his new Canadian friendships PredictiveImplicationLink <? > 3 Months To 1 Year      Inferring the Causes of Observed Changes 229 BobCollectingPokemonCards AverageLink X CanadianFriendBobNotWife 1) By the denition of PredictiveImplicationLink, the target is equivalent to ImplicationLink BobCollectingPokemonCards SequentialAnd 3 Months To 1 Year BobCollectingPokemonCards AverageLink X CanadianFriendBobNotWife To simplify, let us dene a short hand for the sequentialAnd term Def SeqAndTerm = SequentialAnd 3 Months To 1 Year BobCollectingPokemonCards AverageLink X CanadianFriendBobNotWife So the above is equivalent to ImplicationLink BobCollectingPokemonCards SeqAndTerm      230 Real-World Reasoning 2) An ImplicationLink encapsulates both extensional and intensional implication, that is the above","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 16
is equivalent to OrLink ExtensionalImplicationLink BobCollectingPokemonCards SeqAndTerm IntensionalImplicationLink BobCollectingPokemonCards SeqAndTerm 2.1) To infer the extensional part ExtensionalImplicationLink BobCollectingPokemonCards SeqAndTerm we can rewrite SeqAndTerm according to the denition of SequentialAnd (see Section 13.3). ExtensionalImplicationLink BobCollectingPokemonCards Average < TV > t And AtTime t BobCollectingPokemonCards AtTime t+3 Months To 1 Year AverageLink X CanadianFriendBobNotWife      Inferring the Causes of Observed Changes 231 2.2) Then we can evaluate that directly from the axioms 2 and 3. But since BobCollectingPokemonCards happens only once the truth value of such implication will have a very low condence.,"Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 16
"This is where intensional reasoning may come handy, when little data are available intensional inference can lead to less biased and more condent results. 3) To infer the intensional implication part one can expand it into an extensional inheritance of the set of patterns of BobCollectingPokemonCards and SeqAndTerm as explained in Section 12.5.5. 3.1) Using axiom 4, that if X collects Y then shares associations with Y, we can infer that Pokemon cards is a pattern of BobCollectingPokemonCards (assuming that the complexity of Pokemon cards is less than the one of BobCollectingPokemonCards), formally MemberLink PokemonCards PAT(BobCollectingPokemonCards) 3.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 16
"2) Next, using axiom 5 (and few more steps involving the denition of SequentialAnd that we do not detail), we can infer that Pokemon card is also a pattern of SeqAndTerm that is formally MemberLink PokemonCards PAT(SeqAndTerm) 4) Given the above we can conclude that      232 Real-World Reasoning ExtensionalInheritance PAT(BobCollectingPokemonCards PAT(SeqAndTerm) Which is equivalent to the intensional part of the predictive implication we were trying to infer. Here we have only used one possible pattern, Pokemon cards, in practice there might be hundreds or even thousands of patterns. 14.1.2.2 Target Theorem 14.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 16
"2 The inference of target 2 Bob starting learning French is the cause of his new Canadian friendships: PredictiveImplicationLink <? > 3 Months To 1 Year BobLearningFrench AverageLink X CanadianFriendBobNotWife is very similar but instead it uses axiom 6 and axiom 2 to infer the extensional implication. It uses axiom 7 to infer that FrenchLanguage is a pattern of BobLearningFrench, and axiom 9 to infer that FrenchLanguage is a pattern of Quebecois. 14.1.2.3 Target Theorem 14.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 16
"3 Then the target 3 The conjunction of Bob starting collecting Pokemon cards and learning French is the cause of his new Canadian friendships PredictiveImplicationLink <? > 3 Months To 1 Year AndLink BobCollectingPockemonCards BobLearningFrench AverageLink      Inferring the Causes of Observed Changes 233 X CanadianFriendBobNotWife is the most interesting because it involves both patterns, Pokemon cards and French language for the intensional implication part. Note that the extensional implication part will provide a condence as low as for target theorem 14.1 and 14.2 because the conjunction Bob starting collecting Pokemon cards and learning French, with respect to the data, is just as a rare event, it happens only once. But the intensional part however will have an increase of condence because 2 patterns instead of one are involved in the implication. 14.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 16
"2 Incorporating Spatial Inference into Analysis of Change In this section we consider a variation of the inference given above, modied to include spatial reasoning involving Canada and its neighbors. This is an illustration of how temporal and spatial inference rules may be combined to carry out commonsense reasoning regarding potential causes of changes in large knowledge bases. See Figure 14.1 for the map used in the example. Fig. 14.1 Canada and its neighbors      234 Real-World Reasoning We consider a query similar to one discussed above, but slightly relaxed; instead of the causes of Bobs new Canadian friendships we are interested in the causes of Bobs new friends who are Canadian and associated with Canadian; that is more, formally speaking, who inherit extensionally and/or intensionally from Canadian (instead of only extensionally as in the previous inference).","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 16
"So, the denition of Bobs Canadian friend in the previous inference is replaced by: Def CanadianFriendBob = AndLink EvaluationLink FriendOf ListLink Bob $X InheritanceLink $X CanadianPeople Then we introduce some spatial knowledge by adding the predicate near(X,Y) and a curried version of it, nearCanada(X)=near(X,Canada), represented by a fuzzy grid in Figure 14.2, where the level of opacity of each cell of the grid corresponds to the degree of its proximity to Canada. And we extract from that grid two regions, Canada and Ottawa as shown in Figure 14.3, to illustrate the use of the Region Connection Calculus (RCC) as discussed above. And nally we add an axiom that relates geographic proximity and intensional association. 14.2.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 16
"1 New Axioms We can now formalize the above knowledge in PLN (one should consider it as a set of additional axioms, to be added to the ones from the previous example, which is why they are numbered from 11): 10) Nome Alaska is not near Canada Lets rst dene the predicate nearCanada:      Inferring the Causes of Observed Changes 235 Fig. 14.2 Fuzzy Grid of the predicate near Canada Def nearCanada = Evaluation near List $X Canada and the axiom (according to the fuzzy grid of Figure 14.2): Evaluation < 0,.9 > nearCanada Nome Actually in practice this knowledge would be deduced from lower-level knowledge such as the following: Evaluation < 0,.9 >      236 Real-World Reasoning Fig. 14.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 16
"3 Canada Region in dark gray, Ottawa Region in light gray occupiesCell List Nome Cell(8,8) where the predicate occupiesCell indicates whether (or to what degree) a location occupies a cell of the grid, and using the axiom: AverageLink < 1,.9 > $X, $Y, $Z ExtensionalImplication Evaluation occupiesCell List $X Cell($Y,$Z) Evaluation nearCanada      Inferring the Causes of Observed Changes 237 $X But for the sake of simplicity we will directly use the predicate nearCanada. 11) How near McCarthy Alaska is to Canada Evaluation < .6,.9 > nearCanada McCarthy 12) How near Northern Maine is to Canada (the north of ME as indicated in the map Figure 14.1). Evaluation < .8,.9 > nearCanada NorthernMaine 13) Nearby locations have intensional similarity AverageLink < .5,.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 16
"9 > $X, $Y ExtensionalImplication Evaluation near $X $Y IntensionalSimilarity $X $Y 14) People of a given location intensionally inherit from that location AverageLink < .7,.9 > $X, $Y ExtensionalImplication Evaluation      238 Real-World Reasoning liveIn List $X $Y IntensionalInheritance $X $Y 15) Canadians live in Canada Evaluation < .9,.9 > liveIn List Canadian Canada 16) The disjunction of NTPP (Non-Tangential Proper Part) and TPP (Tangential Proper Part) is transitive (from the Region Connection Calculus dened above) Lets dene rst PP for Proper Part AverageLink < 1,.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 16
"9 > $X, $Y ExtentionalEquivalence Evaluation PP List $X $Y OR Evaluation NTPP List $X $Y      Inferring the Causes of Observed Changes 239 Evaluation TPP List $X $Y And then axioms stating the transitivity of PP AverageLink < 1,.9 > $X, $Y, $Z ExtensionalImplication And Evaluation PP List $X $Y Evaluation PP List $Y $Z Evaluation PP List $X $Z $Z 17) If someone is a proper part of a place he/she lives there AverageLink < .9,.9 > $X, $Y ExtensionalImplication      240 Real-World Reasoning Evaluation PP List $X $Y Evaluation liveIn List $X $Y 18) Jack lives in McCarthy Evaluation < .9,.9 > liveIn List Jack McCarthy 19) Jim lives in Nome Evaluation < .9,.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 16
"9 > liveIn List Jim Nome 20) John lives in Ottawa Evaluation < .9,.9 > liveIn List John Ottawa      Inferring the Causes of Observed Changes 241 21) Ottawa is a proper part of Canada (as represented in Figure 14.3) Evaluation < .9,.9 > PP List Ottawa Canada 22) Someone living in Canada is Canadian AverageLink < .8,.9 > $X ExtensionalImplication Evaluation liveIn List $X Canada ExtensionalInheritance $X Canadian And thats it for the axioms. The target theorems are unchanged from the prior example. We will not describe the entire inference here, but only the additional steps dealing with the spatial knowledge introduced and connecting them to the previous inference. What we will show is how to infer that Jack (living in McCarthy) and John (living in Ottawa) inherit from Canadian, but Jim (living further away in Nome) does not.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 16
"Then this knowledge might be used as sub-step to infer what was dened as axiom 2 in the previous inference, that is that new friends on Bob are Canadian or related to Canada. 14.2.2 Evaluating the Theorems As previously, we now have the following 3 sub-target theorems: Inheritance <? > Jack      242 Real-World Reasoning Canadian Inheritance <? > John Canadian Inheritance <? > Jim Canadian We now discuss each of these, using the new spatial information available. 14.2.2.1 Evaluation of Theorem 14.1 1) Using the denition of near and axiom 11 Evaluation < .6,.9 > near List McCarthy Canada 2) Using the conclusion of step 1 and an instantiation of axiom 13 with $X=McCarthy and $Y=Canada and the PLN deduction rule IntensionalSimilarity < .5,.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 16
"9 > McCarthy Canada 3) Using axiom 18, an instantiation of axiom 14 with $X=Jack and $Y=McCarthy and the PLN deduction rule IntensionalInheritance < .9,.9 > Jack McCarthy      Inferring the Causes of Observed Changes 243 4) Using the denition of IntensionalSimilarity and the conclusion of step 2 And < .5,.9 > IntensionalInheritance McCarthy Canada IntensionalInheritance Canada McCarthy 5) Using the conclusion of step 4 (stripping away the second term of the conjunction), the conclusion of step 3 and the PLN deduction rule IntensionalInheritance < .7,.8 > Jack Canada 6) Using axioms 15 as antecedent of the implication gotten from instantiating axiom 14 with $X=Canadian and $Y=Canada IntensionalInheritance < .67,.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 16
"9 > Canadian Canada 7) Using the conclusions of steps 5, 6 and the PLN abduction rule IntensionalInheritance < .56,.6 > Jack Canadian 8) Using the denition of Inheritance (disjunction of Extensional and Intensional inheritance) and the results of step 7 and Inheritance < .56,.6 > Jack Canadian      244 Real-World Reasoning So due to the additional items of spatial knowledge we can conclude that Jack inherits from Canadian, therefore assuming that after March 2007 Jack is a friend of Bob and not a friend of his wife  knowledge which may have an inuence in the calculation of the axiom 2 of the inference of the previous section, even though Jack isnt in fact from Canada strictly speaking.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 16
"Of course it is clear that Jack inherits from Canadian with a lower strength than a real Canadian individual because the latter extensionally inherits from Canadian, but nevertheless Jacks (mixed) inheritance from Canadian has a non null strength. A similar inference could be built for someone living in northern Maine. 14.2.2.2 Evaluation of Theorem 14.2 1) Using axiom 20 and an instantiation of axiom 17 with $X=John, $Y=Ottawa Evaluation < .9,.9 > PP List John Ottawa 2) Using step 1, axiom 21 and the axiom 16 of the transitivity of PP Evaluation < .9,.9 > PP List John Canada 3) Using step 2, axiom 17 with $X=John, $Y=Canada Evaluation < .8,.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 16
"9 > liveIn List John Canada      Inferring the Causes of Observed Changes 245 4) Using step 3 and an instantiation of axiom 22 with $X=John ExtensionalInheritance < .72,.9 > John Canadian 5) Using the denition of Inheritance (disjunction of extensional and intensional inheritance) Inheritance < .72,.9 > John Canadian So using the Region Connection Calculus within PLN, one can conclude that John is Canadian and assuming that John is a friend of Bob after March 2007 and not a friend of his wife may have a inuence over the strength of axiom 2 in the initial example. 14.2.2.3 Evaluation of Theorem 14.3 This inference is essentially identical to the inference of Theorem 1, so we will not recall it.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 16
"The only difference is that the strength of (IntensionalInheritance Jim Canadian) is null, and since Nome is not in Canada this ExtensionalInheritance is null as well, therefore Inheritance < 0,.9 > Jim Canada        ","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 16
"  Chapter 15 Adaptive Inference Control As the discussion in the above chapters has hopefully made clear, every one of the tasks involved in logic-based RWR has been carefully approached by computer scientists, using various formalisms and software prototypes. We have produced more fully eshed-out examples using PLN than other formalisms, because PLN is the inference framework were most familiar with and because we believe it most adequately integrates rich uncertainty representations with powerful inference mechanisms; but similar explorations could be produced using other inference formalisms, and of course one could also go into far greater detail than has been done above. But theres a catch, of course: None of the formalisms or prototypes described in the literature (including PLN) comes along with fully condent theoretical or empirical knowledge telling one how to deal effectively with real-world-scale data stores. This is not just a quantitative problem: its a qualitative problem.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 17
"Addressing this problem adequately requires the implementation and adoption of fundamentally new ideas, complementing existing approaches, Assuming a logic-based approach, scalability of pattern mining, query processing and analysis boils down to efcient inference control. Which of course, is in general terms a monstrously hard problem. Making inference control generally efcient across all possible large knowledge stores is almost surely impossible. The need for efcient inference control should be very clear via looking at the detailed PLN inference trails supplied above. At each step in any one of those inference trails, there were many other inferences that could have been done, aside from the ones shown. The question becomes how to choose these exact ones, or others with similar effect. If there are 20 steps in an inference, and 100 possibilities for each step (actually a terrible underestimate of the real situation), then there are 10020 possible inference trails to be theoretically considered.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 17
"If there are several thousand viable possibilities for each step, 247      248 Real-World Reasoning which is more likely the case in an inference involving a large knowledge store, then the number of possible inference trails becomes even more astronomical. Fortunately there are also many paths leading to useful conclusions, not just one  but even so, if you view it as a search problem, nding useful inference trails amidst the space of all possible ones is a lot worse than nding a few needles in a planet-sized haystack. Even without the complications of large and uncertain knowledge stores, the inference trail pruning problem is a very difcult one. This is the essential reason why automated theorem proving has not yet obsoleted human mathematicians. Automated theorem provers, using formal logic, can carry out individual inference steps adeptly (and without making the stupid mistakes that even smart humans are sometimes prone to), but in most cases they are vastly inferior to humans at choosing which inference steps to take.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 17
"Some of the biggest successes in automated theorem proving have occurred in highly restricted contexts (where there arent that many possibilities to choose from) or else using a semi-automated methodology, where a human expert intervenes to make choices at places where the AI gets confused at the variety of choices and lacks the experience to make an informed decision. So, clearly, in its fully general scope, the problem of inference control is practically unsolvable! But, does that mean the whole research programme outlined in this book is infeasible? No, because we dont need to solve the general problem. To make the programme described here work, the problem needs to be solved only in the context of commonsensical inferences involving the types of knowledge actually stored in the knowledge store. And, to put the point mathematically, its clear that real-world temporal and spatiotemporal knowledge stores have special statistical properties, which have mostly not been carefully characterized.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 17
"So, we suggest that a key focus for research going forward must be the creation of inference control mechanisms that adaptively exploit the statistical structure of real-world temporal and spatiotemporal data, so as to achieve efcient inference control over large real-world knowledge stores. 15.1 Specic Examples Requiring Adaptive Inference Control The above point may sound like a very abstract one, so to make it concrete, lets consider a couple inference steps drawn from one of the above inference trails: the proof of Theorem 1 given above. Quoting directly from there (but leaving out a small amount of explanatory text), we had the following inference step      Adaptive Inference Control 249 3) To infer the intensional implication part one can expand it into an extensional inheritance of the set of patterns of BobCollectingPokemonCards and SeqAndTerm as explained in Section 12.5.5. 3.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 17
"1) Using axiom 4, that if X collects Y then shares associations with Y, we can infer that Pokemon cards is a pattern of BobCollectingPokemonCards (assuming that the complexity of Pokemon cards is less than the one of BobCollectingPokemonCards), formally MemberLink PokemonCards PAT(BobCollectingPokemonCards) 3.2) Next, using axiom 5 (and few more steps involving the denition of SequentialAnd that we do not detail), we can infer that Pokemon card is also a pattern of SeqAndTerm that is formally MemberLink PokemonCards PAT(SeqAndTerm) 4) Given the above we can conclude that ExtensionalInheritance PAT(BobCollectingPokemonCards PAT(SeqAndTerm)      250 Real-World Reasoning The above is perfectly straightforward once youve decided to do it, but where does that decision come from? Basically the decision in moving from Step 3 to Step 4 in this inference is to expand the IntensionalImp","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 17
"lication in 3 into an ExtensionalInheritance as shown in 4. However, the utility of this step is obvious only in hindsight. One doesnt always want to handle an IntensionalImplication by expanding it into an ExtensionalInheritance. Sometimes, for instance, one might want to try to produce it via deduction from other IntensionalImplication  or via unication by binding values to variables in some other IntensionalImplication with a similar form and some overlapping terms, but a lot more free variables. One could also try to derive it using Bayes rule from some other existing IntensionalImplications. Et cetera. In many contexts, expanding an IntensionalImplication into an ExtensionalInheritance may be viewed as a tactic of last resort, because its often going to devolve into a detailed analysis of many special cases.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 17
"So in many contexts (not necessarily all) it will be an advantageous strategy to rst try to evaluate an IntensionalImplication via rst-orderinference-style rules from other IntensionalImplications, or via unication from more abstract knowledge, before resorting to the reduction to extensionality. But even if this strategy is followed, theres the question of how much effort to expend on other methods before resorting to extensionality. And this is bound to be context-dependent. For instance, it may happen that in certain contexts, reasoning purely on the level of intensional relationships has systematically proved difcult, and reductions to extensionality have proved necessary very frequently; whereas in other contexts, reduction to extensionality has rarely proved necessary. The other interesting aspect of that inference is the computation of the set of patterns of BobCollectingPokemonCards. As explained this set could be much larger and actually potentially contains all concepts of the knowledge base.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 17
"So without some kind of mechanism to restrict focus, the inference process will consume an excessive amount of computational resources evaluating irrelevant associations. An example of the kind of mechanism that can be helpful here is activation spreading (see e.g. (Collins et al. , 1975; Anderson, 1983; Crestani, 1997; Nilsson, 1998)), which is carried out in various existing AI systems using a variety of mechanisms (for instance, in neural nets it uses activation spreading; in the NCE/OpenCog architecture it uses articial economics methods (Goertzel et al., 2007); etc.).","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 17
"Using activation spreading, if all the terms involved in Steps 1-4 were to spread some sort of activation to the terms and relationships related to them, and this activation were to then further propagate to the relatives of these terms and relationships (and so forth)       Adaptive Inference Control 251 then, quite likely, PokemonCards would get more activation than nearly any other category in the overall knowledge store, and would be nominated for investigation as indicated in the above text. Note that in principle one could do the above purely by inference, without introducing an external mechanism such as activation spreading. For instance, one could do inference on links such as (Association A B) denoting generic associative semantics, using probabilistic inference rules designed for such inference. In fact the NCE/OpenCog design contains this possibility. However, unless one adopts an extremely constrained control mechanism for this associational inference, then one is faced once again here with a potential combinatorial explosion problem.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 17
"And if one does adopt an extremely restrained control mechanism here, then in effect one is basically using uncertain inferential algebra to carry out spreading activation (which may be a ne thing to do). If such an associative link exists, and there is an inference control heuristic biasing PLN toward building inferential links that generally follow the path laid down by associational links, then the above inference step would indeed emerge as obvious. We could similarly psychoanalyze every step in the above inference trails, studying the various alternatives and how they could be explored using intelligent inference control heuristics. However, we will restrict ourselves to giving two more examples, intended to illustrate the way in which commonsense knowledge about time and space is important for inference control. 15.1.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 17
"1 Using Commonsense Knowledge about Space in Inference Control As an example of using commonsense knowledge about spatial relations to control inference, let us rst recall the following axiom from the above spatial PLN inference: 17) If someone is a proper part of a place he/she lives there AverageLink < .9,.9 > $X, $Y ExtensionalImplication Evaluation PP List $X $Y      252 Real-World Reasoning Evaluation liveIn List $X $Y This axiom was used in the following inference step: 2) Using step 1, axiom 21 and the axiom 16 of the transitivity of PP Evaluation < .9,.9 > PP List John Canada 3) Using step 2, axiom 17 with $X=John, $Y=Canada Evaluation < .8,.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 17
"9 > liveIn List John Canada The question here pertaining to inference control is: why would a PLN inference engine choose to make this step rather than some other one? Obviously, not every proper-part relationships is going to be equally promising for hypothetical transformation into a live-in relationship. The answer here however is conceptually obvious: where people and places are concerned, living-in is a common variety of proper-part relationship. This conceptually obvious answer may however be expressed formally in a number of different ways. It could be expressed explicitly as an implication, e.g. 17) If someone is a proper part of a place he/she lives there AverageLink < .9,.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 17
"9 > $X, $Y ExtensionalImplication      Adaptive Inference Control 253 And Inheritance $X Human Inheritance $Y Place Evaluation PP List $X $Y Evaluation liveIn List $X $Y In this case, this more specialized axiom 17 could be used instead of Axiom 17. It would be more likely to be chosen if there were an inference control heuristic in use stating that: The weight assigned to an application of the variable unication rule between a more general Atom A and a more specic Atom B, should be proportional to how surprising it is to nd Atoms that bind with A as well as B does. In this case, the point would be that the assignment $X=John, $Y=Canada matches 17 surprisingly well (relative to most possibly assignments); and that it matches 17 more surprisingly well than it matches 17. However, the same effect could be achieved via an activation spreading mechanism.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 17
"All one needs is for associative linkages such as the following to exist:  Association John Human  Association Canada Place  Association Human liveIn  Association Place liveIn Of course many other variants are also possible; for instance one could have Inheritance relations in place of the rst two Association relations in the above list; or one could have an association Association (Human And Place) liveIn      254 Real-World Reasoning and so forth. This is a simpler approach than the approach of introducing 17 in place of 17, but ultimately, in this context, the two achieve the same thing. One way or another, the key here is that the system has specialized knowledge about the connections between spatial relationships (ProperPart in this case) and specic content (people living in places, in this case), and that it deploys this specialized knowledge to guide it along the right inference trail, thus avoiding the combinatorial explosion of getting dragged down irrelevant trajectories. 15.1.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 17
"2 Using Commonsense Knowledge about Time in Inference Control Time plays a role quite similar to space in inference control, yet even more fundamental. Without inference control heuristics that specically take into account the habitual connections between temporal relationships and specic content, doing scalable temporal reasoning is not likely to be possible. Let us consider the following inference step, drawn from Step 2.1 in Theorem 1 in Section 14.1.2.1 above To infer the extensional part ExtensionalImplicationLink BobCollectingPokemonCards SeqAndTerm we can rewrite SeqAndTerm according to the denition of SequentialAnd (see Section 13.3).","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 17
"ExtensionalImplicationLink BobCollectingPokemonCards Average < TV > t And AtTime t BobCollectingPokemonCards AtTime t+3 Months To 1 Year AverageLink      Adaptive Inference Control 255 X CanadianFriendBobNotWife The question here, control-wise, is why the decision would be made to expand the SequentialAnd into its ner grained denition. Again this should be a last resort choice because computing the Average of the above formula might be quite expensive. One may perhaps infer it using some less expensive rst-order deduction rules, or some general temporal pattern to infer a more specic one.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 17
"For instance, given SequentialAnd Evaluation TurnOn ($X, CarOf($X)) Evaluation Drive ($X, CarOf($X)) on may infer SequentialAnd Evaluation TurnOn (Ben, CarOf(Ben)) Evaluation Drive (Ben, CarOf(Ben)) To address this combinatorial explosion problem again one can use some assumptions to guess beforehand what temporal relationships are more likely to occur. And example of such assumptions could be: Events of a given temporal scale relate more often to events of the same temporal scale. For instance SleepingTonight might show a strong correlation with BeingInShapeTomorrow. But one would consider absurd to temporally correlate SleepingTonight and BeingInShapeNextYear, or even more absurd SleepingTonight and IraqWar. An even better assumption might be that Events of a given spatio-temporal scale relate more often to events of the same spatiotemporal scale.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 17
"One could then represent that knowledge using Association as explained above which would translate into more activation for events belonging to a given spatio-temporal time scale.      256 Real-World Reasoning There are of course more assumptions one could nd and use. This specic knowledge embodies the particular statistics of real-world knowledge stores, which is fortunately not the same as the statistics of a random knowledge store, or else real-world inference would not be feasible. 15.2 General Issues Raised by the Above Examples A key point we wish to emphasize is that the full, realistic problem of inference control is not encountered when one runs inference engines as toy systems or prototypes. If one adds a relatively small number of knowledge into an inference engine, its not terribly hard to supply it with simple, general inference control heuristics allowing it to do inferences like the examples given in previous chapters.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 17
"The really hard part comes when you couple an inference engine with a massive knowledge store, because then the problem of drawing inferences becomes inextricably tangled up with the problem of guring out which data items are relevant enough to be sensibly used in a given inference. To handle the scalable inference control problem, we have hypothesized, will require a combination of specialized heuristics (such as the ones described in the immediately preceding sections) and the integration of inference with non-inferential methods such as activation spreading. But whether or not this hypothesis is correct, what is plain is that any logic-based approach that is going to handle large knowledge stores (especially large, complex spatiotemporal knowledge stores) is going to need to deal with this problem somehow  and there is not much current research explicitly addressing this area. As noted above, the idea of exploiting the statistics implicit in real-world knowledge stores arises here implicitly.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 17
"For instance, if drawing inferences that follow the lines of associations works effectively, this is an example of special statistics  in a general, mathematically random knowledge store, this kind of heuristic would provide little if any value. And if there are certain contexts in which, systematically, reducing intensional relations to extensional ones is differentially more useful than in the average context  again, this is an example of special statistics that would not likely occur in a random knowledge store. To the extent that appropriate heuristics and design principles can be created, implicitly or explicitly representing systematic biases that tend to be present in real-world data, the inference control problem will be solvable.      Adaptive Inference Control 257 15.2.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 17
"1 Inference Control and Cognitive Architectures Work in the area of cognitive architecture also has some relevance here; architectures like SOAR (Wray & Jones, 2005) and ACT-R (Stewart & West, 2006) are specically aimed at processing information in a way that avoids combinatorial explosions via restricting cognitions attention to information items that are directly relevant to the tasks at hand. Underlying such architectures is the idea of implicitly embedding assumptions about the statistical regularities of the real world in the cognitive architecture itself. However, these architectures have not yet been neither applied in a really scalable way, nor have they been tested in combination with highly powerful formal inference systems. SOAR has been used in conjunction with real-world spatiotemporal data in some contexts, e.g. the TacAir ight simulator project (Jones et al., 1993), which is promising.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 17
"So it seems that, to some extent, the nal solution to the all-important inference control problem may involve a fusion of ideas from the temporal, spatial and uncertain logic literatures with ideas from the cognitive architecture literature. This is one way of describing aspects of the direction weve taken in our own work with the NCE and OpenCog; but of course its an idea that can be eshed out in many different ways. However, the elaboration of the ways that various cognitive architectures might enable effective scalable inference control would lead us too far beyond the scope of this book. 15.3 Inference Control in the OpenCog Cognitive Architecture We end this chapter with some brief remarks about the specic avor that adaptive inference control takes in the OpenCog cognitive architecture, within which PLN is currently embedded. 15.3.1 Activation Spreading and Inference Control in OpenCog A number of our remarks on inference control above mentioned activation spreading.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 17
"This is a general notion with a long history in cognitive science and AI, which may be implemented in many different ways. In this section we will briey describe one such way, ECAN, which is implemented in the OpenCog framework and is currently being integrated with the PLN inference engine. ECAN, or Economic Attention Networks, constitutes a novel method for simultaneously storing memories and allocating resources in AI systems. It bears some resemblance to the spread of activation in attractor neural networks, but differs via explicitly differen     258 Real-World Reasoning tiating two kinds of activation (Short Term Importance, related to processor allocation; and Long Term Importance, related to memory allocation), and in using equations that are based on ideas from economics rather than approximative neural modeling. An ECAN is a graph, consisting of untyped nodes and links, and also links that may be typed either HebbianLink or InverseHebbianLink.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 17
"It is also useful sometimes to consider ECANs that extend the traditional graph formalism and involve links that point to links as well as to nodes. The term Atom will be used to refer to nodes and links collectively. Each Atom in an ECAN is weighted with two numbers, called STI (short-term importance) and LTI (long-term importance). Each Hebbian or InverseHebbian link is weighted with a probability value. The equations of an ECAN explain how the STI, LTI and Hebbian probability values get updated over time. The metaphor underlying these equations is the interpretation of STI and LTI values as (separate) articial currencies. The motivation for this metaphor has been elaborated somewhat in (Goertzel et al., 2007) and will not be recapitulated here.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 17
"The fact that STI (for instance) is a currency means that the total amount of STI in the system is conserved (except in unusual instances where the ECAN controller decides to introduce ination or deation and explicitly manipulate the amount of currency in circulation), a fact that makes the dynamics of an ECAN dramatically different than that of, say, an attractor neural network (in which there is no law of conservation of activation). Conceptually, the STI value of an Atom is interpreted to indicate the immediate urgency of the Atom to the ECAN at a certain point in time; whereas the LTI value of an Atom indicates the amount of value the ECAN perceives in the retention of the Atom in memory (RAM). An ECAN will often be coupled with a Forgetting process that removes low-LTI Atoms from memory according to certain heuristics.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 17
"STI and LTI values will generally vary continuously, but the ECAN equations we introduce below contain the notion of an AttentionalFocus (AF), consisting of those Atoms in the ECAN with the highest STI value. The AF is given its meaning by the existence of equations that treat Atoms with STI above a certain threshold differently. Conceptually, the probability value of a HebbianLink from A to B is the odds that if A is in the AF, so is B; and correspondingly, the InverseHebbianLink from A to B is weighted with the odds that if A is in the AF, then B is not. A critical aspect of the ECAN equations is that Atoms periodically spread their STI and LTI to other Atoms that connect to them via Hebbian and InverseHebbianLinks; this is the ECAN analogue of activation spreading in neural networks.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 17
"     Adaptive Inference Control 259 In an OpenCog context, ECAN consists of a set of Atom types, and then a set of MindAgents carrying out ECAN operations such as HebbianLinkUpdating and ImportanceUpdating. OCP also requires many other MindAgents carrying out other cognitive processes such as probabilistic logical inference according to the PLN system (Goertzel, 2008) and evolutionary procedure learning according to the MOSES system (Looks, 2006). The interoperation of the ECAN MindAgents with these other MindAgents is a subtle issue that will be briey discussed in the nal section of the chapter, but the crux is simple to understand. The CogServer is understood to maintain a kind of central bank of STI and LTI funds.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 17
"When a non-ECAN MindAgent nds an Atom valuable, it sends that Atom a certain amount of Stimulus, which results in that Atoms STI and LTI values being increased (via equations to be presented below, that transfer STI and LTI funds from the CogServer to the Atoms in question). Then, the ECAN ImportanceUpdating MindAgent carries out multiple operations, including some that transfer STI and LTI funds from some Atoms back to the CogServer  keeping the ow of money going. All this represents one among many possible ways of implementing the general notion of activation spreading mentioned in the above inference control examples. A key point is that making activation spreading work well for inference control will likely require extremely tight integration between the inference engine and the association-spreading engine; and the OpenCog approach presents one way of providing such integration. 15.3.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 17
"2 Working around the Frame Problem via Integrative AGI We noted in Chapter 2 that nonmonotonic logic is not the only route for circumventing the frame problem; and in fact in our own work with PLN in the NCE and OpenCog, we have taken a signicantly different approach. Our own approach involves two key ingredients:  Heavy use of activation spreading to guide inference, as illustrated in several of the examples given in the previous section. This mitigates against a reasoning system actually spending its time doing inferences that are useless because they pertain to assumptions that are implicit and considered obvious. The idea is that these background assumptions just dont get stimulated with much juice (which may formally be an activation level, an importance value, or take some other form depending on the AI system in question; in NCE or OCP it is a ShortTermImportance currency value, as briey discussed above).","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 17
"     260 Real-World Reasoning  Use of a hierarchical ontology within inference control, to provide a form of default logic that is contained in the inference control mechanism rather than, in nonmonotonic logic, in the logical formalism itself. Effective use of the hierarchical ontology relies on the existence of a robust activation spreading mechanism, in a way that will be described below. To exemplify the notion of default inheritance, consider again the case of penguins, which do not y, although they are a subclass of birds, which do y. When one discovers a new type of penguin, say an Emperor penguin, one reasons initially that they do not y  i.e., one reasons by reference to the new types immediate parent in the ontological hierarchy, rather than its grandparent. In standard default logic frameworks, the notion of hierarchy is primary and default inheritance is wired in at the inference rule level.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 17
"But this is not the case with PLN  in PLN, correct treatment of default inheritance must come indirectly out of other mechanisms, this can be achieved in a fairly simple and natural way. Consider the two inferences A) Implication penguin fly < 0 > Implication bird penguin < .02 > Implication bird fly B) Implication penguin bird < 1 > Implication bird fly < .9 > Implication penguin fly The correct behavior in these cases, according to the default inheritance idea is that, in a system that already knows at least a moderate amount about the ight behavior of birds and penguins, inference A should be accepted but inference B should not. That is, evidence about penguins should be included in determining whether birds can y  even if there is already some general knowledge about the ight behavior of birds in the system.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 17
"But evidence about birds in general should not be included in estimating whether penguins      Adaptive Inference Control 261 can y, if there is already at least a moderate level of knowledge that in fact penguins are atypical birds in regard to ight. But how can the choice of A over B be motivated in terms of PLN theory? The essence of the answer is simple: in case B the independence assumption at the heart of the deduction rule is a bad one. Within the scope of birds, being a penguin and being a ier are not at all independent. On the other hand, looking at A, we see that within the scope of penguins, being a bird and being a ier are independent. So the reason B is ruled out is that if there is even a moderate amount of knowledge about the truth-value of (Inheritance penguin y), this gives a hint that applying the deduction rules independence assumption in this case is badly wrong.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 17
"On the other hand, what if a mistake is made and the inference B is done anyway? In this case the outcome could be that the system erroneously increases its estimate of the strength of the statement that penguins can y. On the other hand, the revision rule may come to the rescue here. If the prior strength of (Inheritance penguin y) is 0, and inference B yields a strength of .9 for the same proposition, then the special case of the revision rule that handles wildly different truth-value estimates may be triggered. If the 0 strength has much more condence attached to it than the .9, then they wont be merged together, because it will be assumed that the .9 is an observational or inference error. Either the .9 will be thrown out, or it will be provisionally held as an alternate, non-merged, low-condence hypothesis, awaiting further validation or refutation.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 17
"What is more interesting, however, is to consider the implications of the default inference notion for inference control. It seems that the following may be a valuable inference control heuristic: 1. Arrange terms in a hierarchy; e.g., by nding a spanning DAG of the terms in a knowledge base, satisfying certain criteria (e.g., maximizing total strengthcondence within a xed limitation on the number of links). 2. When reasoning about a term, rst do deductive reasoning involving the terms immediate parents in the hierarchy, and then ascend the hierarchy, looking at each hierarchical level only at terms that were not visited at lower hierarchical levels. This is precisely the default reasoning idea  but the key point is that in PLN it lives at the level of inference control, not inference rules or formulas. In PLN, default reasoning is a timesaving heuristic, not an elementary aspect of the logic itself.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 17
"Rather, the practical viability of the default-reasoning inference-control heuristic is a consequence of various other elementary aspects of the logic, such as the ability to detect dependencies rendering      262 Real-World Reasoning the deduction rule inapplicable, and the way the revision rule deals with wildly disparate estimates. One way to embody the above ideas in concrete AI system design is to dene a notion of OntologicalInheritance, based on the spanning DAG mentioned above. One can build a default logic based on the spanning DAG G, as follows. Suppose X lies below A in the DAG G. And, suppose that, for predicate F, we have F(A) < TV > meaning that F applies to A with truth value tv. Then, we may say that If  F(X) is not known, then F(X). In other words, we may assume by default that X possesses the properties of the terms above it in the hierarchy D.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 17
"More formally, we might propose a default inference rule such as: Implication And OntologicalInheritance X A Evaluation F A Not Known( Not (Evaluation F X ) ) Evaluation F X There is no reason that a rule like this cant be implemented within PLN. Note that implementing this rule within PLN gives you something nice, which is that all the relationships involved (OntologicalInheritance, Known, Not, etc.) may be probabilistically quantied, so that the outcome of the inference rule may be probabilistically quantied. The Known predicate is basically the K predicate from standard epistemic logic (commonly denoted Ka, where a is the reasoning system itself in this case) (Fagin et al., 2003; Rescher, 2005). The hierarchy G needs to be periodically rebuilt as it is based on abstracting a DAG from a graph of probabilistic logical relations.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 17
"And, the results of the above default inference rule may be probabilistic and may be merged with the results of other inference rules. The results from using this rule, in principle, should not be so different from if the rule were not present. However, the use of an ontological hierarchy in this way leads to a completely different dynamics of inference control, which in practice will lead to different results.      Adaptive Inference Control 263 And, note nally that for this sort of ontological approach can also be used in the context of activation spreading. In this case, it may take the form (for example) of ontology-based pruning of Assocation relationships.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 17
"The Association between pigeon and yer may be pruned because it is essentially redundant with a set of two Association relationships that are implicit in the hierarchy: Association pigeon bird Association pigeon flyer Using an ontology-pruned set of Association links to guide inference that is partially based on ontology-driven default inference rules, provides an alternative approach to solving the frame problem, that doesnt require introducing a notion of hierarchy into the underlying logic. In this section we have already, perhaps, ventured too far from the focus of this book, which is logic, into the domain of integrative cognitive architecture. But we have done so with a purpose  to illustrate our belief that, in fact, the most likely route to effective control of RWR lies in integrative cognitive architecture, i.e. in mixing up reasoning with other cognitive processes that are not explicitly inferential.","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 17
"This may be done in different ways within different cognitive architectures, and is in our view an extremely important subject of research in the quest for effective RWR for both AGI and near-term application purposes.        ","Real-World Reasoning Toward Scalable, Uncertain Spatiotemporal,  Contextual and Causal Inference",chapter 17
"  Chapter 1 Introduction 1.1 AI Returns to Its Roots Our goal in this book is straightforward, albeit ambitious: to present a conceptual and technical design for a thinking machine, a software program capable of the same qualitative sort of general intelligence as human beings. Its not certain exactly how far the design outlined here will be able to take us, but it seems plausible that once fully implemented, tuned and tested, it will be able to achieve general intelligence at the human level and in some respects beyond. Our ultimate aim is Articial General Intelligence construed in the broadest sense, including articial creativity and articial genius. We feel it is important to emphasize the extremely broad potential of Articial General Intelligence systems. The human brain is not built to be modied, except via the slow process of evolution. Engineered AGI systems, built according to designs like the one outlined here, will be much more susceptible to rapid improvement from their initial state.",Engineering General  Intelligence Part 1,chapter 1
"It seems reasonable to us to expect that, relatively shortly after achieving the rst roughly human-level AGI system, AGI systems with various sorts of beyond-human-level capabilities will be achieved. Though these long-term goals are core to our motivations, we will spend much of our time here explaining how we think we can make AGI systems do relatively simple things, like the things human children do in preschool. Chapter 3 describes a thought-experiment involving a robot playing with blocks, responding to the request Build me something I havent seen before. We believe that preschool creativity contains the seeds of, and the core structures and dynamics underlying, adult human level genius... and new, as yet unforeseen forms of articial innovation. Much of the book focuses on a specic AGI architecture, which we call CogPrime, and which is currently in the midst of implementation using the OpenCog software framework.",Engineering General  Intelligence Part 1,chapter 1
"CogPrime is large and complex and embodies a host of specic decisions regarding the various aspects of intelligence. We dont view CogPrime as the unique path to advanced AGI, nor as the ultimate end-all of AGI research. We feel condent there are multiple possible paths to advanced AGI, and that in following any of B. Goertzel et al., Engineering General Intelligence, Part 1, 1 Atlantis Thinking Machines 5, DOI: 10.2991/978-94-6239-027-0_1,  Atlantis Press and the authors 2014      2 1 Introduction these paths, multiple theoretical and practical lessons will be learned, leading to modications of the ideas possessed while along the early stages of the path. But our goal here is to articulate one path that we believe makes sense to follow, one overall design that we believe can work. 1.",Engineering General  Intelligence Part 1,chapter 1
"2 AGI Versus Narrow AI An outsider to the AI eld might think this sort of book commonplace in the research literature, but insiders know thats far from the truth. The eld of Articial Intelligence (AI) was founded in the mid 1950s with the aim of constructing thinking machinesthat is, computer systems with human-like general intelligence, including humanoid robots that not only look but act and think with intelligence equal to and ultimately greater than human beings. But in the intervening years, the eld has drifted far from its ambitious roots, and this book represents part of a movement aimed at restoring the initial goals of the AI eld, but in a manner powered by new tools and new ideas far beyond those available half a century ago.",Engineering General  Intelligence Part 1,chapter 1
"After the rst generation of AI researchers found the task of creating humanlevel AGI very difcult given the technology of their time, the AI eld shifted focus toward what Ray Kurzweil has called narrow AIthe understanding of particular specialized aspects of intelligence; and the creation of AI systems displaying intelligence regarding specic tasks in relatively narrow domains. In recent years, however, the situation has been changing. More and more researchers have recognized the necessityand feasibilityof returning to the original goals of the eld. In the decades since the 1950s, cognitive science and neuroscience have taught us a lot about what a cognitive architecture needs to look like to support roughly humanlike general intelligence. Computer hardware has advanced to the point where we can build distributed systems containing large amounts of RAM and large numbers of processors, carrying out complex tasks in real time. The AI eld has spawned a host of ingenious algorithms and data structures, which have been successfully deployed for a huge variety of purposes.",Engineering General  Intelligence Part 1,chapter 1
"Due to all this progress, increasingly, there has been a call for a transition from the current focus on highly specialized narrow AI problem solving systems, back to confronting the more difcult issues of human level intelligence and more broadly articial general intelligence (AGI). Recent years have seen a growing number of special sessions, workshops and conferences devoted specically to AGI, including the annual BICA (Biologically Inspired Cognitive Architectures) AAAI Symposium, and the international AGI conference series (one in 2006, and annual since 2008). And, even more exciting, as reviewed in Chap.6, there are a number of contemporary projects focused directly and explicitly on AGI (sometimes under the name AGI, sometimes using related terms such as Human Level Intelligence). In spite of all this progress, however, we feel that no one has yet clearly articulated a detailed, systematic design for an AGI, with potential to yield general intelligence at the human level and ultimately beyond.",Engineering General  Intelligence Part 1,chapter 1
"In this spirit, our main goal in this lengthy      1.2 AGI Versus Narrow AI 3 two-part book is to outline a novel design for a thinking machinean AGI design which we believe has the capability to produce software systems with intelligence at the human adult level and ultimately beyond. Many of the technical details of this design have been previously presented online in a wikibook [Goewiki]; and the basic ideas of the design have been presented briey in a series of conference papers [GPSL03, GPPG06, Goe09c]. But the overall design has not been presented in a coherent and systematic way before this book. In order to frame this design properly, we also present a considerable number of broader theoretical and conceptual ideas here, some more and some less technical in nature. 1.",Engineering General  Intelligence Part 1,chapter 1
"3 CogPrime The AGI design presented here has not previously been granted a name independently of its particular software implementations, but for the purposes of this book it needs one, so weve christened it CogPrime. This ts with the name OpenCogPrime that has already been used to describe the software implementation of CogPrime within the open-source OpenCog AGI software framework. The OpenCogPrime software, right now, implements only a small fraction of the CogPrime design as described here. However, OpenCog was designed specically to enable efcient, scalable implementation of the full CogPrime design (as well as to serve as a more general framework for AGI R&D); and work currently proceeds in this direction, though there is a lot of work still to be done and many challenges remain.1 The CogPrime design is more comprehensive and thorough than anything that has been presented in the literature previously, including the work of others reviewed in Chap6.",Engineering General  Intelligence Part 1,chapter 1
"It covers all the key aspects of human intelligence, and explains how they interoperate and how they can be implemented in digital computer software. Volume 5 of this work outlines CogPrime at a high level, and makes a number of more general points about articial general intelligence and the path thereto; then Part 2 digs deeply into the technical particulars of CogPrime. Even Part 2, however, doesnt explain all the details of CogPrime that have been worked out so far, and it denitely doesnt explain all the implementation details that have gone into designing and building OpenCogPrime. Creating a thinking machine is a large task, and even the intermediate level of detail takes up a lot of pages. 1 This brings up a terminological note: At several places in this Volume and the next we will refer to the current CogPrime or OpenCog implementation; in all cases this refers to OpenCog as of late 2013.",Engineering General  Intelligence Part 1,chapter 1
"We realize the risk of mentioning the state of our software system at time of writing: for future readers this may give the wrong impression, because if our project goes well, more and more of CogPrime will get implemented and tested as time goes on (e.g. within the OpenCog framework, under active development at time of writing). However, not mentioning the current implementation at all seems an even worse course to us, since we feel readers will be interested to know which of our ideasat time of writinghave been honed via practice and which have not. Online resources such as http://www.opencog.org may be consulted by readers curious about the current state of the main OpenCog implementation; though in future forks of the code may be created, or other systems may be built using some or all of the ideas in this book, etc.      4 1 Introduction 1.",Engineering General  Intelligence Part 1,chapter 1
"4 The Secret Sauce There is no consensus on why all the related technological and scientic progress mentioned above has not yet yielded AI software systems with human-like general intelligence (or even greater levels of brilliance!). However, we hypothesize that the core reason boils down to the following three points:  Intelligence depends on the emergence of certain high-level structures and dynamics across a systems whole knowledge base;  We have not discovered any one algorithm or approach capable of yielding the emergence of these structures;  Achieving the emergence of these structures within a system formed by integrating a number of different AI algorithms and structures requires careful attention to the manner in which these algorithms and structures are integrated; and so far the integration has not been done in the correct way. The human brain appears to be an integration of an assemblage of diverse structures and dynamics, built using common components and arranged according to a sensible cognitive architecture.",Engineering General  Intelligence Part 1,chapter 1
"However, its algorithms and structures have been honed by evolution to work closely togetherthey are very tightly inter-adapted, in the same way that the different organs of the body are adapted to work together. Due to their close interoperation they give rise to the overall systemic behaviors that characterize human-like general intelligence. We believe that the main missing ingredient in AI so far is cognitive synergy: the tting-together of different intelligent components into an appropriate cognitive architecture, in such a way that the components richly and dynamically support and assist each other, interrelating very closely in a similar manner to the components of the brain or body and thus giving rise to appropriate emergent structures and dynamics. This leads us to one of the central hypotheses underlying the CogPrime approach to AGI: that the cognitive synergy ensuing from integrating multiple symbolic and subsymbolic learning and memory components in an appropriate cognitive architecture and environment, can yield robust intelligence at the human level and ultimately beyond.",Engineering General  Intelligence Part 1,chapter 1
"The reason this sort of intimate integration has not yet been explored much is that its difcult on multiple levels, requiring the design of an architecture and its component algorithms with a view toward the structures and dynamics that will arise in the system once it is coupled with an appropriate environment. Typically, the AI algorithms and structures corresponding to different cognitive functions have been developed based on divergent theoretical principles, by disparate communities of researchers, and have been tuned for effective performance on different tasks in different environments. Making such diverse components work together in a truly synergetic and cooperative way is a tall order, yet we believe that thisrather than some particular algorithm, structure or architectural principleis the secret sauce needed to create human-level AGI based on technologies available today.      1.5 Extraordinary Proof? 5 1.",Engineering General  Intelligence Part 1,chapter 1
"5 Extraordinary Proof? There is a saying that extraordinary claims require extraordinary proof and by that standard, if one believes that having a design for an advanced AGI is an extraordinary claim, this book must be rated a failure. We dont offer extraordinary proof that CogPrime, once fully implemented and educated, will be capable of human-level general intelligence and more. It would be nice if we could offer mathematical proof that CogPrime has the potential we think it does, but at the current time mathematics is simply not up to the job. Well pursue this direction briey in Chap. 8 and other chapters, where well clarify exactly what kind of mathematical claim CogPrime has the potential for human-level intelligence turns out to be. Once this has been claried, it will be clear that current mathematical knowledge does not yet let us evaluate, or even fully formalize, this kind of claim.",Engineering General  Intelligence Part 1,chapter 1
"Perhaps one day rigorous and detailed analyses of practical AGI designs will be feasibleand we look forward to that daybut its not here yet. Also, it would of course be profoundly exciting if we could offer dramatic practical demonstrations of CogPrimes capabilities. We do have a partial software implementation, in the OpenCogPrime system, but currently the things OpenCogPrime does are too simple to really serve as proofs of CogPrimes power for advanced AGI. We have used some CogPrime ideas in the OpenCog framework to do things like natural language understanding and data mining, and to control virtual dogs in online virtual worlds; and this has been very useful work in multiple senses. It has taught us more about the CogPrime design; it has produced some useful software systems; and it constitutes fractional work building toward a full OpenCog based implementation of CogPrime.",Engineering General  Intelligence Part 1,chapter 1
"However, to date, the things OpenCogPrime has done are all things that could have been done in different ways without the CogPrime architecture (though perhaps not as elegantly nor with as much room for interesting expansion). The bottom line is that building an AGI is a big job. Software companies like Microsoft spend dozens to hundreds of man-years building software products like word processors and operating systems, so it should be no surprise that creating a digital intelligence is also a relatively large-scale software engineering project. As time advances and software tools improve, the number of man-hours required to develop advanced AGI gradually decreasesbut right now, as we write these words, its still a rather big job. In the OpenCogPrime project we are making a serious attempt to create a CogPrime based AGI using an open-source development methodology,withtheopen-sourceLinuxoperatingsystemasoneofourinspirations.",Engineering General  Intelligence Part 1,chapter 1
"But the open-source methodology doesnt work magic either, and it remains a large project, currently at an early stage. I emphasize this point so that readers lacking software engineering expertise dont take the currently fairly limited capabilities of OpenCogPrime as somehow a damning indictment of the potential of the CogPrime design.Thedesignisonething,theimplementationanotherandtheOpenCogPrime implementation currently encompasses perhaps one third to one half of the key ideas in this book.      6 1 Introduction So we dont have extraordinary proof to offer. What we aim to offer instead are clearly-constructed conceptual and technical arguments as to why we think the CogPrime design has dramatic AGI potential. It is also possible to push back a bit on the common intuition that having a design for human-level AGI is such an extraordinary claim.",Engineering General  Intelligence Part 1,chapter 1
"It may be extraordinary relative to contemporary science and culture, but we have a strong feeling that the AGI problem is not difcult in the same ways that most people (including most AI researchers) think it is. We suspect that in hindsight, after human-level AGI has been achieved, people will look back in shock that it took humanity so long to come up with a workable AGI design. As youll understand once youve nished Part 1 of the book, we dont think general intelligence is nearly as extraordinary and mysterious as its commonly made out to be. Yes, building a thinking machine is hardbut humanity has done a lot of other hard things before. It may seem difcult to believe that human-level general intelligence could be achieved by something as simple as a collection of algorithms linked together in an appropriate way and used to control an agent.",Engineering General  Intelligence Part 1,chapter 1
"But we suggest that, once the rst powerful AGI systems are produced, it will become apparent that engineering human-level minds is not so profoundly different from engineering other complex systems. All in all, well consider the book successful if a signicant percentage of openminded, appropriately-educated readers come away from it scratching their chins and pondering: Hmm. You know, that just might work. and a small percentage come away thinking Now thats an initiative Id really like to help with!. 1.6 Potential Approaches to AGI In principle, there is a large number of approaches one might take to building an AGI, starting from the knowledge, software and machinery now available. This is not the place to review them in detail, but a brief list seems apropos, including commentary on why these are not the approaches we have chosen for our own research.",Engineering General  Intelligence Part 1,chapter 1
"Our intent here is not to insult or dismiss these other potential approaches, but merely to indicate why, as researchers with limited time and resources, we have made a different choice regarding where to focus our own energies. 1.6.1 Build AGI from Narrow AI Most of the AI programs around today are narrow AI programsthey carry out one particular kind of task intelligently. One could try to make an advanced AGI by combining a bunch of enhanced narrow AI programs inside some kind of overall framework.      1.6 Potential Approaches to AGI 7 However, were rather skeptical of this approach because none of these narrow AI programs have the ability to generalize across domainsand we dont see how combining them or extending them is going to cause this to magically emerge. 1.6.2 Enhancing Chatbots One could seek to make an advanced AGI by taking a chatbot, and trying to improve its code to make it actually understand what its talking about.",Engineering General  Intelligence Part 1,chapter 1
"We have some direct experience with this route, as in 2010 our AI consulting rm was contracted to improve Ray Kurzweils online chatbot Ramona. Our new Ramona understands a lot more than the previous Ramona version or a typical chatbot, due to using Wikipedia and other online resources, but still its far from an AGI. A more ambitious attempt in this direction was Jason Hutchens a-i.com project, which sought to create a human child level AGI via development and teaching of a statistical learning based chatbot (rather than the typical rule-based kind). The difculty with this approach, however, is that the architecture of a chatbot is fundamentally different from the architecture of a generally intelligent mind. Much of whats important about the human mind is not directly observable in conversations, so if you start from conversation and try to work toward an AGI architecture from there, youre likely to miss many critical aspects. 1.6.",Engineering General  Intelligence Part 1,chapter 1
"3 Emulating the Brain One can approach AGI by trying to gure out how the brain works, using brain imaging and other tools from neuroscience, and then emulating the brain in hardware or software. One rather substantial problem with this approach is that we dont really understand how the brain works yet, because our software for measuring the brain is still relatively crude. There is no brain scanning method that combines high spatial and temporal accuracy, and none is likely to come about for a decade or two. So to do brain-emulation AGI seriously, one needs to wait a while until brain scanning technology improves. Current AI methods like neural nets that are loosely based on the brain, are really not brain-like enough to make a serious claim at emulating the brains approach to general intelligence.",Engineering General  Intelligence Part 1,chapter 1
"We dont yet have any real understanding of how the brain represents abstract knowledge, for example, or how it does reasoning (though the authors, like many others, have made some speculations in this regard [GMIH08]). Another problem with this approach is that once youre done, what you get is something with a very humanlike mind, and we already have enough of those! However, this is perhaps not such a serious objection, because a digital-computerbased version of a human mind could be studied much more thoroughly than a      8 1 Introduction biology-based human mind. We could observe its dynamics in real-time in perfect precision, and could then learn things that would allow us to build other sorts of digital minds. 1.6.4 Evolve an AGI Another approach is to try to run an evolutionary process inside the computer, and wait for advanced AGI to evolve. One problem with this is that we dont know how evolution works all that well.",Engineering General  Intelligence Part 1,chapter 1
"Theres a eld of articial life, but so far its results have been fairly disappointing. Its not yet clear how much one can vary on the chemical structures that underly real biology, and still get powerful evolution like we see in real biology. If we need good articial chemistry to get good articial biology, then do we need good articial physics to get good articial chemistry? Another problem with this approach, of course, is that it might take a really long time. Evolution took billions of years on Earth, using a massive amount of computational power. To make the evolutionary approach to AGI effective, one would need some radical innovations to the evolutionary process (such as, perhaps, using probabilistic methods like BOA [Pel05] or MOSES [Loo06] in place of traditional evolution). 1.6.",Engineering General  Intelligence Part 1,chapter 1
"5 Derive an AGI Design Mathematically One can try to use the mathematical theory of intelligence to gure out how to make advanced AGI. This interests us greatly, but theres a huge gap between the rigorous math of intelligence as it exists today and anything of practical value. As well discuss in Chap. 8, most of the rigorous math of intelligence right now is about how to make AI on computers with dramatically unrealistic amounts of memory or processing power. When one tries to create a theoretical understanding of real-world general intelligence, one arrives at quite different sorts of considerations, as we will roughly outline in Chap. 11. Ideally we would like to be able to study the CogPrime design using a rigorous mathematical theory of real-world general intelligence, but at the moment thats not realistic.",Engineering General  Intelligence Part 1,chapter 1
"The best we can do is to conceptually analyze CogPrime and its various components in terms of relevant mathematical and theoretical ideas; and perform analysis of CogPrimes individual structures and components at varying levels of rigor.      1.6 Potential Approaches to AGI 9 1.6.6 Use Heuristic Computer Science Methods The computer science eld contains a number of abstract formalisms, algorithms and structures that have relevance beyond specic narrow AI applications, yet arent necessarily understood as thoroughly as would be required to integrate them into the rigorous mathematical theory of intelligence. Based on these formalisms, algorithms and structures, a number of single formalism/algorithm focused AGI approaches have been outlined, some of which will be reviewed in Chap. 6.",Engineering General  Intelligence Part 1,chapter 1
"For example Pei Wangs NARS (Non-Axiomatic Reasoning System) approach is based on a specic logic which he argues to be the logic of general intelligenceso, while his system contains many other aspects than this logic, he considers this logic to be the crux of the system and the source of its potential power as an AGI system. The basic intuition on the part of these single formalism/algorithm focused researchers seems to be that there is one key formalism or algorithm underlying intelligence, and if you achieve this key aspect in your AGI program, youre going to get something that fundamentally thinks like a person, even if it has some differences due to its different implementation and embodiment. On the other hand, its also possible that this idea is philosophically incorrect: that there is no one key formalism, algorithm, structure or idea underlying general intelligence.",Engineering General  Intelligence Part 1,chapter 1
"The CogPrime approach is based on the intuition that to achieve human-level, roughly human-like general intelligence based on feasible computational resources, one needs an appropriate heterogeneous combination of algorithms and structures, each coping with different typesofknowledgeanddifferentaspectsoftheproblemofachievinggoalsincomplex environments. 1.6.7 Integrative Cognitive Architecture Finally, to create advanced AGI one can try to build some sort of integrative cognitive architecture: a software system with multiple components that each carry out some cognitive function, and that connect together in a specic way to try to yield overall intelligence. Cognitive science gives us some guidance about the overall architecture, and computer science and neuroscience give us a lot of ideas about what to put in the different components. But still this approach is very complex and there is a lot of need for creative invention. This is the approach we consider most serious at present (at least until neuroscience advances further).",Engineering General  Intelligence Part 1,chapter 1
"And, as will be discussed in depth in these pages, this is the approach weve chosen: CogPrime is an integrative AGI architecture.      10 1 Introduction 1.6.8 Can Digital Computers Really Be Intelligent? All the AGI approaches weve just mentioned assume that its possible to make AGI on digital computers. While we suspect this is correct, we must note that it isnt proven. It might be thatas Penrose [Pen96], Hameroff [Ham87] and others have arguedwe need quantum computers or quantum gravity computers to make AGI. However, there is no evidence of this at this stage. Of course the brain like all matter is described by quantum mechanics, but this doesnt imply that the brain is a macroscopic quantum system in a strong sense (like, say, a Bose-Einstein condensate).",Engineering General  Intelligence Part 1,chapter 1
"And even if the brain does use quantum phenomena in a dramatic way to carry out some of its cognitive processes (a hypothesis for which there is no current evidence), this doesnt imply that these quantum phenomena are necessary in order to carry out the given cognitive processes. For example there is evidence that birds use quantum nonlocal phenomena to carry out navigation based on the Earths magnetic elds [Gau11]; yet scientists have built instruments that carry out the same functions without using any special quantum effects. The importance of quantum phenomena in biology (except via their obvious role in giving rise to biological phenomena describable via classical physics) remains a subject of debate [Abb08]. Quantum magic aside, it is also conceivable that building AGI is fundamentally impossible for some other reason we dont understand. Without getting religious about it, it is rationally quite possible that some aspects of the universe are beyond the scope of scientic methods. Science is fundamentally about recognizing patterns in nite sets of bits (e.g.",Engineering General  Intelligence Part 1,chapter 1
"nite sets of nite-precision observations), whereas mathematics recognizes many sets much larger than this. Selmer Bringsjord [Bri03], and other advocates of hypercomputing approaches to intelligence, argue that the human mind depends on massively large innite sets and therefore can never be simulated on digital computers nor understood via nite sets of nite-precision measurements such as science deals with. But again, while this sort of possibility is interesting to speculate about, theres no real reason to believe it at this time. Brain science and AI are both very young sciences and the working hypothesis that digital computers can manifest advanced AGI has hardly been explored at all yet, relative to what will be possible in the next decades as computers get more and more powerful and our understanding of neuroscience and cognitive science gets more and more complete. The CogPrime AGI design presented here is based on this working hypothesis.",Engineering General  Intelligence Part 1,chapter 1
"Many of the ideas in the book are actually independent of the mind can be implemented digitally working hypothesis, and could apply to AGI systems built on analog, quantum or other non-digital frameworksbut we will not pursue these possibilities here. For the moment, outlining an AGI design for digital computers is hard enough! Regardless of speculations about quantum computing in the brain, it seems clear that AGI on quantum computers is part of our future and will be a powerful thing; but the description of a CogPrime analogue for quantum computers will be left for a later work.      1.7 Five Key Words 11 1.7 Five Key Words As noted, the CogPrime approach lies squarely in the integrative cognitive architecture camp. But it is not a haphazard or opportunistic combination of algorithms and data structures.",Engineering General  Intelligence Part 1,chapter 1
"At bottom it is motivated by the patternist philosophy of mind laid out in Ben Goertzels book The Hidden Pattern [Goe06a], which was in large part a summary and reformulation of ideas presented in a series of books published earlier by the same author [Goe94], [Goe93], [Goe93], [Goe97], [Goe01]. A few of the core ideas of this philosophy are laid out in Chap. 5, though that chapter is by no means a thorough summary. One way to summarize some of the most important yet commonsensical parts of the patternist philosophy of mind, in an AGI context, is to list ve words: perception, memory, prediction, action, goals. In a phrase: A mind uses perception and memory to make predictions about which actions will help it achieve its goals.",Engineering General  Intelligence Part 1,chapter 1
"This ties in with the ideas of many other thinkers, including Jeff Hawkins memory/prediction theory [HB06], and it also speaks directly to the formal characterization of intelligence presented in Chap. 8: general intelligence as the ability to achieve complex goals in complex environments. Naturally the goals involved in the above phrase may be explicit or implicit to the intelligent agent, and they may shift over time as the agent develops. Perception is taken to mean pattern recognition: the recognition of (novel or familiar) patterns in the environment or in the system itself. Memory is the storage ofalready-recognizedpatterns,enablingrecollectionorregenerationofthesepatterns as needed. Action is the formation of patterns in the body and world.",Engineering General  Intelligence Part 1,chapter 1
"Prediction is the utilization of temporal patterns to guess what perceptions will be seen in the future, and what actions will achieve what effects in the futurein essence, prediction consists of temporal pattern recognition, plus the (implicit or explicit) assumption that the universe possesses a habitual tendency according to which previously observed patterns continue to apply. 1.7.1 Memory and Cognition in CogPrime Each of these ve concepts has a lot of depth to it, and we wont say too much about them in this brief introductory overview; but we will take a little time to say something about memory in particular. As well see in Chap. 8, one of the things that the mathematical theory of general intelligence makes clear is that, if you assume your AI system has a huge amount of computational resources, then creating general intelligence is not a big trick. Given enough computing power, a very brief and simple program can achieve any computable goal in any computable environment, quite effectively.",Engineering General  Intelligence Part 1,chapter 1
"Marcus Hutters AIXItl design [Hut05] gives one way of doing this, backed up by rigorous mathe     12 1 Introduction matics. Put informally, what this means is: the problem of AGI is really a problem of coping with inadequate compute resources, just as the problem of natural intelligence is really a problem of coping with inadequate energetic resources. One of the key ideas underlying CogPrime is a principle called cognitive synergy, which explains how real-world minds achieve general intelligence using limited resources, by appropriately organizing and utilizing their memories. This principle says that there are many different kinds of memory in the mind: sensory, episodic, procedural, declarative, attentional, intentional. Each of them has certain learning processes associated with it; for example, reasoning is associated with declarative memory.",Engineering General  Intelligence Part 1,chapter 1
"Synergy arises here in the way the learning processes associated with each kind of memory have got to help each other out when they get stuck, rather than working at cross-purposes. Cognitive synergy is a fundamental principle of general intelligenceit doesnt tend to play a central role when youre building narrow-AI systems. In the CogPrime approach all the different kinds of memory are linked together in a single meta-representation, a sort of combined semantic/neural network called the AtomSpace. It represents everything from perceptions and actions to abstract relationships and concepts and even a systems model of itself and others. When specialized representations are used for other types of knowledge (e.g. program trees forproceduralknowledge,spatiotemporalhierarchiesforperceptualknowledge)then the knowledge stored outside the AtomSpace is represented via tokens (Atoms) in the AtomSpace, allowing it to be located by various cognitive processes, and associated with other memory items of any type.",Engineering General  Intelligence Part 1,chapter 1
"So for instance an OpenCog AI system has an AtomSpace, plus some specialized knowledge stores linked into the AtomSpace; and it also has specic algorithms acting on the AtomSpace and appropriate specialized stores corresponding to each type of memory. Each of these algorithms is complex and has its own story; for instance (an incomplete list, for more detail see the following section of this Introduction):  Declarative knowledge is handled using Probabilistic Logic Networks, described in Chap. 16 (Part 2) and others;  ProceduralknowledgeishandledusingMOSES,aprobabilisticevolutionarylearning algorithm described in Chap. 3 (Part 2) and others;  Attentional knowledge is handled by ECAN (economic attention allocation), described in Chap. 5 (Part 2) and others;  OpenCog contains a language comprehension system called RelEx that takes English sentences and turns them into nodes and links in the AtomSpace.",Engineering General  Intelligence Part 1,chapter 1
"Its currently being extended to handle Chinese. RelEx handles mostly declarative knowledge but also involves some procedural knowledge for linguistic phenomena like reference resolution and semantic disambiguation. But the crux of the CogPrime cognitive architecture is not any particular cognitive process, but rather the way they all work together using cognitive synergy.      1.8 Virtually and Robotically Embodied AI 13 1.8 Virtually and Robotically Embodied AI Another issue that will arise frequently in these pages is embodiment. Theres a lot of debate in the AI community over whether embodiment is necessary for advanced AGI or not. Personally, we doubt its necessary but we think its extremely convenient, and are thus considerably interested in both virtual world and robotic embodiment. The CogPrime architecture itself is neutral on the issue of embodiment, and it could be used to build a mathematical theorem prover or an intelligent chat bot just as easily as an embodied AGI system.",Engineering General  Intelligence Part 1,chapter 1
"However, most of our attention has gone into guring out how to use CogPrime to control embodied agents in virtual worlds, or else (to a lesser extent) physical robots. For instance, during 20112012 we are involved in a Hong Kong government funded project using OpenCog to control video game agents in a simple game world modeled on the game Minecraft [Goe11x]. Current virtual world technology has signicant limitations that make them far less than ideal from an AGI perspective, and in Chap. 17 we will discuss how they can be remedied. However, for the medium-term future virtual worlds are not going to match the natural world in terms of richness and complexityand so theres also something to be said for physical robots that interact with all the messiness of the real world.",Engineering General  Intelligence Part 1,chapter 1
"With this in mind, in the Articial Brain Lab at Xiamen University in 20092010, we conducted some experiments using OpenCog to control the Nao humanoid robot [Goe09c]. The goal of that work was to take the same code that controls the virtual dog and use it to control the physical robot. But its harder because in this context we need to do real vision processing and real motor control. A similar project is being undertaken in Hong Kong at time of writing, involving a collaboration between OpenCog AI developers and David Hansons robotics group. One of the key ideas involved in this project is explicit integration of subsymbolic and more symbolic subsystems. For instance, one can use a purely subsymbolic, hierarchical pattern recognition network for vision processing, and then link its internal structures into the nodes and links in the AtomSpace that represent concepts.",Engineering General  Intelligence Part 1,chapter 1
"So the subsymbolic and symbolic systems can work harmoniously and productively together, a notion we will review in more detail in Chap. 8 (Part 2). 1.9 Language Learning One of the subtler aspects of our current approach to teaching CogPrime is language learning. Three relatively crisp and simple approaches to language learning would be:  Build a language processing system using hand-coded grammatical rules, based on linguistic theory;  Train a language processing system using supervised, unsupervised or semisupervised learning, based on computational linguistics;      14 1 Introduction  Have an AI system learn language via experience, based on imitation and reinforcement and experimentation, without any built-in distinction between linguistic behaviors and other behaviors. While the third approach is conceptually appealing, our current approach in CogPrime (described in a series of chapters in Part 2) is none of the above, but rather a combination of the above.",Engineering General  Intelligence Part 1,chapter 1
"OpenCog contains a natural language processing system built using a combination of the rule-based and statistical approaches, which has reasonably adequate functionality; and our plan is to use it as an initial condition for ongoing adaptive improvement based on embodied communicative experience. 1.10 AGI Ethics When discussing AGI work with the general public, ethical concerns often arise. Science ction lms like the Terminator series have raised public awareness of the possible dangers of advanced AGI systems without correspondingly advanced ethics. Non-prot organizations like the Singularity Institute for AI (http://www.singinst. org) have arisen specically to raise attention about, and foster research on, these potential dangers. Our main focus here is on how to create AGI, not how to teach an AGI human ethical principles. However, we will address the latter issue explicitly in Chap.",Engineering General  Intelligence Part 1,chapter 1
"13, and we do think its important to emphasize that AGI ethics has been at the center of the design process throughout the conception and development of CogPrime and OpenCog. Broadly speaking there are (at least) two major threats related to advanced AGI. One is that people might use AGIs for bad ends; and the other is that, even if an AGI is made with the best intentions, it might reprogram itself in a way that causes it to do something terrible. If its smarter than us, we might be watching it carefully while it does this, and have no idea whats going on. The best way to deal with this second bad AGI problem is to build ethics into your AGI architectureand we have done this with CogPrime, via creating a goal structure that explicitly supports ethics-directed behavior, and via creating an overall architecture that supports ethical synergy along with cognitive synergy.",Engineering General  Intelligence Part 1,chapter 1
"In short, the notion of ethical synergy is that there are different kinds of ethical thinking associated with the different kinds of memory and you want to be sure your AGI has all of them, and that it uses them together effectively. In order to create AGI that is not only intelligent but benecial to other sentient beings, ethics has got to be part of the design and the roadmap. As we teach our AGI systems, we need to lead them through a series of instructional and evaluative tasks that move from a primitive level to the mature human levelin intelligence, but also in ethical judgment.      1.11 Structure of the Book 15 1.11 Structure of the Book The book is divided into two parts.",Engineering General  Intelligence Part 1,chapter 1
"The technical particulars of CogPrime are discussed in Part 2; what we deal with in Part 1 are important preliminary and related matters such as:  The nature of real-world general intelligence, both conceptually and from the perspective of formal modeling (Articial and Natural General Intelligence).  The nature of cognitive and ethical development for humans and AGIs (Cognitive and Ethical Development).  The high-level properties of CogPrime, including the overall architecture and the various sorts of memory involved (Networks for Explicit and Implicit Knowledge Representation).  What kind of path may viably lead us from here to AGI, with focus laid on preschool-type environments that easily foster humanlike cognitive development.",Engineering General  Intelligence Part 1,chapter 1
"Various advanced aspects of AGI systems, such as the network and algebraic structures that may emerge from them, the ways in which they may self-modify, and the degree to which their initial design may constrain or guide their future state even after long periods of radical self-improvement (A Path to Human-Level AGI). One point made repeatedly throughout Part 1, which is worth emphasizing here, is the current lack of a really rigorous and thorough general technical theory of general intelligence. Such a theory, if complete, would be incredibly helpful for understanding complex AGI architectures like CogPrime. Lacking such a theory, we must work on CogPrime and other such systems using a combination of theory, experiment and intuition. This is not a bad thing, but it will be very helpful if the theory and practice of AGI are able to grow collaboratively together. 1.",Engineering General  Intelligence Part 1,chapter 1
"12 Key Claims of the Book We will wrap up this Introduction with a systematic list of some of the key claims to be argued for in these pages. Not all the terms and ideas in these claims have been mentioned in the preceding portions of this Introduction, but we hope they will be reasonably clear to the reader anyway, at least in a general sense. This list of claims will be revisited in Chap. 31 (Part 2) near the end of Part 2, where we will look back at the ideas and arguments that have been put forth in favor of them, in the intervening chapters. In essence this is a list of claims such that, if the reader accepts these claims, they should probably accept that the CogPrime approach to AGI is a viable one. On the other hand if the reader rejects one or more of these claims, they may nd one or more aspects of CogPrime unacceptable for some reason.",Engineering General  Intelligence Part 1,chapter 1
"Without further ado, now, the claims:      16 1 Introduction 1. General intelligence (at the human level and ultimately beyond) can be achieved via creating a computational system that seeks to achieve its goals, via using perception and memory to predict which actions will achieve its goals in the contexts in which it nds itself. 2. To achieve general intelligence in the context of human-intelligence-friendly environments and goals using feasible computational resources, its important that an AGI system can handle different kinds of memory (declarative, procedural, episodic, sensory, intentional, attentional) in customized but interoperable ways. 3. Cognitive synergy: Its important that the cognitive processes associated with different kinds of memory can appeal to each other for assistance in overcoming bottlenecks in a manner that enables each cognitive process to act in a manner that is sensitive to the particularities of each others internal representations, and that doesnt impose unreasonable delays on the overall cognitive dynamics. 4.",Engineering General  Intelligence Part 1,chapter 1
"As a general principle, neither purely localized nor purely global memory is sufcient for general intelligence under feasible computational resources; glocal memory will be required. 5. To achieve human-like general intelligence, its important for an intelligent agent to have sensory data and motoric affordances that roughly emulate those available to humans. We dont know exactly how close this emulation needs to be, which means that our AGI systems and platforms need to support fairly exible experimentation with virtual-world and/or robotic infrastructures. 6. To work toward adult human-level, roughly human-like general intelligence, one fairly easily comprehensible path is to use environments and goals reminiscent of human childhood, and seek to advance ones AGI system along a path roughly comparable to that followed by human children. 7.",Engineering General  Intelligence Part 1,chapter 1
"It is most effective to teach an AGI system aimed at roughly human-like general intelligence via a mix of spontaneous learning and explicit instruction, and to instruct it via a combination of imitation, reinforcement and correction, and a combination of linguistic and nonlinguistic instruction. 8. One effective approach to teaching an AGI system human language is to supply it with some in-built linguistic facility, in the form of rule-based and statisticallinguistics-based NLP systems, and then allow it to improve and revise this facility based on experience. 9.",Engineering General  Intelligence Part 1,chapter 1
"An AGI system with adequate mechanisms for handling the key types of knowledge mentioned above, and the capability to explicitly recognize large-scale patterns in itself, should, upon sustained interaction with an appropriate environment in pursuit of appropriate goals, emerge a variety of complex structures in its internal knowledge network, including, but not limited to:  a hierarchical network, representing both a spatiotemporal hierarchy and an approximate default inheritance hierarchy, cross-linked  a heterarchical network of associativity, roughly aligned with the hierarchical network  a self network which is an approximate micro image of the whole network      1.12 Key Claims of the Book 17  inter-reecting networks modeling self and others, reecting a mirrorhouse design pattern 10. Given the strengths and weaknesses of current and near-future digital computers, a. A (loosely) neural-symbolic network is a good representation for directly storing many kinds of memory, and interfacing between those that it doesnt store directly; b.",Engineering General  Intelligence Part 1,chapter 1
"Uncertain logic is a good way to handle declarative knowledge. To deal with the problems facing a human-level AGI, an uncertain logic must integrate imprecise probability and fuzziness with a broad scope of logical constructs. PLN is one good realization. c. Programs are a good way to represent procedures (both cognitive and physical-action, but perhaps not including low-level motor-control procedures). d. Evolutionary program learning is a good way to handle difcult program learning problems. Probabilistic learning on normalized programs is one effective approach to evolutionary program learning. MOSES is one good realization of this approach. e. Multistart hill-climbing, with a strong Occam prior, is a good way to handle relatively straightforward program learning problems. f. Activation spreading and Hebbian learning comprise a reasonable way to handle attentional knowledge (though other approaches, with greater overhead cost, may provide better accuracy and may be appropriate in some situations).",Engineering General  Intelligence Part 1,chapter 1
Articial economics is an effective approach to activation spreading and Hebbian learning in the context of neural-symbolic networks;  ECAN is one good realization of articial economics;  A good trade-off between comprehensiveness and efciency is to focus on two kinds of attention: processor attention (represented in CogPrime by ShortTermImportance) and memory attention (represented in CogPrime by LongTermImportance). g. Simulation is a good way to handle episodic knowledge (remembered and imagined). Running an internal world simulation engine is an effective way to handle simulation. h. Hybridization of ones integrative neural-symbolic system with a spatiotemporally hierarchical deep learning system is an effective way to handle representation and learning of low-level sensorimotor knowledge. DeSTIN is one example of a deep learning system of this nature that can be effective in this context. i.,Engineering General  Intelligence Part 1,chapter 1
"One effective way to handle goals is to represent them declaratively, and allocate attention among them economically. CogPrimes PLN/ECAN based framework for handling intentional knowledge is one good realization.      18 1 Introduction 11. It is important for an intelligent system to have some way of recognizing largescale patterns in itself, and then embodying these patterns as new, localized knowledge items in its memory. Given the use of a neural-symbolic network for knowledge representation, a graph-mining based map formation heuristic is one good way to do this. 12. Occams Razor: Intelligence is closely tied to the creation of procedures that achieve goals in environments in the simplest possible way. Each of an AGI systems cognitive algorithms should embody a simplicity bias in some explicit or implicit form. 13.",Engineering General  Intelligence Part 1,chapter 1
"An AGI system, if supplied with a commonsensically ethical goal system and an intentional component based on rigorous uncertain inference, should be able to reliably achieve a much higher level of commonsensically ethical behavior than any human being. 14. Once sufciently advanced, an AGI system with a logic-based declarative knowledge approach and a program-learning-based procedural knowledge approach should be able to radically self-improve via a variety of methods, including supercompilation and automated theorem-proving.",Engineering General  Intelligence Part 1,chapter 1
     Part I Overview of the CogPrime Architecture   ,Engineering General  Intelligence Part 1,chapter 1
"  Chapter 2 A Brief Overview of CogPrime 2.1 Introduction Just as there are many different approaches to human ightairplanes, helicopters, balloons, spacecraft, and doubtless many methods no person has thought of yet similarly, there are likely many different approaches to advanced articial general intelligence. All the different approaches to ight exploit the same core principles of aerodynamics in different ways; and similarly, the various different approaches to AGI will exploit the same core principles of general intelligence in different ways. The thorough presentation of the CogPrime design is the job of Part 2 of this bookwhere, not only are the algorithms and structures involved in CogPrime reviewed in more detail, but their relationship to the theoretical ideas underlying CogPrime is pursued more deeply.",Engineering General  Intelligence Part 1,chapter 2
"The job of this chapter is a smaller one: to give a high-level overview of some key aspects the CogPrime architecture at a mostly nontechnical level, so as to enable you to approach Part 2 with a little more idea of what to expect. The remainder of Part 1, following this chapter, will present various theoretical notions enabling the particulars, intent and consequences of the CogPrime design to be more thoroughly understood. 2.2 High-Level Architecture of CogPrime Figures2.1, 2.2, 2.4 and 2.5 depict the high-level architecture of CogPrime, which involves the use of multiple cognitive processes associated with multiple types of memory to enable an intelligent agent to execute the procedures that it believes have the best probability of working toward its goals in its current context.",Engineering General  Intelligence Part 1,chapter 2
"In a robot preschool context, for example, the top-level goals will be simple things such as pleasing the teacher, learning new information and skills, and protecting the robots body. Figure2.3 shows part of the architecture via which cognitive processes interact with each other, via commonly acting on the AtomSpace knowledge repository. B. Goertzel et al., Engineering General Intelligence, Part 1, 21 Atlantis Thinking Machines 5, DOI: 10.2991/978-94-6239-027-0_2,  Atlantis Press and the authors 2014      22 2 A Brief Overview of CogPrime Fig. 2.1 High-level architecture of CogPrime. This is a conceptual depiction, not a detailed owchart (which would be too complex for a single image). Figures2.2, 2.4 and 2.",Engineering General  Intelligence Part 1,chapter 2
"5 highlight specic aspects of this diagram Comparing these diagrams to the integrative human cognitive architecture diagrams to be given in Chap.7, one sees the main difference is that the CogPrime diagrams commit to specic structures (e.g. knowledge representations) and processes, whereas the generic integrative architecture diagram refers merely to types of structures and processes. For instance, the integrative diagram refers generally to declarative knowledge and learning, whereas the CogPrime diagram refers to PLN, as a      2.2 High-Level Architecture of CogPrime 23 specic system for reasoning and learning about declarative knowledge. Table2.1 articulates the key connections between the components of the CogPrime diagram and those of the integrative diagram, thus indicating the general cognitive functions instantiated by each of the CogPrime components. 2.",Engineering General  Intelligence Part 1,chapter 2
"3 Current and Prior Applications of OpenCog Before digging deeper into the theory, and elaborating some of the dynamics underlying the above diagrams, we pause to briey discuss some of the practicalities of work done with the OpenCog system currently implementing parts of the CogPrime architecture. OpenCog, the open-source software framework underlying the OpenCogPrime (currently partial) implementation of the CogPrime architecture, has been used for commercial applications in the area of natural language processing and data mining; for instance, see [GPPG06] where OpenCogPrimes PLN reasoning and RelEx language processing are combined to do automated biological hypothesis generation based on information gathered from PubMed abstracts. Most relevantly to the present work, it has also been used to control virtual agents in virtual worlds [GEA08]. Prototype work done during 20072008 involved using an OpenCog variant called the OpenPetBrain to control virtual dogs in a virtual world (see Fig.2.",Engineering General  Intelligence Part 1,chapter 2
"6 for a screenshot of an OpenPetBrain-controlled virtual dog). While these OpenCog virtual dogs did not display intelligence closely comparable to that of real dogs (or human children), they did demonstrate a variety of interesting and relevant functionalities including:  learning new behaviors based on imitation and reinforcement  responding to natural language commands and questions, with appropriate actions and natural language replies  spontaneous exploration of their world, remembering their experiences and using them to bias future learning and linguistic interaction. One current OpenCog initiative involves extending the virtual dog work via using OpenCog to control virtual agents in a game world inspired by the game Minecraft. These agents are initially specically concerned with achieving goals in a game world via constructing structures with blocks and carrying out simple English communications.",Engineering General  Intelligence Part 1,chapter 2
"Representative example tasks would be:  Learning to build steps or ladders to get desired objects that are high up  Learning to build a shelter to protect itself from aggressors  Learning to build structures resembling structures that its shown (even if the available materials are a bit different)  Learning how to build bridges to cross chasms. Of course, the AI signicance of learning tasks like this all depends on what kind of feedback the system is given, and how complex its environment is. It would be      24 2 A Brief Overview of CogPrime Fig. 2.2 Key explicitly implemented processes of CogPrime. The large box at the center is the Atomspace, the systems central store of various forms of (long-term and working) memory, which contains a weighted labeled hypergraph whose nodes and links are Atoms of various sorts.",Engineering General  Intelligence Part 1,chapter 2
"The hexagonal boxes at the bottom denote various hierarchies devoted to recognition and generation of patterns: perception, action and linguistic. Intervening between these recognition/generation hierarchies and the Atomspace, we have a pattern mining/imprinting component (that recognizes patterns in the hierarchies and passes them to the Atomspace; and imprints patterns from the Atomspace on the hierarchies); and also OpenPsi, a special dynamical framework for choosing actions based on motivations. Above the Atomspace we have a host of cognitive processes, which act on the Atomspace, some continually and some only as context dictates, carrying out various sorts of learning and reasoning (pertinent to various sorts of memory) that help the system fulll its goals and motivations      2.3 Current and Prior Applications of OpenCog 25 Fig. 2.3 MindAgents and AtomSpace in OpenCog.",Engineering General  Intelligence Part 1,chapter 2
"This is a conceptual depiction of one way cognitive processes may interact in OpenCogthey may be wrapped in MindAgent objects, which interact via cooperatively acting on the AtomSpace relatively simple to make an AI system do things like this in a trivial and highly specialized way, but that is not the intent of the project the goal is to have the system learn to carry out tasks like this using general learning mechanisms and a general cognitive architecture, based on embodied experience and only scant feedback from human teachers. If successful, this will provide an outstanding platform for ongoing AGI development, as well as a visually appealing and immediately meaningful demo for OpenCog.",Engineering General  Intelligence Part 1,chapter 2
"Specic, particularly simple tasks that are the focus of this project teams current work at time of writing include:  Watch another character build steps to reach a high-up object  Figure out via imitation of this that, in a different context, building steps to reach a high up object may be a good idea  Also gure out that, if it wants a certain high-up object but there are no materials for building steps available, nding some other way to get elevated will be a good idea that may help it get the object. 2.3.1 Transitioning from Virtual Agents to a Physical Robot Preliminary experiments have also been conducted using OpenCog to control a Nao robot as well as a virtual dog [GdG08]. This involves hybridizing OpenCog with a      26 2 A Brief Overview of CogPrime Table 2.1 Connections between the CogPrime architecture diagram and the integrative architecture diagram CogPrime component Int. diag.",Engineering General  Intelligence Part 1,chapter 2
sub-diagram Int. diag. component Procedure repository Long-term memory Procedural Procedure repository Working memory Active procedural Associative episodic memory Long-term memory Episodic Associative episodic memory Working memory Transient episodic Backup store Long-term memory No correlate: a function not necessarily possessed by the human mind Spacetime server Long-term memory Declarative and sensorimotor Dimensional embedding space No clear correlate: a tool for helping multiple types of LTM Dimensional embedding agent No clear correlate Blending Long-term and working memory Concept formation Clustering Long-term and working memory Concept formation PLN probabilistic inference Long-term and working memory Reasoning and plan learning/optimization MOSES/Hillclimbing Long-term and working memory Procedure learning World simulation Long-term and working memory Simulation Episodic encoding/recall Long-term g memory Story-telling Episodic encoding/recall Working memory Consolidation Forgetting/freezing/defrosting Long-term and working memory No correlate: a function not necessarily,Engineering General  Intelligence Part 1,chapter 2
"possessed by the human mind Map formation Long-term memory Concept formation and pattern mining Attention allocation Long-term and working memory Hebbian/attentional learning Attention allocation High-level mind architecture Reinforcement Attention allocation Working memory Perceptual associative memory and local association AtomSpace High-level mind architecture No clear correlate: a general tool for representing memory including long-term and working, plus some of perception and action AtomSpace Working memory Global workspace (the high-STI portion of AtomSpace) and other workspaces Declarative atoms Long-term and working memory Declarative and sensorimotor (continued)      2.3 Current and Prior Applications of OpenCog 27 Table 2.1 (continued) CogPrime component Int. diag. sub-diagram Int. diag.",Engineering General  Intelligence Part 1,chapter 2
"component Procedure atoms Long-term and working memory Procedural Hebbian atoms Long-term and working memory Attentional Goal atoms Long-term and working memory Intentional Feeling atoms Long-term and working memory Spanning declarative, intentional and sensorimotor OpenPsi High-level mind architecture Motivation/action selection OpenPsi Working memory Action selection Pattern miner High-level mind architecture Arrows between perception and working and long-term memory Pattern miner Working memory Arrows between sensory memory and perceptual associative and transient episodic memory Pattern imprinter Working memory Arrows between action selection and sensorimotor memory, and between the latter and perception/action subsystems Pattern imprinter High-level mind architecture Arrows pointing to action subsystem from working and long-term memories Perception hierarchy High-level mind architecture Perception subsystems Perception hierarchy Working memory Perception/action subsystems and sensory and sensorimotor memory Language comprehension hierarchy Language Comprehension hierarchy Language generation hierarchy Language Generation hierarchy Reinforcement hierarchy High-level mind architecture Reinforcement Reinforcement hierarchy Action Reinforcement hierarchy Action hierarchy Action",Engineering General  Intelligence Part 1,chapter 2
"Collection of specialized action hierarchies There is a row for each component in the CogPrime architecture diagram, which tells the corresponding sub-diagrams and components of the integrative architecture diagram given in Chap.7. Note that the description Long Term and Working Memory indicates occurrence in two separate sub diagrams of the integrative diagram, Long Term Memory and Working Memory      28 2 A Brief Overview of CogPrime Fig. 2.4 Links between cognitive processes and the Atomspace. The cognitive processes depicted all act on the Atomspace, in the sense that they operate by observing certain Atoms in the Atomspace and then modifying (or in rare cases deleting) them, and potentially adding new Atoms as well.",Engineering General  Intelligence Part 1,chapter 2
"Atoms represent all forms of knowledge, but some forms of knowledge are additionally represented by external data stores connected to the Atomspace, such as the Procedure Repository; these are also shown as linked to the Atomspace separate (but interlinked) subsystem handling low-level perception and action. In the experiments done so far, this has been accomplished in an extremely simplistic way. How to do this right is a topic treated in detail in Chap.8 of Part 2.      2.3 Current and Prior Applications of OpenCog 29 Fig. 2.5 Invocation of Atom operations by cognitive processes. This diagram depicts some of the Atom modication, creation and deletion operations carried out by the abstract cognitive processes in the CogPrime architecture We suspect that reasonable level of capability will be achievable by simply interposing DeSTIN, as described in Chaps.810 of Part 2 (or some other similar system), as a perception/action black box between OpenCog and a robot.",Engineering General  Intelligence Part 1,chapter 2
"Some preliminary experiments in this direction have already been carried out, connecting the      30 2 A Brief Overview of CogPrime Fig. 2.6 Screenshot of OpenCog-controlled virtual dog OpenPetBrain to a Nao robot using simpler, less capable software than DeSTIN in the intermediary role (off-the-shelf speech-to-text, text-to-speech and visual object recognition software). However, we also suspect that to achieve robustly intelligent robotics we must go beyond this approach, and connect robot perception and actuation software with OpenCogPrime in a white box manner that allows intimate dynamic feedback between perceptual, motoric, cognitive and linguistic functions. We will achieve this via the creation and real-time utilization of links between the nodes in CogPrimes and DeSTINs internal networks (a topic to be explored in more depth later in this chapter). 2.",Engineering General  Intelligence Part 1,chapter 2
"4 Memory Types and Associated Cognitive Processes in CogPrime Now we return to the basic description of the CogPrime approach, turning to aspects of the relationship between structure and dynamics. Architecture diagrams are all very well, but, ultimately it is dynamics that makes an architecture come alive. Intelligence is all about learning, which is by denition about change, about dynamical response to the environment and internal self-organizing dynamics.      2.4 Memory Types and Associated Cognitive Processes in CogPrime 31 CogPrime relies on multiple memory types and, as discussed above, is founded on the premise that the right course in architecting a pragmatic, roughly human-like AGI system is to handle different types of memory differently in terms of both structure and dynamics.",Engineering General  Intelligence Part 1,chapter 2
"CogPrimes memory types are the declarative, procedural, sensory, and episodic memory types that are widely discussed in cognitive neuroscience [TC05], plus attentional memory for allocating system resources generically, and intentional memory for allocating system resources in a goal-directed way. Table2.2 overviews these memory types, giving key references and indicating the corresponding cognitive processes, and also indicating which of the generic patternist cognitive dynamics each cognitive process corresponds to (pattern creation, association, etc.). Figure2.7 illustrates the relationships between several of the key memory types in the context of a simple situation involving an OpenCogPrime-controlled agent in a virtual world. In terms of patternist cognitive theory, the multiple types of memory in CogPrime should be considered as specialized ways of storing particular types of patterns, optimized for spacetime efciency. The cognitive processes associated with a certain type of memory deal with creating and recognizing patterns of the type for which the memory is specialized.",Engineering General  Intelligence Part 1,chapter 2
"While in principle all the different sorts of pattern could be handled in a unied memory and processing architecture, the sort of specialization used in CogPrime is necessary in order to achieve acceptable efcient general intelligence using currently available computational resources. And as we have argued in detail in Chap.8, efciency is not a side-issue but rather the essence of real-world AGI (since as Hutter (2005) has shown, if one casts efciency aside, arbitrary levels of general intelligence can be achieved via a trivially simple program). The essence of the CogPrime design lies in the way the structures and processes associated with each type of memory are designed to work together in a closely coupled way, yielding cooperative intelligence going beyond what could be achieved by an architecture merely containing the same structures and processes in separate black boxes. The inter-cognitive-process interactions in OpenCog are designed so that  conversion between different types of memory is possible, though sometimes computationally costly (e.",Engineering General  Intelligence Part 1,chapter 2
". an item of declarative knowledge may with some effort be interpreted procedurally or episodically, etc.)  when a learning process concerned centrally with one type of memory encounters a situation where it learns very slowly, it can often resolve the issue by converting some of the relevant knowledge into a different type of memory: i.e. cognitive synergy.      32 2 A Brief Overview of CogPrime Table 2.",Engineering General  Intelligence Part 1,chapter 2
"2 Memory types and cognitive processes in CogPrime Memory type Specic cognitive processes General cognitive functions Declarative Probabilistic logic networks (PLN) [GMIH08]; conceptual blending [FT02] Pattern creation Procedural MOSES (a novel probabilistic evolutionary program learning algorithm) [Loo06] Pattern creation Episodic Internal simulation engine [GEA08] Association, pattern creation Attentional Economic attention networks (ECAN) [GPI+10] Association, credit assignment Intentional Probabilistic goal hierarchy rened by PLN and ECAN, structured according to MicroPsi [Bac09] Credit assignment, pattern creation Sensory DeSTIN, and corresponding structures in the atomspace Association, attention allocation, pattern creation, credit assignment The third column indicates the general cognitive function that each specic cognitive process carries out, according to the patternist theory of cognition 2.4.",Engineering General  Intelligence Part 1,chapter 2
"1 Cognitive Synergy in PLN To put a little meat on the bones of the cognitive synergy idea, discussed briey above and more extensively in latter chapters, we now elaborate a little on the role it plays in the interaction between procedural and declarative learning. While the probabilistic evolutionary learning algorithm MOSES handles much of CogPrimes procedural learning, and CogPrimes internal simulation engine handles most episodic knowledge, CogPrimes primary tool for handling declarative knowledge is an uncertain inference framework called Probabilistic Logic Networks (PLN). The complexities of PLN are the topic of a lengthy technical monograph [GMIH08], and are summarized in Chap.16 (Part 2); here we will eschew most details and focus mainly on pointing out how PLN seeks to achieve efcient inference control via integration with other cognitive processes.",Engineering General  Intelligence Part 1,chapter 2
"As a logic, PLN is broadly integrative: it combines certain term logic rules with more standard predicate logic rules, and utilizes both fuzzy truth values and a variant of imprecise probabilities called indenite probabilities. PLN mathematics tells how these uncertain truth values propagate through its logic rules, so that uncertain premises give rise to conclusions with reasonably accurately estimated uncertainty values. This careful management of uncertainty is critical for the application of      2.4 Memory Types and Associated Cognitive Processes in CogPrime 33 Fig. 2.7 Relationship between multiple memory types. The bottom left corner shows a program tree, constituting procedural knowledge. The upper left shows declarative nodes and links in the Atomspace. The upper right corner shows a relevant system goal. The lower right corner contains an image symbolizing relevant episodic and sensory knowledge.",Engineering General  Intelligence Part 1,chapter 2
"All the various types of knowledge link to each other and can be approximatively converted to each other logical inference in the robotics context, where most knowledge is abstracted from experience and is hence highly uncertain. PLN can be used in either forward or backward chaining mode; and in the language introduced above, it can be used for either analysis or synthesis. As an example, we will consider backward chaining analysis, exemplied by the problem of a robot preschool-student trying to determine whether a new playmate Bob is likely to be a regular visitor to is preschool or not (evaluating the truth value of the implication Bob  regular_visitor). The basic backward chaining process for PLN analysis looks like: 1. Given an implication L  A  B whose truth value must be estimated (for instance L  Concept  Procedure  Goal as discussed above), create a list (A1, ...",Engineering General  Intelligence Part 1,chapter 2
"An) of (inference rule, stored knowledge) pairs that might be used to produce L 2. Using analogical reasoning to prior inferences, assign each Ai a probability of success      34 2 A Brief Overview of CogPrime  If some of the Ai are estimated to have reasonable probability of success at generating reasonably condent estimates of Ls truth value, then invoke Step 1 with Ai in place of L (at this point the inference process becomes recursive)  If none of the Ai looks sufciently likely to succeed, then inference has gotten stuck and another cognitive process should be invoked, e.g.  Concept creation may be used to infer new concepts related to A and B, and then Step 1 may be revisited, in the hope of nding a new, more promising Ai involving one of the new concepts  MOSES may be invoked with one of several special goals, e.g.",Engineering General  Intelligence Part 1,chapter 2
"the goal of nding a procedure P so that P(X) predicts whether X  B. If MOSES nds such a procedure P then this can be converted to declarative knowledge understandable by PLN and Step 1 may be revisited....  Simulations may be run in CogPrimes internal simulation engine, so as to observe the truth value of A  B in the simulations; and then Step 1 may be revisited.... The combinatorial explosion of inference control is combatted by the capability to defer to other cognitive processes when the inference control procedure is unable to make a sufciently condent choice of which inference steps to take next. Note that just as MOSES may rely on PLN to model its evolving populations of procedures, PLN may rely on MOSES to create complex knowledge about the terms in its logical implications.",Engineering General  Intelligence Part 1,chapter 2
"This is just one example of the multiple ways in which the different cognitive processes in CogPrime interact synergetically; a more thorough treatment of these interactions is given in [Goe09a]. In the new playmate example, the interesting case is where the robot initially seems not to know enough about Bob to make a solid inferential judgment (so that none of the Ai seem particularly promising). For instance, it might carry out a number of possible inferences and not come to any reasonably condent conclusion, so that the reason none of the Ai seem promising is that all the decent-looking ones have been tried already. So it might then recourse to MOSES, simulation or concept creation. For instance, the PLN controller could make a list of everyone who has been a regular visitor, and everyone who has not been, and pose MOSES the task of guring out a procedure for distinguishing these two categories.",Engineering General  Intelligence Part 1,chapter 2
"This procedure could then be used directly to make the needed assessment, or else be translated into logical rules to be used within PLN inference. For example, perhaps MOSES would discover that older males wearing ties tend not to become regular visitors. If the new playmate is an older male wearing a tie, this is directly applicable. But if the current playmate is wearing a tuxedo, then PLN may be helpful via reasoning that even though a tuxedo is not a tie, its a similar form of fancy dressso PLN may extend the MOSES-learned rule to the present case and infer that the new playmate is not likely to be a regular visitor.      2.5 Goal-Oriented Dynamics in CogPrime 35 2.5 Goal-Oriented Dynamics in CogPrime CogPrimes dynamics has both goal-oriented and spontaneous aspects; here for simplicitys sake we will focus on the goal-oriented ones.",Engineering General  Intelligence Part 1,chapter 2
"The basic goal-oriented dynamic of the CogPrime system, within which the various types of memory are utilized, is driven by implications known as cognitive schematics, which take the form Context  Procedure  Goal < p > (summarized C  P  G). Semi-formally, this implication may be interpreted to mean: If the context C appears to hold currently, then if I enact the procedure P, I can expect to achieve the goal G with certainty p. Cognitive synergy means that the learning processes corresponding to the different types of memory actively cooperate in guring out what procedures will achieve the systems goals in the relevant contexts within its environment. CogPrimes cognitive schematic is signicantly similar to production rules in classical architectures like SOAR and ACT-R (as reviewed in Chap.6; however, there are signicant differences which are important to CogPrimes functionality.",Engineering General  Intelligence Part 1,chapter 2
"Unlike with classical production rules systems, uncertainty is core to CogPrimes knowledge representation, and each CogPrime cognitive schematic is labeled with an uncertain truth value, which is critical to its utilization by CogPrimes cognitive processes. Also, in CogPrime, cognitive schematics may be incomplete, missing one or two of the terms, which may then be lled in by various cognitive processes (generally in an uncertain way). A stronger similarity is to MicroPsis triplets; the differences in this case are more low-level and technical and have already been mentioned in Chap.6. Finally, the biggest difference between CogPrimes cognitive schematics and production rules or other similar constructs, is that in CogPrime this level of knowledge representation is not the only important one. CLARION [SZ04], as will be reviewed in Chap.",Engineering General  Intelligence Part 1,chapter 2
"6 below, is an example of a cognitive architecture that uses production rules for explicit knowledge representation and then uses a totally separate subsymbolic knowledge store for implicit knowledge. In CogPrime both explicit and implicit knowledge are stored in the same graph of nodes and links, with  explicit knowledge stored in probabilistic logic based nodes and links such as cognitive schematics (see Fig.2.8 for a depiction of some explicit linguistic knowledge.)  implicit knowledge stored in patterns of activity among these same nodes and links, dened via the activity of the importance values (see Fig.2.9 for an illustrative example thereof) associated with nodes and links and propagated by the ECAN attention allocation process. The meaning of a cognitive schematic in CogPrime is hence not entirely encapsulated in its explicit logical form, but resides largely in the activity patterns that ECAN causes its activation or exploration to give rise to.",Engineering General  Intelligence Part 1,chapter 2
"And this fact is important because the synergetic interactions of system components are in large part modulated by      36 2 A Brief Overview of CogPrime Fig. 2.8 Example of explicit knowledge in the Atomspace. One simple example of explicitly represented knowledge in the Atomspace is linguistic knowledge, such as words and the concepts directly linked to them. Not all of a CogPrime systems concepts correlate to words, but some do ECAN activity. Without the real-time combination of explicit and implicit knowledge in the systems knowledge graph, the synergetic interaction of different cognitive processes would not work so smoothly, and the emergence of effective high-level hierarchical, heterarchical and self structures would be less likely. 2.6 Analysis and Synthesis Processes in CogPrime We now return to CogPrimes fundamental cognitive dynamics, using examples from the virtual dog application to motivate the discussion.",Engineering General  Intelligence Part 1,chapter 2
"The cognitive schematic Context  Procedure  Goal leads to a conceptualization of the internal action of an intelligent system as involving two key categories of learning:      2.6 Analysis and Synthesis Processes in CogPrime 37 Fig.2.9 ExampleofimplicitknowledgeintheAtomspace.Asimpleexampleofimplicitknowledge in the Atomspace. The chicken and food concepts are represented by maps of ConceptNodes interconnected by HebbianLinks, where the latter tend to form between ConceptNodes that are often simultaneously important. The bundle of links between nodes in the chicken map and nodes in the food map, represents an implicit, emergent link between the two concept maps. This diagram also illustrates glocal knowledge representation, in that the chicken and food concepts are each represented by individual nodes, but also by distributed maps. The chicken ConceptNode, when important, will tend to make the rest of the map importantand vice versa.",Engineering General  Intelligence Part 1,chapter 2
"Part of the overall chicken concept possessed by the system is expressed by the explicit links coming out of the chicken ConceptNode, and part is represented only by the distributed chicken map as a whole      38 2 A Brief Overview of CogPrime  Analysis: Estimating the probability p of a posited C  P  G relationship  Synthesis: Filling in one or two of the variables in the cognitive schematic, given assumptions regarding the remaining variables, and directed by the goal of maximizing the probability of the cognitive schematic. More specically, where synthesis is concerned,  The MOSES probabilistic evolutionary program learning algorithm is applied to nd P, given xed C and G. Internal simulation is also used, for the purpose of creating a simulation embodying C and seeing which P lead to the simulated achievement of G.",Engineering General  Intelligence Part 1,chapter 2
"Example: A virtual dog learns a procedure P to please its owner (the goal G) in the context C where there is a ball or stick present and the owner is saying fetch.  PLN inference, acting on declarative knowledge, is used for choosing C, given xed P and G (also incorporating sensory and episodic knowledge as appropriate). Simulation may also be used for this purpose.  Example: A virtual dog wants to achieve the goal G of getting food, and it knows that the procedure P of begging has been successful at this before, so it seeks a context C where begging can be expected to get it food. Probably this will be a context involving a friendly person.  PLN-based goal renement is used to create new subgoals G to sit on the right hand side of instances of the cognitive schematic.",Engineering General  Intelligence Part 1,chapter 2
"Example: Given that a virtual dog has a goal of nding food, it may learn a subgoal of following other dogs, due to observing that other dogs are often heading toward their food.  Concept formation heuristics are used for choosing G and for fueling goal renement, but especially for choosing C (via providing new candidates for C). They are also used for choosing P, via a process called predicate schematization that turns logical predicates (declarative knowledge) into procedures.  Example: At rst a virtual dog may have a hard time predicting which other dogs are going to be mean to it. But it may eventually observe common features among a number of mean dogs, and thus form its own concept of pit bull, without anyone ever teaching it this concept explicitly. Where analysis is concerned:  PLN inference, acting on declarative knowledge, is used for estimating the probabilityoftheimplicationinthecognitiveschematic,givenxedC,P andG.",Engineering General  Intelligence Part 1,chapter 2
"pisodic knowledge is also used in this regard, via enabling estimation of the probability via simple similarity matching against past experience. Simulation is also used: multiple simulations may be run, and statistics may be captured therefrom.      2.6 Analysis and Synthesis Processes in CogPrime 39  Example: To estimate the degree to which asking Bob for food (the procedure P is asking for food, the context C is being with Bob) will achieve the goal G of getting food, the virtual dog may study its memory to see what happened on previous occasions where it or other dogs asked Bob for food or other things, and then integrate the evidence from these occasions.  Procedural knowledge, mapped into declarative knowledge and then acted on by PLN inference, can be useful for estimating the probability of the implication C  P  G, in cases where the probability of C  P1  G is known for some P1 related to P.",Engineering General  Intelligence Part 1,chapter 2
"Example: knowledge of the internal similarity between the procedure of asking for food and the procedure of asking for toys, allows the virtual dog to reason that if asking Bob for toys has been successful, maybe asking Bob for food will be successful too.  Inference, acting on declarative or sensory knowledge, can be useful for estimating the probability of the implication C  P  G, in cases where the probability of C1  P  G is known for some C1 related to C.  Example: if Bob and Jim have a lot of features in common, and Bob often responds positively when asked for food, then maybe Jim will too.  Inference can be used similarly for estimating the probability of the implication C  P  G, in cases where the probability of C  P  G1 is known for some G1 related to G.",Engineering General  Intelligence Part 1,chapter 2
"Concept creation can be useful indirectly in calculating these probability estimates, via providing new concepts that can be used to make useful inference trails more compact and hence easier to construct.  Example: The dog may reason that because Jack likes to play, and Jack and Jill are both children, maybe Jill likes to play too. It can carry out this reasoning only if its concept creation process has invented the concept of child via analysis of observed data. In these examples we have focused on cases where two terms in the cognitive schematic are xed and the third must be lled in; but just as often, the situation is that only one of the terms is xed. For instance, if we x G, sometimes the best approach will be to collectively learn C and P. This requires either a procedure learning method that works interactively with a declarative-knowledge-focused concept learning or reasoning method; or a declarative learning method that works interactively with a procedure learning method.",Engineering General  Intelligence Part 1,chapter 2
"That is, it requires the sort of cognitive synergy built into the CogPrime design.      40 2 A Brief Overview of CogPrime 2.7 Conclusion To thoroughly describe a comprehensive, integrative AGI architecture in a brief chapter would be an impossible task; all we have attempted here is a brief overview, to be elaborated on in Part 2 of this book. We do not expect this brief summary to be enough to convince the skeptical reader that the approach described here has a reasonable odds of success at achieving its stated goals. However, we hope to have given the reader at least a rough idea of what sort of AGI design we are advocating, and why and in what sense we believe it can lead to advanced articial general intelligence. For more details on the structure, dynamics and underlying concepts of CogPrime, the reader is encouraged to proceed to Part 2after completing Part 1, of course.",Engineering General  Intelligence Part 1,chapter 2
"Please be patientbuilding a thinking machine is a big topic, and we have a lot to say about it!   ",Engineering General  Intelligence Part 1,chapter 2
"  Chapter 3 Build Me Something I Havent Seen: A CogPrime Thought Experiment 3.1 Introduction AGI design necessarily leads one into some rather abstract spacesbut being a human-like intelligence in the everyday world is a pretty concrete thing. If the CogPrime research program is successful, it will result not just in abstract ideas and equations, but rather in real AGI robots carrying out tasks in the world, and AGI agents in virtual worlds and online digital spaces conducting important business, doing science, entertaining and being entertained by us, and so forth. With this in mind, in this chapter we will ground the discussion rmly in the concrete and everyday, and pursue a thought experiment of the form How would a completed CogPrime system carry out this specic task? The task we will use for this thought-experiment is one we will a running example now and then in the following chapters.",Engineering General  Intelligence Part 1,chapter 3
"We consider the case of a robotically or virtually embodied CogPrime system, operating in a preschool type environment, interacting with a human whom it already knows and given the task of Build me something with blocks that I havent seen before. This target task is fairly simple, but it is complex enough to involve essentially every one of CogPrimes processes, interacting in a unied way. It involves simple, grounded creativity of the sort that normal human children display every day and which, we conjecture, is structurally and dynamically basically the same as the creativity underlying the genius of adult human creators like Einstein, Dali, Dostoevsky, Hendrix, and so forth ... and as the creativity that will power massively capable genius machines in future. We will consider the case of a simple interaction based on the above task where: 1. The human teacher tells the CogPrime agent Build me something with blocks that I havent seen before. 2.",Engineering General  Intelligence Part 1,chapter 3
"After a few false starts, the agent builds something it thinks is appropriate and says Do you like it? 3. The human teacher says Its beautiful. What is it? B. Goertzel et al., Engineering General Intelligence, Part 1, 41 Atlantis Thinking Machines 5, DOI: 10.2991/978-94-6239-027-0_3,  Atlantis Press and the authors 2014      42 3 Build Me Something I Havent Seen: A CogPrime Thought Experiment 4. The agent says Its a car man [and indeed, the construct has 4 wheels and a chassis vaguely like a car, but also a torso, arms and head vaguely like a person]. Of course, a complex system like CogPrime could carry out an interaction like this internally in many different ways, and what is roughly described here is just one among many possibilities.",Engineering General  Intelligence Part 1,chapter 3
"First we will enumerate a number of CogPrime processes and explain some ways that each one may help CogPrime carry out the target task. Then we will give a more evocative narrative, conveying the dynamics that would occur in CogPrime while carrying out the target task, and mentioning how each of the enumerated cognitive processes as it arises in the narrative. Coming as it does at the beginning of the book, yet referring to numerous cognitive processes and structures that will only be dened later in the book, the chapter will necessarily be somewhat opaque to the reader with no prior exposure to CogPrime ideas. However, it has been placed here at the start for a reasonso serve as motivation and conceptual guidance for the complex and voluminous material to follow.",Engineering General  Intelligence Part 1,chapter 3
"You may wish to skim this chapter over relatively lightly the rst time around, getting a general idea of how the different processes and structures in CogPrime t together in a practical contextand then return to the chapter again once youve nished with Part Two (Part 2) and have a fuller understanding of what all the different parts of the design are supposed to do and how theyre supposed to work. 3.2 Roles of Selected Cognitive Processes Now we review a number of the more interesting CogPrime cognitive processes to be reviewed in the following chapters of the book, for each one indicating one or more of the roles it might play in helping a CogPrime system carry out the target task. Note that this list is incomplete in many senses, e.g. it doesnt list all the cognitive processes, nor all the roles played by the ones listed. The purpose is to give an evocative sense of the roles played by the different parts of the design in carrying out the task.",Engineering General  Intelligence Part 1,chapter 3
"Chapter1 Part 2 (OpenCog Framework)  Freezing/Defrosting. When the agent builds a structure from blocks and decides its not good enough to show off to the teacher, what does it do with the detailed ideas and thought process underlying the structure it built? If it doesnt like the structure so much, it may just leave this to the generic forgetting process. But if it likes the structure a lot, it may want to increase the VLTI (Very Long Term Importance) of the Atoms related to the structure in question, to be sure that these are stored on disk or other long-term storage, even after theyre deemed sufciently irrelevant to be pushed out of RAM by the forgetting mechanism.      3.2 Roles of Selected Cognitive Processes 43 When given the target task, the agent may decide to revive from disk the mind-states it went through when building crowd-pleasing structures from blocks before, so as to provide it with guidance.",Engineering General  Intelligence Part 1,chapter 3
"Chapter4 of Part 2 (Emotion, Motivation, Attention and Control)  Cognitive Cycle. While building with blocks, the agents cognitive cycle will be dominated by perceiving, acting on, and thinking about the blocks it is building with. When interacting with the teacher, then interaction-relevant linguistic, perceptual and gestural processes will also enter into the cognitive cycle.  Emotion. The agents emotions will uctuate naturally as it carries out the task. If it has a goal of pleasing the teacher, then it will experience happiness as its expectation of pleasing the teacher increases. If it has a goal of experiencing novelty, then it will experience happiness as it creates structures that are novel in its experience. If it has a goal of learning, then it will experience happiness as it learns new things about blocks construction. On the other hand, it will experience unhappiness as its experienced or predicted satisfaction of these goals decreases.",Engineering General  Intelligence Part 1,chapter 3
"Action Selection In dialoguing with the teacher, action selection will select one or more DialogueController schema to control the conversational interaction (based on which DC schema have proved most effective in prior similar situations.  When the agent wants to know the teachers opinion of its construct, what is happening internally is that the please teacher Goal Atom gets a link of the conceptual form (Implication nd out teachers opinion of my current construct please teacher). This link may be created by PLN inference, probably largely by analogy to previously encountered similar situations. Then, GoalImportance is spread from the please teacher Goal Atom to the nd out teachers opinion of my current construct Atom (via the mechanism of sending an RFS package to the latter Atom). More inference causes a link (Implication ask the teacher for their opinion of my current construct nd out teachers opinion of my current construct) to be formed, and the ask the teacher for their opinion of my current construct Atom to get GoalImportance also.",Engineering General  Intelligence Part 1,chapter 3
"Then Predicate Schematization causes the predicate ask the teacher for their opinion of my current construct to get turned into an actionable schema, which gets GoalImportance, and which gets pushed into the ActiveSchemaPool via Goal-driven action selection. Once the schema version of ask the teacher for their opinion of my current construct is in the ActiveSchemaPool, it then invokes natural language generation Tasks, which lead to the formulation of an English sentence such as Do you like it?      44 3 Build Me Something I Havent Seen: A CogPrime Thought Experiment  When the teacher asks Its beautiful. What is it?, then the NL comprehension MindAgent identies this as a question, and the please teacher Goal Atom gets a link of the conceptual form (Implication answer the question the teacher just asked please teacher).",Engineering General  Intelligence Part 1,chapter 3
"This follows simply from the knowledge ( Implication (teacher has just asked a question AND I answer the teachers question) (please teacher)), or else from more complex knowledge rening this Implication. From this point, things proceed much as in the case Do you like it? described just above. Consider a schema such as pick up a red cube and place it on top of the long red block currently at the top of the structure (lets call this P). Once P is placed in the ActiveSchemaPool, then it runs and generates more specic procedures, such as the ones needed to nd a red cube, to move the agents arm toward the red cube and grasp it, etc. But the execution of these specic low-level procedures is done via the ExecutionManager, analogously to the execution of the specics of generating a natural language sentence from a collection of semantic relationships.",Engineering General  Intelligence Part 1,chapter 3
"Loosely speaking, reaching for the red cube and turning simple relationships into a simple sentences, are considered as automated processes not requiring holistic engagement of the agents mind. What the generic, more holistic Action Selection mechanism does in the present context is to gure out to put P in the ActiveSchemaPool in the rst place. This occurs because of a chain such as: P predictively implies (with a certain probabilistic weight) completion of the car-man structure, which in turn predictively implies completion of a structure that is novel to the teacher, which in turn predictively implies please the teacher, which in turn implies please others, which is assumed an Ubergoal (a top-level system goal).  Goal Atoms. As the above items make clear, the scenario in question requires the initial Goal Atoms to be specialized, via the creation of more and more particular subgoals suiting the situation at hand.  Context Atoms.",Engineering General  Intelligence Part 1,chapter 3
"Knowledge of the context the agent is in can help it disambiguate language it hears, e.g. knowing the context is blocks-building helps it understand which sense of the word blocks is meant. On the other hand, if the context is that the teacher is in a bad mood, then the agent might know via experience that in this context, the strength of (Implication ask the teacher for their opinion of my current construct nd out teachers opinion of my current construct) is lower than in other contexts.  Context Formation. A context like blocks-building or teacher in a bad mood may be formed by clustering over multiple experience-sets, i.e. forming Atoms that refer to spatiotemporally grouped sets of percepts/concepts/actions, and grouping together similar Atoms of this nature into clusters. The Atom referring to the cluster of experience-sets involving blocks-building will then survive as an Atom if it gets involved in relationships that are important or have surprising truth values.",Engineering General  Intelligence Part 1,chapter 3
"If many relationships have signicantly      3.2 Roles of Selected Cognitive Processes 45 different truth-value inside the blocks-building context than outside it, this means its likely that the blocks-building ConceptNode will remain as an Atom with reasonably high LTI, so it can be used as a context in future.  Time-Dependence of Goals. Many of the agents goals in this scenario have different importances over different time scales. For instance please the teacher is important on multiple time-scales: the agent wants to please the teacher in the near term but also in the longer term. But a goal like answer the question the teacher just asked has an intrinsic time-scale to it; if its not fullled fairly rapidly then its importance goes away.  Chapter5 of Part 2 (Attention allocation)  ShortTermImportance versus LongTermImportance.",Engineering General  Intelligence Part 1,chapter 3
"While conversing, the concepts and immediately involved in the conversation (including the Atoms describing the agents in the conversation) have very high STI. While building, Atoms representing to the blocks and related ideas about the structures being built (e.g. images of cars and people perceived or imagined in the past) have very high STI. But the reason these Atoms are in RAM prior to having their STI boosted due to their involvement in the agents activities, is because they had their LTI boosted at some point in the past. And after these Atoms leave the AttentionalFocus and their STI reduces, they will have boosted LTI and hence likely remain in RAM for a long while, to be involved in background thought, and in case theyre useful in the AttentionalFocus again.  HebbianLink Formation.",Engineering General  Intelligence Part 1,chapter 3
"As a single example, the car-man has both wheels and arms, so now a Hebbian association between wheels and arms will exist in the agents memory, to potentially pop up again and guide future thinking. The very idea of a car-man likely emerged partly due to previously formed HebbianLinksbecause people were often seen sitting in cars, the association between person and car existed, which made the car concept and the human concept natural candidates for blending.  Data Mining the System Activity Table. The HebbianLinks mentioned above may have been formed via mining the SystemActivityTable  ECAN Based Associative Memory. When the agent thinks about making a car, thisspreadsimportancetovariousAtomsrelatedtothecarconcept,andonething this does is lead to the emergence of the car attractor into the AttentionalFocus.",Engineering General  Intelligence Part 1,chapter 3
"The different aspects of a car are represented by heavily interlinked Atoms, so that when some of them become important, theres a strong tendency for the others to also become importantand for car to then emerge as an attractor of importance dynamics.  Schema Credit Assignment. Suppose the agent has a subgoal of placing a certain blue block on top of a certain red block. It may use a particular motor schema for carrying out this actioninvolving, for instance, holding the blue block above the red block and then gradually lowering it. If this schema results in success (rather than in, say, knocking down the red block), then it should get rewarded via having      46 3 Build Me Something I Havent Seen: A CogPrime Thought Experiment its STI and LTI boosted and also having the strength of the link between it and the subgoal increased.",Engineering General  Intelligence Part 1,chapter 3
"Next, suppose that a certain cognitive schema (say, the schema of running multiple related simulations and averaging the results, to estimate the success probability of a motor procedure) was used to arrive at the motor schema in question. Then this cognitive schema may get passed some importance from the motor schema, and it will get the strength of its link to the goal increased. In this way credit passes backwards from the goal to the various schema directly or indirectly involved in fullling it.  Forgetting. If the agent builds many structures from blocks during its lifespan, it will accumulate a large amount of perceptual memory.  Chapter6 of Part 2 (Goal and Action Selection). Much of the use of the material in this chapter was covered above in the bullet point for Chap.6 of Part 2, but a few more notes are:  Transfer of RFS Between Goals.",Engineering General  Intelligence Part 1,chapter 3
"Above it was noted that the link (Implication ask the teacher for their opinion of my current construct nd out teachers opinion of my current construct) might be formed and used as a channel for GoalImportance spreading.  Schema Activation. Supposing the agent is building a man-car, it may have car-building schema and man-building schema in its ActiveSchemaPool at the same time, and it may enact both of them in an interleaved manner. But if each tend to require two hands for their real-time enaction, then schema activation will have to pass back and forth between the two of them, so that at any one time, one is active whereas the other one is sitting in the ActiveSchemaPool waiting to get activated.  Goal Based Schema Learning. To take a fairly low-level example, suppose the agent has the (sub)goal of making an arm for a blocks-based person (or man-car), given the presence of a blocks-based torso.",Engineering General  Intelligence Part 1,chapter 3
"Suppose it nds a long block that seems suitable to be an arm. It then has the problem of guring out how to attach the arm to the body. It may try out several procedures in its internal simulation world, until it nds one that works: hold the arm in the right position while one end of it rests on top of some block that is part of the torso, then place some other block on top of that end, then slightly release the arm and see if it falls. If it doesnt fall, leave it. If it seems about to fall, then place something heavier atop it, or shove it further in toward the center of the torso. The procedure learning process could be MOSES here, or it could be PLN.  Chapter7 of Part 2 (Procedure Evaluation)  Inference Based Procedure Evaluation.",Engineering General  Intelligence Part 1,chapter 3
"A procedure for man-building such as rst put up feet, then put up legs, then put up torso, then put up arms and head may be synthesized from logical knowledge (via predicate schematization) but without lling in the details of how to carry out the individual steps, such as put up legs. If a procedure with abstract (ungrounded) schema like PutUpTorso      3.2 Roles of Selected Cognitive Processes 47 is chosen for execution and placed into the ActiveSchemaPool, then in the course of execution, inferential procedure evaluation must be used to gure out how to make the abstract schema actionable. The GoalDrivenActionSelection MindAgent must make the choice whether to put a not-fully-grounded schema into the ActiveSchemaPool, rather than grounding it rst and then making it active; this is thesort of choicethat maybemadeeffectivelyvialearnedcognitive schema.",Engineering General  Intelligence Part 1,chapter 3
"Chapter8 of Part 2 (Perception and Action)  ExperienceDB. No person remembers every blocks structure they ever saw or built, except maybe some autists. But a CogPrime can store all this information fairly easily, in its ExperienceDB, even if it doesnt keep it all in RAM in its AtomSpace. It can also store everything anyone ever said about blocks structures in its vicinity.  Perceptual Pattern Mining.  Object Recognition. Recognizing structures made of blocks as cars, people, houses, etc. requires fairly abstract object recognition, involving identifying the key shapes and features involved in an object-type, rather than just going by simple visual similarity.  Hierarchical Perception Networks. If the room is well-lit, its easy to visually identify individual blocks within a blocks structure. If the room is darker, then more top-down processing may be neededidentifying the overall shape of the blocks structure may guide one in making out the individual blocks.",Engineering General  Intelligence Part 1,chapter 3
"Hierarchical Action Networks. Top-down action processing tells the agent that, if it wants to pick up a block, it should move its arm in such a way as to get its hand near the block, and then move its hand.But if its still learning how to do that sort of motion, more likely it will do this, but then start moving its hand and nd that its hard to get a grip on the blockand then have to go back and move its arm a little differently. Iterating between broader arm/hand movements and more ne-grained hand/nger movements is an instance of information iteratively passing up and down a hierarchical action network.  Coupling of Perception and Action Networks. Picking up a block in the dark is a perfect example of rich coupling of perception and action networks. Feeling the block with the ngers helps with identifying blocks that cant be clearly seen.  Chapter12 of Part 2 (Procedure Learning)  Specication Based Procedure Learning.",Engineering General  Intelligence Part 1,chapter 3
"Suppose the agent has never seen a horse, but the teacher builds a number of blocks structures and calls them horses, and draws a number of pictures and calls them horses. This may cause a procedure learning problem to be spawned, where the tness function is accuracy at distinguishing horses from non-horses.      48 3 Build Me Something I Havent Seen: A CogPrime Thought Experiment Learning to pick up a block is specication-based procedure learning, where the specication is to pick up the block and grip it and move it without knocking down the other stuff near the block.  Representation Building.",Engineering General  Intelligence Part 1,chapter 3
"In the midst of building a procedure to recognize horses, MOSES would experimentally vary program nodes recognizing visual features into other program nodes recognizing other visual features In the midst of building a procedure to pick up blocks, MOSES would experimentally vary program nodes representing physical movements into other nodes representing physical movements In both of these cases, MOSES would also carry out the standard experimental variations of mathematical and control operators according to its standard representation-building framework  Chapter13 Part 2 (Imitative, Reinforcement and Corrective Learning)  Reinforcement Learning. Motor procedures for placing blocks (in simulations or reality) will get rewarded if they dont result in the blocks structure falling down, punished otherwise. Procedures leading to the teacher being pleased, in internal simulations (or in repeated trials of scenarios like the one under consideration), will get rewarded; procedures leading to the teacher being displeased will get punished.  Imitation Learning.",Engineering General  Intelligence Part 1,chapter 3
"If the agent has seen others build with blocks before, it may summon these memories and then imitate the actions it has seen others take.  Corrective Learning. This would occur if the teacher intervened in the agents block-building and guided him physicallye.g. steadying his shaky arm to prevent him from knocking the blocks structure over.  Chapter14 of Part 2 (Hillclimbing)  Complexity Penalty. In learning procedures for manipulating blocks, the complexity penalty will militate against procedures that contain extraneous steps.  Chapter15 of Part 2 (Probabilistic Evolutionary Procedure Learning)  Supplying Evolutionary Learning with Long-Term Memory. Suppose the agent has previously built people from clay, but never from blocks. It may then have learned a classication model predicting which clay people will look appealing to humans, and which wont.",Engineering General  Intelligence Part 1,chapter 3
"It may then transfer this knowledge, using PLN, to form a classication model predicting which blocks-people will look appealing to humans, and which wont.  Fitness Function Estimation via Integrative Intelligence. To estimate the tness of a procedure for, say, putting an arm on a blocks-built human, the agent may try out the procedure in the internal simulation world; or it may use PLN inference to reason by analogy to prior physical situations its observed. These      3.2 Roles of Selected Cognitive Processes 49 allow tness to be estimated without actually trying out the procedure in the environment.  Chapter16 of Part 2 (Probabilistic Logic Networks)  Deduction. This is a tall skinny structure; tall skinny structures fall down easily; thus this structure may fall down easily.  Induction. This teacher is talkative; this teacher is friendly; therefore the talkative are generally friendly.  Abduction.",Engineering General  Intelligence Part 1,chapter 3
"This structure has a head and arms and torso; a person has a head and arms and torso; therefore this structure is a person.  PLN Forward Chaining. What properties might a car-man have, based on inference from the properties of cars and the properties of men?  PLN Backward Chaining. An inference target might be: Find X so that X looks something like a wheel and can be attached to this blocks-chassis, and I can nd four fairly similar copies. Or: Find the truth value of the proposition that this structure looks like a car.  Indenite Truth Values. Consider the deductive inference This is a tall skinny structure; tall skinny structures fall down easily; thus this structure may fall down easily.",Engineering General  Intelligence Part 1,chapter 3
"In this case, the condence of the second premise may be greater than the condence of the rst premise, which may result in an intermediate condence for the conclusion, according to the propagation of indenite probabilities through the PLN deduction rule.  Intensional Inference. Is the blocks-structure a person? According to the denition of intensional inheritance, it shares many informative properties with people (e.g. having arms, torso and head), so to a signicant extent, it is a person.  Condence Decay. The agents condence in propositions regarding building things with blocks should remain nearly constant. The agents condence in propositions regarding the teachers taste should decay more rapidly. This should occur because the agent should observe that, in general, propositions regarding physical object manipulation tend to retain fairly constant truth value, whereas propositions regarding human tastes tend to have more rapidly decaying truth value.",Engineering General  Intelligence Part 1,chapter 3
"Chapter17 Part 2 (Spatiotemporal Inference)  Temporal Reasoning. Suppose, after the teacher asks What is it?, the agent needs to think a while to gure out a good answer. But maybe the agent knows that its rude to pause too long before answering something to a direct question. Temporal reasoning helps gure out how long is too long to wait before answering.  Spatial Reasoning. Suppose the agent puts shoes on the wheels of the car. This is a joke relying on the understanding that wheels hold a car up, whereas feet hold a person up, and the structure is a car-man. But it also relies on the spatial inferences that: the cars wheels are in the right position for the mans feet (below      50 3 Build Me Something I Havent Seen: A CogPrime Thought Experiment the torso); and, the wheels are below the cars chassis just like a persons feet are below its torso.",Engineering General  Intelligence Part 1,chapter 3
"Chapter18 of Part 2 (Inference Control)  Evaluator Choice as a Bandit Problem. In doing inference regarding how to make a suitably humanlike arm for the blocks-man, there may be a choice between multiple inference pathways, perhaps one that relies on analogy to other situations building arms, versus one that relies on more general reasoning about lengths and weights of blocks. The choice between these two pathways will be made randomly with a certain probabilistic bias assigned to each one, via prior experience.  Inference Pattern Mining. The probabilities used in choosing which inference path to take, are determined in part by prior experiencee.g. maybe its the case that in prior situations of building complex blocks structures, analogy has proved a better guide than naive physics, thus the prior probability of the analogy inference pathway will be nudged up.  PLN and Bayes Nets.",Engineering General  Intelligence Part 1,chapter 3
"Whats the probability that the blocks-mans hat will fall off if the man-car is pushed a little bit to simulate driving? This question could be resolved in many ways (e.g. by internal simulation), but one possibility is inference. If this is resolved by inference, its the sort of conditional probability calculation that could potentially be done faster if a lot of the probabilistic knowledge from the AtomSpace were summarized in a Bayes Net. Updating the Bayesnetstructurecanbeslow,sothisisprobablynotappropriateforknowledge that is rapidly shifting; but knowledge about properties of blocks structures may be fairly persistent after the agent has gained a fair bit of knowledge by playing with blocks a lot.  Chapter19 of Part 2 (Pattern Mining)  Greedy Pattern Mining. Push a tall structure of blocks and it tends to fall down is the sort of repetitive pattern that could easily be extracted from a historical record of perceptions and (the agents and others) actions via simple greedy pattern mining algorithm.",Engineering General  Intelligence Part 1,chapter 3
"If there is a block that is shaped like a babys rattle, with a long slender handle and then a circular shape at the end, then greedy pattern mining may be helpful due to having recognized the pattern that structures like this are sometimes rattlesand also that structures like this are often stuck together, with the handle part connected sturdily to the circular part.  Evolutionary Pattern Mining. Push a tall structure of blocks with a wide base and a gradual narrowing toward the top and it may not fall too badly is a more complex pattern that may not be found via greedy mining, unless the agent has dealt with a lot of pyramids.      3.2 Roles of Selected Cognitive Processes 51  Chapter20 of Part 2 (Concept Formation)  Formal Concept Analysis. Suppose there are many long, slender blocks of different colors and different shapes (some cylindrical, some purely rectangular for example).",Engineering General  Intelligence Part 1,chapter 3
"Learning this sort of concept based on common features is exactly what FCA is good at (and when the features are dened fuzzily or probabilistically, its exactly what uncertain FCA is good at). Learning the property of slender itself is another example of something uncertain FCA is good atit would learn this if there were many concepts that preferentially involved slender things (even though formed on the basis of concepts other than slenderness)  Conceptual Blending. The concept of a car-man or man-car is an obvious instance of conceptual blending. The agents know that building a man wont surprise the teacher, and nor will building a car ... but both man and car may pop to the forefront of its mind (i.e. get a briey high STI) when it thinks about what to build.",Engineering General  Intelligence Part 1,chapter 3
"But since it knows it has to do something new or surprising, there may be a cognitive schema that boosts the amount of funds to the ConceptBlending MindAgent, causing it to be extra-active. In any event, the ConceptBlending agent seeks to nd ways to combine important concepts; and then PLN explores these to see which ones may be able to achieve the given goal of surprising the teacher (which includes subgoals such as actually being buildable).  Chapter21 of Part 2 (Dimensional Embedding)  Dimensional Embedding. When the agent needs to search its memory for a previously seen blocks structure similar to the currently observed oneor for a previously articulated thought similar to the one its currently trying to articulatethen it needs to a search through its large memory for an entity similar to X (where X is a structure or a thought).",Engineering General  Intelligence Part 1,chapter 3
"This kind of search can be quite computationally difcultbut if the entities in question have been projected into an embedding space, then its quite rapid. (The cost is shifted to the continual maintenance of the embedding space, and its periodic updating; and there is some error incurred in the projection, but in many cases this error is not a show-stopper).  Embedding Based Inference Control. Rapid search for answers to similarity or inheritance queries can be key for guiding inference in appropriate directions; for instance reasoning about how to build a structure with certain properties, can benet greatly from rapid search for previously-encountered substructures currently structurally or functionally similar to the substructures one desires to build.  Chapter22 of Part 2 (Simulation and Episodic Memory)  Fitness Estimation via Simulation.",Engineering General  Intelligence Part 1,chapter 3
"One way to estimate whether a certain blocks structure is likely to fall down or not, is to build it in ones minds eye and see if the physics engine in ones minds-eye causes it to fall down. This is something that in many cases will work better for CogPrime than for humans,      52 3 Build Me Something I Havent Seen: A CogPrime Thought Experiment because CogPrime has a more mathematically accurate physics engine than the human mind does; however, in cases that rely heavily on naive physics rather than, say, direct applications of Newtons Laws, then CogPrimes simulation engine may underperform the typical human mind.  Concept Formation via Simulation. Objects may be joined into categories using uncertain FCA, based on features that they are identied to have via simulation experiments rather than physical world observations.",Engineering General  Intelligence Part 1,chapter 3
"For instance, it may be observed that pyramid-shaped structures fall less easily than pencil-shaped tower structuresand the concepts corresponding to these two categories may be formedfrom experiments run in the internal simulation world, perhaps inspired by isolated observations in the physical world.  Episodic Memory. Previous situations in which the agent has seen similar structures built, or been given similar problems to solve, may be brought to mind as episodic movies playing in the agents memory. By watching what happens in these replayed episodic movies, the agent may learn new declarative or procedural knowledge about what to do. For example, maybe there was some situation in the agents past where it saw someone asked to do something surprising, and that someone created something funny. This might (via a simple PLN step) bias the agent to create something now, which it has reason to suspect will cause others to laugh.  Chapter23 of Part 2 (Integrative Procedure Learning)  Concept-Driven Procedure Learning.",Engineering General  Intelligence Part 1,chapter 3
"Learning the concept of horse, as discussed above in the context of Chap.12 of Part 2, is an example of this.  Predicate Schematization. The synthesis of a schema for man-building, as discussed above in the context of Chap.7 of Part 2, is an example of this.  Chapter24 of Part 2 (Map Formation)  Map Formation. The notion of a car involves many aspects: the physical appearance of cars, the way people get in and out of cars, the ways cars drive, the noises they make, etc. All these aspects are represented by Atoms that are part of the car map, and are richly interconnected via HebbianLinks as well as other links.  Map Encapsulation. The car map forms implicitly via the interaction of multiple cognitive dynamics, especially ECAN. But then the Anticoagulation MindAgent may do its pattern mining and recognize this map explicitly, and form a PredicateNode encapsulating it.",Engineering General  Intelligence Part 1,chapter 3
"This PredicateNode may then be used in PLN inference, conceptual blending, and so forth (e.g. helping with the formation of a concept like car-man via blending).  Chapter26 of Part 2 (Natural Language Comprehension)  Experience Based Disambiguation. The particular dialogue involved in the present example doesnt require any nontrivial word sense disambiguation. But it does require parse selection, and semantic interpretation selection:      3.2 Roles of Selected Cognitive Processes 53 In Build me something with blocks, the agent has no trouble understanding that blocks means toy building blocks rather than, say, city blocks, based on many possible mechanisms, but most simply importance spreading. Build me something with blocks has at least three interpretations: the building could be carried out using blocks with a tool; or the thing built could be presented alongside blocks; or the thing built could be composed of blocks.",Engineering General  Intelligence Part 1,chapter 3
"The latter is the most commonsensical interpretation for most humans, but that is because we have heard the phrase building with blocks used in a similarly grounded way before (as well as other similar phrases such as playing with Legos, etc., whose meaning helps militate toward the right interpretation via PLN inference and importance spreading). So here we have a simple example of experience-based disambiguation, where experiences at various distances of association from the current one are used to help select the correct parse. A subtler form of semantic disambiguation is involved in interpreting the clause that I havent seen before. A literal-minded interpretation would say that this requirement is fullled by any blocks construction thats not precisely identical to one the teacher has seen before. But of course, any sensible human knows this is an idiomatic clause that means signicantly different from anything Ive seen before.",Engineering General  Intelligence Part 1,chapter 3
"This could be determined by the CogPrime agent if it has heard the idiomatic clause before, or if its heard a similar idiomatic phrase such as something Ive never done before. Or, even if the agent has never heard such an idiom before, it could potentially gure out the intended meaning simply because the literal-minded interpretation would be a pointless thing for the teacher to say. So if it knows the teacher usually doesnt add useless modicatory clauses onto their statements, then potentially the agent could guess the correct meaning of the phrase.  Chapter28 of Part 2 (Language Generation)  Experience-Based Knowledge Selection for Language Generation. When the teacher asks What is it?, the agent must decide what sort of answer to give. Within the connes of the QuestionAnswering DialogueController, the agent could answer A structure of blocks, or A part of the physical world, or A thing, or Mine.",Engineering General  Intelligence Part 1,chapter 3
"(Or, if it were running another DC, it could answer more broadly, e.g. None of your business, etc.). However, the QA DC tells it that, in the present context, the most likely desired answer is one that the teacher doesnt already know; and the most important property of the structure that the teacher doesnt obviously already know is the fact that it depicts a car man. Also, memory of prior conversations may bring up statements like Its a horse in reference to a horse built of blocks, or a drawing of a horse, etc.  Experience-Based Guidance of Word and Syntax Choice. The choice of phrase car man requires some choices to be made. The agent could just as well say Its a man with a car for feet or Its a car with a human upper body and head or Its a car centaur, etc. A bias toward simple expressions would      54 3 Build Me Something I Havent Seen: A CogPrime Thought Experiment lead to car man.",Engineering General  Intelligence Part 1,chapter 3
"If the teacher were known to prefer complex expressions, then the agent might be biased toward expressing the idea in a different way.  Chapter30 of Part 2 (Natural Language Dialogue)  Adaptation of Dialogue Controllers. The QuestionAsking and QuestionAnswering DialogueControllers both get reinforcement from this interaction, for the specic internal rules that led to the given statements being made. 3.3 A Semi-Narrative Treatment Now we describe how a CogPrime system might carry out the specied task in a seminarrative form, weaving in the material from the previous section as we go along, and making some more basic points as well. The semi-narrative covers most but not all of the bullet points from the previous section, but with some of the technical details removed; and it introduces a handful of new examples not given in the bullet points.",Engineering General  Intelligence Part 1,chapter 3
"The reason this is called a semi-narrative rather than a narrative is that there is no particular linear order to the processes occurring in each phase of the situation described here. CogPrimes internal cognitive processes do not occur in a linear narrative; rather, what we have is a complex network of interlocking events. But still, describing some of these events concretely in a manner correlated with the different stages of a simple interaction, may have some expository value. The human teacher tells the CogPrime agent Build me something with blocks that I havent seen before. Uponhearingthis,theagentscognitivecyclesaredominatedbylanguageprocessing and retrieval from episodic and sensory memory. The agent may decide to revive from disk the mind-states it went through when building human-pleasing structures from blocks before, so as to provide it with guidance It will likely experience the emotion of happiness, because it anticipates the pleasure of getting rewarded for the task in future.",Engineering General  Intelligence Part 1,chapter 3
"The ubergoal of pleasing the teacher gets active (gets funded signicantly with STI currency), as it becomes apparent there are fairly clear ways of fullling that goal (via the subgoal S of building blocks structures that will get positive response from the teacher). Other ubergoals like gaining knowledge are not funded as much with STI currency just now, as they are not immediately relevant. Action selection, based on ImplicationLinks derived via PLN (between various possible activities and the subgoal S) causes it to start experimentally building some blocks structures. Past experience with building (turned into ImplicationLinks via mining the SystemActivityTable) tells it that it may want to build a little bit in its internal simulation world before building in the external world, causing STI currently to ow to the simulation MindAgent.",Engineering General  Intelligence Part 1,chapter 3
"The Atom corresponding to the context blocks-building gets high STI and is pushed into the AttentionalFocus, making it likely that many future inferences will      3.3 A Semi-Narrative Treatment 55 occur in this context. Other Atoms related to this one also get high STI (the ones in the blocks-building map, and others that are especially related to blocks-building in this particular context). After a few false starts, the agent builds something it thinks is appropriate and says Do you like it? Now that the agent has decided what to do to fulll its well-funded goal, its cognitive cycles are dominated by action, perception and related memory access and concept creation. An obvious subgoal is spawned: build a new structure now, and make this particular structure under construction appealing and novel to the teacher. This subgoal has a shorter time scale than the high level goal. The subgoal gets some currency from its supergoal using the mechanism of RFS spreading.",Engineering General  Intelligence Part 1,chapter 3
"Action selection must tell it when to continue building the same structure and when to try a new one, as well as more micro level choices. Atoms related to the currently pursued blocks structure get high STI. After a failed structure (a false start) is disassembled, the corresponding Atoms lose STI dramatically (leaving AF) but may still have signicant LTI, so they can be recalled later as appropriate. They may also have VLTI so they will be saved to disk later on if other things push them out of RAM due to getting higher LTI. Meanwhile everything thats experienced from the external world goes into the ExperienceDB. Atoms representing different parts of aspects of the same blocks structure will get Hebbian links between them, which will guide future reasoning and importance spreading.",Engineering General  Intelligence Part 1,chapter 3
"Importance spreading helps the system go from an idea for something to build (say, a rock or a car) to the specic plans and ideas about how to build it, via increasing the STI of the Atoms that will be involved in these plans and ideas. If something apparently good is done in building a blocks structure, then other processes and actions that helped lead to or support that good thing, get passed some STI from the Atoms representing the good thing, and also may get linked to the Goal Atom representing good in this context. This leads to reinforcement learning. The agent may play with building structures and then seeing what they most look like, thus exercising abstract object recognition (that uses procedures learned by MOSES or hillclimbing, or uncertain relations learned by inference, to guess what object category a given observed collection of percepts most likely falls into).",Engineering General  Intelligence Part 1,chapter 3
"Since the agent has been asked to come up with something surprising, it knows it should probably try to formulate some new conceptsbecause it has learned in the past, via SystemActivityTable mining, that often newly formed concepts are surprising to others. So, more STI currency is given to concept formation MindAgents, such as the ConceptualBlending Mind Agent (which, along with a lot of stuff that gets thrown out or stored for later use, comes up with car-man). When the notion of car is brought to mind, the distributed map of nodes corresponding to car get high STI. When car-man is formed, it is reasoned about (producing new Atoms), but it also serves as a nexus of importance-spreading, causing the creation of a distributed car-man map.",Engineering General  Intelligence Part 1,chapter 3
"     56 3 Build Me Something I Havent Seen: A CogPrime Thought Experiment If the goal of making an arm for a man-car occurs, then goal-driven schema learning may be done to learn a procedure for arm-making (where the actual learning is done by MOSES or hill-climbing). If the agent is building a man-car, it may have man-building and car-building schema in its ActiveSchemaPool at the same time, and SchemaActivation may spread back and forth between the different modules of these two schema. If the agent wants to build a horse, but has never seen a horse made of blocks (only various pictures and movies of horses), it may uses MOSES or hillclimbing internally to solve the problem of creating a horse-recognizer or a horse-generator which embodies appropriate abstract properties of horses. Here as in all cases of procedure learning, a complexity penalty rewards simpler programs, from among all programs that approximately fulll the goals of the learning process.",Engineering General  Intelligence Part 1,chapter 3
"If a procedure being executed has some abstract parts, then these may be executed by inferential procedure evaluation (which makes the abstract parts concrete on the y in the course of execution). To guess the tness of a procedure for doing something (say, building an arm or recognizing a horse), inference or simulation may be used, as well as direct evaluation in the world. Deductive, inductive and abductive PLN inference may be used in guring out what a blocks structure will look or act like before building it (its tall and thin so it may fall down; it wont be bilaterally symmetric so it wont look much like a person; etc.) Backward-chaining inference control will help gure out how to assemble something matching a certain specication e.g. how to build a chassis based on knowledge of what a chassis looks like.",Engineering General  Intelligence Part 1,chapter 3
"Forward chaining inference (critically including intensional relationships) will be used to estimate the properties that the teacher will perceive a given specic structure to have. Spatial and temporal algebra will be used extensively in this reasoning, within the PLN framework. Coordinating different parts of the bodysay an arm and a handwill involve importance spreading (both up and down) within the hierarchical action network, and from this network to the hierarchical perception network and the heterarchical cognitive network. In looking up Atoms in the AtomSpace, some have truth values whose condences have decayed signicantly (e.g. those regarding the teachers tastes), whereas others have condences that have hardly decayed at all (e.g. those regarding general physical properties of blocks). Finding previous blocks structures similar to the current one (useful for guiding buildingbyanalogytopastexperience)maybedonerapidlybysearchingthesystems internal dimensional-embedding space.",Engineering General  Intelligence Part 1,chapter 3
"As the building process occurs, patterns mined via past experience (tall things often fall down) are used within various cognitive processes (reasoning, procedure learning, concept creation, etc.); and new pattern mining also occurs based on the new observations made as different structures are build and experimented with and destroyed.      3.3 A Semi-Narrative Treatment 57 Simulation of teacher reactions, based on inference from prior examples, helps with the evaluation of possible structures, and also of procedures for creating structures. As the agent does all this, it experiences the emotion of curiosity (likely among other emotions), because as it builds each new structure it has questions about what it will look like and how the teacher would react to it. The human teacher says Its beautiful. What is it? The agent says Its a car man Now that the building is done and the teacher says something, the agents cognitive cycles are dominated by language understanding and generation.",Engineering General  Intelligence Part 1,chapter 3
"The Atom representing the context of talking to the teacher gets high STI, and is used as the context for many ensuing inferences. Comprehension of it uses anaphor resolution based on a combination of ECAN and PLN inference based on a combination of previously interpreted language and observation of the external world situation. The agent experiences the emotion of happiness because the teacher has called its creation beautiful, which is recognizes as a positive evaluationso the agent knows one of its ubergoals (please the teacher) has been signicantly fullled. The goal of pleasing the teacher causes the system to want to answer the question. So the QuestionAnswering DialogueController schema gets paid a lot and gets put into the ActiveSchemaPool. In reaction to the question asked, this DC chooses a semantic graph to speak, then invokes NL generation to say it.",Engineering General  Intelligence Part 1,chapter 3
"NL generation chooses the most compact expression that seems to adequately convey the intended meaning, so it decides on car man as the best simple verbalization to match the newly created conceptual blend that it thinks effectively describes the newly created blocks structure. The positive feedback from the user leads to reinforcement of the Atoms and processes that led to the construction of the blocks structure that has been judged beautiful (via importance spreading and SystemActivityTable mining). 3.4 Conclusion The simple situation considered in this chapter is complex enough to involve nearly all the different cognitive processes in the CogPrime systemand many interactions between these processes. This fact illustrates one of the main difculties of designing, building and testing an articial mind like CogPrimeuntil nearly all of the system is build and made to operate in an integrated way, its hard to do any meaningful test of the system.",Engineering General  Intelligence Part 1,chapter 3
"Testing PLN or MOSES or conceptual blending in isolation may be interesting computer science, but it doesnt tell you much about CogPrime as a design for a thinking machine. According to the CogPrime approach, getting a simple child-like interaction like build me something with blocks that I havent seen before to work properly requires a holistic, integrated cognitive system. Once one has built a system capable of this      58 3 Build Me Something I Havent Seen: A CogPrime Thought Experiment sort of simple interaction then, according to the theory underlying CogPrime, one is not that far from a system with adult human-level intelligence. And once one has an adult human-level AGI built according to a highly exible design like CogPrime, given the potential of such systems to self-analyze and self-modify, one is not far off from a dramatically powerful Genius Machine.",Engineering General  Intelligence Part 1,chapter 3
"Of course there will be a lot of work to do to get from a child-level system to an adult-level systemit wont necessarily unfold as automatically as seems to happen with a human child, because CogPrime lacks the suite of developmental processes and mechanisms that the young human brain has. But still, a child CogPrime mind capable of doing the things outlined in this chapter will have all the basic components and interactions in place, all the ones that are needed for a much more advanced articial mind. Of course, one could concoct a narrow-AI system carrying out the specic activities described in this chapter, much more simply than one could build a CogPrime system capable of doing these activities. But thats not the pointthe point of this chapter is not to explain how to achieve some particular narrow set of activities by any means necessary, but rather to explain how these activities might be achieved within the CogPrime framework, which has been designed with much more generality in mind.",Engineering General  Intelligence Part 1,chapter 3
"It would be worthwhile to elaborate a number of other situations similar to the one described in this chapter, and to work through the various cognitive processes and structures in CogPrime carefully in the context of each of these situations. In fact this sort of exercise has frequently been carried out informally in the context of developing CogPrime. But the burden of this book is already large enough, so we will leave this for future worksemphasizing that it is via intimate interplay between concrete considerations like the ones presented in this chapter, and general algorithmic and conceptual considerations as presented in most of the chapters of this book, that we have the greatest hope of creating advanced AGI. The value of this sort of interplay actually follows from the theory of real-world general intelligence presented several of the following chapters in Part 1.",Engineering General  Intelligence Part 1,chapter 3
"Thoroughly general intelligence is only possible given unrealistic computational resources, so real-world general intelligence is about achieving high generality given limited resources relative to the specic classes of environments relevant to a given agent. Specic situations like building surprising things with blocks are particularly important insofar as they embody broader information about the classes of environments relevant to broadly human-like general intelligence. No doubt, once a CogPrime system is completed, the specics of its handling of the situation described here will differ somewhat from the treatment presented in this chapter. Furthermore, the nal CogPrime system may differ algorithmically and structurally in some respects from the specics given in this bookit would be surprising if the process of building, testing and interacting with CogPrime didnt teach us some new things about various of the topics covered.",Engineering General  Intelligence Part 1,chapter 3
"But our conjecture is that, if sufcient effort is deployed appropriately, then a system much like the CogPrime system described in this book will be able to handle the situation described in this chapter in a roughly similar manner to the one described in this chapterand that this will serve as a natural precursor to much more dramatic AGI achievements.",Engineering General  Intelligence Part 1,chapter 3
     Part II Artificial and Natural General Intelligence   ,Engineering General  Intelligence Part 1,chapter 3
"  Chapter 4 What is Human-Like General Intelligence? 4.1 Introduction CogPrime, the AGI architecture on which the bulk of this book focuses, is aimed at the creation of articial general intelligence that is vaguely human-like in nature, and possesses capabilities at the human level and ultimately beyond. Obviously this description begs some foundational questions, such as, for starters: What is general intelligence? What is human-like general intelligence? What is intelligence at all? Perhapsinthefuturetherewillexistarigoroustheoryofgeneralintelligencewhich applies usefully to real-world biological and digital intelligences. In later chapters we will give some ideas in this direction. But such a theory is currently nascent at best. So, given the present state of science, these two questions about intelligence must be handled via a combination of formal and informal methods.",Engineering General  Intelligence Part 1,chapter 4
"This brief, informal chapter attempts to explain our view on the nature of intelligence in sufcient detail to place the discussion of CogPrime in appropriate context, without trying to resolve all the subtleties. Psychologists sometimes dene human general intelligence using IQ tests and related instrumentsso one might wonder: why not just go with that? But these sortsofintelligencetestingapproacheshavedifcultyevenextendingtohumansfrom diverse cultures [HHPO12] [Fis01]. So its clear that to ground AGI approaches that are not based on precise modeling of human cognition, one requires a more fundamental understanding of the nature of general intelligence. On the other hand, if one conceives intelligence too broadly and mathematically, theres a risk of leaving the real human world too far behind. In this chapter (followed up in Chaps.",Engineering General  Intelligence Part 1,chapter 4
"10 and 8 with more rigor), we present a highly abstract understanding of intelligence-ingeneral, and then portray human-like general intelligence as a (particularly relevant) special case. B. Goertzel et al., Engineering General Intelligence, Part 1, 61 Atlantis Thinking Machines 5, DOI: 10.2991/978-94-6239-027-0_4,  Atlantis Press and the authors 2014      62 4 What Is Human-Like General Intelligence? 4.1.1 What is General Intelligence? Many attempts to characterize general intelligence have been made; Legg and Hutter [LH07a] review over 70! Our preferred abstract characterization of intelligence is: the capability of a system to choose actions maximizing its goal-achievement, basedonitsperceptionsandmemories,andmakingreasonablyefcientuseofits computational resources [Goe10b].",Engineering General  Intelligence Part 1,chapter 4
"A general intelligence is then understood as one that can do this for a variety of complex goals in a variety of complex environments. However, apart from positing denitions, it is difcult to say anything nontrivial about general intelligence in general. Marcus Hutter [Hut05a] has demonstrated, using a characterization of general intelligence similar to the one above, that a very simple algorithm called AIXItl can demonstrate arbitrarily high levels of general intelligence, if given sufciently immense computational resources. This is interesting because it shows that (if we assume the universe can effectively be modeled as a computational system) general intelligence is basically a problem of computational efciency. The particular structures and dynamics that characterize real-world general intelligences like humans arise because of the need to achieve reasonable levels of intelligence using modest space and time resources. The patternist theory of mind presented in [Goe06a] and briey summarized in Chap.",Engineering General  Intelligence Part 1,chapter 4
"5 presents a number of emergent structures and dynamics that are hypothesized to characterize pragmatic general intelligence, including such things as systemwide hierarchical and heterarchical knowledge networks, and a dynamic and selfmaintaining self-model. Much of the thinking underlying CogPrime has centered on how to make multiple learning components combine to give rise to these emergent structures and dynamics. 4.1.2 What is Human-Like General Intelligence? General principles like complex goals in complex environments and patternism are not sufcient to specify the nature of human-like general intelligence. Due to the harsh reality of computational resource restrictions, real-world general intelligences are necessarily biased to particular classes of environments. Human intelligence is biased toward the physical, social and linguistic environments in which humanity evolved, and if AI systems are to possess humanlike general intelligence they must to some extent share these biases.",Engineering General  Intelligence Part 1,chapter 4
"But what are these biases, specically? This is a large and complex question, which we seek to answer in a theoretically grounded way in Chap. 10. However, before turning to abstract theory, one may also approach the question in a pragmatic way, by looking at the categories of things that humans do to manifest their particular variety of general intelligence. This is the task of the following section.      4.2 Commonly Recognized Aspects of Human-Like Intelligence 63 4.2 Commonly Recognized Aspects of Human-Like Intelligence It would be nice if we could give some sort of standard model of human intelligence in this chapter, to set the context for our approach to articial general intelligence but the truth is that there isnt any. What the cognitive science eld has produced so far is better described as: a broad set of principles and platitudes, plus a long, looselyorganized list of ideas and results.",Engineering General  Intelligence Part 1,chapter 4
"Chapter 7 constitutes an attempt to present an integrative architecture diagram for human-like general intelligence, synthesizing the ideas of a number of different AGI and cognitive theorists. However, though the diagram given there attempts to be inclusive, it nonetheless contains many features that are accepted by only a plurality of the research community. The following list of key aspects of human-like intelligence has a better claim at truly being generic and representing the consensus understanding of contemporary science. It was produced by a very simple method: starting with the Wikipedia page for cognitive psychology, and then adding a few items onto it based on scrutinizing the tables of contents of some top-ranked cognitive psychology textbooks.",Engineering General  Intelligence Part 1,chapter 4
"There is some redundancy among list items, and perhaps also some minor omissions (depending on how broadly one construes some of the items), but the point is to give a broad indication of human mental functions as standardly identied in the psychology eld:  Perception  General perception  Psychophysics  Pattern recognition (the ability to correctly interpret ambiguous sensory information)  Object and event recognition  Time sensation (awareness and estimation of the passage of time)  Motor Control  Motor planning  Motor execution  Sensorimotor integration  Categorization  Category induction and acquisition  Categorical judgement and classication  Category representation and structure  Similarity  Memory  Aging and memory  Autobiographical memory  Constructive memory  Emotion and memory      64 4 What Is Human-Like General Intelligence?  False memories  Memory biases  Long-term memory  Episodic memory  Semantic memory  Procedural memory  Short-term memory  Sensory memory  Working memory  Knowledge",Engineering General  Intelligence Part 1,chapter 4
"representation  Mental imagery  Propositional encoding  Imagery versus propositions as representational mechanisms  Dual-coding theories  Mental models  Language  Grammar and linguistics  Phonetics and phonology  Language acquisition  Thinking  Choice  Concept formation  Judgment and decision making  Logic, formal and natural reasoning  Problem solving  Planning  Numerical cognition  Creativity  Consciousness  Attention and Filtering (the ability to focus mental effort on specic stimuli whilst excluding other stimuli from consideration)  Access consciousness  Phenomenal consciousness  Social Intelligence  Distributed Cognition  Empathy If theres nothing surprising to you in the above list, Im not surprised! If youve read a bit in the modern cognitive science literature, the list may even seem trivial. But its worth reecting that 50 years ago, no such list could have been produced with      4.2 Commonly Recognized Aspects of Human-Like Intelligence 65 the same level of broad acceptance.",Engineering General  Intelligence Part 1,chapter 4
"And less than 100 years ago, the Western worlds scientic understanding of the mind was dominated by Freudian thinking; and not too long after that, by behaviorist thinking, which argued that theorizing about what went on inside the mind made no sense, and science should focus entirely on analyzing external behavior. The progress of cognitive science hasnt made as many headlines as contemporaneous progress in neuroscience or computing hardware and software, but its certainly been dramatic. One of the reasons that AGI is more achievable now than in the 1950s and 1960s when the AI eld began, is that now we understand the structures and processes characterizing human thinking a lot better. In spite of all the theoretical and empirical progress in the cognitive science eld, however, there is still no consensus among experts on how the various aspects of intelligence in the above human intelligence feature list are achieved and interrelated.",Engineering General  Intelligence Part 1,chapter 4
"In these pages, however, for the purpose of motivating CogPrime, we assume a broad integrative understanding roughly as follows:  Perception: There is signicant evidence that human visual perception occurs using a spatiotemporal hierarchy of pattern recognition modules, in which higherlevel modules deal with broader spacetime regions, roughly as in the DeSTIN AGI architecture discussed in Chap. 6. Further, there is evidence that each module carries out temporal predictive pattern recognition as well as static pattern recognition. Audition likely utilizes a similar hierarchy. Olfaction may use something more like a Hopeld attractor neural network, as described in Chap. 14. The networks corresponding to different sense modalities have multiple cross-linkages, more at the upper levels than the lower, and also link richly into the parts of the mind dealing with other functions.",Engineering General  Intelligence Part 1,chapter 4
"Motor Control: This appears to be handled by a spatiotemporal hierarchy as well, in which each level of the hierarchy corresponds to higher-level (in space and time) movements. The hierarchy is very tightly linked in with the perceptual hierarchies, allowing sensorimotor learning and coordination.  Memory: There appear to be multiple distinct but tightly cross-linked memory systems, corresponding to different sorts of knowledge such as declarative (facts andbeliefs),procedural,episodic,sensorimotor,attentionalandintentional(goals).  Knowledge Representation: There appear to be multiple base-level representational systems; at least one corresponding to each memory system, but perhaps more than that. Additionally there must be the capability to dynamically create newcontext-specicrepresentationalsystemsfoundedonthebaserepresentational system.",Engineering General  Intelligence Part 1,chapter 4
"Language: While there is surely some innate biasing in the human mind toward learning certain types of linguistic structure, its also notable that language shares a great deal of structure with other aspects of intelligence like social roles [CB00] and the physical world [Cas07]. Language appears to be learned based on biases toward learning certain types of relational role systems; and language processing seems a complex mix of generic reasoning and pattern recognition processes with specialized acoustic and syntactic processing routines.      66 4 What Is Human-Like General Intelligence?  Consciousness is pragmatically well-understood using Baars global workspace theory, in which a small subset of the minds content is summoned at each time into a working memory aka workspace aka attentional focus where it is heavily processed and used to guide action selection.",Engineering General  Intelligence Part 1,chapter 4
"Thinking is a diverse combination of processes encompassing things like categorization, (crisp and uncertain) reasoning, concept creation, pattern recognition, and others; these processes must work well with all the different types of memory and must effectively integrate knowledge in the global workspace with knowledge in long-term memory.  Social Intelligence seems closely tied with language and also with self-modeling; we model ourselves in large part using the same specialized biases we use to help us model others. None of the points in the above bullet list is particularly controversial, but neither are any of them universally agreed-upon by experts. However, in order to make any progress on AGI design one must make some commitments to particular cognitiontheoretic understandings, at this level and ultimately at more precise levels as well. Further, general philosophical analyses like the patternist philosophy to be reviewed in the following chapter only provide limited guidance here.",Engineering General  Intelligence Part 1,chapter 4
"Patternism provides a lter for theories about specic cognitive functionsit rules out assemblages of cognitive-function-specic theories that dont t together to yield a mind that could act effectively as a pattern-recognizing, goal-achieving system with the right internal emergent structures. But its not a precise enough lter to serve as a sole guide for cognitive theory even at the high level. The above list of points leads naturally into the integrative architecture diagram presented in Chap. 7. But that generic architecture diagram is fairly involved, and before presenting it, we will go through some more background regarding humanlike intelligence (in the rest of this chapter), philosophy of mind (in Chap. 5) and contemporary AGI architectures (in Chap. 6). 4.3 Further Characterizations of Human-Like Intelligence We now present a few complementary approaches to characterizing the key aspects of humanlike intelligence, drawn from different perspectives in the psychology and AI literature.",Engineering General  Intelligence Part 1,chapter 4
"These different approaches all overlap substantially, which is good, yet each gives a slightly different slant. 4.3.1 Competencies Characterizing Human-Like Intelligence First we give a list of key competencies characterizing human level intelligence resulting from the AGI Roadmap Workshop held at the University of Knoxville in      4.3 Further Characterizations of Human-Like Intelligence 67 October 2008,1 which was organized by Ben Goertzel and Itamar Arel. In this list, each broad competency area is listed together with a number of specic competencies sub-areas within its scope: 1. Perception: vision, hearing, touch, proprioception, crossmodal 2. Actuation: physical skills, navigation, tool use 3. Memory: episodic, declarative, behavioral 4. Learning:imitation,reinforcement,interactiveverbalinstruction,writtenmedia, experimentation 5.",Engineering General  Intelligence Part 1,chapter 4
"Reasoning: deductive, abductive, inductive, causal, physical, associational, categorization 6. Planning: strategic, tactical, physical, social 7. Attention: visual, social, behavioral 8. Motivation: subgoal creation, affect-based motivation, control of emotions 9. Emotion: expressing emotion, understanding emotion 10. Self: self-awareness, self-control, other-awareness 11. Social: empathy, appropriate social behavior, social communication, social inference, group play, theory of mind 12. Communication: gestural, pictorial, verbal, language acquisition, cross-modal 13. Quantitative: counting, grounded arithmetic, comparison, measurement 14. Building/Creation: concept formation, verbal invention, physical construction, social group formation. Clearly this list is getting at the same things as the textbook headings given in Sect.4.2, but with a different emphasis due to its origin among AGI researchers rather than cognitive psychologists.",Engineering General  Intelligence Part 1,chapter 4
"As part of the AGI Roadmap project, specic tasks were created corresponding to each of the sub-areas in the above list; we will describe some of these tasks in Chap. 18. 4.3.2 Gardners Theory of Multiple Intelligences The diverse list of human-level competencies given above is reminiscent of Gardners[Gar99]multipleintelligences(MI)frameworkapsychologicalapproach to intelligence assessment based on the idea that different people have mental strengths in different high-level domains, so that intelligence tests should contain aspects that focus on each of these domains separately. MI does not contradict the 1 See http://www.ece.utk.edu/~itamar/AGI_Roadmap.",Engineering General  Intelligence Part 1,chapter 4
"; participants included: Sam Adams, IBM Research; Ben Goertzel, Novamente LLC; Itamar Arel, University of Tennessee; Joscha Bach, Institute of Cognitive Science, University of Osnabruck, Germany; Robert Coop, University of Tennessee; Rod Furlan, Singularity Institute; Matthias Scheutz, Indiana University; J. Storrs Hall, Foresight Institute; Alexei Samsonovich, George Mason University; Matt Schlesinger, Southern Illinois University; John Sowa, Vivomind Intelligence, Inc.; Stuart C. Shapiro, University at Buffalo.      68 4 What Is Human-Like General Intelligence? complex goals in complex environments view of intelligence, but rather may be interpreted as making specic commitments regarding which complex tasks and which complex environments are most important for roughly human-like intelligence.",Engineering General  Intelligence Part 1,chapter 4
"MI does not seek an extreme generality, in the sense that it explicitly focuses on domains in which humans have strong innate capability as well as generalintelligence capability; there could easily be non-human intelligences that would exceed humans according to both the commonsense human notion of general intelligence and the generic complex goals in complex environments or Hutter/ Legg-style denitions, yet would not equal humans on the MI criteria. This strong anthropocentrism of MI is not a problem from an AGI perspective so long as one uses MI in an appropriate way, i.e. only for assessing the extent to which an AGI system displays specically human-like general intelligence. This restrictiveness is the price one pays for having an easily articulable and relatively easily implementable evaluation framework. Table4.1 summarizes the types of intelligence included in Gardners MI theory. 4.3.",Engineering General  Intelligence Part 1,chapter 4
"3 Newells Criteria for a Human Cognitive Architecture Finally, another related perspective is given by Alan Newells functional criteria for a human cognitive architecture [New90], which require that a humanlike AGI system should: Table 4.1 Types of intelligence in Gardners multiple intelligence theory Intelligence Type Aspects Linguistic Words and language, written and spoken; retention, interpretation and explanation of ideas and information via language; understands relationship between communication and meaning Logical-Mathematical Logical thinking, detecting patterns, scientic reasoning and deduction; analyse problems, perform mathematical calculations, understands relationship between cause and effect towards a tangible outcome Musical Musical ability, awareness, appreciation and use of sound; recognition of tonal and rhythmic patterns, understands relationship between sound and feeling Bodily-Kinesthetic Body movement control, manual dexterity, physical agility and balance; eye and body coordination Spatial-Visual Visual and spatial perception; interpretation and creation of images; pictorial imagination and expression; understands relationship between images and meanings, and between",Engineering General  Intelligence Part 1,chapter 4
"space and effect Interpersonal Perception of other peoples feelings; relates to others; interpretation of behaviour and communications; understands relationships between people and their situations      4.3 Further Characterizations of Human-Like Intelligence 69 1. Behave as an (almost) arbitrary function of the environment 2. Operate in real time 3. Exhibit rational, i.e., effective adaptive behavior 4. Use vast amounts of knowledge about the environment 5. Behave robustly in the face of error, the unexpected, and the unknown 6. Integrate diverse knowledge 7. Use (natural) language 8. Exhibit self-awareness and a sense of self 9. Learn from its environment 10. Acquire capabilities through development 11. Arise through evolution 12. Be realizable within the brain.",Engineering General  Intelligence Part 1,chapter 4
"In our view, Newells criterion 1 is poorly-formulated, for while universal Turing computing power is easy to come by, any nite AI system must inevitably be heavily adapted to some particular class of environments for straightforward mathematical reasons [Hut05a, GPI+10]. On the other hand, his criteria 11 and 12 are not relevant to the CogPrime approach as we are not doing biological modeling but rather AGI engineering. However, Newells criteria 210 are essential in our view, and all will be covered in the following chapters. 4.3.4 Intelligence and Creativity Creativity is a key aspect of intelligence. While sometimes associated especially with genius-level intelligence in science or the arts, actually creativity is pervasive throughout intelligence, at all levels.",Engineering General  Intelligence Part 1,chapter 4
"When a child makes a ying toy car by pasting paper bird wings on his toy car, and when a bird gures out how to use a curved stick to get a piece of food out of a difcult cornerthis is creativity, just as much as the invention of a new physics theory or the design of a new fashion line. The very nature of intelligenceachieving complex goals in complex environments requires creativity for its achievement, because the nature of complex environments and goals is that they are always unveiling new aspects, so that dealing with them involves inventing things beyond what worked for previously known aspects.",Engineering General  Intelligence Part 1,chapter 4
"CogPrime contains a number of cognitive dynamics that are especially effective at creating new ideas, such as: concept creation (which synthesizes new concepts via combining aspects of previous ones), probabilistic evolutionary learning (which simulates evolution by natural selection, creating new procedures via mutation, combinationandprobabilisticmodelingbasedonpreviousones),andanalogicalinference (an aspect of the Probabilistic Logic Networks subsystems). But ultimately creativity is about how a system combines all the processes at its disposal to synthesize novel solutions to the problems posed by its goals in its environment. There are times, of course, when the same goal can be achieved in multiple wayssome more creative than others. In CogPrime this relates to the existence of      70 4 What Is Human-Like General Intelligence? multiple top-level goals, one of which may be novelty.",Engineering General  Intelligence Part 1,chapter 4
"A system with novelty as one of its goals, alongside other more specic goals, will have a tendency to solve other problems in creative ways, thus fullling its novelty goal along with its other goals. This can be seen at the level of childlike behaviors, and also at a much more advanced level. Salvador Dali wanted to depict his thoughts and feelings, but he also wanted to do so in a striking and unusual way; this combination of aspirations spurred him to produce his amazing art. A child who is asked to draw a house, but has a goal of novelty, may draw a tower with a swimming pool on the roof rather than a typical Colonial structure. A physical motivated by novelty will seek a non-obvious solution to the equation at hand, rather than just applying tried and true methods, and perhaps discover some new phenomenon.",Engineering General  Intelligence Part 1,chapter 4
"Novelty can be measured formally in terms of information-theoretic surprisingness based upon a given basis of knowledge and experience [Sch06]; something that is novel and creative to a child may be familiar to the adult world, and a solution that seems novel and creative to a brilliant scientist today, may seem like cliche elementary school level work 100 years from now. Measuring creativity is even more difcult and subjective than measuring intelligence. Qualitatively, however, we humans can recognize it; and we suspect that the qualitative emergence of dramatic, multidisciplinary computational creativity will be one of the things that makes the human population feel emotionally that advanced AGI has nally arrived. 4.4 Preschool as a View into Human-Like General Intelligence One issue that arises when pursuing the grand goal of human-level general intelligence is how to measure partial progress.",Engineering General  Intelligence Part 1,chapter 4
"The classic Turing Test of imitating human conversation remains too difcult to usefully motivate immediate-term AI research (see [HF95] [Fre90] for arguments that it has been counterproductive for the AI eld). The same holds true for comparable alternatives like the Robot College Test of creating a robot that can attend a semester of university and obtain passing grades. However, some researchers have suggested intermediary goals, that constitute partial progress toward the grand goal and yet are qualitatively different from the highly specialized problems to which most current AI systems are applied. In this vein, Sam Adams and his team at IBM have outlined a so-called Toddler Turing Test, in which one seeks to use AI to control a robot qualitatively displaying similar cognitive behaviors to a young human child (say, a 3 year old) [AABL02].",Engineering General  Intelligence Part 1,chapter 4
"In fact this sort of idea has a long and venerable history in the AI eldAlan Turings original 1950 paper on AI [Tur50], where he proposed the Turing Test, contains the suggestion that Instead of trying to produce a programme to simulate the adult mind, why not rather try to produce one which simulates the childs?      4.4 Preschool as a View into Human-Like General Intelligence 71 We nd this childlike cognition based approach promising for many reasons, including its integrative nature: what a young child does involves a combination of perception, actuation, linguistic and pictorial communication, social interaction, conceptual problem solving and creative imagination. Specically, inspired by these ideas, in Chap. 17 we will suggest the approach of teaching and testing early-stage AGI systems in environments that emulate the preschools used for teaching human children.",Engineering General  Intelligence Part 1,chapter 4
"Human intelligence evolved in response to the demands of richly interactive environments, and a preschool is specically designed to be a richly interactive environment with the capability to stimulate diverse mental growth. So, we are currently exploring the use of CogPrime to control virtual agents in preschool-like virtual world environments, as well as commercial humanoid robot platforms such as the Nao (see Fig.4.1) or Robokind (Fig.4.2) in physical preschool-like robot labs. Another advantage of focusing on childlike cognition is that child psychologists have created a variety of instruments for measuring child intelligence. In Chap. 18, we will discuss an approach to evaluating the general intelligence of human childlike AGI systems via combining tests typically used to measure the intelligence of young human children, with additional tests crafted based on cognitive science and the standard preschool curriculum. Fig. 4.1 The Nao humanoid robot      72 4 What Is Human-Like General Intelligence? Fig.",Engineering General  Intelligence Part 1,chapter 4
"4.2 Robokind robot, designed by David Hanson To put it differently: While our long-term goal is the creation of genius machines with general intelligence at the human level and beyond, we believe that every young child has a certain genius; and by beginning with this childlike genius, we can built a platform capable of developing into a genius machine with far more dramatic capabilities. 4.4.1 Design for an AGI Preschool More precisely, we dont suggest to place a CogPrime system in an environment that is an exact imitation of a human preschoolthis would be inappropriate since current robotic or virtual bodies are very differently abled than the body of a young human child. But we aim to place CogPrime in an environment emulating the basic diversity and educational character of a typical human preschool. We stress this now, at this early point in the book, because we will use running examples throughout the book drawn from the preschool context.      4.",Engineering General  Intelligence Part 1,chapter 4
"4 Preschool as a View into Human-Like General Intelligence 73 The key notion in modern preschool design is the learning center, an area designed and outtted with appropriate materials for teaching a specic skill. Learning centers are designed to encourage learning by doing, which greatly facilitates learning processes based on reinforcement, imitation and correction; and also to provide multiple techniques for teaching the same skills, to accommodate different learning styles and prevent overtting and overspecialization in the learning of new skills. Centersarealsodesignedtocross-developrelatedskills.Amanipulativescenter, for example, provides physical objects such as drawing implements, toys and puzzles, to facilitate development of motor manipulation, visual discrimination, and (through sequencing and classication games) basic logical reasoning. A dramatics center cross-trains interpersonal and empathetic skills along with bodily-kinesthetic, linguistic, and musical skills.",Engineering General  Intelligence Part 1,chapter 4
"Other centers, such as art, reading, writing, science and math centers are also designed to train not just one area, but to center around a primary intelligence type while also cross-developing related areas. For specic examples of the learning centers associated with particular contemporary preschools, see [Nei98]. In many progressive, student-centered preschools, students are left largely to their own devices to move from one center to another throughout the preschool room. Generally, each center will be staffed by an instructor at some points in the day but not others, providing a variety of learning experiences. To imitate the general character of a human preschool, we will create several centers in our robot lab.",Engineering General  Intelligence Part 1,chapter 4
"The precise architecture will be adapted via experience but initial centers will likely be:  a blocks center: a table with blocks on it  a language center: a circle of chairs, intended for people to sit around and talk with the robot  a manipulatives center, with a variety of different objects of different shapes and sizes, intended to teach visual and motor skills  a ball play center: where balls are kept in chests and there is space for the robot to kick the balls around  a dramatics center where the robot can observe and enact various movements. One Running Example As we proceed through the various component structures and dynamics of CogPrime in the following chapters, it will be useful to have a few running examples to use to explain how the various parts of the system are supposed to work.",Engineering General  Intelligence Part 1,chapter 4
"One example we will use fairly frequently is drawn from the preschool context: the somewhat open-ended task of Build me something out of blocks, that you havent built for me before, and then tell me what it is. This is a relatively simple task that combines multiple aspects of cognition in a richly interconnected way, and is the sort of thing that young children will naturally do in a preschool setting.      74 4 What Is Human-Like General Intelligence? 4.5 Integrative and Synergetic Approaches to Articial General Intelligence CogPrime constitutes an integrative approach to AGI. And we suggest that the naturalness of integrative approaches to AGI follows directly from comparing above lists of capabilities and criteria to the array of available AI technologies.",Engineering General  Intelligence Part 1,chapter 4
"No single known algorithm or data structure appears easily capable of carrying out all these functions, so if one wants to proceed now with creating a general intelligence that is even vaguely humanlike, one must integrate various AI technologies within some sort of unifying architecture. For this reason and others, an increasing amount of work in the AI community these days is integrative in one sense or another. Estimation of Distribution Algorithms integrate probabilistic reasoning with evolutionary learning [Pel05]. Markov Logic Networks [RD06] integrate formal logic and probabilistic inference, as does the Probabilistic Logic Networks framework [GIGH08] utilized in CogPrime and explained further in the book, and other works in the Progic area such as [WW06]. Leslie Pack Kaelbling has synthesized low-level robotics methods (particle ltering) with logical inference [ZPK07]. Dozens of further examples could be given.",Engineering General  Intelligence Part 1,chapter 4
"The construction of practical robotic systems like the Stanley system that won the DARPA Grand Challenge [Tea06] involve the integration of numerous components based on different principles. These algorithmic and pragmatic innovations provide ample raw materials for the construction of integrative cognitive architectures and are part of the reason why childlike AGI is more approachable now than it was 50 or even 10 years ago. Further, many of the cognitive architectures described in the current AI literature are integrative in the sense of combining multiple, qualitatively different, interoperating algorithms. Chapter 6 gives a high-level overview of existing cognitive architectures, dividing them into symbolic, emergentist (e.g. neural network) and hybrid architectures. The hybrid architectures generally integrate symbolic and neural components, often with multiple subcomponents within each of these broad categories.",Engineering General  Intelligence Part 1,chapter 4
"However, we believe that even these excellent architectures are not integrative enough, in the sense that they lack sufciently rich and nuanced interactions between the learning components associated with different kinds of memory, and hence are unlikely to give rise to the emergent structures and dynamics characterizing general intelligence. One of the central ideas underlying CogPrime is that with an integrative cognitive architecture that combines multiple aspects of intelligence, achieved by diverse structures and algorithms, within a common framework designed specically to support robust synergetic interactions between these aspects. The simplest way to create an integrative AI architecture is to loosely couple multiple components carrying out various functions, in such a way that the different components pass inputs and outputs amongst each other but do not interfere with or modulate each others internal functioning in real-time. However, the human brain appears to be integrative in a much tighter sense, involving rich real-time dynamical coupling between various components with distinct but related functions.      4.",Engineering General  Intelligence Part 1,chapter 4
"5 Integrative and Synergetic Approaches to Articial General Intelligence 75 In [Goe09a] we have hypothesized that the brain displays a property of cognitive synergy, according to which multiple learning processes can not only dispatch subproblems to each other, but also share contextual understanding in real-time, so that each one can get help from the others in a contextually savvy way. By imbuing AI architectures with cognitive synergy, we hypothesize, one can get past the bottlenecks that have plagued AI in the past. Part of the reasoning here, as elaborated in Chap. 10 and [Goe09b], is that real physical and social environments display a rich dynamic interconnection between their various aspects, so that richly dynamically interconnected integrative AI architectures will be able to achieve goals within them more effectively. And this brings us to the patternist perspective on intelligent systems, alluded to above and eshed out further in Chap.",Engineering General  Intelligence Part 1,chapter 4
"5 with its focus on the emergence of hierarchically and heterarchically structured networks of patterns, and pattern-systems modeling self and others. Ultimately the purpose of cognitive synergy in an AGI system is to enable the various AI algorithms and structures composing the system to work together effectively enough to give rise to the right system-wide emergent structures characterizing real-world general intelligence. The underlying theory is that intelligence is not reliant on any particular structure or algorithm, but is reliant on the emergence of appropriately structured networks of patterns, which can then be used to guide ongoing dynamics of pattern recognition and creation. And the underlying hypothesis is that the emergence of these structures cannot be achieved by a loosely interconnected assemblage of components, no matter how sensible the architecture; it requires a tightly connected, synergetic system.",Engineering General  Intelligence Part 1,chapter 4
"It is possible to make these theoretical ideas about cognition mathematically rigorous; for instance, Appendix B briey presents a formal denition of cognitive synergy that has been analyzed as part of an effort to prove theorems about the importance of cognitive synergy for giving rise to emergent system properties associated with general intelligence. However, while we have found such formal analyses valuable for clarifying our designs and understanding their qualitative properties, we have concluded that, for the present, the best way to explore our hypotheses about cognitive synergy and human-like general intelligence is empiricallyvia building and testing systems like CogPrime. 4.5.1 Achieving Human-Like Intelligence via Cognitive Synergy Summing up: at the broadest level, there are four primary challenges in constructing an integrative, cognitive synergy based approach to AGI: 1. Choosing an overall cognitive architecture that possesses adequate richness and exibility for the task of achieving childlike cognition. 2.",Engineering General  Intelligence Part 1,chapter 4
"Choosing appropriate AI algorithms and data structures to fulll each of the functions identied in the cognitive architecture. (e.g. visual perception, audition, episodic memory, language generation, analogy,...)      76 4 What Is Human-Like General Intelligence? 3. Ensuring that these algorithms and structures, within the chosen cognitive architecture, are able to cooperate in such a way as to provide appropriate coordinated, synergetic intelligent behavior (a critical aspect since childlike cognition is an integrated functional response to the world, rather than a loosely coupled collection of capabilities.) 4. Embedding ones system in an environment that provides sufciently rich stimuli and interactions to enable the system to use this cooperation to ongoingly, creatively develop an intelligent internal world-model and self-model. We argue that CogPrime provides a viable way to address these challenges.",Engineering General  Intelligence Part 1,chapter 4
"  Chapter 5 A Patternist Philosophy of Mind 5.1 Introduction In the last chapter we discussed human intelligence from a fairly down-to-earth perspective, looking at the particular intelligent functions that human beings carry out in their everyday lives. And we strongly feel this practical perspective is important: Without this concreteness, its too easy for AGI research to get distracted by appealing (or frightening) abstractions of various sorts. However, its also important to look at the nature of mind and intelligence from a more general and conceptual perspective, to avoid falling into an approach that follows the particulars of human capability but ignores the deeper structures and dynamics of mind that ultimately allow human minds to be so capable.",Engineering General  Intelligence Part 1,chapter 5
"In this chapter we very briey review some ideas from the patternist philosophy of mind, a general conceptual framework on intelligence which has been inspirational for many key aspects of the CogPrime design, and which has been ongoingly developed by one of the authors (Ben Goertzel) during the last two decades (in a series of publications beginning in 1991, most recently The Hidden Pattern [Goe06a]). Some of the ideas described are quite broad and conceptual, and are related to CogPrime only via serving as general inspirations; others are more concrete and technical, and are actually utilized within the design itself. CogPrime is an integrative design formed via the combination of a number of different philosophical, scientic and engineering ideas. The success or failure of the design doesnt depend on any particular philosophical understanding of intelligence. In that sense, the more abstract notions presented in this chapter should be considered optional rather than critical in a CogPrime context.",Engineering General  Intelligence Part 1,chapter 5
"However, due to the core role patternism has played in the development of CogPrime, understanding a few things about general patternist philosophy will be helpful for understanding CogPrime, even for those readers who are not philosophically inclined. Those readers who are philosophically inclined, on the other hand, are urged to read The Hidden Pattern and then interpret the particulars of CogPrime in this light. B. Goertzel et al., Engineering General Intelligence, Part 1, 77 Atlantis Thinking Machines 5, DOI: 10.2991/978-94-6239-027-0_5,  Atlantis Press and the authors 2014      78 5 A Patternist Philosophy of Mind 5.2 Some Patternist Principles The patternist philosophy of mind is a general approach to thinking about intelligent systems.",Engineering General  Intelligence Part 1,chapter 5
"It is based on the very simple premise that mind is made of patternand that a mind is a system for recognizing patterns in itself and the world, critically including patterns regarding which procedures are likely to lead to the achievement of which goals in which contexts. Pattern as the basis of mind is not in itself is a very novel idea; this concept is present, for instance, in the nineteenth-century philosophy of Charles Peirce [Pei34], in the writings of contemporary philosophers Daniel Dennett [Den91] and Douglas Hofstadter [Hof79, Hof96], in Benjamin Whorfs [Who64] linguistic philosophy and Gregory Batesons [Bat79] systems theory of mind and nature. Bateson spoke of the Metapattern: that it is pattern which connects.",Engineering General  Intelligence Part 1,chapter 5
"In Goertzels writings on philosophy of mind, an effort has been made to pursue this theme more thoroughly than has been done before, and to articulate in detail how various aspects of human mind and mind in general can be well-understood by explicitly adopting a patternist perspective.1 In the patternist perspective, pattern is generally dened as representation as something simpler. Thus, for example, if one measures simplicity in terms of bitcount, then a program compressing an image would be a pattern in that image. But if one uses a simplicity measure incorporating run-time as well as bit-count, then the compressed version may or may not be a pattern in the image, depending on how ones simplicity measure weights the two factors. This denition encompasses simple repeated patterns, but also much more complex ones.",Engineering General  Intelligence Part 1,chapter 5
"While pattern theory has typically been elaborated in the context of computational theory, it is not intrinsically tied to computation; rather, it can be developed in any context where there is a notion of representation or production and a way of measuring simplicity. One just needs to be able to assess the extent to which f represents or produces X, and then to compare the simplicity of f and X; and then one can assess whether f is a pattern in X. A formalization of this notion of pattern is given in [Goe06a] and briey summarized at the end of this chapter. Next, in patternism the mind of an intelligent system is conceived as the (fuzzy) set of patterns in that system, and the set of patterns emergent between that system and other systems with which it interacts. The latter clause means that the patternist perspective is inclusive of notions of distributed intelligence [Hut96].",Engineering General  Intelligence Part 1,chapter 5
"Basically, the mind of a system is the fuzzy set of different simplifying representations of that system that may be adopted. Intelligence is conceived, similarly to in Marcus Hutters [Hut05] work (and as elaborated informally in Chap.4, and formally in Chap.8), as the ability to achieve complex goals in complex environments; where complexity itself may be dened as the possession of a rich variety of patterns. A mind is thus a collection of patterns 1 In some prior writings the term psynet model of mind has been used to refer to the application of patternist philosophy to cognitive theory, but this term has been deprecated in recent publications as it seemed to introduce more confusion than clarication.      5.2 Some Patternist Principles 79 that is associated with a persistent dynamical process that achieves highly-patterned goals in highly-patterned environments. An additional hypothesis made within the patternist philosophy of mind is that reection is critical to intelligence.",Engineering General  Intelligence Part 1,chapter 5
"This lets us conceive an intelligent system as a dynamical system that recognizes patterns in its environment and itself, as part of its quest to achieve complex goals. While this approach is quite general, it is not vacuous; it gives a particular structure to the tasks of analyzing and synthesizing intelligent systems. About any would-be intelligent system, we are led to ask questions such as:  How are patterns represented in the system? That is, how does the underlying infrastructure of the system give rise to the displaying of a particular pattern in the systems behavior?  What kinds of patterns are most compactly represented within the system?  What kinds of patterns are most simply learned?  What learning processes are utilized for recognizing patterns?  What mechanisms are used to give the system the ability to introspect (so that it can recognize patterns in itself)? Now, these same sorts of questions could be asked if one substituted the word pattern with other words like knowledge or information.",Engineering General  Intelligence Part 1,chapter 5
"However, we have found that asking these questions in the context of pattern leads to more productive answers, avoiding confusing byways and also tying in very nicely with the details of various existing formalisms and algorithms for knowledge representation and learning. Among the many kinds of patterns in intelligent systems, semiotic patterns are particularly interesting ones. Peirce decomposed these into three categories:  iconic patterns, which are patterns of contextually important internal similarity between two entities (e.g. an iconic pattern binds a picture of a person to that person)  indexical patterns, which are patterns of spatiotemporal co-occurrence (e.g. an indexical pattern binds a wedding dress and a wedding)  symbolic patterns, which are patterns indicating that two entities are often involved in the same relationships (e.g.",Engineering General  Intelligence Part 1,chapter 5
"a symbolic pattern between the number 5 (the symbol) and various sets of 5 objects (the entities that the symbol is taken to represent) Of course, some patterns may span more than one of these semiotic categories; and there are also some patterns that dont fall neatly into any of these categories. But the semiotic patterns are particularly important ones; and symbolic patterns have played an especially large role in the history of AI, because of the radically different approaches different researchers have taken to handling them in their AI systems. Mathematical logic and related formalisms provide sophisticated mechanisms for combining and relating symbolic patterns (symbols), and some AI approaches have focused heavily on these, sometimes more so than on the identication of      80 5 A Patternist Philosophy of Mind symbolic patterns in experience or the use of them to achieve practical goals. We will look fairly carefully at these differences in Chap.6.",Engineering General  Intelligence Part 1,chapter 5
"Pursuing the patternist philosophy in detail leads to a variety of particular hypotheses and conclusions about the nature of mind. Following from the view of intelligence in terms of achieving complex goals in complex environments, comes a view in which the dynamics of a cognitive system are understood to be governed by two main forces:  self-organization, via which system dynamics cause existing system patterns to give rise to new ones  goal-oriented behavior, which will be dened more rigorously in Chap.8, but basicallyamounts toasysteminteractingwithits environment inawaythat appears like an attempt to maximize some reasonably simple function Self-organizedandgoal-orientedbehaviormustbeunderstoodascooperativeaspects. If an agent is asked to build a surprising structure out of blocks and does so, this is goal-oriented. But the agents ability to carry out this goal-oriented task will be greater if it has previously played around with blocks a lot in an unstructured, spontaneous way.",Engineering General  Intelligence Part 1,chapter 5
"And the nudge toward creativity given to it by asking it to build a surprising blocks structure may cause it to explore some novel patterns, which then feed into its future unstructured blocks play. Based on these concepts, as argued in detail in [Goe06a], several primary dynamical principles may be posited, including:  Evolution, conceived as a general process via which patterns within a large population thereof are differentially selected and used as the basis for formation of new patterns, based on some tness function that is generally tied to the goals of the agent.  Example: If trying to build a blocks structure that will surprise Bob, an agent may simulate several procedures for building blocks structures in its minds eye, assessing for each one the expected degree to which it might surprise Bob. The search through procedure space could be conducted as a form of evolution, via an algorithm such as MOSES (see Chap.16 of Part 2).  Autopoiesis.",Engineering General  Intelligence Part 1,chapter 5
"The process by which a system of interrelated patterns maintains its integrity, via a dynamic in which whenever one of the patterns in the system begins to decrease in intensity, some of the other patterns increase their intensity in a manner that causes the troubled pattern to increase in intensity again.  Example: An agents set of strategies for building the base of a tower, and its set of strategies for building the middle part of a tower, are likely to relate autopoietically. If the system partially forgets how to build the base of a tower, then it may regenerate this missing knowledge via using its knowledge about how to build the middle part (i.e., it knows it needs to build the base in a way that will support good middle parts). Similarly if it partially forgets how to build the middle part, then it may regenerate this missing knowledge via using its knowledge about how to build the base (i.e.",Engineering General  Intelligence Part 1,chapter 5
"it knows a good middle part should t in well with the sorts of base it knows are good).      5.2 Some Patternist Principles 81  This same sort of interdependence occurs between pattern-sets containing more than two elements.  Sometimes (as in the above example) autopoietic interdependence in the mind is tied to interdependencies in the physical world, sometimes not.  Association. Patterns, when given attention, spread some of this attention to other patterns that they have previously been associated with in some way. Furthermore, there is Peirces law of mind [Pei34], which could be paraphrased in modern terms as stating that the mind is an associative memory network, whose dynamics dictate that every idea in the memory is an active agent, continually acting on those ideas with which the memory associates it.",Engineering General  Intelligence Part 1,chapter 5
"Example: Building a blocks structure that resembles a tower, spreads attention to memories of prior towers the agents has seen, and also to memories of people the agent knows have seen towers, and structures it has built at the same time as towers, structures that resemble towers in various respects, etc.  Differentialattentionallocation/creditassignment.Patternsthathavebeenvaluable for goal-achievement are given more attention, and are encouraged to participate in giving rise to new patterns.  Example: Perhaps in a prior instance of the task build me a surprising structure out of blocks, searching through memory for non-blocks structures that the agent has played with has proved a useful cognitive strategy. In that case, when the task is posed to the agent again, it should tend to allocate disproportionate resources to this strategy.  Pattern creation. Patterns that have been valuable for goal-achievement are mutated and combined with each other to yield new patterns.",Engineering General  Intelligence Part 1,chapter 5
"Example: Building towers has been useful in a certain context, but so has building structures with a large number of triangles. Why not build a tower out of triangles? Or maybe a vaguely tower-like structure that uses more triangles than a tower easily could?  Example: Building an elongated block structure resembling a table was successful in the past, as was building a structure resembling a very at version of a chair. Generalizing, maybe building distorted versions of furniture is good. Or maybe it is building distorted version of any previously perceived objects that is good. Or maybe both, to different degrees.... Next, for a variety of reasons outlined in [Goe06a] it becomes appealing to hypothesizethat thenetworkof patterns inanintelligent systemmust giverisetothefollowing large-scale emergent structures  Hierarchical network. Patterns are habitually in relations of control over other patterns that represent more specialized aspects of themselves.",Engineering General  Intelligence Part 1,chapter 5
"     82 5 A Patternist Philosophy of Mind  Example: The pattern associated with tall building has some control over the pattern associated with tower, as the former represents a more general concept ... and tower has some control over Eiffel tower, etc.  Heterarchical network. The system retains a memory of which patterns have previously been associated with each other in any way.  Example: Tower and snake are distant in the natural pattern hierarchy, but may be associatively/heterarchically linked due to having a common elongated structure. This heterarchical linkage may be used for many things, e.g. it might inspire the creative construction of a tower with a snakes head.  Dual network. Hierarchical and heterarchical structures are combined, with the dynamics of the two structures working together harmoniously.",Engineering General  Intelligence Part 1,chapter 5
"Among many possiblewaystohierarchicallyorganizeasetofpatterns,theoneusedshouldbeone that causes hierarchically nearby patterns to have many meaningful heterarchical connections; and of course, there should be a tendency to search for heterarchical connections among hierarchically nearby patterns.  Example: While the set of patterns hierarchically nearby tower and the set of patterns heterarchically nearby tower will be quite different, they should still have more overlap than random pattern-sets of similar sizes. So, if looking for something else heterarchically near tower, using the hierarchical information about tower should be of some use, and vice versa.  In CogPrimes probabilistic logic subsystem, PLN (see Chap.17 of Part 2) hierarchical relationships correspond to Atoms A and B so that Inheritance AB and Inheritance BA have highly dissimilar strength; and heterarchical relationships correspond to IntensionalSimilarity relationships.",Engineering General  Intelligence Part 1,chapter 5
"The dual network structure then arises when intensional and extensional inheritance approximately correlate with each other, so that inference about either kind of inheritance assists with guring out about the other kind.  Self structure. A portion of the network of patterns forms into an approximate image of the overall network of patterns.  Example:Eachtimetheagentbuildsacertainstructure,itobservesitselfbuilding the structure, and its role as builder of a tall tower (or whatever the structure is) becomes part of its self-model. Then when it is asked to build something new, it may consult its self-model to see if it believes itself capable of building that sort of thing (for instance, if it is asked to build something very large, its self-model may tell it that it lacks persistence for such projects, so it may reply I can try, but I may wind up not nishing it).",Engineering General  Intelligence Part 1,chapter 5
"As we proceed through the CogPrime design in the following pages, we will see how each of these abstract concepts arises concretely from CogPrimes structures and algorithms. If the theory of [Goe06a] is correct, then the success of CogPrime as a design will depend largely on whether these high-level structures and dynamics can be made to emerge from the synergetic interaction of CogPrimes representation and algorithms, when they are utilized to control an appropriate agent in an appropriate environment.      5.3 Cognitive Synergy 83 5.3 Cognitive Synergy Now we dig a little deeper and present a different sort of general principle of feasible general intelligence, already hinted in earlier chapters: the cognitive synergy principle,2 which is both a conceptual hypothesis about the structure of generally intelligent systems in certain classes of environments, and a design principle used to guide the design of CogPrime.",Engineering General  Intelligence Part 1,chapter 5
"Chapter9 presents a mathematical formalization of the notion of cognitive synergy; here we present the conceptual idea informally, which makes it more easily digestible but also more vague-sounding. We will focus here on cognitive synergy specically in the case of multi-memory systems, which we dene as intelligent systems whose combination of environment, embodimentandmotivationalsystemmakeitimportantforthemtopossessmemories that divide into partially but not wholly distinct components corresponding to the categories of:  Declarative memory  Examples of declarative knowledge: Towers on average are taller than buildings. I generally am better at building structures I imagine, than at imitating structures Im shown in pictures.  Procedural memory (memory about how to do certain things)  Examples of procedural knowledge: Practical know-how regarding how to pick up an elongated rectangular block, or a square one.",Engineering General  Intelligence Part 1,chapter 5
"Know-how regarding when to approach a problem by asking What would one of my teachers do in this situation versus by thinking through the problem from rst principles.  Sensory and episodic memory  Example of sensory knowledge: Memory of Bobs face; memory of what a specic tall blocks tower looked like.  Example of episodic knowledge: Memory of the situation in which the agent rst met Bob; memory of a situation in which a specic tall blocks tower was built.  Attentional memory (knowledge about what to pay attention to in what contexts)  Example of attentional knowledge: When involved with a new person, its useful to pay attention to whatever that person looks at.  Intentional memory (knowledge about the systems own goals and subgoals)  Example of intentional knowledge: If my goal is to please some person whom I dont know that well, then a subgoal may be guring out what makes that person smile.",Engineering General  Intelligence Part 1,chapter 5
"2 While these points are implicit in the theory of mind given in [Goe06a], they are not articulated in this specic form there. So the material presented in this section is a new development within patternist philosophy, developed since [Goe06a] in a series of conference papers such as [Goe09a].      84 5 A Patternist Philosophy of Mind In Chap.10 we present a detailed argument as to how the requirement for a multi-memory underpinning for general intelligence emerges from certain underlying assumptions regarding the measurement of the simplicity of goals and environments. Specically we argue that each of these memory types corresponds to certain modes of communication, so that intelligent agents which have to efciently handle a sufcient variety of types of communication with other agents, are going to have to handle all these types of memory.",Engineering General  Intelligence Part 1,chapter 5
"These types of communication overlap and are often used together, which implies that the different memories and their associated cognitive processes need to work together. The points made in this section do not rely on that argument regarding the relation of multiple memory types to the environmental situation of multiple communication types. What they do rely on is the assumption that, in the intelligence agent in question, the different components of memory are signicantly but not wholly distinct. That is, there are signicant family resemblances between the memories of a single type, yet there are also thoroughgoing connections between memories of different types. Repeatingtheabovepointsinaslightlymoreorganizedmannerandthenextending them, the essential idea of cognitive synergy, in the context of multi-memory systems, may be expressed in terms of the following points 1. Intelligence, relative to a certain set of environments, may be understood as the capability to achieve complex goals in these environments. 2.",Engineering General  Intelligence Part 1,chapter 5
"With respect to certain classes of goals and environments, an intelligent system requires a multi-memory architecture, meaning the possession of a number of specialized yet interconnected knowledge types, including: declarative, procedural, attentional, sensory, episodic and intentional (goal-related). These knowledge types may be viewed as different sorts of patterns that a system recognizes in itself and its environment. 3. Such a system must possess knowledge creation (i.e. pattern recognition / formation) mechanisms corresponding to each of these memory types. These mechanisms are also called cognitive processes. 4. Each of these cognitive processes, to be effective, must have the capability to recognize when it lacks the information to perform effectively on its own; and in this case, to dynamically and interactively draw information from knowledge creation mechanisms dealing with other types of knowledge. 5. This cross-mechanism interaction must have the result of enabling the knowledge creation mechanisms to perform much more effectively in combination than they would if operated non-interactively.",Engineering General  Intelligence Part 1,chapter 5
"This is cognitive synergy. Interactions as mentioned in Points 4 and 5 in the above list are the real conceptual meat of the cognitive synergy idea. One way to express the key idea here, in an AI context, is that most AI algorithms suffer from combinatorial explosions: the number of possible elements to be combined in a synthesis or analysis is just too great, and the algorithms are unable to lter through all the possibilities, given the lack of intrinsic constraint that comes along with a general intelligence context (as opposed to a narrow-AI problem like chess-playing, where the context is constrained and hence restricts the scope of possible combinations that needs to be considered). In an AGI      5.",Engineering General  Intelligence Part 1,chapter 5
"3 Cognitive Synergy 85 architecture based on cognitive synergy, the different learning mechanisms must be designed specically to interact in such a way as to palliate each others combinatorial explosionsso that, for instance, each learning mechanism dealing with a certain sort of knowledge, must synergize with learning mechanisms dealing with the other sorts of knowledge, in a way that decreases the severity of combinatorial explosion. One prerequisite for cognitive synergy to work is that each learning mechanism must recognize when it is stuck, meaning its in a situation where it has inadequate information to make a condent judgment about what steps to take next. Then, when it does recognize that its stuck, it may request help from other, complementary cognitive mechanisms. 5.4 The General Structure of Cognitive Dynamics: Analysis and Synthesis We have discussed the need for synergetic interrelation between cognitive processes corresponding to different types of memory ...",Engineering General  Intelligence Part 1,chapter 5
"and the general high-level cognitive dynamics that a mind must possess (evolution, autopoiesis). The next step is to dig further into the nature of the cognitive processes associated with different memory types and how they give rise to the needed high-level cognitive dynamics. In this section we present a general theory of cognitive processes based on a decomposition of cognitive processes into the two categories of analysis and synthesis, and a general formulation of each of these categories.3 Specically we concentrate here on what we call focused cognitive processes; that is, cognitive processes that selectively focus attention on a subset of the patterns making up a mind. In general these are not the only kind, there may also be global cognitive processes that act on every pattern in a mind. An example of a global cognitive process in CogPrime is the basic attention allocation process, which spreads importance among all knowledge in the systems memory.",Engineering General  Intelligence Part 1,chapter 5
"Global cognitive processes are also important, but focused cognitive processes are subtler to understand which is why we spend more time on them here. 5.4.1 Component-Systems and Self-Generating Systems Webeginwithautopoiesisand,morespecically,withtheconceptofacomponentsystem, as described in George Kampiss book Self-Modifying Systems in Biology and Cognitive Science [Kam91], and as modied into the concept of a self-generating system or SGS in Goertzels book Chaotic Logic [Goe94]. Roughly 3 While these points are highly compatible with theory of mind given in [Goe06a], they are not articulated there. The material presented in this section is a new development within patternist philosophy, presented previously only in the article [GPPG06].",Engineering General  Intelligence Part 1,chapter 5
"     86 5 A Patternist Philosophy of Mind speaking, a Kampis-style component-system consists of a set of components that combine with each other to form other compound components. The metaphor Kampis uses is that of Lego blocks, combining to form bigger Lego structures. Compound structures may in turn be combined together to form yet bigger compound structures. A self-generating system is basically the same concept as a component-system, but understood to be computable, whereas Kampis claims that component-systems are uncomputable. Next, in SGS theory there is also a notion of reduction (not present in the Lego metaphor): sometimes when components are combined in a certain way, a reaction happens, which may lead to the elimination of some of the components. One relevant metaphor here is chemistry. Another is abstract algebra: for instance, if we combine a component f with its inverse component f 1, both components are eliminated.",Engineering General  Intelligence Part 1,chapter 5
"Thus, we may think about two stages in the interaction of sets of components: combination, and reduction. Reduction may be thought of as algebraic simplication, governed by a set of rules that apply to a newly created compound component, based on the components that are assembled within it. Formally, suppose C1, C2, ... is the set of components present in a discrete-time component-system at time t. Then, the components present at time t + 1 are a subset of the set of components of the form Reduce(Join(Ci(1), ..., Ci(r))) where Join is a joining operation, and Reduce is a reduction operator. The joining operationisassumedtomaptuplesofcomponentsintocomponents,andthereduction operator is assumed to map the space of components into itself.",Engineering General  Intelligence Part 1,chapter 5
"Of course, the specic nature of a component system is totally dependent on the particular denitions of the reduction and joining operators; in following chapters we will specify these for the CogPrime system, but for the purpose of the broader theoretical discussion in this section they may be left general. What is called the cognitive equation in Chaotic Logic [Goe94] is the case of a SGS where the patterns in the system at time t have a tendency to correspond to components of the system at future times t + s. So, part of the action of the system is to transform implicit knowledge (patterns among system components) into explicit knowledge (specic system components). We will see one version of this phenomenon in Chap.15 where we model implicit knowledge using mathematical structures called derived hypergraphs; and we will also later review several ways in which CogPrimes dynamics explicitly encourage cognitive-equation type dynamics, e.g.",Engineering General  Intelligence Part 1,chapter 5
"inference, which takes conclusions implicit in the combination of logical relationships, and makes them implicit by deriving new logical relationships from them  map formation, which takes concepts that have often been active together, and creates new concepts grouping them  association learning, which creates links representing patterns of association between entities      5.4 The General Structure of Cognitive Dynamics: Analysis and Synthesis 87  probabilistic procedure learning, which creates new models embodying patterns regarding which procedures tend to perform well according to particular tness functions. 5.4.2 Analysis and Synthesis Now we move on to the main point of this section: the argument that all or nearly all focused cognitive processes are expressible using two general process-schemata we call synthesis and analysis.",Engineering General  Intelligence Part 1,chapter 5
"4 The notion of focused cognitive process will be exemplied more thoroughly below, but in essence what is meant is a cognitive process that begins with a small number of items (drawn from memory) as its focus, and has as its goal discovering something about these items, or discovering something about something else in the context of these items or in a way strongly biased by these items. This is different from a global cognitive process whose goal is more broadly-based and explicitly involves all or a large percentage of the knowledge in an intelligent systems memory store. Among the focused cognitive processes are those governed by the so-called cognitive schematic implication Context  Procedure  Goal where the Context involves sensory, episodic and/or declarative knowledge; and attentional knowledge is used to regulate how much resource is given to each such schematic implication in memory. Synergy among the learning processes dealing with the context, the procedure and the goal is critical to the adequate execution of the cognitive schematic using feasible computational resources.",Engineering General  Intelligence Part 1,chapter 5
"This sort of explicitly goal-driven cognition plays a signicant though not necessarily dominant role in CogPrime, and is also related to production rules systems and other traditional AI systems, as will be articulated in Chap.6. The synthesis and analysis processes as we conceive them, in the general framework of SGS theory, are as follows. First, synthesis, as shown in Fig.5.1, is dened as Synthesis: Iteratively build compounds from the initial component pool using the combinators, greedily seeking compounds that seem likely to achieve the goal. Or in more detail: 1.",Engineering General  Intelligence Part 1,chapter 5
"Begin with some initial components (the initial current pool), an additional set of components identied as combinators (combination operators), and a goal function 4 In [GPPG06], what is here called analysis was called backward synthesis, a name which has some advantages since it indicated that whats happening is a form of creation; but here we have opted for the more traditional analysis/synthesis terminology.      88 5 A Patternist Philosophy of Mind Fig. 5.1 The general process of synthesis 2. Combine the components in the current pool, utilizing the combinators, to form product components in various ways, carrying out reductions as appropriate, and calculating relevant quantities associated with components as needed 3. Select the product components that seem most promising according to the goal function, and add these to the current pool (or else simply dene these as the current pool) 4. Return to Step 2 And analysis, as shown in Fig.5.",Engineering General  Intelligence Part 1,chapter 5
"2, is dened as Analysis: Iteratively search (the systems long-term memory) for component-sets that combine using the combinators to form the initial component pool (or subsets thereof), greedily seeking component-sets that seem likely to achieve the goal. Or in more detail: 1. Begin with some components (the initial current pool) and a goal function 2. Seek components so that, if one combines them to form product components using the combinators and then performs appropriate reductions, one obtains (as many as possible of) the components in the current pool 3. Use the newly found constructions of the components in the current pool, to update the quantitative properties of the components in the current pool, and also (via the current pool) the quantitative properties of the components in the initial pool 4.",Engineering General  Intelligence Part 1,chapter 5
"Out of the components found in Step 2, select the ones that seem most promising according to the goal function, and add these to the current pool (or else simply dene these as the current pool)      5.4 The General Structure of Cognitive Dynamics: Analysis and Synthesis 89 Fig. 5.2 The general process of analysis 5. Return to Step 2 More formally, synthesis may be specied as follows. Let X denote the set of combinators, and let Y0 denote the initial pool of components (the initial focus of the cognitive process). Given Yi, let Zi denote the set Reduce(Join(Ci(1), ..., Ci(r))) where the Ci are drawn from Yi or from X. We may then say Yi+1 = Filter(Zi) where Filter is a function that selects a subset of its arguments.",Engineering General  Intelligence Part 1,chapter 5
"Analysis, on the other hand, begins with a set W of components, and a set X of combinators, and tries to nd a series Yi so that according to the process of synthesis, Yn = W. In practice, of course, the implementation of a synthesis process need not involve the explicit construction of the full set Zi. Rather, the ltering operation takes place implicitly during the construction of Yi+1. The result, however, is that one gets some subset of the compounds producible via joining and reduction from the set of components present in Yi plus the combinators X. Conceptually one may view synthesis as a very generic sort of growth process, and analysis as a very generic sort of guring out how to grow something.",Engineering General  Intelligence Part 1,chapter 5
"     90 5 A Patternist Philosophy of Mind The intuitive idea underlying the present proposal is that these forward-going and backward-going growth processes are among the essential foundations of cognitive control, and that a conceptually sound design for cognitive control should explicitly make use of this fact. To abstract away from the details, what these processes are about is:  taking the general dynamic of compound-formation and reduction as outlined in Kampis and Chaotic Logic  introducing goal-directed pruning (ltering) into this dynamic so as to account for the limitations of computational resources that are a necessary part of pragmatic intelligence. 5.4.3 The Dynamic of Iterative Analysis and Synthesis While synthesis and analysis are both very useful on their own, they achieve their greatest power when harnessed together. It is my hypothesis that the dynamic pattern of alternating synthesis and analysis has a fundamental role in cognition. Put simply, synthesis creates new mental forms by combining existing ones.",Engineering General  Intelligence Part 1,chapter 5
"Then, analysis seeks simple explanations for the forms in the mind, including the newly created ones; and, this explanation itself then comprises additional new forms in the mind, to be used as fodder for the next round of synthesis. Or, to put it yet more simply:  Combine  Explain  Combine  Explain  Combine  It is not hard to express this alternating dynamic more formally, as well.  Let X denote any set of components.  Let F(X) denote a set of components which is the result of synthesis on X.  Let B(X) denote a set of components which is the result of analysis of X. We assume also a heuristic biasing the synthesis process toward simple constructs.  Let S(t) denote a set of components at time t, representing part of a systems knowledge base.  Let I(t) denote components resulting from the external environment at time t.",Engineering General  Intelligence Part 1,chapter 5
"Then, we may consider a dynamical iteration of the form S(t + 1) = B(F(S(t) + I(t))) This expresses the notion of alternating synthesis and analysis formally, as a dynamical iteration on the space of sets of components. We may then speak about attractors of this iteration: xed points, limit cycles and strange attractors. One of the key hypotheses we wish to put forward here is that some key emergent cognitive structures are strange attractors of this equation. The iterative dynamic of combination and explanation leads to the emergence of certain complex structures that are, in essence, maintained when one recombines their parts and then seeks to explain the      5.4 The General Structure of Cognitive Dynamics: Analysis and Synthesis 91 recombinations. These structures are built in the rst place through iterative recombination and explanation, and then survive in the mind because they are conserved by this process.",Engineering General  Intelligence Part 1,chapter 5
"They then ongoingly guide the construction and destruction of various other temporary mental structures that are not so conserved. 5.4.4 Self and Focused Attention as Approximate Attractors of the Dynamic of Iterated Forward-Analysis As noted above, patternist philosophy argues that two key aspects of intelligence are emergent structures that may be called the self and the attentional focus. These, it is suggested, are aspects of intelligence that may not effectively be wired into the infrastructure of an intelligent system, though of course the infrastructure may be congured in such a way as to encourage their emergence. Rather, these aspects, by their nature, are only likely to be effective if they emerge from the cooperative activity of various cognitive processes acting within a broad base of knowledge. Above we have described the pattern of ongoing habitual oscillation between synthesis and analysis as a kind of dynamical iteration. Here we will argue that both self and attentional focus may be viewed as strange attractors of this iteration.",Engineering General  Intelligence Part 1,chapter 5
"The mode of argument is relatively informal. The essential processes under consideration are ones that are poorly understood from an empirical perspective, due to the extreme difculty involved in studying them experimentally. For understanding self and attentional focus, we are stuck in large part with introspection, which is famously unreliable in some contexts, yet still dramatically better than having no information at all. So, the philosophical perspective on self and attentional focus given here is a synthesis of empirical and introspective notions, drawn largely from the published thinking and research of others but with a few original twists. From a CogPrime perspective, its use has been to guide the design process, to provide a grounding for what otherwise would have been fairly arbitrary choices. 5.4.4.1 Self Another high-level intelligent system pattern mentioned above is the self, which we here will tie in with analysis and synthesis processes. The term self as used here refers to the phenomenal self [Met04] or self-model.",Engineering General  Intelligence Part 1,chapter 5
"That is, the self is the model that a system builds internally, reecting the patterns observed in the (external and internal) world that directly pertain to the system itself. As is well known in everyday human life, self-models need not be completely accurate to be useful; and in the presence of certain psychological factors, a more accurate self-model may not necessarily be advantageous. But a self-model that is too badly inaccurate will lead to a badly-functioning system that is unable to effectively act toward the achievement of its own goals.      92 5 A Patternist Philosophy of Mind The value of a self-model for any intelligent system carrying out embodied agentive cognition is obvious. And beyond this, another primary use of the self is as a foundation for metaphors and analogies in various domains. Patterns recognized pertaining to the self are analogically extended to other entities.",Engineering General  Intelligence Part 1,chapter 5
"In some cases this leads to conceptual pathologies, such as the anthropomorphization of trees, rocks and other such objects that one sees in some precivilized cultures. But in other cases this kind of analogy leads to robust sorts of reasoningfor instance, in reading Lakoff and Nunezs [LN00] intriguing explorations of the cognitive foundations of mathematics, it is pretty easy to see that most of the metaphors on which they hypothesize mathematics to be based, are grounded in the minds conceptualization of itself as a spatiotemporally embedded entity, which in turn is predicated on the minds having a conceptualization of itself (a self) in the rst place. A self-model can in many cases form a self-fullling prophecy (to make an obvious double-entendre!). Actions are generated based on ones model of what sorts of actions one can and/or should take; and the results of these actions are then incorporated into ones self-model.",Engineering General  Intelligence Part 1,chapter 5
"If a self-model proves a generally bad guide to action selection, this may never be discovered, unless said self-model includes the knowledge that semi-random experimentation is often useful. In what sense, then, may it be said that self is an attractor of iterated analysis? Analysis infers the self from observations of system behavior. The system asks: What kind of system might I be, in order to give rise to these behaviors that I observe myself carrying out? Based on asking itself this question, it constructs a model of itself, i.e. it constructs a self. Then, this self guides the systems behavior: it builds new logical relationships its self-model and various other entities, in order to guide its future actions oriented toward achieving its goals. Based on the behaviors newly induced via this constructive, forward-synthesis activity, the system may then engage in analysis again and ask: What must I be now, in order to have carried out these new actions? And so on.",Engineering General  Intelligence Part 1,chapter 5
"Our hypothesis is that after repeated iterations of this sort, in infancy, nally during early childhood a kind of self-reinforcing attractor occurs, and we have a self-model that is resilient and doesnt change dramatically when new instances of actionor explanation-generation occur. This is not strictly a mathematical attractor, though, because over a long period of time the self may well shift signicantly. But, for a mature self, many hundreds of thousands or millions of forward-analysis cycles may occur before the self-model is dramatically modied. For relatively long periods of time, small changes within the context of the existing self may sufce to allow the system to control itself intelligently. Humans can also develop what are known as subselves [Row90]. A subself is a partially autonomous self-network focused on particular tasks, environments or interactions.",Engineering General  Intelligence Part 1,chapter 5
"It contains a unique model of the whole organism, and generally has its own set of episodic memories, consisting of memories of those intervals during which it was the primary dynamic mode controlling the organism. One common example is the creative subselfthe subpersonality that takes over when a creative person launches into the process of creating something. In these times, a whole different personality sometimes emerges, with a different sort of relationship to the      5.4 The General Structure of Cognitive Dynamics: Analysis and Synthesis 93 world. Among other factors, creativity requires a certain open-ness that is not always productive in an everyday life context, so its natural for the self-system of a highly creative person to bifurcate into one self-system for everyday life, and another for the protected context of creative activity. This sort of phenomenon might emerge naturally in CogPrime systems as well if they were exposed to appropriate environments and social situations.",Engineering General  Intelligence Part 1,chapter 5
"Finally, it is interesting to speculate regarding how self may differ in future AI systems as opposed to in humans. The relative stability we see in human selves may not exist in AI systems that can self-improve and change more fundamentally and rapidly than humans can. There may be a situation in which, as soon as a system has understood itself decently, it radically modies itself and hence violates its existing self-model. Thus: intelligence without a long-term stable self. In this case the attractor-ish nature of the self holds only over much shorter time scales than for human minds or human-like minds. But the alternating process of synthesis and analysis for self-construction is still critical, even though no reasonably stable selfconstituting attractor ever emerges. The psychology of such intelligent systems will almost surely be beyond human beings capacity for comprehension and empathy. 5.4.4.",Engineering General  Intelligence Part 1,chapter 5
"2 Attentional Focus Now, we turn to the notion of an attentional focus similar to Baars [Baa97] notion of a Global Workspace, which will be reviewed in more detail in Chap.6: a collection of mental entities that are, at a given moment, receiving far more than the usual share of an intelligent systems computational resources. Due to the amount of attention paid to items in the attentional focus, at any given moment these items are in large part driving the cognitive processes going on elsewhere in the mind as wellbecause the cognitive processes acting on the items in the attentional focus are often involved in other mental items, not in attentional focus, as well (and sometimes this results in pulling these other items into attentional focus). An intelligent system must constantly shift its attentional focus from one set of entities to another based on changes in its environment and based on its own shifting discoveries.",Engineering General  Intelligence Part 1,chapter 5
"In the human mind, there is a self-reinforcing dynamic pertaining to the collection of entities in the attentional focus at any given point in time, resulting from the observation that: If A is in the attentional focus, and A and B have often been associated in the past, then odds are increased that B will soon be in the attentional focus. This basic observation has been rened tremendously via a large body of cognitive psychology work; and neurologically it follows not only from Hebbs [Heb49] classic work on neural reinforcement learning, but also from numerous more modern renements [SB98]. But it implies that two items A and B, if both in the attentional focus, can reinforce each others presence in the attentional focus, hence forming a kind of conspiracy to keep each other in the limelight.",Engineering General  Intelligence Part 1,chapter 5
"But of course, this kind of dynamic must be counteracted by a pragmatic tendency to remove items from the attentional focus if giving them attention is not providing sufcient utility in terms of the achievement of system goals.      94 5 A Patternist Philosophy of Mind The synthesis and analysis perspective provides a more systematic perspective on this self-reinforcing dynamic. Synthesis occurs in the attentional focus when two or more items in the focus are combined to form new items, new relationships, new ideas. This happens continually, as one of the main purposes of the attentional focus is combinational. On the other hand, Analysis then occurs when a combination that has been speculatively formed is then linked in with the remainder of the mind (the unconscious, the vast body of knowledge that is not in the attentional focus at the given moment in time). Analysis basically checks to see what support the new combination has within the existing knowledge store of the system.",Engineering General  Intelligence Part 1,chapter 5
"Thus, forward-analysis basically comes down to generate and test, where the testing takes the form of attempting to integrate the generated structures with the ideas in the unconscious long-term memory. One of the most obvious examples of this kind of dynamic is creative thinking [Bod03, Goe97], where the attentional focus continually combinationally creates new ideas, which are then tested via checking which ones can be validated in terms of (built up from) existing knowledge. The analysis stage may result in items being pushed out of the attentional focus, to be replaced by others. Likewise may the synthesis stage: the combinations may overshadow and then replace the things combined. However, in human minds and functional AI minds, the attentional focus will not be a complete chaos with constant turnover: Sometimes the same set of ideasor a shifting set of ideas within the same overall family of ideaswill remain in focus for a while.",Engineering General  Intelligence Part 1,chapter 5
"When this occurs it is because this set or family of ideas forms an approximate attractor for the dynamics of the attentional focus, in particular for the forward-analysis dynamic of speculative combination and integrative explanation. Often, for instance, a small core set of ideas will remain in the attentional focus for a while, but will not exhaust the attentionalfocus:therestoftheattentionalfocuswillthen,atanypointintime,beoccupied with other ideas related to the ones in the core set. Often this may mean that, for a while, the whole of the attentional focus will move around quasi-randomly through a strange attractor consisting of the set of ideas related to those in the core set. 5.4.",Engineering General  Intelligence Part 1,chapter 5
"5 Conclusion The ideas presented above (the notions of synthesis and analysis, and the hypothesis of self and attentional focus as attractors of the iterative forward-analysis dynamic) are quite generic and are hypothetically proposed to be applicable to any cognitive system, natural or articial. Later chapters will discuss the manifestation of the above ideas in the context of CogPrime. We have found that the analysis/synthesis approach is a valuable tool for conceptualizing CogPrimes cognitive dynamics, and we conjecture that a similar utility may be found more generally. Next, soas not toendthesectionontooblasof anote, wewill alsomakeastronger hypothesis: that, in order for a physical or software system to achieve intelligence that is roughly human-level in both capability and generality, using computational resources on the same order of magnitude as the human brain, this system must      5.",Engineering General  Intelligence Part 1,chapter 5
"4 The General Structure of Cognitive Dynamics: Analysis and Synthesis 95  manifest the dynamic of iterated synthesis and analysis, as modes of an underlying self-generating system dynamic  do so in such a way as to lead to self and attentional focus as emergent structures that serve as approximate attractors of this dynamic, over time periods that are long relative to the basic cognitive cycle time of the systems forward-analysis dynamics. To prove the truth of a hypothesis of this nature would seem to require mathematics fairly far beyond anything that currently exists. Nonetheless, however, we feel it is important to formulate and discuss such hypotheses, so as to point the way for future investigations both theoretical and pragmatic. 5.5 Perspectives on Machine Consciousness We cant let a chapter on philosophyeven a brief oneend without some discussion of the thorniest topic in the philosophy of mind: consciousness.",Engineering General  Intelligence Part 1,chapter 5
"Rather than seeking to resolve or comprehensively review this most delicate issue, we will restrict ourselves to discussing the relationship between consciousness theory and patternist philosophy of cognition, the practical work of designing and building AGI. One fairly concrete idea about consciousness, that relates closely to certain aspects of the CogPrime design, is that the subjective experience of being conscious of some entityX,iscorrelatedwiththepresenceofaveryintensepatterninonesoverallmindstate, corresponding to X. This simple idea is also the essence of neuroscientist Susan Greenelds theory of consciousness [Gre01] (but in her theory, overall mind-state is replaced with brain-state), and has much deeper historical roots in philosophy of mind which we shall not venture to unravel here. Thisobservationrelatestotheideaofmovingbubblesofawarenessinintelligent systems.",Engineering General  Intelligence Part 1,chapter 5
"If an intelligent system consists of multiple processing or data elements, and during each (sufciently long) interval of time some of these elements get much more attention than others, then one may view the system as having a certain attentional focus during each interval. The attentional focus is itself a signicant pattern in the system (the pattern being these elements habitually get more processor and memory, roughly speaking). As the attentional focus shifts over time one has a moving bubble of pattern which then corresponds experientially to a moving bubble of awareness. This notion of a moving bubble of awareness ties in very closely to global workspace theory [Baa97] (briey mentioned above), a cognitive theory that has broad support from neuroscience and cognitive science and has also served as the motivation for Stan Franklins LIDA AI system [BF09], to be discussed in Chap.6.",Engineering General  Intelligence Part 1,chapter 5
"The global workspace theory views the mind as consisting of a large population of small, specialized processesa society of agents. These agents organize themselves into coalitions, and coalitions that are relevant to contextually novel phenomena, or contextuallyimportantgoals,arepulledintotheglobalworkspace(whichisidentied      96 5 A Patternist Philosophy of Mind with consciousness). This workspace broadcasts the message of the coalition to all the unconscious agents, and recruits other agents into consciousness. Various sorts of contextse.g. goal contexts, perceptual contexts, conceptual contexts and cultural contextsplay a role in determining which coalitions are relevant, and form the unconscious background of the conscious global workspace. New perceptions are often, but not necessarily, pushed into the workspace. Some of the agents in the global workspace are concerned with action selection, i.e. with controlling and passing parameters to a population of possible actions.",Engineering General  Intelligence Part 1,chapter 5
"The contents of the workspace at any given time have a certain cohesiveness and interdependency, the so-called unity of consciousness. In essence the contents of the global workspace form a moving bubble of attention or awareness. In CogPrime, this moving bubble is achieved largely via economic attention network (ECAN) equations [GPI+10] that propagate virtual currency between nodes and links representing elements of memories, so that the attentional focus consists of the wealthiest nodes and links. Figures5.3 and 5.4 illustrate the existence and ow of attentional focus in OpenCog. On the other hand, in Hameroffs recent model of the brain [Ham10], the brains moving bubble of attention is achieved through dendro-dendritic connections and the emergent dendritic web. In this perspective, self, free will and reective consciousness are specic phenomena occurring within the moving bubble of awareness.",Engineering General  Intelligence Part 1,chapter 5
"They are specic ways of experiencing awareness, corresponding to certain abstract types of physical structures and dynamics, which we shall endeavor to identify in detail in AppendixC. 5.6 Postscript: Formalizing Pattern Finally, before winding up our very brief tour through patternist philosophy of mind, we will briey visit patternisms more formal side. Many of the key aspects of patternism have been rigorously formalized. Here we give only a few very basic elements of the relevant mathematics, which will be used later on in the exposition of CogPrime. (Specically, the formal denition of pattern emerges in the CogPrime design in the denition of a tness function for pattern mining algorithms and Occam-based concept creation algorithms, and the denition of intensional inheritance within PLN.",Engineering General  Intelligence Part 1,chapter 5
"We give some denitions, drawn from Appendix1 of [Goe06a]: Denition 1 Given a metric space (M, d), and two functions c : M  [0, ] (the simplicity measure) and F : M  M (the production relationship), we say that P  M is a pattern in X  M to the degree P X =  1  d(F(P), X) c(X)  c(X)  c(P) c(X) +      5.6 Postscript: Formalizing Pattern 97 Fig. 5.3 Graphical depiction of the momentary bubble of attention in the memory of an OpenCog AI system. Circles and lines represent nodes and links in OpenCogPrimes memory, and stars denote those nodes with a high level of attention (represented in OpenCog by the ShortTermImportance node variable) at the particular point in time This degree is called the pattern intensity of P in X.",Engineering General  Intelligence Part 1,chapter 5
"It quanties the extent to which P is a pattern in X. Supposing that F(P) = X, then the rst factor in the denition equals 1, and we are left with only the second term, which measures the degree of compression obtained via representing X as the result of P rather than simply representing X directly. The greater the compression ratio obtained via using P to represent X, the greater the intensity of P as a pattern in X. The rst time, in the case F(P) = X, adjusts the pattern intensity downwards to account for the amount of error with which F(P) approximates = X. If one holds the second factor xed and thinks about varying the rst factor, then: The greater the error, the lossier the compression, and the lower the pattern intensity.",Engineering General  Intelligence Part 1,chapter 5
"For instance, if one wishes one may take c to denote algorithmic information measured on some reference Turing machine, and F(X) to denote what appears on the second tape of a two-tape Turing machine t time-steps after placing X on its rst tape. Other more naturalistic computational models are also possible here and are discussed extensively in Appendix1 of [Goe06a].      98 5 A Patternist Philosophy of Mind Fig. 5.4 Graphical depiction of the momentary bubble of attention in the memory of an OpenCog AI system, a few moments after the bubble shown in Fig.5.3, indicating the moving of the bubble of attention. Depictive conventions are the same as in Fig.5.3. This shows an idealized situation where the declarative knowledge remains invariant from one moment to the next but only the focus of attention shifts.",Engineering General  Intelligence Part 1,chapter 5
"In reality both will evolve together Denition 2 The structure of X  M is the fuzzy set StX dened via the membership function StX(P) = P X This lets us formalize our denition of mind alluded to above: the mind of X as the set of patterns associated with X. We can formalize this, for instance, by considering P to belong to the mind of X if it is a pattern in some Y that includes X. There are then two numbers to look at: P X and P(Y|X) (the percentage of Y that is also contained in X). To dene the degree to which P belongs to the mind of X we can then combine these two numbers using some function f that is monotone increasing in both arguments. This highlights the somewhat arbitrary semantics of of in the phrase the mind of X.",Engineering General  Intelligence Part 1,chapter 5
"Which of the patterns binding X to its environment are part of Xs mind, and which are part of the world? This isnt necessarily a good question, and the answer seems to depend on what perspective you choose, represented formally in the present framework by what combination function f you choose (for instance if f (a, b) = arb2r then it depends on the choice of 0 < r < 1).      5.6 Postscript: Formalizing Pattern 99 Next, we can formalize the notion of a pattern space by positing a metric on patterns, thus making pattern space a metric space, which will come in handy in some places in later chapters: Denition 3 Assuming M is a countable space, the structural distance is a metric dSt dened on M via dSt(X, Y) = T(StX, StY ) where T is any metric on countable-length vectors.",Engineering General  Intelligence Part 1,chapter 5
"Using this denition of pattern, combined with the formal theory of intelligence given in Chap.8, one may formalize the various hypotheses made in the previous section, regarding the emergence of different kinds of networks and structures as patterns in intelligent systems. However, it appears quite difcult to prove the formal versions of these hypotheses given current mathematical tools, which renders such formalizations of limited use. Finally, consider the case where the metric space M has a partial ordering < on it; we may then dene Denition 5.1 R  M is a subpattern in X  M to the degree R X =  PM true(R < P)dP X  PM dP X This degree is called the subpattern intensity of P in X. Roughly speaking, the subpattern intensity measures the percentage of patterns in X that contain R (where containment is judged by the partial ordering <).",Engineering General  Intelligence Part 1,chapter 5
"But the percentage is measured using a weighted average, where each pattern is weighted by its intensity as a pattern in X. A subpattern may or may not be a pattern on its own. A nonpattern that happens to occur within many patterns may be an intense subpattern. Whether the subpatterns in X are to be considered part of the mind of X is a somewhat superuous question of semantics. Here we choose to extend the denition of mind given in [Goe06a] to include subpatterns as well as patterns, because this makes it simpler to describe the relationship between hypersets and minds, as we will do in AppendixC.",Engineering General  Intelligence Part 1,chapter 5
"  Chapter 6 Brief Survey of Cognitive Architectures 6.1 Introduction While we believe CogPrime is the most thorough attempt at an architecture for advanced AGI, to date, we certainly recognize there have been many valuable attempts in the past with similar aims; and we also have great respect for other AGI efforts occurring in parallel with CogPrime development, based on alternative, sometimes overlapping, theoretical presuppositions and practical choices. In most of this book we will ignore these other current and historical efforts except where they are directly useful for CogPrimethere are many literature reviews already published, and this is a research treatise not a textbook. In this chapter, however, we will break from this pattern and give a rough high-level overview of the various AGI architectures at play in the eld today.",Engineering General  Intelligence Part 1,chapter 6
"The overview denitely has a bias toward other work with some direct relevance to CogPrime, but not an overwhelming bias; we also discuss a number of approaches that are unrelated to, and even in some cases conceptually orthogonal to, our own. CogPrime builds on prior AI efforts in a variety of ways. Most of the specic algorithms and structures in CogPrime have their roots in prior AI work; and in addition,theCogPrimecognitivearchitecturehasbeenheavilyinspiredbysomeother holistic cognitive architectures, especially (but not exclusively) MicroPsi [Bac09], LIDA [BF09] and DeSTIN [ARK09a, ARC09]. We will articulate some rough mappings between elements of these other architectures and elements of CogPrimesome in this chapter, and some in Chap.7. However, these mappings will mostly be left informal and very incompletely specied.",Engineering General  Intelligence Part 1,chapter 6
"The articulation of detailed inter-architecture mappings is an important project, but would be a substantial additional project going well beyond the scope of this book. We will not give a thorough review of the similarities and differences between CogPrime and each of these architectures, but only mention some of the highlights. The reader desiring a more thorough review of cognitive architectures is referred to Wlodek Duchs review paper from the AGI-08 conference [DOP08]; and also to Alexei Samsonovichs review paper [Sam10], which compares a number of cognitive B. Goertzel et al., Engineering General Intelligence, Part 1, 101 Atlantis Thinking Machines 5, DOI: 10.2991/978-94-6239-027-0_6,  Atlantis Press and the authors 2014      102 6 Brief Survey of Cognitive Architectures Fig. 6.1 Duchs simplied taxonomy of cognitive architectures.",Engineering General  Intelligence Part 1,chapter 6
"CogPrime falls into the hybrid category, but differs from other hybrid architectures in its focus on synergetic interactions between components and their potential to give rise to appropriate system-wide emergent structures enabling general intelligence architectures in terms of a feature checklist, and was created collaboratively with the creators of the architectures. Duch, in his survey of cognitive architectures [DOP08], divides existing approaches into three paradigmssymbolic, emergentist and hybridas broadly indicated in Fig.6.1. Drawing on his survey and updating slightly, we give here some key examples of each, and then explain why CogPrime represents a signicantly more effective approach to embodied human-like general intelligence. In our treatment of emergentist architectures, we pay particular attention to developmental robotics architectures, which share considerably with CogPrime in terms of underlying philosophy, but differ via not integrating a symbolic language and inference component such as CogPrime includes.",Engineering General  Intelligence Part 1,chapter 6
"In brief, we believe that the hybrid approach is the most pragmatic one given the current state of AI technology, but that the emergentist approach gets something fundamentally right, by focusing on the emergence of complex dynamics and structures from the interactions of simple components. So CogPrime is a hybrid architecture which (according to the cognitive synergy principle) binds its components together very tightly dynamically, allowing the emergence of complex dynamics and structures in the integrated system. Most other hybrid architectures are less tightly coupled and hence seem ill-suited to give rise to the needed emergent complexity. The other hybrid architectures that do possess the needed tight coupling, such as MicroPsi [Bac09], strike us as underdeveloped and founded on insufciently powerful learning algorithms.      6.2 Symbolic Cognitive Architectures 103 6.",Engineering General  Intelligence Part 1,chapter 6
"2 Symbolic Cognitive Architectures A venerable tradition in AI focuses on the physical symbol system hypothesis [New90], which states that minds exist mainly to manipulate symbols that represent aspects of the world or themselves. A physical symbol system has the ability to input, output, store and alter symbolic entities, and to execute appropriate actions in order to reach its goals. Generally, symbolic cognitive architectures focus on working memory that draws on long-term memory as needed, and utilize a centralized control over perception, cognition and action. Although in principle such architectures could be arbitrarily capable (since symbolic systems have universal representational and computational power, in theory), in practice symbolic architectures tend to be weak in learning, creativity, procedure learning, and episodic and associative memory. Decades of work in this tradition have not resolved these issues, which has led many researchers to explore other options.",Engineering General  Intelligence Part 1,chapter 6
"A few of the more important symbolic cognitive architectures are:  SOAR [LRN87], a classic example of expert rule-based cognitive architecture designed to model general intelligence. It has recently been extended to handle sensorimotor functions, though in a somewhat cognitively unnatural way; and is not yet strong in areas such as episodic memory, creativity, handling uncertain knowledge, and reinforcement learning.  ACT-R [AL03] is fundamentally a symbolic system, but Duch classies it as a hybrid system because it incorporates connectionist-style activation spreading in a signicantrole;andthereisanexperimentalthoroughlyconnectionistimplementation to complement the primary mainly-symbolic implementation. Its combination of SOAR-style production rules with large-scale connectionist dynamics allows it to simulate a variety of human psychological phenomena, but abstract reasoning, creativity and transfer learning are still missing.",Engineering General  Intelligence Part 1,chapter 6
"EPIC [RCK01], a cognitive architecture aimed at capturing human perceptual, cognitive and motor activities through several interconnected processors working in parallel. The system is controlled by production rules for cognitive processors and a set of perceptual (visual, auditory, tactile) and motor processors operating on symbolically coded features rather than raw sensory data. It has been connected to SOAR for problem solving, planning and learning.  ICARUS [Lan05], an integrated cognitive architecture for physical agents, with knowledge specied in the form of reactive skills, each denoting goal-relevant reactions to a class of problems. The architecture includes a number of modules: a perceptual system, a planning system, an execution system, and several memory systems. Concurrent processing is absent, attention allocation is fairly crude, and uncertain knowledge is not thoroughly handled.",Engineering General  Intelligence Part 1,chapter 6
"SNePS (Semantic Network Processing System) [SE07] is a logic, frame and network-based knowledge representation, reasoning, and acting system that has undergone over three decades of development. While it has been used for some interesting prototype experiments in language processing and virtual agent control, it has not yet been used for any large-scale or real-world application.      104 6 Brief Survey of Cognitive Architectures  Cyc [LG90] is an AGI architecture based on predicate logic as a knowledge representation, and using logical reasoning techniques to answer questions and derive new knowledge from old. It has been connected to a natural language engine, and designs have been created for the connection of Cyc with Albuss 4D-RCS [AM01].",Engineering General  Intelligence Part 1,chapter 6
"Cycs most unique aspect is the large database of commonsense knowledge that Cycorp has accumulated (millions of pieces of knowledge, entered by specially trained humans in predicate logic format); part of the philosophy underlying Cyc is that once a sufcient quantity of knowledge is accumulated in the knowledge base, the problem of creating human-level general intelligence will become much less difcult due to the ability to leverage this knowledge. While these architectures contain many valuable ideas and have yielded some interesting results, we feel they are incapable on their own of giving rise to the emergent structures and dynamics required to yield humanlike general intelligence using feasible computational resources. However, we are more sanguine about the possibility of ideas and components from symbolic architectures playing a role in human-level AGI via incorporation in hybrid architectures. We now review a few symbolic architectures in slightly more detail. 6.2.",Engineering General  Intelligence Part 1,chapter 6
"1 SOAR The cognitive architectures best known among AI academics are probably Soar and ACT-R, both of which are explicitly being developed with the dual goals of creating human-level AGI and modeling all aspects of human psychology. Neither the Soar nor ACT-R communities feel themselves particularly near these long-term goals, yet they do take them seriously. Soar is based on IF-THEN rules, otherwise known as production rules. On the surface this makes it similar to old-style expert systems, but Soar is much more than an expert system; its at minimum a sophisticated problem-solving engine. Soar explicitly conceives problem solving as a search through solution space for a goal state representing a (precise or approximate) problem solution. It uses a methodology of incremental search, where each step is supposed to move the system a little closer to its problem-solving goal, and each step involves a potentially complex decision cycle.",Engineering General  Intelligence Part 1,chapter 6
"In the simplest case, the decision cycle has two phases:  Gathering appropriate information from the systems long-term memory (LTM) into its working memory (WM)  A decision procedure that uses the gathered information to decide an action. If the knowledge available in LTM isnt enough to solve the problem, then the decision procedure invokes search heuristics like hill-climbing, which try to create new knowledge (new production rules) that will help move the system closer to a solution. If a solution is found by chaining together multiple production rules, then      6.2 Symbolic Cognitive Architectures 105 a chunking mechanism is used to combine these rules together into a single rule for future use. One could view the chunking mechanism as a way of converting explicit knowledge into implicit knowledge, similar to map formation in CogPrime (see Chap.",Engineering General  Intelligence Part 1,chapter 6
"25 of Part 2), but in the current Soar design and implementation it is a fairly crude mechanism In recent years Soar has acquired a number of additional methods and modalities, including some visual reasoning methods and some mechanisms for handling episodic and procedural knowledge. These expand the scope of the system but the basic production rule and chunking mechanisms as briey described above remain the core cognitive algorithm of the system. From a CogPrime perspective, what Soar offers is certainly valuable, e.g.  heuristics for transferring knowledge from LTM into WM  chaining and chunking of implications  methods for interfacing between other forms of knowledge and implications.",Engineering General  Intelligence Part 1,chapter 6
"However, a very short and very partial list of the major differences between Soar and CogPrime would include  CogPrime contains a variety of other core cognitive mechanisms beyond the management and chunking of implications  The variety of chunking type methods in CogPrime goes far beyond the sort of localized chunking done in Soar  CogPrime is committed to representing uncertainty at the base level whereas Soars production rules are crisp  The mechanisms for LTM-WM interaction are rather different in CogPrime, being based on complex nonlinear dynamics as represented in Economic Attention Allocation (ECAN)  Currently Soar does not contain creativity-focused heuristics like blending or evolutionary learning in its core cognitive dynamic. 6.2.2 ACT-R In the grand scope of cognitive architectures, ACT-R is quite similar to Soar, but there are many micro-level differences.",Engineering General  Intelligence Part 1,chapter 6
"ACT-R is dened in terms of declarative and procedural knowledge, where procedural knowledge takes the form of Soar-like production rules, and declarative knowledge takes the form of chunks. It contains a variety of mechanisms for learning new rules and chunks from old; and also contains sophisticated probabilistic equations for updating the activation levels associated with items of knowledge (these equations being roughly analogous in function to, though quite different from, the ECAN equations in CogPrime). The ow of cognition in the system is in response to the current goal, currently active information from declarative memory, information attended to in perceptual      106 6 Brief Survey of Cognitive Architectures Fig. 6.2 Conjectured mapping between ACT-R and the brain modules (vision and audition are implemented), and the current state of motor modules (hand and speech are implemented).",Engineering General  Intelligence Part 1,chapter 6
"The early work with ACT-R was based on comparing system performance to human behavior, using only behavioral measures, such as the timing of keystrokes or patterns of eye movements. Using such measures, it was not possible to test detailed assumptions about which modules were active in the performance of a task. More recently the ACT-R community has been engaged in a process of using imaging data to provide converging data on module activity. Figure6.2 illustrates the associations they have made between the systems cognitive modules and brain regions. Coordination among all of these components occurs through actions of the procedural module, which is mapped to the basal ganglia. In practice ACT-R, even more so than Soar, seems to be used more as a programming framework for cognitive modeling than as an AI system. One can fairly easily use ACT-R to program models of specic human mental behaviors, which may then be matched against psychological data. Opinions differ as to whether this sort of modeling is valuable for achieving AGI goals.",Engineering General  Intelligence Part 1,chapter 6
"CogPrime is not designed to support this kind of modeling, as it intentionally does many things very differently from humans. ACT-R in its original form did not say much about perceptual and motor operations, but recent versions have incorporated EPIC, an independent cognitive architecture focused on modeling these aspects of human behavior.      6.2 Symbolic Cognitive Architectures 107 6.2.3 Cyc and Texai Our review of cognitive architectures would be incomplete without mentioning Cyc [LG90], one of the best known and best funded AGI-oriented projects in history. While the main focus of the Cyc project has been on the hand-coding of large amounts of declarative knowledge, there is also a cognitive architecture of sorts there. The center of Cyc is an engine for logical deduction, acting on knowledge represented in predicate logic. A natural language engine has been associated with the logic engine, which enables one to ask English questions and get English replies.",Engineering General  Intelligence Part 1,chapter 6
"Stephen Reed, while an engineer at Cycorp, designed a perceptual-motor front end for Cyc based on James Albus Reference Model Architecture; the ensuing system, called CognitiveCyc, would have been the rst full-edged cognitive architecture based on Cyc, but was not implemented. Reed left Cycorp and is now building a system called Texai, which has many similarities to Cyc (and relies upon the OpenCyc knowledge base, a subset of Cycs overall knowledge base), but incorporates a CognitiveCyc style cognitive architecture. 6.2.4 NARS Pei Wangs NARS logic [Wan06] played a large role in the development of PLN, CogPrimes uncertain logic component, a relationship that is discussed in depth in [GMIH08] and wont be re-emphasized here.",Engineering General  Intelligence Part 1,chapter 6
"However, NARS is more than just an uncertain logic, it is also an overall cognitive architecture (which is centered on NARS logic, but also includes other aspects). CogPrime bears little relation to NARS except in the specic similarities between PLN logic and NARS logic, but, the other aspects of NARS are worth briey recounting here. NARS is formulated as a system for processing tasks, where a task consists of a question or a piece of new knowledge. The architecture is focused on declarative knowledge, but some pieces of knowledge may be associated with executable procedures, which allows NARS to carry out control activities (in roughly the same way that a Prolog program can).",Engineering General  Intelligence Part 1,chapter 6
"At any given time a NARS system contains  working memory: a small set of tasks which are active, kept for a short time, and closely related to new questions and new knowledge  long-term memory: a huge set of knowledge which is passive, kept for a long time, and not necessarily related to current questions and knowledge. The working and long term memory spaces of NARS may each be thought of as a set of chunks, where each chunk consists of a set of tasks and a set of knowledge. NARSs basic cognitive process is: 1. choose a chunk 2. choose a task from that chunk      108 6 Brief Survey of Cognitive Architectures 3. choose a piece of knowledge from that chunk 4. use the task and knowledge to do inference 5. send the new tasks to corresponding chunks.",Engineering General  Intelligence Part 1,chapter 6
"Depending on the nature of the task and knowledge, the inference involved may be one of the following:  if the task is a question, and the knowledge happens to be an answer to the question, a copy of the knowledge is generated as a new task  backward inference  revision (merging two pieces of knowledge with the same form but different truth value)  forward inference  execution of a procedure associated with a piece of knowledge. Unlike many other systems, NARS doesnt decide what type of inference is used to process a task when the task is accepted, but works in a data-driven waythat is, it is the task and knowledge that dynamically determine what type of inference will be carried out. The choice processes mentioned above are done via assigning relative priorities to  chunks (where they are called activity)  tasks (where they are called urgency)  knowledge (where they are called importance). and then distributing the systems resources accordingly, based on a probabilistic algorithm.",Engineering General  Intelligence Part 1,chapter 6
"(Its interesting to note that while NARS uses probability theory as part of its control mechanism, the logic it uses to represent its own knowledge about the world is nonprobabilistic. This is considered conceptually consistent, in the context of NARS theory, because system control is viewed as a domain where the systems knowledge is more complete, thus more amenable to probabilistic reasoning.) 6.2.5 GLAIR and SNePS Another logic-focused cognitive architecture, very different from NARS in detail, is Stuart Shapiros GLAIR cognitive architecture, which is centered on the SNePS paraconsistent logic [SE07]. Like NARS, the core cognitive loop of GLAIR is based on reasoning: either thinking about some percept (e.g. linguistic input, or sense data from the virtual or physical world), or answering some question.",Engineering General  Intelligence Part 1,chapter 6
"This inference based cognition process is turned into an intelligent agent control process via coupling it with an acting component, which operates according to a set of policies, each one of which tells the system when to take certain internal or external actions (including internal reasoning actions) in response to its observed internal and external situation. GLAIR contains multiple layers:      6.2 Symbolic Cognitive Architectures 109  the Knowledge Layer (KL), which contains the beliefs of the agent, and is where reasoning, planning, and act selection are performed.  the Sensori-Actuator Layer (SAL), contains the controllers of the sensors and effectors of the hardware or software robot.  the Perceptuo-Motor Layer (PML), which grounds the KL symbols in perceptual structures and subconscious actions, contains various registers for providing the agents sense of situatedness in the environment, and handles translation and communication between the KL and the SAL.",Engineering General  Intelligence Part 1,chapter 6
"The logical Knowledge Layer incorporates multiple memory types using a common representation (including declarative, procedural, episodic, attentional and intentional knowledge, and meta-knowledge). To support this broad range of knowledge types, a broad range of logical inference mechanisms are used, so that the KL may be variously viewed as predicate logic based, frame based, semantic network based, or from other perspectives. What makes GLAIR more robust than most logic based AI approaches is the novel paraconsistent logical formalism used in the knowledge base, which means (among other things) that uncertain, speculative or erroneous knowledge may exist in the systems memory without leading the system to create a broadly erroneous view of the world or carry out egregiously unintelligent actions. CogPrime is not thoroughly logic-focused like GLAIR is, but in its logical aspect it seeks a similar robustness through its use of PLN logic, which embodies properties related to paraconsistency.",Engineering General  Intelligence Part 1,chapter 6
"Compared to CogPrime, we see that GLAIR has a similarly integrative approach, but that the integration of different sorts of cognition is done more strictly within the framework of logical knowledge representation. 6.3 Emergentist Cognitive Architectures Another species of cognitive architecture expects abstract symbolic processing to emerge from lower-level subsymbolic dynamics, which sometimes (but not always) are designed to simulate neural networks or other aspects of human brain function. These architectures are typically strong at recognizing patterns in highdimensional data, reinforcement learning and associative memory; but no one has yet shown how to achieve high-level functions such as abstract reasoning or complex language processing using a purely subsymbolic approach.",Engineering General  Intelligence Part 1,chapter 6
"A few of the more important subsymbolic, emergentist cognitive architectures are:  DeSTIN [ARK09a, ARC09], which is part of CogPrime, may also be considered as anautonomousAGIarchitecture,inwhichcaseitisemergentistandcontains mechanisms to encourage language, high-level reasoning and other abstract aspects of intelligent to emerge from hierarchical pattern recognition and related selforganizing network dynamics. In CogPrime DeSTIN is used as part of a hybrid architecture, which greatly reduces the reliance on DeSTINs emergent properties.      110 6 Brief Survey of Cognitive Architectures  Hierarchical Temporal Memory (HTM) [HB06] is a hierarchical temporal pattern recognition architecture, presented as both an AI approach and a model of the cortex. So far it has been used exclusively for vision processing and we will discuss its shortcomings later in the context of our treatment of DeSTIN.",Engineering General  Intelligence Part 1,chapter 6
"SAL [JL08], based on the earlier and related IBCA (Integrated Biologically-based Cognitive Architecture) is a large-scale emergent architecture that seeks to model distributed information processing in the brain, especially the posterior and frontal cortex and the hippocampus. So far the architectures in this lineage have been used to simulate various human psychological and psycholinguistic behaviors, but havent been shown to give rise to higher-level behaviors like reasoning or subgoaling.  NOMAD (Neurally Organized Mobile Adaptive Device) automata and its successors [KE06] are based on Edelmans Neural Darwinism model of the brain, and feature large numbers of simulated neurons evolving by natural selection into congurations that carry out sensorimotor and categorization tasks. The emergence of higher-level cognition from this approach seems rather unlikely.",Engineering General  Intelligence Part 1,chapter 6
"Ben Kuipers and his colleagues [MK07, MK08, MK09] have pursued an extremely innovative research program which combines qualitative reasoning and reinforcement learning to enable an intelligent agent to learn how to act, perceive and model the world. Kuipers notion of bootstrap learning involves allowing the robot to learn almost everything about its world, including for instance the structure of 3D space and other things that humans and other animals obtain via their genetic endowments. Compared to Kuipers approach, CogPrime falls in line with most other approaches which provide more hard-wired structure, following the analogy to biological organisms that are born with more innate biases. There is also a set of emergentist architectures focused specically on developmental robotics, which we will review below in a separate subsection, as all of these share certain common characteristics. Our general perspective on the emergentist approach is that it is philosophically correct but currently pragmatically inadequate.",Engineering General  Intelligence Part 1,chapter 6
"Eventually, some emergentist approach could surely succeed at giving rise to humanlike general intelligence the human brain, after all, is plainly an emergentist system. However, we currently lack understanding of how the brain gives rise to abstract reasoning and complex language, and none of the existing emergentist systems seem remotely capable of giving rise to such phenomena. It seems to us that the creation of a successful emergentist AGI will have to wait for either a detailed understanding of how the brain gives rise to abstract thought, or a much more thorough mathematical understanding of the dynamics of complex self-organizing systems. The concept of cognitive synergy is more relevant to emergentist than to symbolic architectures. In a complex emergentist architecture with multiple specialized components, much of the emergence is expected to arise via synergy between different richly interacting components.",Engineering General  Intelligence Part 1,chapter 6
"Symbolic systems, at least in the forms currently seen in the literature, seem less likely to give rise to cognitive synergy as their dynamics tend to be simpler. And hybrid systems, as we shall see, are somewhat diverse in this      6.3 Emergentist Cognitive Architectures 111 regard: some rely heavily on cognitive synergies and others consist of more loosely coupled components. We now review the DeSTIN emergentist architecture in more detail, and then turn to the developmental robotics architectures. 6.3.1 DeSTIN: A Deep Reinforcement Learning Approach to AGI The DeSTIN architecture, created by Itamar Arel and his colleagues, addresses the problem of general intelligence using hierarchical spatiotemporal networks designed to enable scalable perception, state inference and reinforcement-learning-guided action in real-world environments.",Engineering General  Intelligence Part 1,chapter 6
"DeSTIN has been developed with the plan of gradually extending it into a complete system for humanoid robot control, founded on the same qualitative information-processing principles as the human brain (though without striving for detailed biological realism). However, the practical work with DeSTIN to date has focused on visual and auditory processing; and in the context of the present proposal, the intention is to utilize DeSTIN for perception and actuation oriented processing, hybridizing it with CogPrime which will handle abstract cognition and language. Here we will discuss DeSTIN primarily in the perception context, only briey mentioning the application to actuation which is conceptually similar. In DeSTIN (see Fig.6.3), perception is carried out by a deep spatiotemporal inference network, which is connected to a similarly architected critic network that provides feedback on the inference networks performance, and an action network that controls actuators based on the activity in the inference network (Fig.6.",Engineering General  Intelligence Part 1,chapter 6
"4 depicts a standard action hierarchy, of which the hierarchy in DeSTIN is an example). The nodes in these networks perform probabilistic pattern recognition according to algorithms to be described below; and the nodes in each of the networks may receive states of nodes in the other networks as inputs, providing rich interconnectivity and synergetic dynamics. 6.3.1.1 Deep Versus Shallow Learning for Perceptual Data Processing The most critical feature of DeSTIN is its uniquely robust approach to modeling the world based on perceptual data. Mimicking the efciency and robustness by which the human brain analyzes and represents information has been a core challenge in AI research for decades. For instance, humans are exposed to massive amounts of visual and auditory data every second of every day, and are somehow able to capture critical aspects of it in a way that allows for appropriate future recollection and action selection.",Engineering General  Intelligence Part 1,chapter 6
"For decades, it has been known that the brain is a massively parallel fabric, in which computation processes and memory storage are highly distributed. But massive parallelism is not in itself a solutionone also needs the right architecture; which DeSTIN provides, building on prior work in the area of deep learning.      112 6 Brief Survey of Cognitive Architectures Fig. 6.3 High-level architecture of DeSTIN Fig. 6.4 A standard, general-purpose hierarchical control architecture. DeSTINs control hierarchy exemplies this architecture, with the difference lying mainly in the DeSTIN control hierarchys tight integration with the state inference (perception) and critic (reinforcement) hierarchies Humanlike intelligence is heavily adapted to the physical environments in which humans evolved; and one key aspect of sensory data coming from our physical environments is its hierarchical structure.",Engineering General  Intelligence Part 1,chapter 6
"However, most machine learning and pattern recognition systems are shallow in structure, not explicitly incorporating thehierarchicalstructureoftheworldintheirarchitecture.Inthecontextofperceptual data processing, the practical result of this is the need to couple each shallow learner      6.3 Emergentist Cognitive Architectures 113 with a pre-processing stage, wherein high-dimensional sensory signals are reduced to a lower-dimension feature space that can be understood by the shallow learner. The hierarchical structure of the world is thus crudely captured in the hierarchy of preprocessor plus shallow learner. In this sort of approach, much of the intelligence of the system shifts to the feature extraction process, which is often imperfect and always application-domain specic. Deep machine learning has emerged as a more promising framework for dealing with complex, high-dimensional real-world data. Deep learning systems possess a hierarchical structure that intrinsically biases them to recognize the hierarchical patterns present in real-world data.",Engineering General  Intelligence Part 1,chapter 6
"Thus, they hierarchically form a feature space that is driven by regularities in the observations, rather than by hand-crafted techniques. They also offer robustness to many of the distortions and transformations that characterize real-world signals, such as noise, displacement, scaling, etc. Deep belief networks [HOT06] and Convolutional Neural Networks [LBDE90] have been demonstrated to successfully address pattern inference in high dimensional data (e.g. images). They owe their success to their underlying paradigm of partitioning large data structures into smaller, more manageable units, and discovering the dependencies that may or may not exist between such units. However, this paradigm has its limitations; for instance, these approaches do not represent temporal information with the same ease as spatial structure. Moreover, some key constraints are imposed on the learning schemes driving these architectures, namely the need for layer-by-layer training, and oftentimes pre-training.",Engineering General  Intelligence Part 1,chapter 6
"DeSTIN overcomes the limitations of prior deep learning approaches to perception processing, and also extends beyond perception to action and reinforcement learning. 6.3.1.2 DeSTIN for Perception Processing The hierarchical architecture of DeSTINs spatiotemporal inference network comprises an arrangement into multiple layers of nodes comprising multiple instantiations of an identical cortical circuit. Each node corresponds to a particular spatiotemporal region, and uses a statistical learning algorithm to characterize the sequences of patterns that are presented to it by nodes in the layer beneath it. More specically,  At the very lowest layer of the hierarchy nodes receive as input raw data (e.g. pixels of an image) and continuously construct a belief state that attempts to characterize the sequences of patterns viewed.  The second layer, and all those above it, receive as input the belief states of nodes at their corresponding lower layers, and attempt to construct belief states that capture regularities in their inputs.",Engineering General  Intelligence Part 1,chapter 6
"Each node also receives as input the belief state of the node above it in the hierarchy (which constitutes contextual information) (Fig.6.5). More specically, each of the DeSTIN nodes, referring to a specic spacetime region, contains a set of state variables conceived as clusters, each corresponding to      114 6 Brief Survey of Cognitive Architectures Fig. 6.5 Small-scale instantiation of the DeSTIN perceptual hierarchy. Each box represents a node, which corresponds to a spatiotemporal region (nodes higher in the hierarchy corresponding to larger regions). O denotes the current observation in the region, C is the state of the higher-layer node, and S and S denote state variables pertaining to two subsequent time steps. In each node, a statistical learning algorithm is used to predict subsequent states based on prior states, current observations, and the state of the higher-layer node a set of previously-observed sequences of events.",Engineering General  Intelligence Part 1,chapter 6
"These clusters are characterized by centroids (and are hence assumed roughly spherical in shape), and each of them comprises a certain spatiotemporal form recognized by the system in that region. Each node then contains the task of predicting the likelihood of a certain centroid being most apropos in the near future, based on the past history of observations in the node. This prediction may be done by simple probability tabulation, or via application of supervised learning algorithms such as recurrent neural networks. These clustering and prediction processes occur separately in each node, but the nodes are linked together via bidirectional dynamics: each node feeds input to its parents, and receives advice from its parents that is used to condition its probability calculations in a contextual way. These processes are executed formally by the following basic belief update rule, which governs the learning process and is identical for every node in the architecture. The belief state is a probability mass function over the sequences of stimuli that the nodes learns to represent.",Engineering General  Intelligence Part 1,chapter 6
"Consequently, each node is allocated a predened number of state variables each denoting a dynamic pattern, or sequence, that is autonomously learned. The DeSTIN update rule maps the current observation (o), belief state (b), and the belief state of a higher-layer node or context (c), to a new (updated) belief state (b), such that      6.3 Emergentist Cognitive Architectures 115 b  s = Pr  s|o, b, c  = Pr  s  o  b  c  Pr (o  b  c) , (6.1) alternatively expressed as b  s = Pr(o|s, b, c) Pr  s|b, c  Pr (b, c) Pr (o|b, c) Pr (b, c) . (6.",Engineering General  Intelligence Part 1,chapter 6
"2) Under the assumption that observations depend only on the true state, or Pr(o|s, b, c) = Pr(o|s), we can further simplify the expression such that b  s = Pr(o|s) Pr  s|b, c  Pr (o|b, c) , (6.3) where Pr  s|b, c  =  sS Pr  s|s, c  b (s), yielding the belief update rule b  s = Pr  o|s  sS Pr  s|s, c  b (s)  sS Pr (o|s)  sS Pr (s|s, c) b (s), (6.4) where S denotes the sequence set (i.e. belief dimension) such that the denominator term is a normalization factor. One interpretation of Eq.(6.",Engineering General  Intelligence Part 1,chapter 6
"4) would be that the static pattern similarity metric, Pr  o|s , is modulated by a construct that reects the system dynamics, Pr  s|s, c  . As such, the belief state inherently captures both spatial and temporal information. In our implementation, the belief state of the parent node, c, is chosen using the selection rule c = arg max s bp(s), (6.5) where bp is the belief distribution of the parent node. A close look at Eq.(6.4) reveals that there are two core constructs to be learned, Pr(o|s) andPr(s|s, c).Inthecurrent DeSTINdesign, theformer is learnedviaonline clustering while the latter is learned based on experience by inductively learning a rule that predicts the next state s given the prior state s and c. The overall result is a robust framework that autonomously (i.e.",Engineering General  Intelligence Part 1,chapter 6
"with no human engineered pre-processing of any type) learns to represent complex data patterns, and thus serves the critical role of building and maintaining a model of the state of the world. In a vision processing context, for example, it allows for powerful unsupervised classication. If shown a variety of real-world scenes, it will automatically form internal structures corresponding to the various natural categories of objects shown in the scenes, such as trees, chairs, people, etc.; and also the various natural categories of events it sees, such as reaching, pointing, falling. And, as will be      116 6 Brief Survey of Cognitive Architectures discussed below, it can use feedback from DeSTINs action and critic networks to further shape its internal world-representation based on reinforcement signals. Benets of DeSTIN for Perception Processing DeSTINs perceptual network offers multiple key attributes that render it more powerful than other deep machine learning approaches to sensory data processing: 1.",Engineering General  Intelligence Part 1,chapter 6
"The belief space that is formed across the layers of the perceptual network inherently captures both spatial and temporal regularities in the data. Given that many applications require that temporal information be discovered for robust inference, this is a key advantage over existing schemes. 2. Spatiotemporal regularities in the observations are captured in a coherent manner (rather than being represented via two separate mechanisms). 3. All processing is both top-down and bottom-up, and both hierarchical and heterarchical, based on nonlinear feedback connections directing activity and modulating learning in multiple directions through DeSTINs cortical circuits. 4. Support for multi-modal fusing is intrinsic within the framework, yielding a powerful state inference system for real-world, partially-observable settings. 5. Each node is identical, which makes it easy to map the design to massively parallel platforms, such as graphics processing units.",Engineering General  Intelligence Part 1,chapter 6
"Points 24 in the above list describe how DeSTINs perceptual network displays its own cognitive synergy in a way that ts naturally into the overall synergetic dynamics of the overall CogPrime architecture. Using this cognitive synergy, DeSTINs perceptual network addresses a key aspect of general intelligence: the ability to robustly infer the state of the world, with which the system interacts, in an accurate and timely manner. 6.3.1.3 DeSTIN for Action and Control DeSTINs perceptual network performs unsupervised world-modeling, which is a critical aspect of intelligence but of course is not the whole story. DeSTINs action network, coupled with the perceptual network, orchestrates actuator commands into complex movements, but also carries out other functions that are more cognitive in nature.",Engineering General  Intelligence Part 1,chapter 6
"Forinstance,peoplelearntodistinguishbetweencupsandbowlsinpartviahearing other people describe some objects as cups and others as bowls. To emulate this kind of learning, DeSTINs critic network provides positive or negative reinforcement signals based on whether the action network has correctly identied a given object as a cup or a bowl, and this signal then impacts the nodes in the action network. The critic network takes a simple external degree of success or failure signal and turns it into multiple reinforcement signals to be fed into the multiple layers of the action network. The result is that the action network self-organizes so as to include      6.3 Emergentist Cognitive Architectures 117 an implicit cup versus bowl classier, whose inputs are the outputs of some of the nodes in the higher levels of the perceptual network.",Engineering General  Intelligence Part 1,chapter 6
"This classier belongs in the action network because it is part of the procedure by which the DeSTIN system carries out the action of identifying an object as a cup or a bowl. This example illustrates how the learning of complex concepts and procedures is divided uidly between the perceptual network, which builds a model of the world in an unsupervised way, and the action network, which learns how to respond to the world in a manner that will receive positive reinforcement from the critic network. 6.3.2 Developmental Robotics Architectures A particular subset of emergentist cognitive architectures are sufciently important that we consider them separately here: these are developmental robotics architectures, focused on controlling robots without signicant hard-wiring of knowledge or capabilities, allowing robots to learn (and learn how to learn, etc.) via their engagement with the world.",Engineering General  Intelligence Part 1,chapter 6
"A signicant focus is often placed here on intrinsic motivation, wherein the robot explores the world guided by internal goals like novelty or curiosity, forming a model of the world as it goes along, based on the modeling requirements implied by its goals. Many of the foundations of this research area were laid by Juergen Schmidhubers work in the 1990s [Sch91b, Sch91a, Sch95, Sch02], but now with more powerful computers and robots the area is leading to more impressive practical demonstrations. We mention here a handful of the important initiatives in this area:  Juyang Wengs Dav [HZT+02] and SAIL [WHZ+00] projects involve mobile robots that explore their environments autonomously, and learn to carry out simple tasks by building up their own world-representations through both unsupervised and teacher-driven processing of high-dimensional sensorimotor data.",Engineering General  Intelligence Part 1,chapter 6
"The underlying philosophy is based on human child development [WH06], the knowledge representations involved are neural network based, and a number of novel learning algorithms are involved, especially in the area of vision processing.  FLOWERS [BO09], an initiative at the French research institute INRIA, led by Pierre-Yves Oudeyer, is also based on a principle of trying to reconstruct the processes of development of the human childs mind, spontaneously driven by intrinsic motivations. Kaplan [Kap08] has taken this project in a direction closely related to our own via the creation of a robot playroom. Experiential language learning has also been a focus of the project [OK06], driven by innovations in speech understanding.",Engineering General  Intelligence Part 1,chapter 6
"IM-CLEVER,1 a new European project coordinated by Gianluca Baldassarre and conducted by a large team of researchers at different institutions, is focused on creating software enabling an iCub [MSV+08] humanoid robot to explore the 1 http://im-clever.noze.it/project/project-description      118 6 Brief Survey of Cognitive Architectures environment and learn to carry out human childlike behaviors based on its own intrinsic motivations. As this project is the closest to our own we will discuss it in more depth below. Like CogPrime, IM-CLEVER is a humanoid robot intelligence architecture guided by intrinsic motivations, and using hierarchical architectures for reinforcementlearningandsensoryabstraction.",Engineering General  Intelligence Part 1,chapter 6
"-CLEVERsmotivationalstructureisbased in part on Schmidhubers information-theoretic model of curiosity [Sch06]; and CogPrimes Psi-based motivational structure utilizes probabilistic measures of novelty, which are mathematically related to Schmidhubers measures. On the other hand, IM-CLEVERs use of reinforcement learning follows Schmidhubers earlier work RL for cognitive robotics [BS04, BZGS06], Bartos work on intrinsically motivated reinforcement learning [SB06, SM05], and Lees [LMC07a, LMC07b] work on developmental reinforcement learning; whereas CogPrimes assemblage of learning algorithms is more diverse, including probabilistic logic, concept blending and other symbolic methods (in the OCP component) as well as more conventional reinforcement learning methods (in the DeSTIN component).",Engineering General  Intelligence Part 1,chapter 6
"InmanyrespectsIM-CLEVERbearsamoderatelystrongresemblancetoDeSTIN, whose integration with CogPrime is discussed in Chap.9 of Part 2 (although IMCLEVER has much more focus on biological realism than DeSTIN). Apart from numerous technical differences, the really big distinction between IM-CLEVER and CogPrime is that in the latter we are proposing to hybridize a hierarchicalabstraction/reinforcement-learning system (such as DeSTIN) with a more abstract symbolic cognition engine that explicitly handles probabilistic logic and language. IM-CLEVER lacks the aspect of hybridization with a symbolic system, taking more of a pure emergentist strategy. Like DeSTIN considered as a standalone architecture IM-CLEVER does entail a high degree of cognitive synergy, between components dealing with perception, world-modeling, action and motivation. However, the emergentist versus hybrid is a large qualitative difference between the two approaches.",Engineering General  Intelligence Part 1,chapter 6
"In all, while we largely agree with the philosophy underlying developmental robotics, our intuition is that the learning and representational mechanisms underlying the current systems in this area are probably not powerful enough to lead to human child level intelligence. We expect that these systems will develop interesting behaviors but fall short of robust preschool level competency, especially in areas like language and reasoning where symbolic systems have typically proved more effective. This intuition is what impels us to pursue a hybrid approach, such as CogPrime. But we do feel that eventually, once the mechanisms underlying brains are better understood and robotic bodies are richer in sensation and more adept in actuation, some sort of emergentist, developmental-robotics approach can be successful at creating humanlike, human-level AGI.      6.4 Hybrid Cognitive Architectures 119 6.",Engineering General  Intelligence Part 1,chapter 6
"4 Hybrid Cognitive Architectures In response to the complementary strengths and weaknesses of the symbolic and emergentist approaches, in recent years a number of researchers have turned to integrative, hybrid architectures, which combine subsystems operating according to the two different paradigms. The combination may be done in many different ways, e.g. connection of a large symbolic subsystem with a large subsymbolic system, or the creation of a population of small agents each of which is both symbolic and subsymbolic in nature. Nils Nilsson expressed the motivation for hybrid AGI systems very clearly in his article at the AI-50 conference (which celebrated the 50th anniversary of the AI eld) [Nil09]. While afrming the value of the Physical Symbol System Hypothesis that underlies symbolic AI, he argues that the PSSH explicitly assumes that, whenever necessary, symbols will be grounded in objects in the environment through the perceptual and effector capabilities of a physical symbol system.",Engineering General  Intelligence Part 1,chapter 6
"Thus, he continues, I grant the need for non-symbolic processes in some intelligent systems, but I think they supplement rather than replace symbol systems. I know of no examples of reasoning, understanding language, or generating complex plans that are best understood as being performed by systems using exclusively non-symbolic processes.... AI systems that achieve human-level intelligence will involve a combination of symbolic and non-symbolic processing. A few of the more important hybrid cognitive architectures are:  CLARION [SZ04] is a hybrid architecture that combines a symbolic component for reasoning on explicit knowledge with a connectionist component for managing implicit knowledge. Learning of implicit knowledge may be done via neural net, reinforcement learning, or other methods. The integration of symbolic and subsymbolic methods is powerful, but a great deal is still missing such as episodic knowledge and learning and creativity. Learning in the symbolic and subsymbolic portions is carried out separately rather than dynamically coupled, minimizing cognitive synergy effects.",Engineering General  Intelligence Part 1,chapter 6
"DUAL [NK04] is the most impressive system to come out of Marvin Minskys Society of Mind paradigm. It features a population of agents, each of which combines symbolic and connectionist representation, self-organizing to collectively carry out tasks such as perception, analogy and associative memory. The approach seems innovative and promising, but it is unclear how the approach will scale to high-dimensional data or complex reasoning problems due to the lack of a more structured high-level cognitive architecture.  LIDA [BF09] is a comprehensive cognitive architecture heavily based on Bernard Baars Global Workspace Theory. It articulates a cognitive cycle integrating various forms of memory and intelligent processing in a single processing loop. The architecture ties in well with both neuroscience and cognitive psychology, but it deals most thoroughly with lower level aspects of intelligence, handling more advanced aspects like language and reasoning only somewhat sketchily.",Engineering General  Intelligence Part 1,chapter 6
"There      120 6 Brief Survey of Cognitive Architectures is a clear mapping between LIDA structures and processes and corresponding structures and processing in OCP; so that its only a mild stretch to view CogPrime as an instantiation of the general LIDA approach that extends further both in the lower level (to enable robot action and sensation via DeSTIN) and the higher level (to enable advanced language and reasoning via OCP mechanisms that have no direct LIDA analogues).  MicroPsi [Bac09] is an integrative architecture based on Dietrich Dorners Psi model of motivation, emotion and intelligence. It has been tested on some practical control applications, and also on simulating articial agents in a simple virtual world. MicroPsis comprehensiveness and basis in neuroscience and psychology are impressive, but in the current version of MicroPsi, learning and reasoning are carried out by algorithms that seem unlikely to scale.",Engineering General  Intelligence Part 1,chapter 6
"OCP incorporates the Psi model for motivation and emotion, so that MicroPsi and CogPrime may be considered very closely related systems. But similar to LIDA, MicroPsi currently focuses on the lower level aspects of intelligence, not yet directly handling advanced processes like language and abstract reasoning.  PolyScheme [Cas07] integrates multiple methods of representation, reasoning and inference schemes for general problem solving. Each Polyscheme specialist models a different aspect of the world using specic representation and inference techniques, interacting with other specialists and learning from them. Polyscheme has been used to model infant reasoning including object identity, events, causality, and spatial relations. The integration of reasoning methods is powerful, but the overall cognitive architecture is simplistic compared to other systems and seems focused more on problem-solving than on the broader problem of intelligent agent control.",Engineering General  Intelligence Part 1,chapter 6
"Shruti [SA93] is a fascinating biologically-inspired model of human reexive inference, which represents in connectionist architecture relations, types, entities and causal rules using focal-clusters. However, much like Hofstadters earlier Copycat architecture [Hof95], Shruti seems more interesting as a prototype exploration of ideas than as a practical AGI system; at least, after a signicant time of development it has not proved signicantly effective in any applications.  James Albuss 4D/RCS robotics architecture shares a great deal with some of the emergentist architectures discussed above, e.g. it has the same hierarchical pattern recognition structure as DeSTIN and HTM, and the same three crossconnected hierarchies as DeSTIN, and shares with the developmental robotics architectures a focus on real-time adaptation to the structure of the world.",Engineering General  Intelligence Part 1,chapter 6
"However, 4D/RCS is not foundationally learning-based but relies on hard-wired architecture and algorithms, intended to mimic the qualitative structure of relevant parts of the brain (and intended to be augmented by learning, which differentiates it from emergentist approaches). As our own CogPrime approach is a hybrid architecture, it will come as no surprise that we believe several of the existing hybrid architectures are fundamentally going in the right direction. However, nearly all the existing hybrid architectures have severe shortcomings which we feel will prevent them from achieving robust humanlike AGI.      6.4 Hybrid Cognitive Architectures 121 Many of the hybrid architectures are in essence multiple, disparate algorithms carrying out separate functions, encapsulated in black boxes and communicating resultswitheachother.Forinstance,PolyScheme,ACT-RandCLARIONalldisplay this modularity property to a signicant extent.",Engineering General  Intelligence Part 1,chapter 6
"These architectures lack the rich, real-time interaction between the internal dynamics of various memory and learning processes that we believe is critical to achieving humanlike general intelligence using realistic computational resources. On the other hand, those architectures that feature richer integrationsuch as DUAL, Shruti, LIDA and MicroPsihave the aw of relying (at least in their current versions) on overly simplistic learning algorithms, which drastically limits their scalability. It does seem plausible to us that some of these hybrid architectures could be dramatically extended or modied so as to produce humanlike general intelligence. Forinstance,onecouldreplaceLIDAslearningalgorithmswithothersthatinterrelate with each other in a nuanced synergetic way; or one could replace MicroPsis simple learning and reasoning methods with much more powerful and scalable ones acting on the same data structures. However, making these changes would dramatically alter the cognitive architectures in question on multiple levels. 6.4.",Engineering General  Intelligence Part 1,chapter 6
"1 Neural Versus Symbolic; Global Versus Local The symbolic versus emergentist dichotomy that we have used to structure our review of cognitive architectures is not absolute nor fully precisely dened; it is more of a heuristic distinction. In this section, before plunging into the details of particular hybrid cognitive architectures, we review two other related dichotomies that are useful for understanding hybrid systems: neural versus symbolic systems, and globalist versus localist knowledge representation. 6.4.1.1 Neural-Symbolic Integration The distinction between neural and symbolic systems has gotten fuzzier and fuzzier in recent years, with developments such as  Logic-based systems being used to control embodied agents (hence using logical terms to deal with data that is apparently perception or actuation-oriented in nature, rather than being symbolic in the semiotic sense), see [SS03a] and [GMIH08].",Engineering General  Intelligence Part 1,chapter 6
"Hybrid systems combining neural net and logical parts, or using logical or neural net components interchangeably in the same role [LAon].  Neural net systems being used for strongly symbolic tasks such as automated grammar learning ([Elm91], plus more recent work.) Figure6.6 presents a schematic diagram of a generic neural-symbolic system, generalizing from [BH05], a paper that gives an elegant categorization of neural     122 6 Brief Survey of Cognitive Architectures Fig. 6.6 Generic neural-symbolic architecture Fig. 6.7 Broad categories of neural-symbolic architecture symbolic AI systems. Figure6.7 depicts several broad categories of neural-symbolic architecture. Bader and Hitzler categorize neural-symbolic systems according to three orthogonal axes: interrelation, language and usage. Language refers to the type of language used in the symbolic component, which may be logical, automata-based, formal grammar-based, etc.",Engineering General  Intelligence Part 1,chapter 6
"Usage refers to the purpose to which the neural-symbolic interrelation is put. We tend to use learning as an encompassing term for all forms of ongoing knowledge-creation, whereas Bader and Hitzler distinguish learning from reasoning. Of Bader and Hitzlers three axes the one that interests us most here is interrelation, which refers to the way the neural and symbolic components of the architecture intersect with each other. They distinguish hybrid architectures which contain separate but equal, interacting neural and symbolic components; versus integrative architectures in which the symbolic component essentially rides      6.4 Hybrid Cognitive Architectures 123 piggyback on the neural component, extracting information from it and helping it carry out its learning, but playing a clearly derived and secondary role. We prefer Suns (2001) term monolithic to Bader and Hitzlers integrative to describe this type of system, as the latter term seems best preserved in its broader meaning.",Engineering General  Intelligence Part 1,chapter 6
"Within the scope of hybrid neural-symbolic systems, there is another axis which Bader and Hitzler do not focus on, because the main interest of their review is in monolithic systems. We call this axis interactivity, and what we are referring to is the frequency of high-information-content, high-inuence interaction between the neural and symbolic components in the hybrid system. In a low-interaction hybrid system, the neural and symbolic components dont exchange large amounts of mutually inuential information all that frequently, and basically act like independent system components that do their learning/reasoning/thinking periodically sending each other their conclusions. In some cases, interaction may be asymmetric: one component may frequently send a lot of inuential information to the other, but not vice versa. However, our hypothesis is that the most capable neural-symbolic systems are going to be the symmetrically highly interactive ones.",Engineering General  Intelligence Part 1,chapter 6
"In a symmetric high-interaction hybrid neural-symbolic system, the neural and symbolic components exchange inuential information sufciently frequently that each one plays a major role in the other ones learning/reasoning/thinking processes. Thus, the learning processes of each component must be considered as part of the overall dynamic of the hybrid system. The two components arent just feeding their outputstoeachotherasinputs,theyremutuallyguidingeachothersinternalprocessing. One can make a speculative argument for the relevance of this kind of architecture to neuroscience. It seems plausible that this kind of neural-symbolic system roughly emulates the kind of interaction that exists between the brains neural subsystems implementing localist symbolic processing, and the brains neural subsystems implementing globalist, classically connectionist processing. It seems most likely that, in the brain, symbolic functionality emerges from an underlying layer of neural dynamics.",Engineering General  Intelligence Part 1,chapter 6
"However, it is also reasonable to conjecture that this symbolic functionality is conned to a functionally distinct subsystem of the brain, which then interacts with other subsystems in the brain much in the manner that the symbolic and neural components of a symmetric high-interaction neural-symbolic system interact. Neuroscience speculations aside, however, our key conjecture regarding neuralsymbolic integration is that this sort of neural-symbolic system presents a promising direction for articial general intelligence research. In Chap.9 of Part 2, we will give a more concrete idea of what a symmetric high-interaction hybrid neural-symbolic architecture might look like, exploring the potential for this sort of hybridization between the OpenCogPrime AGI architecture (which is heavily symbolic in nature) and hierarchical attractor neural net based architectures such as DeSTIN.      124 6 Brief Survey of Cognitive Architectures 6.",Engineering General  Intelligence Part 1,chapter 6
"5 Globalist Versus Localist Representations Another interesting distinction, related to but different from symbolic versus emergentist and neural versus symbolic, may be drawn between cognitive systems (or subsystems) where memory is essentially global, and those where memory is essentially local. In this section we will pursue this distinction in various guises, along with the less familiar notion of glocal memory. This globalist/localist distinction is most easily conceptualized by reference to memoriescorrespondingtocategoriesofentitiesoreventsinanexternalenvironment. In an AI system that has an internal notion of activationi.e. in which some of its internal elements are more active than others, at any given point in time one can dene the internal image of an external event or entity as the fuzzy set of internal elements that tend to be active when that event or entity is presented to the systems sensors.",Engineering General  Intelligence Part 1,chapter 6
"If one has a particular set S of external entities or events of interest, then, the degree of memory localization of such an AI system relative to S may be conceived as the percentage of the systems internal elements that have a high degree of membership in the internal image of an average element of S. Of course, this characterization of localization has its limitations, such as the possibility of ambiguity regarding what are the system elements of a given AI system; and the exclusive focus on internal images of external phenomena rather than representation of internal abstract concepts. However, our goal here is not to formulate an ultimate, rigorous and thorough ontology of memory systems, but only to pose a rough and ready categorization so as to properly frame our discussion of some specic AGI issues relevant to CogPrime. Clearly the ideas pursued here will benet from further theoretical exploration and elaboration.",Engineering General  Intelligence Part 1,chapter 6
"In this sense, a Hopeld neural net [Ami89] would be considered globalist since it has a low degree of memory localization (most internal images heavily involve a large number of system elements); whereas Cyc would be considered localist as it has a very high degree of memory localization (most internal images are heavily focused on a small set of system elements). However, although Hopeld nets and Cyc form handy examples, the globalist versus localist distinction as described above is not identical to the neural versus symbolic distinction. For it is in principle quite possible to create localist systems using formal neurons, and also to create globalist systems using formal logic. And globalist-localist is not quite identical to symbolic versus emergentist either, because the latter is about coordinated system dynamics and behavior not just about knowledge representation.",Engineering General  Intelligence Part 1,chapter 6
"CogPrime combines both symbolic and (loosely) neural representations, and also combines globalist and localist representations in a way that we will call glocal and analyze more deeply in Chap.14; but there are many other ways these various properties could be manifested by AI systems. Rigorously studying the corpus of existing (or hypothetical!) cognitive architectures using these ideas would be a large task, which we do not undertake here.      6.5 Globalist Versus Localist Representations 125 Fig. 6.8 The CLARION cognitive architecture In the next sections we review several hybrid architectures in more detail, focusing most deeply on LIDA and MicroPsi which have been directly inspirational for CogPrime. 6.5.1 CLARION Ron Suns CLARION architecture (see Fig.6.",Engineering General  Intelligence Part 1,chapter 6
"8) is interesting in its combination of symbolic and neural aspectsa combination that is used in a sophisticated way to embody the distinction and interaction between implicit and explicit mental processes. From a CLARION perspective, architectures like Soar and ACT-R are severelylimitedinthattheydealonlywithexplicitknowledgeandassociatedlearning processes. CLARION consists of a number of distinct subsystems, each of which contains a dual representational structure, including a rules and chunks symbolic knowledge store somewhat similar to ACT-R, and a neural net knowledge store embodying implicit knowledge. The main subsystems are:  An action-centered subsystem to control actions;  A non-action-centered subsystem to maintain general knowledge;  Amotivationalsubsystemtoprovideunderlyingmotivationsforperception,action, and cognition;  A meta-cognitive subsystem to monitor, direct, and modify the operations of all the other subsystems.      126 6 Brief Survey of Cognitive Architectures 6.5.",Engineering General  Intelligence Part 1,chapter 6
"2 The Society of Mind and the Emotion Machine In his inuential but controversial book The Society of Mind [Min88], Marvin Minsky described a model of human intelligence as something that is built up from the interactions of numerous simple agents. He spells out in great detail how various particular cognitive functions may be achieved via agents and their interactions. He leaves no room for any central algorithms or structures of thought, famously arguing: What magical trick makes us intelligent? The trick is that there is no trick. The power of intelligence stems from our vast diversity, not from any single, perfect principle. This perspective was extended in the more recent work The Emotion Machine [Min07], where Minsky argued that emotions are ways to think evolved to handle different problem types that exist in the world. The brain is posited to have rulebased mechanisms (selectors) that turns on emotions to deal with various problems.",Engineering General  Intelligence Part 1,chapter 6
"Overall, both of these works serve better as works of speculative cognitive science than as works of AI or cognitive architecture per se. As neurologist Richard Restak said in his review of Emotion Machine, Minsky does a marvelous job parsing other complicated mental activities into simpler elements. ... But he is less effective in relating these emotional functions to whats going on in the brain. As Restak added, he is also not so effective at relating these emotional functions to straightforwardly implementable algorithms or data structures. Push Singh, in his PhD thesis and followup work [SBC05], did the best job so far of creating a concrete AI design based on Minskys ideas. While Singhs system was certainly interesting, it was also noteworthy for its lack of any learning mechanisms, and its exclusive focus on explicit rather than implicit knowledge. Due to Singhs tragic death, his work was never brought anywhere near completion.",Engineering General  Intelligence Part 1,chapter 6
"It seems fair to say that there has not yet been a serious cognitive architecture posed based closely on Minskys ideas. 6.5.3 DUAL The closest thing to a Minsky-ish cognitive architecture is probably DUAL, which takes the Society of Mind concept and adds to it a number of other interesting ideas. DUAL integrates symbolic and connectionist approaches at a deeper level than CLARION, and has been used to model various cognitive functions such as perception, analogy and judgment. Computations in DUAL emerge from the self-organized interaction of many micro-agents, each of which is a hybrid symbolic/connectionist device. Each DUAL agent plays the role of a neural network node, with an activation level and activation spreading dynamics; but also plays the role of a symbol, manipulated using formal rules. The agents exchange messages and activation via links that can be learned and modied, and they form coalitions which collectively represent concepts, episodes, and facts.      6.",Engineering General  Intelligence Part 1,chapter 6
"5 Globalist Versus Localist Representations 127 Fig. 6.9 The three main components of the DUAL model: the retinotopic visual array (RVA), the visual working memory (VWM) and DUALs semantic memory. Attention is allocated to an area of the visual array by the object in VWM controlling attention, while scene and object categories corresponding to the contents of VWM are retrieved from the semantic memory The structure of the model is sketchily depicted in Fig.6.9, which covers the application of DUAL to a toy environment called TextWorld. The visual input corresponding to a stimulus is presented on a two-dimensional visual array representing the front end of the system. Perceptual primitives like blobs and terminations are immediately generated by cheap parallel computations. Attention is controlled at each time by an object which allocates it selectively to some area of the stimulus.",Engineering General  Intelligence Part 1,chapter 6
"A detailed symbolic representation is constructed for this area which tends to fade away as attention is withdrawn from it and allocated to another one. Categorization of visual memory contents takes place by retrieving object and scene categories from DUALs semantic memory and mapping them onto current visual memory representations. In principle the DUAL framework seems quite powerful; using the language of CogPrime, however, it seems to us that the learning mechanisms of DUAL have not been formulated in such a way as to give rise to powerful, scalable cognitive synergy. It would likely be possible to create very powerful AGI systems within DUAL, and perhaps some very CogPrime-like systems as well. But the systems that have been created or designed for use within DUAL so far seem not to be that powerful in their potential or scope.      128 6 Brief Survey of Cognitive Architectures Fig. 6.10 Albuss 4D-RCS architecture for a single vehicle.",Engineering General  Intelligence Part 1,chapter 6
"Figure from [AM01], used with permission 6.5.4 4D/RCS Inaratherdifferentdirection,JamesAlbus,whileattheNationalBureauofStandards, developed a very thorough and impressive architecture for intelligent robotics called 4D/RCS,whichwasimplementedinanumberofmachinesincludingunmannedautomated vehicles. This architecture lacks critical aspects of intelligence such as learning and creativity, but combines perception, action, planning and world-modeling in a highly effective and tightly-integrated fashion. The architecture has three hierarchies of memory/processing units: one for perception, one for action and one for modeling and guidance. Each unit has a certain spatiotemporal scope, and (except for the lowest level) supervenes over children whose spatiotemporal scope is a subset of its own. The action hierarchy takes care of decomposing tasks into subtasks; whereas the sensation hierarchy takes care of grouping signals into entities and events.",Engineering General  Intelligence Part 1,chapter 6
"The modeling/guidance hierarchy mediates interactions between perception and action based on its understanding of the world and the systems goals. In his book [AM01] Albus describes methods for extending 4D/RCS into a complete cognitive architecture, but these extensions have not been elaborated in full detail nor implemented (Figs.6.10 and 6.11).      6.5 Globalist Versus Localist Representations 129 Fig. 6.11 Albuss perceptual, motor and modeling hierarchies. Figure from [AM01], used with permission      130 6 Brief Survey of Cognitive Architectures 6.5.5 PolyScheme Nick Cassimatiss PolyScheme architecture [Cas07] shares with GLAIR the use of multiple logical reasoning methods on a common knowledge store. While its underlying ideas are quite general, currently PolyScheme is being developed in the context of the object tracking domain (construed very broadly).",Engineering General  Intelligence Part 1,chapter 6
"As a logic framework PolyScheme is fairly conventional (unlike GLAIR or NARS with their novel underlying formalisms), but PolyScheme has some unique conceptual aspects, for instance its connection with Cassimatiss theory of mind, which holds that the same core set of logical concepts and relationships underlies both language and physical reasoning [Cas04]. This ties in with the use of a common knowledge store for multiple cognitive processes; for instance it suggests that  the same core relationships can be used for physical reasoning and parsing, but that each of these domains may involve some additional relationships.  language processing may be done via physical-reasoning-based cognitive processes, plus the additional activity of some language-specic processes. 6.5.6 Joshua Blue Sam Adams and his colleagues at IBM have created a cognitive architecture called Joshua Blue [AABL02], which has some signicant similarities to CogPrime.",Engineering General  Intelligence Part 1,chapter 6
"Similar to our current research direction with CogPrime, Joshua Blue was created with loose emulation of child cognitive development in mind; and, also similar to CogPrime, it featuresanumberofcognitiveprocessesactingonacommonneural-symbolicknowledge store. The specic cognitive processes involved in Joshua Blue and CogPrime are not particularly similar, however. At time of writing (2012) Joshua Blue is not under active development and has not been for some time; however, the project may be reanimated in future. Joshua Blues core knowledge representation is a semantic network of nodes connected by links along which activation spreads. Although many of the nodes have specic semantic referents, as in a classical semantic net, the spread of activation through the network is designed to lead to the emergence of assemblies (which could also be thought of as dynamical attractors) in a manner more similar to an attractor neural network.",Engineering General  Intelligence Part 1,chapter 6
"A major difference from typical semantic or neural network models is the central role that affect plays in the systems dynamics. The weights of the links in the knowledge base are adjusted dynamically based on the emotional contexta very direct way of ensuring that cognitive processes and mental representations are continuously inuenced by affect. Qualitatively, this mimics the way that particular emotions in the human brain correlate with the dissemination throughout the brain of particular neurotransmitters, which then affect synaptic activity.      6.5 Globalist Versus Localist Representations 131 A result of this architecture is that in Joshua Blue, emotion directs attention in a very direct way: affective weighting is important in determining which associated objects will become part of the focus of attention, or will be retained from memory.",Engineering General  Intelligence Part 1,chapter 6
"AnotablesimilaritybetweenCogPrimeandJoshuaBlueisthatinbothsystems,nodes are assigned two quantitative attention values, one governing allocation of current system resources (mainly processor time; this is CogPrimes ShortTermImportance) and one governing the long-term allocation of memory (CogPrimes LongTermImportance). The concrete work done with Joshua Blue involved using it to control a simple agent in a simulated world, with the goal that via human interaction, the agent would develop a complex and humanlike emotional and motivational structure from its simple in-built emotions and drives, and would then develop complex cognitive capabilities as part of this development process. 6.5.7 LIDA The LIDA architecture developed by Stan Franklin and his colleagues [BF09] is based on the concept of the cognitive cyclea notion that is important to nearly every BICA (Biologically Inspired Cognitive Architectures) and also to the brain, but that plays a particularly central role in LIDA.",Engineering General  Intelligence Part 1,chapter 6
"As Franklin says, as a matter of principle, every autonomous agent, be it human, animal, or articial, must frequently sample (sense) its environment, process (make sense of) this input, and select an appropriate response (action). The agents life can be viewed as consisting of a continual sequence of iterations of these cognitive cycles. Such cycles constitute the indivisible elements of attention, the least sensing and acting to which we can attend. A cognitive cycle can be thought of as a moment of cognition, a cognitive moment. 6.5.8 The Global Workspace LIDA is heavily based on the global workspace concept developed by Bernard Baars. As this concept is also directly relevant to CogPrime it is worth briey describing here. In essence Baars Global Workspace Theory (GWT) is a particular hypothesis about how working memory works and the role it plays in the mind.",Engineering General  Intelligence Part 1,chapter 6
"Baars conceives working memory as the inner domain in which we can rehearse telephone numbers to ourselves or, more interestingly, in which we carry on the narrative of our lives. It is usually thought to include inner speech and visual imagery. Baars uses the term consciousness to refer to the contents of working memorya theoretical commitment that is not part of the CogPrime design. In this section we will use the term consciousness in Baars way, but not throughout the rest of the book.      132 6 Brief Survey of Cognitive Architectures Baars conceives working memory and consciousness in terms of a theater metaphoraccording to which, in the theater of consciousness a spotlight of selective attention shines a bright spot on stage. The bright spot reveals the global workspacethe contents of consciousness, which may be metaphorically considered as a group of actors moving in and out of consciousness, making speeches or interacting with each other. The unconscious is represented by the audience watching the play ...",Engineering General  Intelligence Part 1,chapter 6
"and there is also a role for the director (the minds executive processes) behind the scenes, along with a variety of helpers like stage hands, script writers, scene designers, etc. GWT describes a eeting memory with a duration of a few seconds. This is much shorter than the 1030s of classical working memoryaccording to GWT there is a very brief cognitive cycle in which the global workspace is refreshed, and the time period an item remains in working memory generally spans a large number of these elementary refresh actions. GWT contents are proposed to correspond to what we are conscious of, and are said to be broadcast to a multitude of unconscious cognitive brain processes. Unconscious processes, operating in parallel, can form coalitions which can act as input processes to the global workspace.",Engineering General  Intelligence Part 1,chapter 6
"Each unconscious process is viewed as relating to certain goals, and seeking to get involved with coalitions that will get enough importance to become part of the global workspacebecause once theyre in the global workspace theyll be allowed to broadcast out across the mind as a whole, which include broadcasting to the internal and external actuators that allow the mind to do things. Getting into the global workspace is a processs best shot at achieving its goals. Obviously, the theater metaphor used to describe the GWT is evocative but limited; for instance, the unconscious in the mind does a lot more than the audience in a theater. The unconscious comes up with complex creative ideas sometimes, which feed into consciousnessalmost as if the audience is also the scriptwriter. Baars theory, with its understanding of unconscious dynamics in terms of coalitionbuilding, fails to describe the subtle dynamics occurring within the various forms of long-term memory, which result in subtle nonlinear interactions between long term memory and working memory.",Engineering General  Intelligence Part 1,chapter 6
"But nevertheless, GWT successfully models a number of characteristics of consciousness, including its role in handling novel situations, its limited capacity, its sequential nature, and its ability to trigger a vast range of unconscious brain processes. It is the framework on which LIDAs theory of the cognitive cycle is built. 6.5.9 The LIDA Cognitive Cycle The simplest cognitive cycle is that of an animal, which senses the world, compares sensation to memory, and chooses an action, all in one uid subjective moment. But the same cognitive cycle structure/process applies to higher-level cognitive processes as well. The LIDA architecture is based on the LIDA model of the cognitive cycle,      6.5 Globalist Versus Localist Representations 133 Fig. 6.12 The LIDA cognitive cycle. Copyright Stan Franklin (2006), used with permission which posits a particular structure underlying the cognitive cycle that possess the generality to encompass both simple and complex cognitive moments.",Engineering General  Intelligence Part 1,chapter 6
"The LIDA cognitive cycle itself is a theoretical construct that can be implemented in many ways, and indeed other BICAs like CogPrime and Psi also manifest the LIDA cognitive cycle in their dynamics, though utilizing different particular structures to do so. Figure6.12 shows the cycle pictorially, starting in the upper left corner and proceeding clockwise. At the start of a cycle, the LIDA agent perceives its current situation and allocates attention differentially to various parts of it. It then broadcasts information about the most important parts (which constitute the agents consciousness), and this information gets features extracted from it, when then get passed along to episodic and semantic memory, that interact in the global workspace to create a model for the agents current situation.",Engineering General  Intelligence Part 1,chapter 6
"This model then, in interaction with procedural memory, enables the agent to choose an appropriate action and execute itthe critical action-selection phase!      134 6 Brief Survey of Cognitive Architectures The LIDA Cognitive Cycle in More Depth We now run through the cognitive cycle in more detail.2 It begins with sensory stimuli fromtheagents external internal environment. Low-level featuredetectors insensory memory begin the process of making sense of the incoming stimuli. These low-level features are passed to perceptual memory where higher-level features, objects, categories, relations, actions, situations, etc. are recognized. These recognized entities, called percepts, are passed to the workspace, where a model of the agents current situation is assembled. Workspace structures serve as cues to the two forms of episodic memory, yielding both short and long term remembered local associations.",Engineering General  Intelligence Part 1,chapter 6
"In addition to the current percept, the workspace contains recent percepts that havent yet decayed away, and the agents model of the then-current situation previously assembled from them. The model of the agents current situation is updated from the previous model using the remaining percepts and associations. This updating process will typically require looking back to perceptual memory and even to sensory memory, to enable the understanding of relations and situations. This assembled new model constitutes the agents understanding of its current situation within its world. Via constructing the model, the agent has made sense of the incoming stimuli. Now attention allocation comes into play, because a real agent lacks the computational resources to work with all parts of its world-model with maximal mental focus. Portions of the model compete for attention. These competing portions take the form of (potentially overlapping) coalitions of structures comprising parts the model. Once one such coalition wins the competition, the agent has decided what to focus its attention on.",Engineering General  Intelligence Part 1,chapter 6
"And now comes the purpose of all this processing: to help the agent to decide what to do next. The winning coalition passes to the global workspace, the namesake of Global Workspace Theory, from which it is broadcast globally. Though the contents of this conscious broadcast are available globally, the primary recipient is procedural memory, which stores templates of possible actions including their context and possible results. Procedural memory also stores an activation value for each such templatea value that attempts to measure the likelihood of an action taken within its context producing the expected result. Its worth noting that LIDA makes a rather specic assumption here. LIDAs activation values are like the probabilistic truth values of the implications in CogPrimes Context  Procedure  Goal triples. However, in CogPrime this probability is not the same as the ShortTermImportance attention value associated with the Implication link representing that implication. Here LIDA merges together two concepts that in CogPrime are separate.",Engineering General  Intelligence Part 1,chapter 6
"Templates whose contexts intersect sufciently with the contents of the conscious broadcast instantiate copies of themselves with their variables specied to the current situation. These instantiations are passed to the action selection mechanism, which chooses a single action from these instantiations and those remaining from previous 2 This section paraphrases heavily from [Fra06].      6.5 Globalist Versus Localist Representations 135 cycles. The chosen action then goes to sensorimotor memory, where it picks up the appropriate algorithm by which it is then executed. The action so taken affects the environment, and the cycle is complete. The LIDA model hypothesizes that all human cognitive processing is via a continuing iteration of such cognitive cycles. It acknowledges that other cognitive processes may also occur, rening and building on the knowledge used in the cognitive cycle (for instance, the cognitive cycle itself doesnt mention abstract reasoning or creativity).",Engineering General  Intelligence Part 1,chapter 6
"But the idea is that these other processes occur in the context of the cognitive cycle, which is the main loop driving the internal and external activities of the organism. 6.5.9.1 Avoiding Combinatorial Explosion via Adaptive Attention Allocation LIDA avoids combinatorial explosions in its inference processes via two methods, both of which are also important in CogPrime:  combining reasoning via association with reasoning via deduction  foundational use of uncertainty in reasoning. One can create an analogy between LIDAs workspace structures and codelets and a logic-based architectures assertions and functions. However, LIDAs codelets only operate on the structures that are active in the workspace during any given cycle. This includes recent perceptions, their closest matches in other types of memory, and structures recently created by other codelets. The results with the highest estimate of success, i.e. activation, will then be selected.",Engineering General  Intelligence Part 1,chapter 6
"UncertaintyplaysaroleinLIDAsreasoninginseveralways,mostnotablythrough the base activation of its behavior codelets, which depend on the models estimated probability of the codelets success if triggered. LIDA observes the results of its behaviors and updates the base activation of the responsible codelets dynamically. We note that for this kind of uncertain inference/activation interplay to scale well, some level of cognitive synergy must be present; and based on our understanding of LIDA it is not clear to us whether the particular inference and association algorithms used in LIDA possess the requisite synergy. 6.5.9.2 LIDA Versus CogPrime The LIDA cognitive cycle, broadly construed, exists in CogPrime as in other cognitive architectures. To see how, it sufces to map the key LIDA structures into corresponding CogPrime structures, as is done in Table6.1.",Engineering General  Intelligence Part 1,chapter 6
"Of course this table does not cover all CogPrime processes, as LIDA does not constitute a thorough explanation of CogPrime structure and dynamics. And in most cases the corresponding CogPrime and LIDA processes dont work in exactly the same way; for instance, as noted above, LIDAs action selection relies solely on LIDAs activation values,      136 6 Brief Survey of Cognitive Architectures Table 6.",Engineering General  Intelligence Part 1,chapter 6
"1 CogPrime analogues of key LIDA features LIDA CogPrime Declarative memory Atomspace Attentional codelets Schema that adjust importance of atoms explicitly Coalitions Maps Global workspace Attentional focus Behavior codelets Schema Procedural memory (scheme net) Procedures in ProcedureRepository; and network of SchemaNodes in the Atomspace Action selection (behavior net) Propagation of STICurrency from goals to actions, and action selection process Transient episodic memory Perceptual atoms entering AT with high STI, which rapidly decreases in most cases Local workspaces Bubbles of interlinked atoms with moderate importance, focused on by a subset of MindAgents (dened in Chap.",Engineering General  Intelligence Part 1,chapter 6
"2 of Part 2) for a period of time Perceptual associative memory HebbianLinks in the AT Sensory memory Spaceserver/timeserver, plus auxiliary stores for other senses Sensorimotor memory Atoms storing record of actions taken, linked in with atoms indexed in sensory memory whereas CogPrimes action selection process is more complex, relying on aspects of CogPrime that lack LIDA analogues. 6.5.10 Psi and MicroPsi We have saved for last the architecture that has the most in common with CogPrime: Joscha Bachs MicroPsi architecture, closely based on Dietrich Dorners Psi theory. CogPrime has borrowed substantially from Psi in its handling of emotion and motivation; but Psi also has other aspects that differ considerably from CogPrime. Here we will focus more heavily on the points of overlap, but will mention the key points of difference as well.",Engineering General  Intelligence Part 1,chapter 6
"The overall Psi cognitive architecture, which is centered on the Psi model of the motivational system, is roughly depicted in Fig.6.13. Psis motivational system begins with Demands, which are the basic factors that motivate the agent. For an animal these would include things like food, water, sex, novelty, socialization, protection of ones children, and so forth. For an intelligent robot they might include things like electrical power, novelty, certainty, socialization, well-being of others and mental growth. Psi also species two fairly abstract demands and posits them as psychologically fundamental (see Fig.6.14):  competence, the effectiveness of the agent at fullling its Urges      6.5 Globalist Versus Localist Representations 137 Fig. 6.13 High-level architecture of the Psi model Fig. 6.",Engineering General  Intelligence Part 1,chapter 6
"14 Primary interrelationships between Psi modulators      138 6 Brief Survey of Cognitive Architectures  certainty, the condence of the agents knowledge. Each demand is assumed to come with a certain target level or target range (and these may uctuate over time, or may change as a system matures and develops). An Urge is said to develop when a demand deviates from its target range: the urge then seeks to return the demand to its target range. For instance, in an animal-like agent the demand related to food is more clearly described as fullness, and there is a target range indicating that the agent is neither too hungry nor too full of food. If the agents fullness deviates from this range, an Urge to return the demand to its target range arises.",Engineering General  Intelligence Part 1,chapter 6
"Similarly, if an agents novelty deviates from its target range, this means the agents life has gotten either too boring or too disconcertingly weird, and the agent gets an Urge for either more interesting activities (in the case of below-range novelty) or more familiar ones (in the case of above-range novelty). There is also a primitive notion of Pleasure (and its opposite, displeasure), which is considered as different from the complex emotion of happiness. Pleasure is understood as associated with Urges: pleasure occurs when an Urge is (at least partially) satised, whereas displeasure occurs when an urge gets increasingly severe. The degree to which an Urge is satised is not necessarily dened instantaneously; it may be dened, for instance, as a time-decaying weighted average of the proximity of the demand to its target range over the recent past.",Engineering General  Intelligence Part 1,chapter 6
"So, for instance if an agent is bored and gets a lot of novel stimulation, then it experiences some pleasure. If its bored and then the monotony of its stimulation gets even more extreme, then it experiences some displeasure. Note that, according to this relatively simplistic approach, any decrease in the amount of dissatisfaction causes some pleasure; whereas if everything always continues within its acceptable range, there isnt any pleasure. This may seem a little counterintuitive, but its important to understand that these simple denitions of pleasure and displeasure are not intended to fully capture the natural language concepts associated with those words. The natural language terms are used here simply as heuristics to convey the general character of the processes involved. These are very low level processes whose analogues in human experience are largely below the conscious level. A Goal is considered as a statement that the system may strive to make true at some future time.",Engineering General  Intelligence Part 1,chapter 6
"A Motive is an (urge, goal) pair, consisting of a goal whose satisfaction is predicted to imply the satisfaction of some urge. In fact one may consider Urges as top-level goals, and the agents other goals as their subgoals. In Psi an agent has one ruling motive at any point in time, but this seems an oversimplication more applicable to simple animals than to human-like or other advanced AI systems. In general one may think of different motives having different weights indicating the amount of resources that will be spent on pursuing them. Emotions in Psi are considered as complex systemic response-patterns rather than explicitly constructed entities. An emotion is the set of mental entities activated in response to a certain set of urges. Dorner conceived theories about how various common emotions emerge from the dynamics of urges and motives as described in the Psi model. Intentions are also considered as composite entities: an intention at      6.",Engineering General  Intelligence Part 1,chapter 6
"5 Globalist Versus Localist Representations 139 a given point in time consists of the active motives, together with their related goals, behavior programs and so forth. The basic logic of action in Psi is carried out by triples that are very similar to CogPrimes Context  Procedure  Goal triples. However, an important role is played by four modulators that control how the processes of perception, cognition and action selection are regulated at a given time:  activation, which determines the degree to which the agent is focused on rapid, intensive activity versus reective, cognitive activity  resolution level, which determines how accurately the system tries to perceive the world  certainty, which determines how hard the system tries to achieve denite, certain knowledge  selection threshold, which determines how willing the system is to change its choice of which goals to focus on.",Engineering General  Intelligence Part 1,chapter 6
"These modulators characterize the systems emotional and cognitive state at a very abstract level; they are not emotions per se, but they have a large effect on the agents emotions. Their intended interaction is depicted in Fig.6.14. 6.5.11 The Emergence of Emotion in the Psi Model We now briey review the specics of how Psi models the emergence of emotion. The basic idea is to dene a small set of proto-emotional dimensions in terms of basic Urges and modulators. Then, emotions are identied with regions in the space spanned by these dimensions. The simplest approach uses a six-dimensional continuous space: 1. pleasure 2. arousal 3. resolution level 4. selection threshold (i.e. degree of dominance of the leading motive) 5. level of background checks (the rate of the securing behavior) 6. level of goal-directed behavior. Figure6.",Engineering General  Intelligence Part 1,chapter 6
"15showshowthelatter5ofthesedimensionsarederivedfromunderlying urges and modulators. Note that these dimensions are not orthogonal; for instance resolution is mainly inversely related to arousal. Additional dimensions are also discussed, for instance it is postulated that to deal with social emotions one may wish to introduce two more demands corresponding to inner and outer obedience to social norms, and then dene dimensions in terms of these. Specic emotions are then characterized in terms of these dimensions. According to [Bac09], for instance, Anger ... is characterized by high arousal, low resolution, strong motive dominance, few background checks and strong goal-orientedness;      140 6 Brief Survey of Cognitive Architectures Fig. 6.15 Five proto-emotional dimensions implicit in the Psi model sadness by low arousal, high resolution, strong dominance, few background-checks and low goal-orientedness. Im a bit skeptical of the contention that these dimensions fully characterize the relevant emotions.",Engineering General  Intelligence Part 1,chapter 6
"Anger for instance seems to have some particular characteristics not implied by the above list of dimensional values. The list of dimensional values associated with anger doesnt tell us that an angry person is more likely to punch someone than to bounce up and down, for example. However, it does seem that the dimensional values associated with an emotion are informative about the emotion, so that positioning an emotion on the given dimensions tells one a lot. 6.5.12 Knowledge Representation, Action Selection and Planning in Psi In addition to the basic motivation/emotion architecture of Psi, which has been adopted (with some minor changes) for use in CogPrime, Psi has a number of other aspects that are somewhat different from their CogPrime analogues. First of all, on the micro level, Psi represents knowledge using structures called quads. Each quad is a cluster of 5 neurons containing a core neuron, and four other      6.",Engineering General  Intelligence Part 1,chapter 6
"5 Globalist Versus Localist Representations 141 neurons representing before/after and part-of/has-part relationships in regard to that core neuron. Quads are naturally assembled into spatiotemporal hierarchies, though they are not required to form part of such a structure. Psi stores knowledge using quads arranged in three networks, which are conceptually similar to the networks in Albuss 4D/RCS and Arels DeSTIN architectures:  A sensory network, which stores declarative knowledge: schemas representing images, objects, events and situations as hierarchical structures.  A motor network, which contains procedural knowledge by way of hierarchical behavior programs.  A motivational network handling demands. Perception in Psi, which is centered in the sensory network, follows principles similar to DeSTIN (which are shared also by other systems), for instance the principle of perception as prediction.",Engineering General  Intelligence Part 1,chapter 6
"Psis HyPercept mechanism performs hypothesisbased perception: it attempts to predict what is there to be perceived and then attempts to verify these predictions using sensation and memory. Furthermore HyPercept is intimately coupled with actions in the external world, according to the concept of Neissers perceptual cycle, the cycle between exploration and representation of reality. Perceptually acquired information is translated into schemas capable of guiding behaviors, and these are enacted (sometimes affecting the world in signicant ways) and in the process used to guide further perception. Imaginary perceptions are handled via a mental stage analogous to CogPrimes internal simulation world.",Engineering General  Intelligence Part 1,chapter 6
"Action selection in Psi works based on what are called triplets, each of which consists of  a sensor schema (pre-conditions, condition schema; like CogPrimes context)  a subsequent motor schema (action, effector; like CogPrimes procedure)  a nal sensor schema (post-conditions, expectations; like an CogPrime predicate or goal). What distinguishes these triplets from classic production rules as used in (say) Soar and ACT-R is that the triplets may be partial (some of the three elements may be missing) and may be uncertain. However, there seems no fundamental difference between these triplets and CogPrimes concept/procedure/goal triplets, at a high level; the difference lies in the underlying knowledge representation used for the schemata, and the probabilistic logic used to represent the implication.",Engineering General  Intelligence Part 1,chapter 6
"The work of guring out what schema to execute to achieve the chosen goal in the current context is done in Psi using a combination of processes called the Rasmussen ladder (named after Danish psychologist Jens Rasmussen). The Rasmussen ladder describes the organization of action as a movement between the stages of skill-based behavior, rule-based behavior and knowledge-based behavior, as follows:  If a given task amounts to a trained routine, an automatism or skill is activated; it can usually be executed without conscious attention and deliberative control.  If there is no automatism available, a course of action might be derived from rules; before a known set of strategies can be applied, the situation has to be analyzed and the strategies have to be adapted.      142 6 Brief Survey of Cognitive Architectures  In those cases where the known strategies are not applicable, a way of combining the available manipulations (operators) into reaching a given goal has to be explored at rst.",Engineering General  Intelligence Part 1,chapter 6
"This stage usually requires a recomposition of behaviors, that is, a planning process. The planning algorithm used in the Psi and MicroPsi implementations is a fairly simple hill-climbing planner. While its hypothesized that a more complex planner may be needed for advanced intelligence, part of the Psi theory is the hypothesis that most real-life planning an organism needs to do is fairly simple, once the organism has the right perceptual representations and goals. 6.5.13 Psi Versus CogPrime On a high level, the similarities between Psi and CogPrime are quite strong:  interlinked declarative, procedural and intentional knowledge structures, represented using neural-symbolic methods (though, the knowledge structures have somewhat different high-level structures and low-level representational mechanisms in the two systems)  perception via prediction and perception/action integration  action selection via triplets that resemble uncertain, potentially partial production rules  similar motivation/emotion framework, since CogPrime incorporates a variant of Psi for this.",Engineering General  Intelligence Part 1,chapter 6
"On the nitty-gritty level there are many differences between the systems, but on the big-picture level the main difference lies in the way the cognitive synergy principle is pursued in the two different approaches. Psi and MicroPsi rely on very simple learning algorithms that are closely tied to the quad neurosymbolic knowledge representation, and hence interoperate in a fairly natural way without need for subtle methods of synergy engineering. CogPrime uses much more diverse and sophisticated learning algorithms which thus require more sophisticated methods of interoperation in order to achieve cognitive synergy.",Engineering General  Intelligence Part 1,chapter 6
"  Chapter 7 A Generic Architecture of Human-Like Cognition 7.1 Introduction When writing the rst draft of this book, some years ago, we had the idea to explain CogPrime by aligning its various structures and processes with the ones in the standard architecture diagram of the human mind. After a bit of investigation, though, we gradually came to the realization that no such thing existed. There was no standard owchart or other sort of diagram explaining the modern consensus on how human thought works. Many such diagrams existed, but each one seemed to represent some particular focus or theory, rather than an overall integrative understanding. Since there are multiple opinions regarding nearly every aspect of human intelligence, it would be difcult to get two cognitive scientists to fully agree on every aspect of an overall human cognitive architecture diagram. Prior attempts to outline detailed mind architectures have tended to follow highly specic theories of intelligence, and hence have attracted only moderate interest from researchers not adhering to those theories.",Engineering General  Intelligence Part 1,chapter 7
"An example is Minskys work presented in The Emotion Machine [Min07], which arguably does constitute an architecture diagram for the human mind, but which is only loosely grounded in current empirical knowledge and stands more as a representation of Minskys own intuitive understanding. But nevertheless, it seemed to us that a reasonable attempt at an integrative, relatively theory-neutral human cognitive architecture diagram would be better than nothing. So naturally, we took it on ourselves to create such a diagram. This chapter is the resultit draws on the thinking of a number of cognitive science and AGI researchers, integrating their perspectives in a coherent, overall architecture diagram for human, and human-like, general intelligence. The specic architecture diagram of CogPrime, given in Chap.2, may then be understood as a particular instantiation of this generic architecture diagram of human-like cognition.",Engineering General  Intelligence Part 1,chapter 7
"There is no getting around the fact that, to a certain extent, the diagram presented here reects our particular understanding of how the mind works. However, it was intentionally constructed with the goal of not being just an abstracted version of the CogPrime architecture diagram! It does not reect our own idiosyncratic B. Goertzel et al., Engineering General Intelligence, Part 1, 143 Atlantis Thinking Machines 5, DOI: 10.2991/978-94-6239-027-0_7,  Atlantis Press and the authors 2014      144 7 A Generic Architecture of Human-Like Cognition understanding of human intelligence, as much as a combination of understandings previously presented by multiple researchers (including ourselves), arranged according to our own taste in a manner we nd conceptually coherent. With this in mind, we call it the Integrative Human-Like Cognitive Architecture Diagram, or for short the integrative diagram.",Engineering General  Intelligence Part 1,chapter 7
"We have made an effort to ensure that as many pieces of the integrative diagram as possible are well grounded in psychological and even neuroscientic data, rather than mainly embodying speculative notions; however, given the current state of knowledge, this could not be done to a complete extent, and there is still some speculation involved here and there. While based on understandings of human intelligence, the integrative diagram is intended to serve as an architectural outline for human-like general intelligence more broadly. For example, CogPrime is explicitly not intended as a precise emulation of human intelligence, and does many things quite differently than the human mind, yet can still fairly straightforwardly be mapped into the integrative diagram. The integrative diagram focuses on structure, but this should not be taken to represent a valuation of structure over dynamics in our approach to intelligence. Following chapters treat various dynamical phenomena in depth. 7.",Engineering General  Intelligence Part 1,chapter 7
"2 Key Ingredients of the Integrative Human-Like Cognitive Architecture Diagram Themainingredientsweveusedinassemblingtheintegrativediagramareasfollows:  Our own views on the various types of memory critical for human-like cognition, and the need for tight, synergetic interactions between the cognitive processes focused on these.  Aaron Slomans high-level architecture diagram of human intelligence [Slo01], drawn from his CogAff architecture, which strikes me as a particularly clear embodiment of modern common sense regarding the overall architecture of the human mind. We have added only a couple items to Slomans high-level diagram, which we felt deserved an explicit high-level role that he did not give them: emotion, language and reinforcement.  The LIDA architecture diagram presented by Stan Franklin and Bernard Baars [BF09].WethinkLIDAisanexcellentmodelofworkingmemoryandwhatSloman calls reactive processes, with well-researched grounding in the psychology and neuroscience literature.",Engineering General  Intelligence Part 1,chapter 7
"We have adapted the LIDA diagram only very slightly for use here, changing some of the terminology on the arrows, and indicating where parts of the LIDA diagram indicate processes elaborated in more detail elsewhere in the integrative diagram.  The architecture diagram of the Psi model of motivated cognition, presented by Joscha Bach in [Bac09] based on prior work by Dietrich Dorner [Dr02]. This diagram is presented without signicant modication; however it should be noted that Bach and Dorner present this diagram in the context of larger and richer cognitive      7.2 Key Ingredients of the Integrative Human-Like Cognitive Architecture Diagram 145 models, the other aspects of which are not all incorporated in the integrative diagram.  James Albuss three-hierarchy model of intelligence [AM01], involving coupled perception, action and reinforcement hierarchies.",Engineering General  Intelligence Part 1,chapter 7
"Albuss model, utilized in the creation of intelligent unmanned automated vehicles, is a crisp embodiment of many ideas emergent from the eld of intelligent control systems.  Deep learning networks as a model of perception (and action and reinforcement learning), as embodied for example in the work of Itamar Arel [ARC09] and Jeff Hawkins [HB06]. The integrative diagram adopts this as the basic model of the perception and action subsystems of human intelligence. Language understanding and generation are also modeled according to this paradigm. One possible negative reaction to the integrative diagram might be to say that its a kind of Frankenstein monster diagram, piecing together aspects of different theories in a way that violates the theoretical notions underlying all of them! For example, the integrative diagram takes LIDA as a model of working memory and reactive processing, but from the papers on LIDA its unclear whether the creators of LIDA construe it more broadly than that.",Engineering General  Intelligence Part 1,chapter 7
"The deep learning community tends to believe that the architecture of current deep learning networks, in itself, is close to sufcient for human-level general intelligencewhereas the integrative diagram appropriates the ideas from this community mainly for handling perception, action and language, etc. On the other hand, in a more positive perspective, one could view the integrative diagram as consistent with LIDA, but merely providing much more detail on some of the boxes in the LIDA diagram (e.g. dealing with perception and long-term memory). And one could view the integrative diagram as consistent with the deep learning paradigmvia viewing it, not as a description of components to be explicitly implemented in an AGI system, but rather as a description of the key structures and processes that must emerge in deep learning network, based on its engagement with the world, in order for it to achieve human-like general intelligence.",Engineering General  Intelligence Part 1,chapter 7
"Our own view, underlying the creation of the integrative diagram, is that different communities of cognitive science researchers have focused on different aspects of intelligence, and have thus each created models that are more fully eshed out in some aspects than others. But these various models all link together fairly cleanly, which is not surprising as they are all grounded in the same data regarding human intelligence. Many judgment calls must be made in fusing multiple models in the way that the integrative diagram does, but we feel these can be made without violating the spirit of the component models. In assembling the integrative diagram, we have made these judgment calls as best we can, but were well aware that different judgments would also be feasible and defensible. Revisions are likely as time goes on, not only due to new data about human intelligence but also to evolution of understanding regarding the best approach to model integration. Another possible argument against the ideas presented here is that theres nothing newall the ingredients presented have been given before elsewhere.",Engineering General  Intelligence Part 1,chapter 7
"To this our retort is to quote Pascal: Let no one say that I have said nothing new... the      146 7 A Generic Architecture of Human-Like Cognition Fig. 7.1 High-level architecture of a human-like mind arrangement of the subject is new. The various architecture diagrams incorporated into the integrative diagram are either extremely high level (Slomans diagram) or focus primarily on one aspect of intelligence, treating the others very concisely by summarizing large networks of distinction structures and processes in small boxes. The integrative diagram seeks to cover all aspects of human-like intelligence at a roughly equal granularitya different arrangement. Thiskindofhigh-leveldiagrammingexerciseisnotpreciseenough,nordynamicsfocused enough, to serve as a guide for creating human-level or more advanced AGI. But it can be a useful tool for explaining and interpreting a concrete AGI design, such as CogPrime. 7.",Engineering General  Intelligence Part 1,chapter 7
"3 An Architecture Diagram for Human-Like General Intelligence The integrative diagram is presented here in a series of seven Figures. Figure7.1 gives a high-level breakdown into components, based on Slomans high-level cognitive-architectural sketch [Slo01]. This diagram represents, roughly speaking, modern common sense about how a human-like mind is architected. The separation between structures and processes, embodied in having separate boxes      7.3 An Architecture Diagram for Human-Like General Intelligence 147 Fig. 7.2 Architecture of working memory and reactive processing, closely modeled on the LIDA architecture for Working Memory versus Reactive Processes, and for Long Term Memory versus Deliberative Processes, could be viewed as somewhat articial, since in the human brain and most AGI architectures, memory and processing are closely integrated.",Engineering General  Intelligence Part 1,chapter 7
"However, the tradition in cognitive psychology is to separate out Working Memory and Long Term Memory from the cognitive processes acting thereupon, so we have adhered to that convention. The other changes from Slomans diagram are the explicit inclusion of language, representing the hypothesis that language processing is handled in a somewhat special way in the human brain; and the inclusion of a reinforcement component parallel to the perception and action hierarchies, as inspired by intelligent control systems theory (e.g. Albus as mentioned above) and deep learning theory. Of course Slomans high level diagram in its original form is intended as inclusive of language and reinforcement, but we felt it made sense to give them more emphasis. Figure7.2, modeling working memory and reactive processing, is essentially the LIDA diagram as given in prior papers by Stan Franklin, Bernard Baars and colleagues [BF09].",Engineering General  Intelligence Part 1,chapter 7
"The boxes in the upper left corner of the LIDA diagram pertain to sensory and motor processing, which LIDA does not handle in detail, and which are modeled more carefully by deep learning theory. The bottom left corner box refers to action selection, which in the integrative diagram is modeled in more detail by Psi. The top right corner box refers to Long-Term Memory, which the integrative diagram models in more detail as a synergetic multi-memory system (Fig.7.4). The original LIDA diagram refers to various codelets, a key concept in LIDA theory. We have replaced attention codelets here with attention ow, a more generic term. We suggest one can think of an attention codelet as: a piece of      148 7 A Generic Architecture of Human-Like Cognition Fig. 7.3 Architecture of motivated action information stating that, for a certain group of items, its currently pertinent to pay attention to this group as a collective.",Engineering General  Intelligence Part 1,chapter 7
"Figure7.3, modeling motivation and action selection, is a lightly modied version of the Psi diagram from Joscha Bachs book Principles of Synthetic Intelligence [Bac09]. The main difference from Psi is that in the integrative diagram the Psi motivated action framework is embedded in a larger, more complex cognitive model. Psi comes with its own theory of working and long-term memory, which is related to but different from the one given in the integrative diagramit views the multiple memory types distinguished in the integrative diagram as emergent from a common memory substrate. Psi comes with its own theory of perception and action, which seems broadly consistent with the deep learning approach incorporated in the integrative diagram. Psis handling of working memory lacks the detailed, explicit workow of LIDA, though it seems broadly conceptually consistent with LIDA. In Fig.7.3, the box labeled Other portions of working memory is labeled Protocol and situation memory in the original Psi diagram.",Engineering General  Intelligence Part 1,chapter 7
"The Perception, Action Execution and Action Selection boxes have fairly similar semantics to the similarly labeled boxes in the LIDA-like Fig.7.2, so that these diagrams may be viewed as overlapping. The LIDA model doesnt explain action selection and planning in as much detail as Psi, so the Psi-like Fig.7.3 could be viewed as an elaboration of the action-selection portion of the LIDA-like Fig.7.2. In Psi, reinforcement is considered as part of the learning process involved in action selection and planning; in Fig.7.3 an explicit reinforcement box has been added to the original Psi diagram, to emphasize this. Figure7.4, modeling long-term memory and deliberative processing, is derived from our own prior work studying the cognitive synergy between different      7.3 An Architecture Diagram for Human-Like General Intelligence 149 Fig. 7.",Engineering General  Intelligence Part 1,chapter 7
"4 Architecture of long-term memory and deliberative and metacognitive thinking cognitive processes associated with different types of memory. The division into types of memory is fairly standard. Declarative, procedural, episodic and sensorimotor memory are routinely distinguished; we like to distinguish attentional memory and intentional (goal) memory as well, and view these as the interface between longterm memory and the minds global control systems. One focus of our AGI design work has been on designing learning algorithms, corresponding to these various types of memory, that interact with each other in a synergetic way [Goe09c], helping each other to overcome their intrinsic combinatorial explosions. There is signicant evidence that these various types of long-term memory are differently implemented in the brain, but the degree of structure and dynamical commonality underlying these different implementations remains unclear. Each of these long-term memory types has its analogue in working memory as well.",Engineering General  Intelligence Part 1,chapter 7
"In some cognitive models, the working memory and long-term memory versions of a memory type and corresponding cognitive processes, are basically the same thing. CogPrime is mostly like thisit implements working memory as a subset of long-term memory consisting of items with particularly high importance values. The distinctive nature of working memory is enforced via using slightly different dynamical equations to update the importance values of items with importance above a certain threshold. On the other hand, many cognitive models treat working and long term memory as more distinct than this, and there is evidence for signicant functional and anatomical distinctness in the brain in some cases. So for the purpose of the integrative diagram, it seemed best to leave working and long-term memory subcomponents as parallel but distinguished. Figure7.",Engineering General  Intelligence Part 1,chapter 7
"4 also encompasses metacognition, under the hypothesis that in human beings and human-like minds, metacognitive thinking is carried out using basically the same processes as plain ordinary deliberative thinking, perhaps with various tweaks optimizing them for thinking about thinking. If it turns out that humans      150 7 A Generic Architecture of Human-Like Cognition Fig. 7.5 Architecture for multimodal perception have, say, a special kind of reasoning faculty exclusively for metacognition, then the diagram would need to be modied. Modeling of self and others is understood to occur via a combination of metacognition and deliberative thinking, as well as via implicit adaptation based on reactive processing. Figure7.5 models perception, according to the basic ideas of deep learning theory. Vision and audition are modeled as deep learning hierarchies, with bottom-up and top-down dynamics. The lower layers in each hierarchy refer to more localized patterns recognized in, and abstracted from, sensory data.",Engineering General  Intelligence Part 1,chapter 7
"Output from these hierarchies to the rest of the mind is not just through the top layers, but via some sort of sampling from various layers, with a bias toward the top layers. The different hierarchies cross-connect, and are hence to an extent dynamically coupled together. It is also recognized that there are some sensory modalities that arent strongly hierarchical, e.g touch and smell (the latter being better modeled as something like an asymmetric Hopeld net, prone to frequent chaotic dynamics [LLW+05])these may also crossconnect with each other and with the more hierarchical perceptual subnetworks. Of course the suggested architecture could include any number of sensory modalities; the diagram is restricted to four just for simplicity. The self-organized patterns in the upper layers of perceptual hierarchies may become quite complex and may develop advanced cognitive capabilities like episodic memory, reasoning, language learning, etc.",Engineering General  Intelligence Part 1,chapter 7
"A pure deep learning approach to intelligence argues that all the aspects of intelligence emerge from this kind of dynamics (among perceptual, action and reinforcement hierarchies). Our own view is that the heterogeneity of human brain architecture argues against this perspective, and that      7.3 An Architecture Diagram for Human-Like General Intelligence 151 Fig. 7.6 Architecture for action and reinforcement deep learning systems are probably better as models of perception and action than of general cognition. However, the integrative diagram is not committed to our perspective on thisa deep-learning theorist could accept the integrative diagram, but argue that all the other portions besides the perceptual, action and reinforcement hierarchies should be viewed as descriptions of phenomena that emerge in these hierarchies due to their interaction. Figure7.6 shows an action subsystem and a reinforcement subsystem, parallel to the perception subsystem.",Engineering General  Intelligence Part 1,chapter 7
"Two action hierarchies, one for an arm and one for a leg, are shown for concreteness, but of course the architecture is intended to be extended more broadly. In the hierarchy corresponding to an arm, for example, the lowest level would contain control patterns corresponding to individual joints, the next level up to groupings of joints (like ngers), the next level up to larger parts of the arm (hand, elbow). The different hierarchies corresponding to different body parts cross-link, enabling coordination among body parts; and they also connect at multiple levels to perception hierarchies, enabling sensorimotor coordination. Finally there is a module for motor planning, which links tightly with all the motor hierarchies, and also overlaps with the more cognitive, inferential planning activities of the mind, in a manner that is modeled different ways by different theorists. Albus [AM01] has elaborated this kind of hierarchy quite elaborately. The reward hierarchy in Fig.7.",Engineering General  Intelligence Part 1,chapter 7
"6 provides reinforcement to actions at various levels on the hierarchy, and includes dynamics for propagating information about reinforcement up and down the hierarchy. Figure7.7 deals with language, treating it as a special case of coupled perception and action. The traditional architecture of a computational language comprehension      152 7 A Generic Architecture of Human-Like Cognition Fig. 7.7 Architecture for language processing system is a pipeline [JM09] [Goe10c], which is equivalent to a hierarchy with the lowest-level linguistic features (e.g. sounds, words) at the bottom, and the highest level features (semantic abstractions) at the top, and syntactic features in the middle. Feedback connections enable semantic and cognitive modulation of lowerlevel linguistic processing. Similarly, language generation is commonly modeled hierarchically, with the top levels being the ideas needing verbalization, and the bottom level corresponding to the actual sentence produced.",Engineering General  Intelligence Part 1,chapter 7
"In generation the primary ow is top-down, with bottom-up ow providing modulation of abstract concepts by linguistic surface forms. So, thats itan integrative architecture diagram for human-like general intelligence, split among seven different pictures, formed by judiciously merging together architecture diagrams produced via a number of cognitive theorists with different, overlapping foci and research paradigms. Is anything critical left out of the diagram? A quick perusal of the table of contents of cognitive psychology textbooks suggests to me that if anything major is left out, its also unknown to current cognitive psychology. However, one could certainly make an argument for explicit inclusion of certain other aspects of intelligence, that in the integrative diagram are left as implicit emergent phenomena.",Engineering General  Intelligence Part 1,chapter 7
"For instance, creativity is obviously very important to intelligence, but, there is no creativity box in any of these diagramsbecause in our view, and the view of the cognitive theorists whose work weve directly drawn on here, creativity is best viewed as a process emergent from other processes that are explicitly included in the diagrams. 7.4 Interpretation and Application of the Integrative Diagram A tongue-partly-in-cheek denition of a biological pathway is a subnetwork of a biological network, that ts on a single journal page. Cognitive architecture diagrams have a similar propertythey are crude abstractions of complex structures and dynamics, sculpted in accordance with the size of the printed page, and the tol     7.4 Interpretation and Application of the Integrative Diagram 153 erance of the human eye for absorbing diagrams, and the tolerance of the human author for making diagrams.",Engineering General  Intelligence Part 1,chapter 7
"However, sometimes constraintseven arbitrary onesare useful for guiding creative efforts, due to the fact that they force choices. Creating an architecture for human-like general intelligence that ts in a few (okay, seven) fairly compact diagrams, requires one to make many choices about what features and relationships are most essential. In constructing the integrative diagram, we have sought to make these choices, not purely according to our own tastes in cognitive theory or AGI system design, but according to a sort of blend of the taste and judgment of a number of scientists whose views we respect, and who seem to have fairly compatible, complementary perspectives. What is the use of a cognitive architecture diagram like this? It can help to give newcomers to the eld a basic idea about what is known and suspected about the nature of human-like general intelligence. Also, it could potentially be used as a tool for cross-correlating different AGI architectures.",Engineering General  Intelligence Part 1,chapter 7
"If everyone who authored an AGI architecture would explain how their architecture accounts for each of the structures and processes identied in the integrative diagram, this would give a means of relating the various AGI designs to each other. The integrative diagram could also be used to help connect AGI and cognitive psychology to neuroscience in a more systematic way. In the case of LIDA, a fairly careful correspondence has been drawn up between the LIDA diagram nodes and links and various neural structures and processes [FB08]. Similar knowledge exists for the rest of the integrative diagram, though not organized in such a systematic fashion. A systematic curation of links between the nodes and links in the integrative diagram and current neuroscience knowledge, would constitute an interesting rst approximation of the holistic cognitive behavior of the human brain. Finally (and harking forward to later chapters), the big omission in the integrative diagram is dynamics.",Engineering General  Intelligence Part 1,chapter 7
"Structure alone will only get you so far, and you could build an AGI system with reasonable-looking things in each of the integrative diagrams boxes, interrelating according to the given arrows, and yet still fail to make a viable AGI system. Given the limitations the real world places on computing resources, its not enough to have adequate representations and algorithms in all the boxes, communicating together properly and capable doing the right things given sufcient resources. Rather, one needs to have all the boxes lled in properly with structures and processes that, when they act together using feasible computing resources, will yield appropriately intelligent behaviors via their cooperative activity. And this has to do with the complex interactive dynamics of all the processes in all the different boxeswhich is something the integrative diagram doesnt touch at all. This brings us again to the network of ideas weve discussed under the name of cognitive synergy, to be discussed later on.",Engineering General  Intelligence Part 1,chapter 7
"It might be possible to make something similar to the integrative diagram on the level of dynamics rather than structures, complementing the structural integrative diagram given here; but this would seem signicantly more challenging, because we lack a standard set of tools for depicting system dynamics. Most cognitive theorists and AGI architects describe their structural ideas using boxes-and-lines diagrams of      154 7 A Generic Architecture of Human-Like Cognition some sort, but there is no standard method for depicting complex system dynamics. So to make a dynamical analogue to the integrative diagram, via a similar integrative methodology, one would rst need to create appropriate diagrammatic formalizations of the dynamics of the various cognitive theories being integrateda fascinating but onerous task. When we rst set out to make an integrated cognitive architecture diagram, via combining the complementary insights of various cognitive science and AGI theorists, we werent sure how well it would work.",Engineering General  Intelligence Part 1,chapter 7
"But now we feel the experiment was generally a success the resultant integrated architecture seems sensible and coherent, and reasonably complete. It doesnt come close to telling you everything you need to know to understand or implement a human-like mindbut it tells you the various processes and structures you need to deal with, and which of their interrelations are most critical. And, perhaps just as importantly, it gives a concrete way of understanding the insights of a specic but fairly diverse set of cognitive science and AGI theorists as complementary rather than contradictory. In a CogPrime context, it provides a way of tying in the specic structures and dynamics involved in CogPrime, with a more generic portrayal of the structures and dynamics of humanlike intelligence.",Engineering General  Intelligence Part 1,chapter 7
     Part III Toward a General Theory of General Intelligence   ,Engineering General  Intelligence Part 1,chapter 7
"  Chapter 8 A Formal Model of Intelligent Agents 8.1 Introduction The articial intelligence eld is full of sophisticated mathematical models and equations, but most of these are highly specialized in naturee.g. formalizations of particular logic systems, analyzes of the dynamics of specic sorts of neural nets, etc. On the other hand, a number of highly general models of intelligent systems also exist, including Hutters recent formalization of universal intelligence [Hut05] and a large body of work in the disciplines of systems science and cyberneticsbut these have tended not to yield many specic lessons useful for engineering AGI systems, serving more as conceptual models in mathematical form. It would be fantastic to have a mathematical theory bridging these extremesa real general theory of general intelligence, allowing the derivation and analysis of specic structures and processes playing a role in practical AGI systems, from broad mathematical models of general intelligence in various situations and under various constraints.",Engineering General  Intelligence Part 1,chapter 8
"However, the path to such a theory is not entirely clear at present; and, as valuable as such a theory would be, we dont believe such a thing to be necessary for creating advanced AGI. One possibility is that the development of such a theory will occur contemporaneously and synergetically with the advent of practical AGI technology. Lacking a mature, pragmatically useful general theory of general intelligence, however, we have still found it valuable to articulate certain theoretical ideas about the nature of general intelligence, with a level of rigor a bit greater than the wholly informal discussions of the previous chapters. The chapters in this section of the book articulate some ideas we have developed in pursuit of a general theory of general intelligence; ideas that, even in their current relatively undeveloped form, have been very helpful in guiding our concrete work on the CogPrime design.",Engineering General  Intelligence Part 1,chapter 8
"This chapter presents a more formal version of the notion of intelligence as achieving complex goals in complex environments, based on a formal model of intelligent agents. These formalizations of agents and intelligence will be used in later chapters as a foundation for formalizing other concepts like inference and cognitive B. Goertzel et al., Engineering General Intelligence, Part 1, 157 Atlantis Thinking Machines 5, DOI: 10.2991/978-94-6239-027-0_8,  Atlantis Press and the authors 2014      158 8 A Formal Model of Intelligent Agents synergy. Chapters9 and 10 pursue the notion of cognitive synergy a little more thoroughly than was done in previous chapters.",Engineering General  Intelligence Part 1,chapter 8
"Chapter11 sketches a general theory of general intelligence using tools from category theorynot bringing it to the level where one can use it to derive specic AGI algorithms and structures; but still, presenting ideas that will be helpful in interpreting and explaining specic aspects of the CogPrime design in Part 2 Finally, Appendix A explores an additional theoretical direction, in which the mind of an intelligent system is viewed in terms of certain curved spacesa novel way of thinking about the dynamics of general intelligence, which has been useful in guiding development of the ECAN component of CogPrime, and we expect will have more general value in future. Despite the intermittent use of mathematical formalism, the ideas presented in this section are fairly speculative, and we do not propose them as constituting a well-demonstrated theory of general intelligence.",Engineering General  Intelligence Part 1,chapter 8
"Rather, we propose them as an interesting way of thinking about general intelligence, which appears to be consistent with available data, and which has proved inspirational to us in conceiving concrete structures and dynamics for AGI, as manifested for example in the CogPrime design. Understanding the way of thinking described in these chapters is valuable for understanding why the CogPrime design is the way it is, and for relating CogPrime to other practical and intellectual systems, and extending and improving CogPrime. 8.2 A Simple Formal Agents Model (SRAM) We now present a formalization of the concept of intelligent agentsbeginning with a formalization of agents in general. Drawing on [Hut05, LH07a], we consider a class of active agents which observe and explore their environment and also take actions in it, which may affect the environment.",Engineering General  Intelligence Part 1,chapter 8
"Formally, the agent sends information to the environment by sending symbols from some nite alphabet called the action space ; and the environment sends signals to the agent with symbols from an alphabet called the perception space, denoted P. Agents can also experience rewards, which lie in the reward space, denoted R, which for each agent is a subset of the rational unit interval. The agent and environment are understood to take turns sending signals back and forth, yielding a history of actions, observations and rewards, which may be denoted a1o1r1a2o2r2... or else a1x1a2x2... if x is introduced as a single symbol to denote both an observation and a reward. The complete interaction history up to and including cycle t is denoted ax1:t; and the history before cycle t is denoted ax<t = ax1:t1.      8.",Engineering General  Intelligence Part 1,chapter 8
"2 A Simple Formal Agents Model (SRAM) 159 The agent is represented as a function  which takes the current history as input, and produces an action as output. Agents need not be deterministic, an agent may for instance induce a probability distribution over the space of possible actions, conditioned on the current history. In this case we may characterize the agent by a probability distribution (at|ax<t). Similarly, the environment may be characterized by a probability distribution (xk|ax<kak). Taken together, the distributions  and  dene a probability measure over the space of interaction sequences. Next, we extend this model in a few ways, intended to make it better reect the realities of intelligent computational agents. The rst modication is to allow agents to maintain memories (of nite size), via adding memory actions drawn from a set M into the history of actions, observations and rewards. The second modication is to introduce the notion of goals. 8.2.",Engineering General  Intelligence Part 1,chapter 8
"1 Goals We dene goals as mathematical functions (to be specied below) associated with symbols drawn from the alphabet G; and we consider the environment as sending goal-symbols to the agent along with regular observation-symbols. (Note however that the presentation of a goal-symbol to an agent does not necessarily entail the explicit communication to the agent of the contents of the goal function. This must be provided by other, correlated observations.) We also introduce a conditional distribution (g, ) that gives the weight of a goal g in the context of a particular environment . In this extended framework, an interaction sequence looks like a1o1g1r1a2o2g2r2... or else a1y1a2y2... where gi are symbols corresponding to goals, and y is introduced as a single symbol to denote the combination of an observation, a reward and a goal.",Engineering General  Intelligence Part 1,chapter 8
"Each goal function maps each nite interaction sequence Ig,s,t = ays:t with gs to gt corresponding to g, into a value rg(Ig,s,t)  [0, 1] indicating the value or raw reward of achieving the goal during that interaction sequence. The total reward rt obtained by the agent is the sum of the raw rewards obtained at time t from all goals whose symbols occur in the agents history before t. This formalism of goal-seeking agents allows us to formalize the notion of intelligence as achieving complex goals in complex environmentsa direction that is pursued in Sect.8.3. Note that this is an external perspective of system goals, which is natural from the perspective of formally dening system intelligence in terms of system behavior, but is not necessarily very natural in terms of system design.",Engineering General  Intelligence Part 1,chapter 8
"From the point of      160 8 A Formal Model of Intelligent Agents view of AGI design, one is generally more concerned with the (implicit or explicit) representation of goals inside an AGI system, as in CogPrimes Goal Atoms to be reviewed in Chap.4 of Part 2 Further, it is important to also consider the case where an AGI system has no explicit goals, and the systems environment has no immediately identiable goals either. But in this case, we dont see any clear way to dene a systems intelligence, except via approximating the system in terms of other theoretical systems which do have explicit goals. This approximation approach is developed in Sect.8.3.5. The awkwardness of linking the general formalism of intelligence theory presented here, with the practical business of creating and designing AGI systems, may indicate a shortcoming on the part of contemporary intelligence theory or AGI designs.",Engineering General  Intelligence Part 1,chapter 8
"On the other hand, this sort of situation often occurs in other domains as welle.g. the leap from quantum theory to the analysis of real-world systems like organic molecules involves a lot of awkwardness and large leaps a well. 8.2.2 Memory Stores As well as goals, we introduce into the model a long-term memory and a workspace. Regarding long-term memory we assume the agents memory consists of multiple memory stores corresponding to various types of memory, e.g.: procedural (KProc), declarative (KDec), episodic (KEp), attentional (KAtt) and Intentional (KInt).",Engineering General  Intelligence Part 1,chapter 8
"In Appendix B a category-theoretic model of these memory stores is introduced; but for the moment, we need only assume the existence of  an injective mapping Ep: KEp  H where H is the space of fuzzy sets of subhistories (subhistories being episodes in this formalism)  an injective mapping Proc: KProc  M  W  A, where M is the set of memory states, W is the set of (observation, goal, reward) triples, and A is the set of actions (this maps each procedure object into a function that enacts actions in the environment or memory, based on the memory state and current world-state)  an injective mapping Dec: KDec  L, where L is the set of expressions in some formal language (which may for example be a logical language), which possesses words corresponding to the observations, goals, reward values and actions in our agent formalism  an injective mapping Int: KInt  G",Engineering General  Intelligence Part 1,chapter 8
", where G is the space of goals mentioned above  an injective mapping Att: KInt  KEp  KProc  KEc  V, where V is the space of attention values (structures that gauge the importance of paying attention to an item of knowledge over various time-scales or in various contexts). We also assume that the vocabulary of actions contains memory-actions corresponding to the operations of inserting the current observation, goal, reward or action into the episodic and/or declarative memory store. And, we assume that the activity of the agent, at each time-step, includes the enaction of one or more of the procedures      8.2 A Simple Formal Agents Model (SRAM) 161 in the procedural memory store. If several procedures are enacted at once, then the end result is still formally modeled as a single action a = a[1]      a[k] where  is an operator on action-space that composes multiple actions into a single one.",Engineering General  Intelligence Part 1,chapter 8
"Finally, we assume that, at each time-step, the agent may carry out an external action ai on the environment, a memory action mi on the (long-term) memory, and an action bi on its internal workspace. Among the actions that can be carried out on the workspace, are the ability to insert or delete observations, goals, actions or rewardvalues from the workspace. The workspace can be thought of as a sort of short-term memory or else in terms of Baars global workspace concept mentioned above. The workspace provides a medium for interaction between the different memory types. The workspace provides a mechanism by which declarative, episodic and procedural memory may interact with each other. For this mechanism to work, we must assume that there are actions corresponding to query operations that allow procedures to look into declarative and episodic memory.",Engineering General  Intelligence Part 1,chapter 8
"The nature of these query operations will vary among different agents, but we can assume that in general an agent has  one or more procedures QDec(x) serving as declarative queries, meaning that when QDec is enacted on some x that is an ordered set of items in the workspace, the result is that one or more items from declarative memory is entered into the workspace  one or more procedures QEp(x) serving as episodic queries, meaning that when QEp is enacted on some x that is an ordered set of items in the workspace, the result is that one or more items from episodic memory is entered into the workspace. One additional aspect of CogPrimes knowledge representation that is important to PLN is the attachment of nonnegative weights ni corresponding to elementary observations oi. These weights denote the amount of evidence contained in the observation.",Engineering General  Intelligence Part 1,chapter 8
"For instance, in the context of a robotic agent, one could use these values to encode the assumption that an elementary visual observation has more evidential value than an elementary olfactory observation. We now have a model of an agent with long-term memory comprising procedural, declarative and episodic aspects, an internal cognitive workspace, and the capability to use procedures to drive actions based on items in memory and the workspace, and to move items between long-term memory and the workspace. 8.2.2.1 Modeling CogPrime Of course, this formal model may be realized differently in various real-world AGI systems.",Engineering General  Intelligence Part 1,chapter 8
"In CogPrime we have  a weighted, labeled hypergraph structure called the AtomSpace used to store declarative knowledge (this is the representation used by PLN)  a collection of programs in a LISP-like language called Combo, stored in a Procedure Repository data structure, used to store procedural knowledge  a collection of partial movies of the systems experience, played back using an internal simulation engine, used to store episodic knowledge      162 8 A Formal Model of Intelligent Agents  AttentionValue objects, minimally containing ShortTermImportance (STI) and LongTermImportance (LTI) values used to store attentional knowledge  Goal Atoms for intentional knowledge, stored in the same format as declarative knowledge but whose dynamics involve a special form of articial currency that is used to govern action selection. The AtomSpace is the central repository and procedures and episodes are linked to Atoms in the AtomSpace which serve as their symbolic representatives.",Engineering General  Intelligence Part 1,chapter 8
"The workspace in CogPrime exists only virtually: each item in the AtomSpace has a short term importance (STI) level, and the workspace consists of those items in the AtomSpace with highest STI, and those procedures and episodes whose symbolic representatives in the AtomSpace have highest STI. On the other hand, as we saw above, the LIDA architecture uses separate representations for procedural, declarative and episodic memory, but also has an explicit workspace component, where the most currently contextually relevant items from all different types of memory are gathered and used together in the course of actions. However, compared to CogPrime, it lacks comparably ne-grained methods for integrating the different types of memory. Systematically mapping various existing cognitive architectures, or human brain structure, into this formal agents model would be a substantial though quite plausible exercise; but we will not undertake this here. 8.2.",Engineering General  Intelligence Part 1,chapter 8
"3 The Cognitive Schematic Next we introduce an additional specialization into SRAM: the cognitive schematic, written informally as Context & Procedure  Goal and considered more formally as holds(C) & ex(P)  hi where h may be an externally specied goal gi or an internally specied goal h derived as a (possibly uncertain) subgoal of one of more gi; C is a piece of declarative or episodic knowledge and P is a procedure that the agent can internally execute to generate a series of actions. ex(P) is the proposition that P is successfully executed. If C is episodic then holds(C) may be interpreted as the current context (i.e. some nite slice of the agents history) being similar to C; if C is declarative then holds(C) may be interpreted as the truth value of C evaluated at the current context.",Engineering General  Intelligence Part 1,chapter 8
"Note that C may refer to some part of the world quite distant from the agents current sensory observations; but it may still be formally evaluated based on the agents history. In the standard CogPrime notation as introduced formally in Chap.2 of Part 2 (where indentation has function-argument syntax similar to that in Python, and relationship types are prepended to their relata without parentheses), for the case C is declarative this would be written as      8.2 A Simple Formal Agents Model (SRAM) 163 PredictiveExtensionalImplication AND C Execution P G and in the case C is episodic one replaces C in this formula with a predicate expressing Cs similarity to the current context. The semantics of the PredictiveExtensionalInheritance relation will be discussed below. The Execution relation simply denotes the proposition that procedure P has been executed.",Engineering General  Intelligence Part 1,chapter 8
"For the class of SRAM agents who (like CogPrime) use the cognitive schematic to govern many or all of their actions, a signicant fragment of agent intelligence boils down to estimating the truth values of PredictiveExtensionalImplication relationships. Action selection procedures can be used, which choose procedures to enact based on which ones are judged most likely to achieve the current external goals gi in the current context. Rather than enter into the particularities of action selection or other cognitive architecture issues, we will restrict ourselves to PLN inference, which in the context of the present agent model is a method for handling PredictiveImplication in the cognitive schematic. Consider an agent in a virtual world, such as a virtual dog, one of whose external goals is to please its owner. Suppose its owner has asked it to nd a cat, and it can translate this into a subgoal nd cat.",Engineering General  Intelligence Part 1,chapter 8
"If the agent operates according to the cognitive schematic, it will search for P so that PredictiveExtensionalImplication AND C Execution P Evaluation found cat holds. 8.3 Toward a Formal Characterization of Real-World General Intelligence Having dened what we mean by an agent acting in an environment, we now turn to the question of what it means for such an agent to be intelligent. As we have reviewed extensively in Chap.4, intelligence is a commonsense, folk psychology concept, with all the imprecision and contextuality that this generally entails. One cannot expect any compact, elegant formalism to capture all of its meanings. Even in the psychology and AI research communities, divergent denitions abound; Legg and Hutter [LH07a] lists and organizes 70+ denitions from the literature.",Engineering General  Intelligence Part 1,chapter 8
"     164 8 A Formal Model of Intelligent Agents Practical study of natural intelligence in humans and other organisms, and practical design, creation and instruction of articial intelligences, can proceed perfectly well without an agreed-upon formalization of the intelligence concept. Some researchers may conceive their own formalisms to guide their own work, others may feel no need for any such thing. But nevertheless, it is of interest to seek formalizations of the concept of intelligence, which capture useful fragments of the commonsense notion of intelligence, and provide guidance for practical research in cognitive science and AI. A number of such formalizations have been given in recent decades, with varying degrees of mathematical rigor. Perhaps the most carefully-wrought formalization of intelligence so far is the theory of universal intelligence presented by Shane Legg and Marcus Hutter in [LH07b], which draws on ideas from algorithmic information theory.",Engineering General  Intelligence Part 1,chapter 8
"Universal intelligence captures a certain aspect of the intelligence concept very well, and has the advantage of connecting closely with ideas in learning theory, decision theory and computation theory. However, the kind of general intelligence it captures best, is a kind which is in a sense more general in scope than humanstyle general intelligence. Universal intelligence does capture the sense in which humans are more intelligent than worms, which are more intelligent than rocks; and the sense in which theoretical AGI systems like Hutters AIXI or AIXItl [Hut05] would be much more intelligent than humans. But it misses essential aspects of the intelligence concept as it is used in the context of intelligent natural systems like humans or real-world AI systems. Our main goal in this section is to present variants of universal intelligence that better capture the notion of intelligence as it is typically understood in the context of real-world natural and articial systems.",Engineering General  Intelligence Part 1,chapter 8
"The rst variant we describe is pragmatic general intelligence, which is inspired by the intuitive notion of intelligence as the ability to achieve complex goals in complex environments, given in [Goe93]. After assuming a prior distribution over the space of possible environments, and one over the space of possible goals, one then denes the pragmatic general intelligence as the expected level of goal-achievement of a system relative to these distributions. Rather than measuring truly broad mathematical general intelligence, pragmatic general intelligence measures intelligence in a way thats specically biased toward certain environments and goals. Another variant denition is then presented, the efcient pragmatic general intelligence , which takes into account the amount of computational resources utilized by the system in achieving its intelligence. Some argue that making efcient use of available resources is a dening characteristic of intelligence, see e.g. [Wan06].",Engineering General  Intelligence Part 1,chapter 8
"A critical question left open is the characterization of the prior distributions corresponding to everyday human reality; we give a semi-formal sketch of some ideas on this in Chap.10, where we present the notion of a communication prior, which assigns a probability weight to a situation S based on the ease with which one agent in a society can communicate S to another agent in that society, using multimodal communication(includingverbalization,demonstration,dramaticandpictorialdepiction, etc.).      8.3 Toward a Formal Characterization of Real-World General Intelligence 165 Finally, we present a formal measure of the generality of an intelligence, which precisiates the informal distinction between general AI and narrow AI. 8.3.1 Biased Universal Intelligence To dene universal intelligence, Legg and Hutter consider the class of environments that are reward-summable, meaning that the total amount of reward they return to any agent is bounded by 1.",Engineering General  Intelligence Part 1,chapter 8
"Where ri denotes the reward experienced by the agent from the environment at time i, the expected total reward for the agent  from the environment  is dened as V   E(   1 ri)  1. To extend their denition in the direction of greater realism, we rst introduce a second-order probability distribution , which is a probability distribution over the space of environments . The distribution  assigns each environment a probability. One such distribution  is the Solomonoff-Levin universal distribution in which one sets  = 2K(); but this is not the only distribution  of interest. In fact a great deal of real-world general intelligence consists of the adaptation of intelligent systems to particular distributions  over environment-space, differing from the universal distribution.",Engineering General  Intelligence Part 1,chapter 8
"We then dene Denition 4 The biased universal intelligence of an agent  is its expected performance with respect to the distribution  over the space of all computable rewardsummable environments, E, that is,  ()   E ()V  . Legg and Hutters universal intelligence is obtained by setting  equal to the universal distribution. This framework is more exible than it might seem. E.g. suppose one wants to incorporate agents that die. Then one may create a special action, say a666, corresponding to the state of death, to create agents that  in certain circumstances output action a666  have the property that if their previous action was a666, then all of their subsequent actions must be a666 and to dene a reward structure so that actions a666 always bring zero reward. It then follows that death is generally a bad thing if one wants to maximize intelligence.",Engineering General  Intelligence Part 1,chapter 8
"Agents that die will not get rewarded after theyre dead; and agents that live only 70      166 8 A Formal Model of Intelligent Agents years, say, will be restricted from getting rewards involving long-term patterns and will hence have specic limits on their intelligence. 8.3.2 Connecting Legg and Hutters Model of Intelligent Agents to the Real World A notable aspect of the Legg and Hutter formalism is the separation of the reward mechanism from the cognitive mechanisms of the agent. While commonplace in the reinforcement learning literature, this seems psychologically unrealistic in the context of biological intelligences and many types of machine intelligences. Not all human intelligent activity is specically reward-seeking in nature; and even when it is, humans often pursue complexly constructed rewards, that are dened in terms of their own cognitions rather than separately given.",Engineering General  Intelligence Part 1,chapter 8
"Suppose a certain humans goals are true love, or world peace, and the proving of interesting theoremsthen these goals are dened by the human herself, and only she knows if shes achieved them. An externally-provided reward signal doesnt capture the nature of this kind of goalseeking behavior, which characterizes much human goal-seeking activity (and will presumably characterize much of the goal-seeking activity of advanced engineered intelligences also) ... let alone human behavior that is spontaneous and unrelated to explicit goals, yet may still appear commonsensically intelligent.",Engineering General  Intelligence Part 1,chapter 8
"One could seek to bypass this complaint about the reward mechanisms via a sort of neo-Freudian argument, via  associating the reward signal, not with the external environment as typically conceived, but rather with a portion of the intelligent agents brain that is separate from the cognitive component  viewingcomplexgoalsliketruelove,worldpeaceandprovinginterestingtheorems as indirect ways of achieving the agents basic goals, created within the agents memory via subgoaling mechanisms but it seems to us that a general formalization of intelligence should not rely on such strong assumptions about agents cognitive architectures. So below, after introducing the pragmatic and efcient pragmatic general intelligence measures, we will propose an alternate interpretation wherein the mechanism of external rewards is viewed as a theoretical test framework for assessing agent intelligence, rather than a hypothesis about intelligent agent architecture.",Engineering General  Intelligence Part 1,chapter 8
"In this alternate interpretation, formal measures like the universal, pragmatic and efcient pragmatic general intelligence are viewed as not directly applicable to realworld intelligences, because they involve the behaviors of agents over a wide variety of goals and environments, whereas in real life the opportunities to observe agents are more limited. However, they are viewed as being indirectly applicable to real-world agents, in the sense that an external intelligence can observe an agents real-world behavior and then infer its likely intelligence according to these measures.      8.3 Toward a Formal Characterization of Real-World General Intelligence 167 In a sense, this interpretation makes our formalized measures of intelligence the opposite of real-world IQ tests. An IQ test is a quantied, formalized test which is designed to approximately predict the informal, qualitative achievement of humans in real life.",Engineering General  Intelligence Part 1,chapter 8
"On the other hand, the formal denitions of intelligence we present here are quantied, formalized tests that are designed to capture abstract notions of intelligence, but which can be approximately evaluated on a real-world intelligent system by observing what it does in real life. 8.3.3 Pragmatic General Intelligence The above concept of biased universal intelligence is perfectly adequate for many purposes, but it is also interesting to explicitly introduce the notion of a goal into the calculation. This allows us to formally capture the notion presented in [Goe93] of intelligence as the ability to achieve complex goals in complex environments. If the agent is acting in environment , and is provided with gs corresponding to g at the start and the end of the time-interval T = {i  (s, . . .",Engineering General  Intelligence Part 1,chapter 8
", t)}, then the expected goal-achievement of the agent, relative to g, during the interval is the expectation V ,g,T  E  t i=s rg(Ig,s,i)  where the expectation is taken over all interaction sequences Ig,s,i drawn according to . We then propose Denition 5 The pragmatic general intelligence of an agent , relative to the distribution  over environments and the distribution  over goals, is its expected performance with respect to goals drawn from  in environments drawn from , over the time-scales natural to the goals; that is, ()   E,gG,T ()(g, )V ,g,T (in those cases where this sum is convergent). This denition formally captures the notion that intelligence is achieving complex goals in complex environments, where complexity is gauged by the assumed measures  and .",Engineering General  Intelligence Part 1,chapter 8
"If  is taken to be the universal distribution, and  is dened to weight goals according to the universal distribution, then pragmatic general intelligence reduces to universal intelligence. Furthermore, it is clear that a universal algorithmic agent like AIXI [Hut05] would also have a high pragmatic general intelligence, under fairly broad conditions. As the interaction history grows longer, the pragmatic general intelligence of AIXI      168 8 A Formal Model of Intelligent Agents would approach the theoretical maximum; as AIXI would implicitly infer the relevant distributions via experience. However, if signicant reward discounting is involved, so that near-term rewards are weighted much higher than long-term rewards, then AIXI might compare very unfavorably in pragmatic general intelligence, to other agents designed with prior knowledge of ,  and  in mind.",Engineering General  Intelligence Part 1,chapter 8
"The most interesting case to consider is where  and  are taken to embody some particular bias in a real-world space of environments and goals, and this bias is appropriately reected in the internal structure of an intelligent agent. Note that an agent needs not lack universal intelligence in order to possess pragmatic general intelligence with respect to some non-universal distribution over goals and environments. However, in general, given limited resources, there may be a tradeoff between universal intelligence and pragmatic intelligence. Which leads to the next point: how to encompass resource limitations into the denition. One might argue that the denition of Pragmatic General Intelligence is already encompassed by Legg and Hutters denition because one may bias the distribution of environments within the latter by considering different Turing machines underlying the Kolmogorov complexity. However this is not a general equivalence because the Solomonoff-Levin measure intrinsically decays exponentially, whereas an assumptive distribution over environments might decay at some other rate.",Engineering General  Intelligence Part 1,chapter 8
"This issue seems to merit further mathematical investigation. 8.3.4 Incorporating Computational Cost Let ,,g,T be a probability distribution describing the amount of computational resources consumed by an agent  while achieving goal g over time-scale T. This is a probability distribution because we want to account for the possibility of nondeterministic agents. So, ,,g,T(Q) tells the probability that Q units of resources are consumed. For simplicity we amalgamate space and time resources, energetic resources, etc. into a single number Q, which is assumed to live in some subset of the positive reals. Space resources of course have to do with the size of the systems memory.",Engineering General  Intelligence Part 1,chapter 8
"Then we may dene Denition 6 The efcient pragmatic general intelligence of an agent  with resource consumption ,,g,T, relative to the distribution  over environments and the distribution  over goals, is its expected performance with respect to goals drawn from  in environments drawn from , over the time-scales natural to the goals, normalized by the amount of computational effort expended to achieve each goal; that is, Eff()   E,gG,Q,T ()(g, ),,g,T(Q) Q V ,g,T (in those cases where this sum is convergent).      8.3 Toward a Formal Characterization of Real-World General Intelligence 169 This is a measure that rates an agents intelligence higher if it uses fewer computational resources to do its business. Roughly, it measures reward achieved per spacetime computation unit. Note that, by abandoning the universal prior, we have also abandoned the proof of convergence that comes with it.",Engineering General  Intelligence Part 1,chapter 8
"In general the sums in the above denitions need not converge; and exploration of the conditions under which they do converge is a complex matter. 8.3.5 Assessing the Intelligence of Real-World Agents The pragmatic and efcient pragmatic general intelligence measures are more realistic than the Legg and Hutter universal intelligence measure, in that they take into account the innate biasing and computational resource restrictions that characterize real-world intelligence. But as discussed earlier, they still live in fantasy-land to an extentthey gauge the intelligence of an agent via a weighted average over a wide variety of goals and environments; and they presume a simplistic relationship between agents and rewards that does not reect the complexities of real-world cognitive architectures. It is not obvious from the foregoing how to apply these measures to real-world intelligent systems, which lack the ability to exist in such a wide variety of environments within their often brief lifespan, and mostly go about their lives doing things other than pursuing quantied external rewards.",Engineering General  Intelligence Part 1,chapter 8
"In this brief section we describe an approach to bridging this gap. The treatment is left semi-formal in places. We suggest to view the denitions of pragmatic and efcient pragmatic general intelligence in terms of a possible worlds semanticsi.e. to view them as asking, counterfactually, how an agent would perform, hypothetically, on a series of tests (the tests being goals, dened in relation to environments and reward signals). Real-world intelligent agents dont normally operate in terms of explicit goals and rewards; these are abstractions that we use to think about intelligent agents.",Engineering General  Intelligence Part 1,chapter 8
"However, this is no objection to characterizing various sorts of intelligence in terms of counterfactuals like: how would system S operate if it were trying to achieve this or that goal, in this or that environment, in order to seek reward? We can characterize various sorts of intelligence in terms of how it can be inferred an agent would perform on certain tests, even though the agents real life does not consist of taking these tests. This conceptual approach may seem a bit articial but we dont currently see a better alternative, if one wishes to quantitatively gauge intelligence (which is, in a sense, an articial thing to do in the rst place). Given a real-world agent X and a mandate to assess its intelligence, the obvious alternative to looking at possible worlds in the manner of the above denitions, is just looking directly at the properties of the things X has achieved in the real world during its lifespan.",Engineering General  Intelligence Part 1,chapter 8
"But this isnt an easy solution, because it doesnt disambiguate which aspects of Xs achievements were due to its own actions versus due to the rest of the world that X was interacting with when it made its achievements. To distinguish the amount of achievement that      170 8 A Formal Model of Intelligent Agents X caused via its own actions requires a model of causality, which is a complex can of worms in itself; and, critically, the standard models of causality also involve counterfactuals (asking what would have been achieved in this situation if the agent X hadnt been there, etc.) [MW07]. Regardless of the particulars, it seems difcult to avoid counterfactual realities in assessing intelligence. The approach we suggestgiven a real-world agent X with a history of actions in a particular world, and a mandate to assess its intelligenceis to introduce an additional player, an inference agent , into the picture.",Engineering General  Intelligence Part 1,chapter 8
"The agent  modeled above is then viewed as X: the model of X that  constructs, in order to explore Xs inferred behaviors in various counterfactual environments. In the test situations embodied in the denitions of pragmatic and efcient pragmatic general intelligence, the environment gives X rewards, based on specically congured goals. In Xs real life, the relation between goals, rewards and actions will generally be signicantly subtler and perhaps quite different. We model the real world similarly to the fantasy world of the previous section, but with the omission of goals and rewards. We dene a naturalistic context as one in which all goals and rewards are constant, i.e. gi = g0 and ri = r0 for all i. This is just a mathematical convention for stating that there are no precisely-dened external goals and rewards for the agent.",Engineering General  Intelligence Part 1,chapter 8
"In a naturalistic context, we then have a situation where agents create actions based on the past history of actions and perceptions, and if there is any relevant notion of reward or goal, it is within the cognitive mechanism of some agent. A naturalistic agent X is then an agent  which is restricted to one particular naturalistic context, involving one particular environment  (formally, we may achieve this within the framework of agents described above via dictating that X issues constant null actions a0 in all environments except ). Next,wepositametricspace(, d)ofnaturalisticagentsdenedonanaturalistic context involving environment , and a subspace    of inference agents, which are naturalistic agents that output predictions of other agents behaviors (a notion we will not fully formalize here). If agents are represented as program trees, then d may be taken as edit distance on tree space [Bil05].",Engineering General  Intelligence Part 1,chapter 8
"Then, for each agent   , we may assess  the prior probability () according to some assumed distribution   the effectiveness p(, X) of  at predicting the actions of an agent X  . We may then dene Denition 7 The inference ability of the agent , relative to  and X, is q,X() = ()  Y sim(X, Y)p(, Y)  Y sim(X, Y) where sim is a specied decreasing function of d(X, Y),such as sim(X, Y) = 1 1+d(X,Y).      8.3 Toward a Formal Characterization of Real-World General Intelligence 171 To construct X, we may then use the model of X created by the agent    with the highest inference ability relative to  and X (using some specied ordering, in case of a tie). Having constructed X, we can then say that.",Engineering General  Intelligence Part 1,chapter 8
"Denition 8 The inferred pragmatic general intelligence (relativeto  and ) of a naturalistic agent X dened relative to an environment ,is dened as the pragmatic general intelligence of the model X of Xproduced by the agent    with maximal inference ability relative to (and in the case of a tie, the rst of these in the ordering dened over ). The inferred efcient pragmatic general intelligence of X relative to is dened similarly. This provides a precise characterization of the pragmatic and efcient pragmatic intelligence of real-world systems, based on their observed behaviors. Its a bit messy; but the real world tends to be like that. 8.",Engineering General  Intelligence Part 1,chapter 8
"4 Intellectual Breadth: Quantifying the Generality of an Agents Intelligence We turn now to a related question: How can one quantify the degree of generality that an intelligent agent possesses? Above we have discussed the qualitative distinction between AGI and Narrow AI, and intelligence as we have formalized it above is specically intended as a measure of general intelligence. But quantifying intelligence is different than quantifying generality versus narrowness. To make the discussion simpler, we introduce the term context as a shorthand for environment/interval triple (, g, T). Given a context (, g, T), and a set  of agents, one may construct a fuzzy set Ag,g,T gathering those agents that are intelligent relative to the context; and given a set of contexts, one may also dene a fuzzy set Con gathering those contexts with respect to which a given agent  is intelligent.",Engineering General  Intelligence Part 1,chapter 8
"The relevant formulas are: Ag,g,T () = Con(, g, T) = 1 N  Q ,g,T(Q)V ,g,T Q where N = N(, g, T) is a normalization factor dened appropriately, e.g. via N(, g, T) = max  V ,g,T. One could make similar denitions leaving out the computational cost factor Q, but we suspect that incorporating Q is a more promising direction. We then propose Denition 9 The intellectual breadth of an agent , relative to the distribution  over environments and the distribution  over goals, is H(P Con(, g, T))      172 8 A Formal Model of Intelligent Agents where His the entropy and P Con(, g, T) = ()(g, )Con(, g, T)  (,g.",Engineering General  Intelligence Part 1,chapter 8
") ()(g, )Con(, g, T) is the probability distribution formed by normalizing the fuzzy set Con(, g, T). A similar denition of the intellectual breadth of a context(, g, T), relative to the distribution  over agents, may be posited. A weakness of these denitions is that they dont try to account for dependencies between agents or contexts; perhaps more renedformulationsmaybedevelopedthataccountexplicitlyforthesedependencies. Note that the intellectual breadth of an agent as dened here is largely independent of the (efcient or not) pragmatic general intelligence of that agent. One could have a rather (efciently or not) pragmatically generally intelligent system with little breadth: this would be a system very good at solving a fair number of hard problems, yet wholly incompetent on a larger number of hard problems.",Engineering General  Intelligence Part 1,chapter 8
"On the other hand, one could also have a terribly (efciently or not) pragmatically generally stupid system with great intellectual breadth: i.e a system roughly equally dumb in all contexts! Thus, one can characterize an intelligent agent as narrow with respect to distribution  over environments and the distribution  over goals, based on evaluating it as having low intellectual breadth. A narrow AI relative to  and  would then be an AI agent with a relatively high efcient pragmatic general intelligence but a relatively low intellectual breadth. 8.5 Conclusion Our main goal in this chapter has been to push the formal understanding of intelligence in a more pragmatic direction. Much more work remains to be done, e.g. in specifying the environment, goal and efciency distributions relevant to real-world systems, but we believe that the ideas presented here constitute nontrivial progress.",Engineering General  Intelligence Part 1,chapter 8
"If the line of research suggested in this chapter succeeds, then eventually, one will be able to do AGI research as follows: Specify an AGI architecture formally, and then use the mathematics of general intelligence to derive interesting results about the environments, goals and hardware platforms relative to which the AGI architecture will display signicant pragmatic or efcient pragmatic general intelligence, and intellectual breadth. The remaining chapters in this section present further ideas regarding how to work toward this goal. For the time being, such a mode of AGI research remains mainly for the future, but we have still found the formalism given in these chapters useful for formulating and clarifying various aspects of the CogPrime design as will be presented in later chapters.",Engineering General  Intelligence Part 1,chapter 8
"  Chapter 9 Cognitive Synergy 9.1 Introduction As we have seen, the formal theory of general intelligence, in its current form, doesnt really tell us much thats of use for creating real-world AGI systems. It tells us that creating extraordinarily powerful general intelligence is almost trivial if one has unrealistically huge amounts of computational resources; and that creating moderately powerful general intelligence using feasible computational resources is all about creating AI algorithms and data structures that (explicitly or implicitly) match the restrictions implied by a certain class of situations, to which the general intelligence is biased. Weve also described, in various previous chapters, some non-rigorous, conceptual principles that seem to explain key aspects of feasible general intelligence: the complementary reliance on evolution and autopoiesis, the superposition of hierarchical and heterarchical structures, and so forth. These principles can be considered as broad strategies for achieving general intelligence in certain broad classes of situations.",Engineering General  Intelligence Part 1,chapter 9
"Although, a lot of research needs to be done to gure out nice ways to describe, for instance, in what class of situations evolution is an effective learning strategy, in what class of situations dual hierarchical/heterarchical structure is an effective way to organize memory, etc. In this chapter well dig deeper into one of the general principle of feasible generalintelligencesbrieyalludedtoearlier:thecognitivesynergyprinciple,which is both a conceptual hypothesis about the structure of generally intelligent systems in certain classes of environments, and a design principle used to guide the architecting of CogPrime. We will focus here on cognitive synergy specically in the case of multi-memory systems, which we dene as intelligent systems (like CogPrime) whose combination of environment, embodiment and motivational systems make it important for them to possess memories that divide into partially but not wholly distinct components corresponding to the categories of: B. Goertzel et al.",Engineering General  Intelligence Part 1,chapter 9
"Engineering General Intelligence, Part 1, 173 Atlantis Thinking Machines 5, DOI: 10.2991/978-94-6239-027-0_9,  Atlantis Press and the authors 2014      174 9 Cognitive Synergy  Declarative memory  Procedural memory (memory about how to do certain things)  Sensory and episodic memory  Attentional memory (knowledge about what to pay attention to in what contexts)  Intentional memory (knowledge about the systems own goals and subgoals). In Chap.10 we present a detailed argument as to how the requirement for a multimemory underpinning for general intelligence emerges from certain underlying assumptions regarding the measurement of the simplicity of goals and environments; but the points made here do not rely on that argument. What they do rely on is the assumption that, in the intelligence in question, the different components of memory are signicantly but not wholly distinct.",Engineering General  Intelligence Part 1,chapter 9
"That is, there are signicant family resemblances between the memories of a single type, yet there are also thoroughgoing connections between memories of different types. The cognitive synergy principle, if correct, applies to any AI system demonstrating intelligence in the context of embodied, social communication. However, one may also take the theory as an explicit guide for constructing AGI systems; and of course, the bulk of this book describes one AGI architecture, CogPrime, designed in such a way. It is possible to cast these notions in mathematical form, and we make some efforts in this direction in Appendix B, using the languages of category theory and information geometry. However, this formalization has not yet led to any rigorous proof of the generality of cognitive synergy nor any other exciting theorems; with luck this will come as the mathematics is further developed. In this chapter the presentation is kept on the heuristic level, which is all that is critically needed for motivating the CogPrime design.",Engineering General  Intelligence Part 1,chapter 9
"9.2 Cognitive Synergy The essential idea of cognitive synergy, in the context of multi-memory systems, may be expressed in terms of the following points: 1. Intelligence, relative to a certain set of environments, may be understood as the capability to achieve complex goals in these environments. 2. With respect to certain classes of goals and environments (see Chap.10 for a hypothesis in this regard), an intelligent system requires a multi-memory architecture, meaning the possession of a number of specialized yet interconnected knowledge types, including: declarative, procedural, attentional, sensory, episodic and intentional (goal-related). These knowledge types may be viewed as different sorts of patterns that a system recognizes in itself and its environment. Knowledge of these various different types must be interlinked, and in some cases may represent differing views of the same content (see Fig.9.1).      9.2 Cognitive Synergy 175 Fig. 9.",Engineering General  Intelligence Part 1,chapter 9
"1 Illustrative example of the interactions between multiple types of knowledge, in representing a simple piece of knowledge. Generally speaking, one type of knowledge can be converted to another, at the cost of some loss of information. The synergy between cognitive processes associated with corresponding pieces of knowledge, possessing different type, is a critical aspect of general intelligence 3. Such a system must possess knowledge creation (i.e. pattern recognition / formation) mechanisms corresponding to each of these memory types. These mechanisms are also called cognitive processes. 4. Each of these cognitive processes, to be effective, must have the capability to recognize when it lacks the information to perform effectively on its own; and in this case, to dynamically and interactively draw information from knowledge creation mechanisms dealing with other types of knowledge. 5. This cross-mechanism interaction must have the result of enabling the knowledge creation mechanisms to perform much more effectively in combination than they would if operated non-interactively. This is cognitive synergy.",Engineering General  Intelligence Part 1,chapter 9
"While these points are implicit in the theory of mind given in [Goe06a], they are not articulated in this specic form there. Interactions as mentioned in Points 4 and 5 in the above list are the real conceptual meat of the cognitive synergy idea. One way to express the key idea here is that most AI algorithms suffer from combinatorial explosions: the number of      176 9 Cognitive Synergy possible elements to be combined in a synthesis or analysis is just too great, and the algorithms are unable to lter through all the possibilities, given the lack of intrinsic constraint that comes along with a general intelligence context (as opposed to a narrow-AI problem like chess-playing, where the context is constrained and hence restricts the scope of possible combinations that needs to be considered).",Engineering General  Intelligence Part 1,chapter 9
"In an AGI architecture based on cognitive synergy, the different learning mechanisms must be designed specically to interact in such a way as to palliate each others combinatorial explosionsso that, for instance, each learning mechanism dealing with a certain sort of knowledge, must synergize with learning mechanisms dealing with the other sorts of knowledge, in a way that decreases the severity of combinatorial explosion. One prerequisite for cognitive synergy to work is that each learning mechanism must recognize when it is stuck, meaning its in a situation where it has inadequate information to make a condent judgment about what steps to take next. Then, when it does recognize that its stuck, it may request help from other, complementary cognitive mechanisms. Atheoretical notioncloselyrelatedtocognitivesynergyis thecognitiveschematic, formalized in Chap.",Engineering General  Intelligence Part 1,chapter 9
"8, which states that the activity of the different cognitive processes involved in an intelligent system may be modeled in terms of the schematic implication Context  Procedure  Goal where the Context involves sensory, episodic and/or declarative knowledge; and attentional knowledge is used to regulate how much resource is given to each such schematic implication in memory. Synergy among the learning processes dealing with the context, the procedure and the goal is critical to the adequate execution of the cognitive schematic using feasible computational resources. Overall, the cognitive synergy principle describes the behavior of a system as it pursues a set of goals (which in most cases may be assumed to be supplied to the system a priori, but then rened by inference and other processes). The assumed intelligent agent model is roughly as follows: At each time the system chooses a set of procedures to execute, based on its judgments regarding which procedures will best help it achieve its goals in the current context. These procedures may involve external actions (e.g.",Engineering General  Intelligence Part 1,chapter 9
"involving conversation, or controlling an agent in a simulated world) and/or internal cognitive actions. In order to make these judgments it must effectively manage declarative, procedural, episodic, sensory and attentional memory, each of which is associated with specic algorithms and structures. There are also global processes spanning all the forms of memory, including the allocation of attention to different memory items and cognitive processes, and the identication and reication of system-wide activity patterns (the latter referred to as map formation).      9.3 Cognitive Synergy in CogPrime 177 9.3 Cognitive Synergy in CogPrime Different cognitive systems will use different processes to fulll the various roles identied in Chap.7. Here we briey preview the basic cognitive processes that the CogPrime AGI design uses for these roles, and the synergies that exist between these. 9.3.1 Cognitive Processes in CogPrime Tables9.1 and 9.",Engineering General  Intelligence Part 1,chapter 9
"2 present the key structures and processes involved in CogPrime, identifying each one with a certain memory/process type as considered in cognitive synergy theory. That is: each of these cognitive structures or processes deals with one or more types of memorydeclarative, procedural, sensory, episodic or attentional. Table9.3 describes the key CogPrime processes in terms of the analysis vs. synthesis distinction. Finally, Tables9.4 and 9.5 exemplify these structures and processes in the context of embodied virtual agent control. Table 9.1 The OpenCogPrime data structures used to represent the key knowledge types involved Memory type OpenCogPrime data structure Declarative The AtomTable, which is a special form of weighted, labeled hypergraphi.e.",Engineering General  Intelligence Part 1,chapter 9
"a table of nodes and links (collectively referred to as Atoms) with different types, and each weighted with a multi-dimensional truth value (embodying an indenite probability value that give both probability and condence information). Attentional Atoms in the AtomTable are weighted with AttentionValue objects, which contain both ShortTermImportance values (governing processor time allocation) and LongTerm Importance values (governing memory usage) Procedural This is handled using special Combo tree structures embodying LISP-like programs, in a special program dialect intended to manage behaviors in a virtual world and actions in the AtomTable Sensory Handled via a collection of specialized sense-modalityspecic data structures Episodic Handled via an internal simulation world that allows the system to runminds eye movies of situations it remembers, has heard about, or hypothetically envisions Intentional Goals are represented by Atoms stored in the AtomTable; there is a separate table indicating which At",Engineering General  Intelligence Part 1,chapter 9
"oms are top-level goals, which is used to guide attention allocation and goal renement processes      178 9 Cognitive Synergy Table 9.2 Key cognitive processes, and the algorithms that play their roles in CogPrime Cognitive process OpenCogPrime algorithm Uncertain inference Probabilistic Logic Networks (PLN), a logical inference framework capable of uncertain reasoning about abstract knowledge, everyday commonsense knowledge, and low-level perceptual and motor knowledge Supervised procedure learning MOSES, a probabilistic evolutionary learning algorithm, which learns procedures (represented as LISPlike program trees) based on specications Attention allocation Economic Attention Networks (ECAN), a framework for allocating (memory and processor) attention among items of knowledge and cognitive processes, utilizing a synthesis of ideas from neural networks and articial economics.",Engineering General  Intelligence Part 1,chapter 9
"ECAN also comes with a forgetting agent that either saves to disk or deletes knowledge that is estimated not sufciently valuable to keep in memory Map formation Use of frequent subgraph mining, MOSES and other algorithms to scan the knowledge base of the system for patterns and then embodying these patterns explicitly as new knowledge items Concept creation A collection of heuristics for forming new concepts via combining existing ones, including conceptual blending, mutation and extensional and intensional logical operators Simulation The running of simulations of (remembered or imagined) external-world scenarios in an internal worldsimulation engine Goal renement Transformation of given goals into sets of subgoals, using concept creation, inference and procedure learning In the CogPrime context, a procedure in this cognitive schematic is a program tree stored in the systems procedural knowledge base; and a context is a (fuzzy, probabilistic) logical predicate stored in the AtomSpace, that holds, to a certain extent, during each interval of time.",Engineering General  Intelligence Part 1,chapter 9
"A goal is a fuzzy logical predicate that has a certain value at each interval of time, as well. Attentional knowledge is handled in CogPrime by the ECAN articial economics mechanism, that continually updates ShortTermImportance and LongTerm Importance values associated with each item in the CogPrime systems memory, which control the amount of attention other cognitive mechanisms pay to the item, and how much motive the system has to keep the item in memory. HebbianLinks are then created between knowledge items that often possess ShortTermImportance at the same time; this is CogPrimes version of traditional Hebbian learning. ECAN has deep interactions with other cognitive mechanisms as well, which are essential to its efcient operation; for instance, PLN inference may be used to help ECAN extrapolate conclusions about what is worth paying attention to, and      9.3 Cognitive Synergy in CogPrime 179 Table 9.",Engineering General  Intelligence Part 1,chapter 9
"3 Key CogPrime cognitive processes categorized according to knowledge type and process type Synthesis Analysis PLN (Decl. and Proc.) PLN forward inference PLN backward inference MOSES (Decl. and Proc.) MOSES and hillclimbing procedure learning (combining portions and aspects of prior procedures) Probabilistic modeling to identify patterns among programs fullling a certain goal in a certain context (part of MOSES) Sensory/episodic Imagination of hypothetical episodes based on specied criteria, via combination of aspects of known episodes Filling in gaps in remembered or hypothesized episodes Attentional Hebbian learning Importance spreading Map formation Assignment of credit Intentional Goal synthesis Goal renement MOSES may be used to recognize subtle attentional patterns. ECAN also handles assignment of credit, the guring-out of the causes of an instance of successful goal-achievement, drawing on PLN and MOSES as needed when the causal inference involved here becomes difcult.",Engineering General  Intelligence Part 1,chapter 9
"The synergies between CogPrimes cognitive processes are well summarized in Table9.6 below, which is a 16  16 matrix summarizing a host of interprocess interactions generic to CST. One key aspect of how CogPrime implements cognitive synergy is PLNs sophisticated management of the condence of judgments. This ties in with the way OpenCog Primes PLN inference framework represents truth values in terms of multiple components (as opposed to the single probability values used in many probabilistic inference systems and formalisms): each item in OpenCogPrimes declarative memory has a condence value associated with it, which tells how much weight the system places on its knowledge about that memory item. This assists with cognitive synergy as follows: A learning mechanism may consider itself stuck, generally speaking, when it has no high-condence estimates about the next step it should take.",Engineering General  Intelligence Part 1,chapter 9
"Without reasonably accurate condence assessment to guide it, inter-component interaction could easily lead to increased rather than decreased combinatorial explosion. And of course there is an added recursion here, in that condence assessment is carried out partly via PLN inference, which in itself relies upon these same synergies for its effective operation. To illustrate this point further, consider one of the synergetic aspects described in Table9.6: the role cognitive synergy plays in deductive inference. Deductive inference is a hard problem in generalbut what is hard about it is not carrying out      180 9 Cognitive Synergy Table 9.",Engineering General  Intelligence Part 1,chapter 9
"4 Key CogPrime cognitive structures illustrated in the context of virtual agents Knowledge type Virtual agent example(s) Declarative  The red ball on the table is larger than the blue ball on the oor  Bob becomes angry quickly  Ball roll; blocks dont  Jim knows Bob is not my friend Procedural  A procedure for retrieving an item from a distant location  A procedure for spinning around in a circle  A procedure for stacking a block on top of another one  A procedure for repeatedly asking a question in different ways until an acceptable answer is obtained Sensory  The appearance of Bobs face The specic array of objects on the oor under the table Episodic  The series of actions Bill did when he built a tower on the oor yesterday  The episode in which Bill and Bob repeatedly threw a ball back and forth between each other  The series of actions I just took, between getting up from the chair and Bob saying good Attentional  The set of objects that seem",Engineering General  Intelligence Part 1,chapter 9
"to be important in the context of the game Bob and Bill are playing  The set of words and phrases that are associated with Bob being happy with me while we walk around together Intentional  The goal of making Bob say positive things  The goal of making a tower that does not fall down easily  The goal of getting Jim to answer my question inference steps, but rather inference control (i.e., choosing which inference steps to carry out). Specically, what must happen for deduction to succeed in CogPrime is: 1. the system must recognize when its deductive inference process is stuck, i.e. when the PLN inference control mechanism carrying out deduction has no clear idea regarding which inference step(s) to take next, even after considering all the domain knowledge at is disposal; 2.",Engineering General  Intelligence Part 1,chapter 9
"in this case, the system must defer to another learning mechanism to gather more information about the different choices availableand the other learning mechanism chosen must, a reasonable percentage of the time, actually provide useful information that helps PLN to get unstuck and continue the deductive process.      9.3 Cognitive Synergy in CogPrime 181 Table 9.",Engineering General  Intelligence Part 1,chapter 9
"5 Key CogPrime cognitive processes illustrated in the context of virtual agents Cognitive process Virtual agent example Inference  Tall thin blocks, when stood upright, are less likely to topple over if placed next to each other  Bob hates cursing, and Jim is Bobs friend, and friends often have similar likes and dislikes, so Jim probably hates cursing Procedure learning  Learning a procedure for crawling on the oor, based on imitation of what others do when they describe themselves as crawling, plus reinforcement from others when they nd ones imitation accurate  Learning a procedure embodying some combination of functional and visual features that predicts whether some entity is considered a toy or not Attention allocation  Pictures of women are associated with Bobs happiness, and Bobs happiness is associated with getting reward, therefore pictures of women are associated with getting reward  Asking for help is surprisingly often a precursor to getting reward when Jane is around; so when a reward is gotten when Jane is around, a little extra attention should be given to",Engineering General  Intelligence Part 1,chapter 9
"ongoing improvement of the processes that help in the mechanics of asking for help Goal renement  The goal of making Jim happy, seems to often be achieved by the goal of creating sculptures Jim likes, and Jim likes complicated sculptures; thus I adopt the goal of creating complicated sculptures when Jim is around Declarative pattern mining  The goal of making Jim happy, seems to often be achieved by the goal of creating sculptures Jim likes, and Jim likes complicated sculptures; thus I adopt the goal of creating complicated sculptures when Jim is around Sensory pattern recognition  When Jim builds a castle out of blocks, he identies some portions of the castle as towers and others as walls; its necessary to visually identify which portions of each castle correspond to these descriptors  Its also necessary to visually identify the castle as a whole versus the table, oor or other base its resting on Simulation  Using an internal simulation world to experiment with building various towers rapidly, at a pace faster than is possible in the online simulation world where humans",Engineering General  Intelligence Part 1,chapter 9
"participate  Using an internal simulation world containing a simulation of Bob and Jim, to simulate what Bob will know about what youre doing if you hide behind Jim and build a tower of blocks (Continued)      182 9 Cognitive Synergy Table 9.5 (Continued) Cognitive process Virtual agent example Concept creation  The concept of an unstable structure  The concept of an irritable person  The concept of a happy occasion Map formation  The set of all knowledge items associated with Bob being in a good mood (which may then be used to form a new concept)  The set of all knowledge items associated with (running, walking or crawling) races For instance, deduction might defer to the attentional knowledge subsystem, and make a judgment as to which of the many possible next deductive steps are most associated with the goal of inference and the inference steps taken so far, according to the HebbianLinks constructed by the attention allocation subsystem, based on observed associations.",Engineering General  Intelligence Part 1,chapter 9
"Or, if this fails, deduction might ask MOSES (running in supervised categorization mode) to learn predicates characterizing some of the terms involving the possible next inference steps. Once MOSES provides these new predicates, deduction can then attempt to incorporate these into its inference process, hopefully (though not necessarily) arriving at a higher-condence next step. 9.4 Some Critical Synergies Referring back to Fig.9.2, and summarizing many of the ideas in the previous section, Table9.6 enumerates a number of specic ways in which the cognitive processes mentioned in the gure may synergize with one another, potentially achieving dramatically greater efciency than would be possible on their own. Of course, realizing these synergies on the practical algorithmic level requires signicant inventiveness and may be approached in many different ways. The specics ofhowCogPrimemanifeststhesesynergiesarediscussedinmanyfollowingchapters. 9.",Engineering General  Intelligence Part 1,chapter 9
"5 Cognitive Synergy for Procedural and Declarative Learning We now present a little more algorithmic detail regarding the operation and synergetic interaction of CogPrimes two most sophisticated components: the MOSES procedure learning algorithm (see Chap.16, Part 2), and the PLN uncertain inference framework (see Chap.17, Part 2). The treatment is necessarily quite compact, since we have not yet reviewed the details of either MOSES or PLN; but as well as illustrating the notion of cognitive synergy more concretely, perhaps the high-level discussion here will make clearer how MOSES and PLN t into the big picture of CogPrime.      9.5 Cognitive Synergy for Procedural and Declarative Learning 183 Table 9.",Engineering General  Intelligence Part 1,chapter 9
"6 This table, and the following ones, show some of the synergies between the primary cognitive processes explicitly used in CogPrime How  Helps  Map formation Goal system Simulation Sensorimotor pattern recognition Uncertain inference Creates new concepts and relationships. Enabling briefer useful inference trails Goal renement enables more careful goal-based inference pruning -Simulations provide a method of testing speculative inferential conclusionsSimulations suggest hypotheses to be explored via inference Creates new concepts and relationships. Enabling briefer useful inference trails Supervised procedure learning Creates new procedures to be used as modules in candidate procedures Goal renement allows more precise denition of tness functions.",Engineering General  Intelligence Part 1,chapter 9
"Makind procedure learnings job easier Simulation provides a method of tness estimation allowing inexpensive testing of candidate procedures Extraction of sensorimotor patterns allows creation of abstracted tness functions for (inferentially and simulatively) evaluating procedures guiding real-word actions Attention allocation Creates new concepts grouping attentionally related memory items, enabling AA to nd subtler attentional patterns involving these nodes Goal renement allows more accurately goal-driven allocation of attention Simulation provides data for attention allocation allowing attentional information to be extracted from co-occurrences observed in simulation Creates concepts attentionally related memory items, enabling AA to nd subtler attentional patterns involving these nodes (Continued)      184 9 Cognitive Synergy Table 9.",Engineering General  Intelligence Part 1,chapter 9
"6 (Continued) How  Helps  Map formation Goal system Simulation Sensorimotor pattern recognition Concept creation Creates new concepts to be fed into other concept creation mechanisms Goal renement provides more precise denition of criteria via which new concepts are created Utility of concepts may be assessed via creating simulated entities embodying the new concepts and seeing what they lead to in simulation Creates New concepts to be fed into other concept creation mechanisms Uncertain inference NA When inference gets stuck in an inference trail, it can ask procedure learning to learn new pattern regarding concepts in the inference trail (If there is adequate data regarding the concepts) Importance levels allow pruning of inference trees Provides news concepts. Allowing briefer useful inference trails Supervised procedure learning Inference can be used to allow prior experience to guide each instance of procedure learning NA Importance levels may be used to blas choices made in the course of procedure learning (e.g.",Engineering General  Intelligence Part 1,chapter 9
"in OCP, in the tness evaluation and representation building phases of MOSES) Provides new concepts, allowing compacter programs using new concepts in various roles (Continued)      9.5 Cognitive Synergy for Procedural and Declarative Learning 185 Table 9.6 (Continued) How  Helps  Map formation Goal system Simulation Sensorimotor pattern recognition Attention allocation Enables inference of new HebbianLinks and HebbianPredicates from existing ones Procedure learning can recognize patterns in historical system activity, which are then used to build concepts and relationships guiding attention allocation NA Combination of concepts formed via map formation, may lead to new concepts that even better direct attention Concept creation Allows inferential assessment of the value of new concepts Procedure learning can be used to search for high-quality blends of existing concepts (using e.g.",Engineering General  Intelligence Part 1,chapter 9
"inferential and attentional knowledge in tness functions) Allows assessment of the value of new concepts based on historical attentional knowledge NA Map formation Speculative inference can help map formation guess which map to hunt for Procedure learning can be used to search dor maps that are more complex than mere cooccurrence Attention allocation provides the raw data for map formation No signicant direct synergy (Continued)      186 9 Cognitive Synergy Table 9.6 (Continued) How  Helps  Map formation Goal system Simulation Sensorimotor pattern recognition Goal system Inference can carry out goal renement No signicant direct synergy Flow of importance among subgoals determines which subgoals get used, versus being forgotten Concept creation can be used to provide raw data for goal renement (e.g.",Engineering General  Intelligence Part 1,chapter 9
"a new subgoal that blends two others) Simulation In order to provide data for setting up simulations inference will often be needed No signicant direct synergy Attention allocation tells which portions of a simulation need to be run in more detail No signicant direct synergy Sensorimotor pattern recognition Speculative inference helps ll in gaps in sensory data Procedure learning can be used to nd subtle patterns in sensorimotor data Attention allocation guides parttern recognition via indicating which sensorimotor stimuli and patterns tend to be associatively linked New concepts may be created that then are found to serve as signicant patterns in sensorimotor data Map formation NA Map formation may focus on nding maps relates to subgoals, and good subgoal renement helps here No signicant direct synergy No signicant direct synergy Goal system Concepts formed from maps may be useful raw material for forming subgoals NA No signicant direct synergy No signicant direct synergy (Continued)      9.",Engineering General  Intelligence Part 1,chapter 9
"5 Cognitive Synergy for Procedural and Declarative Learning 187 Table 9.6 (Continued) How  Helps  Map formation Goal system Simulation Sensorimotor pattern recognition Simulation No signicant direct synergy No signicant direct synergy NA Presence of recognized sensorimotor patterns may be used to judge whether a simulation is sufciently accurate Sensorimotor pattern recognition Concepts formed from maps may usefully guide sensorimotor pattern search Directing pattern toward patterns pertinent to subgoals may make the task far easier Patterns recognized in simulations may then be checked for presence in real sensorimotor data NA 9.5.1 Cognitive Synergy in MOSES MOSES, CogPrimes primary algorithm for learning procedural knowledge, has been tested on a variety of application problems including standard GP test problems, virtual agent control, biological data analysis and text classication [Loo06]. It represents procedures internally as program trees.",Engineering General  Intelligence Part 1,chapter 9
"Each node in a MOSES program tree is supplied with a knob, comprising a set of values that may potentially be chosen to replace the data item or operator at that node. So for instance a node containing the number 7 may be supplied with a knob that can take on any integer value. A node containing a while loop may be supplied with a knob that can take on various possible control ow operators including conditionals or the identity. A node containing a procedure representing a particular robot movement, may be supplied with a knob that can take on values corresponding to multiple possible movements. Following a metaphor suggested by Douglas Hofstadter [Hof96], MOSES learning covers both knob twiddling (setting the values of knobs) and knob creation (Fig.9.2).",Engineering General  Intelligence Part 1,chapter 9
"MOSES is invoked within CogPrime in a number of ways, but most commonly for nding a procedure P satisfying a probabilistic implication C&P  G as described above, where C is an observed context and G is a system goal. In this case the probability value of the implication provides the scoring function that MOSES uses to assess the quality of candidate procedures.      188 9 Cognitive Synergy Fig. 9.2 High-level control ow of MOSES algorithm For example, suppose a CogPrime-controlled robot is trying to learn to play the game of tag. (I.e. a multi-agent game in which one agent is specially labeled it, and runs after the other player agents, trying to touch them. Once another agent is touched, it becomes the new it and the previous it becomes just another player agent.",Engineering General  Intelligence Part 1,chapter 9
"Then its context C is that others are trying to play a game they call tag with it; and we may assume its goals are to please them and itself, and that it has gured out that in order to achieve this goal it should learn some procedure to follow when interacting with others who have said they are playing tag. In this case a potential tag-playing procedure might contain nodes for physical actions like step_forward(speed s), as well as control ow nodes containing operators like ifelse (for instance, there would probably be a conditional telling the robot to do something different depending on whether someone seems to be chasing it). Each of these program tree nodes would have an appropriate knob assigned to it. And the scoring function would evaluate a procedure P in terms of how successfully the robot played tag when controlling its behaviors according to P (noting that it may also be using other control procedures concurrently with P).",Engineering General  Intelligence Part 1,chapter 9
"Its worth noting here that evaluating the scoring function in this case involves some inference already, because in order to tell if it is playing tag successfully, in a real-world context, it must watch and understand the behavior of the other players. MOSES follows the high-level control ow depicted in Fig.16.1 (Part 2), which corresponds to the following process for evolving a metapopulation of demes of programs (each deme being a set of relatively similar programs, forming a sort of island in program space): 1. Construct an initial set of knobs based on some prior (e.g., based on an empty program; or more interestingly, using prior knowledge supplied by PLN inference      9.5 Cognitive Synergy for Procedural and Declarative Learning 189 based on the systems memory) and use it to generate an initial random sampling of programs. Add this deme to the metapopulation. 2.",Engineering General  Intelligence Part 1,chapter 9
"Select a deme from the metapopulation and update its sample, as follows: a. Select some promising programs from the demes existing sample to use for modeling, according to the scoring function. b. Considering the promising programs as collections of knob settings, generate new collections of knob settings by applying some (competent) optimization algorithm. For best performance on difcult problems, it is important to use an optimization algorithm that makes use of the systems memory in its choices, consulting PLN inference to help estimate which collections of knob settings will work best. c. Convert the new collections of knob settings into their corresponding programs, reduce the programs to normal form, evaluate their scores, and integrate them into the demes sample, replacing less promising programs. In the case that scoring is expensive, score evaluation may be preceded by score estimation, which may use PLN inference, enaction of procedures in an internal simulation environment, and/or similarity matching against episodic memory. 3.",Engineering General  Intelligence Part 1,chapter 9
"For each new program that meet the criterion for creating a new deme, if any: a. Construct a new set of knobs (a process called representation-building) to dene a region centered around the program (the demes exemplar), and use it to generate a new random sampling of programs, producing a new deme. b. Integrate the new deme into the metapopulation, possibly displacing less promising demes. 4. Repeat from step 2. MOSES is a complex algorithm and each part plays its role; if any one part is removed the performance suffers signicantly [Loo06]. However, the main point we want to highlight here is the role played by synergetic interactions between MOSES and other cognitive components such as PLN, simulation and episodic memory, as indicated in boldface in the above pseudocode.",Engineering General  Intelligence Part 1,chapter 9
"MOSES is a powerful procedure learning algorithm, but used on its own it runs into scalability problems like any other such algorithm; the reason we feel it has potential to play a major role in a human-level AI system is its capacity for productive interoperation with other cognitive components. Continuing the tag example, the power of MOSESs integration with other cognitive processes would come into play if, before learning to play tag, the robot has already played simpler games involving chasing. If the robot already has experience chasing and being chased by other agents, then its episodic and declarative memory will contain knowledge about how to pursue and avoid other agents in the context of running around an environment full of objects, and this knowledge will be deployable within the appropriate parts of MOSESs Steps 1 and 2. Cross-process and crossmemory-type integration make it tractable for MOSES to act as a transfer learning algorithm, not just a task-specic machine-learning algorithm.",Engineering General  Intelligence Part 1,chapter 9
"     190 9 Cognitive Synergy 9.5.2 Cognitive Synergy in PLN While MOSES handles much of CogPrimes procedural learning, and OpenCogPrimes internal simulation engine handles most episodic knowledge, CogPrimes primary tool for handling declarative knowledge is an uncertain inference framework called Probabilistic Logic Networks (PLN). The complexities of PLN are the topic of a lengthy technical monograph [GMIHo8], and here we will eschew most details and focus mainly on pointing out how PLN seeks to achieve efcient inference control via integration with other cognitive processes. As a logic, PLN is broadly integrative: it combines certain term logic rules with more standard predicate logic rules, and utilizes both fuzzy truth values and a variant of imprecise probabilities called indenite probabilities. PLN mathematics tells how these uncertain truth values propagate through its logic rules, so that uncertain premises give rise to conclusions with reasonably accurately estimated uncertainty values.",Engineering General  Intelligence Part 1,chapter 9
"This careful management of uncertainty is critical for the application of logical inference in the robotics context, where most knowledge is abstracted from experience and is hence highly uncertain. PLN can be used in either forward or backward chaining mode; and in the language introduced above, it can be used for either analysis or synthesis. As an example, we will consider backward chaining analysis, exemplied by the problem of a robot preschool-student trying to determine whether a new playmate Bob is likely to be a regular visitor to its preschool or not (evaluating the truth value of the implication Bob  regular_visitor). The basic backward chaining process for PLN analysis looks like: 1. Given an implication L  A  B whose truth value must be estimated (for instance L  C&P  G as discussed above), create a list (A1, . . . , An) of (inference rule, stored knowledge) pairs that might be used to produce L. 2.",Engineering General  Intelligence Part 1,chapter 9
"Using analogical reasoning to prior inferences, assign each Ai a probability of success.  If some of the Ai are estimated to have reasonable probability of success at generating reasonably condent estimates of Ls truth value, then invoke Step 1 with Ai in place of L (at this point the inference process becomes recursive).  If none of the Ai looks sufciently likely to succeed, then inference has gotten stuck and another cognitive process should be invoked, e.g.  Concept creation may be used to infer new concepts related to A and B, and then Step 1 may be revisited, in the hope of nding a new, more promising Ai involving one of the new concepts.  MOSES may be invoked with one of several special goals, e.g. the goal of nding a procedure P so that P(X) predicts whether X  B.",Engineering General  Intelligence Part 1,chapter 9
"If MOSES nds such a procedure P then this can be converted to declarative knowledge understandable by PLN and Step 1 may be revisited . . .      9.5 Cognitive Synergy for Procedural and Declarative Learning 191  Simulations may be run in CogPrimes internal simulation engine, so as to observe the truth value of A  B in the simulations; and then Step 1 may be revisited . . . The combinatorial explosion of inference control is combatted by the capability to defer to other cognitive processes when the inference control procedure is unable to make a sufciently condent choice of which inference steps to take next. Note that just as MOSES may rely on PLN to model its evolving populations of procedures, PLN may rely on MOSES to create complex knowledge about the terms in its logical implications.",Engineering General  Intelligence Part 1,chapter 9
"This is just one example of the multiple ways in which the different cognitive processes in CogPrime interact synergetically; a more thorough treatment of these interactions is given in Chap.32 (Part 2). In the new playmate example, the interesting case is where the robot initially seems not to know enough about Bob to make a solid inferential judgment (so that none of the Ai seem particularly promising). For instance, it might carry out a number of possible inferences and not come to any reasonably condent conclusion, so that the reason none of the Ai seem promising is that all the decent-looking ones have been tried already. So it might then recourse to MOSES, simulation or concept creation. For instance, the PLN controller could make a list of everyone who has been a regular visitor, and everyone who has not been, and pose MOSES the task of guring out a procedure for distinguishing these two categories.",Engineering General  Intelligence Part 1,chapter 9
"This procedure could then used directly to make the needed assessment, or else be translated into logical rules to be used within PLN inference. For example, perhaps MOSES would discover that older males wearing ties tend not to become regular visitors. If the new playmate is an older male wearing a tie, this is directly applicable. But if the current playmate is wearing a tuxedo, then PLN may be helpful via reasoning that even though a tuxedo is not a tie, its a similar form of fancy dressso PLN may extend the MOSES-learned rule to the present case and infer that the new playmate is not likely to be a regular visitor. 9.6 Is Cognitive Synergy Tricky? In this section1 we use the notion of cognitive synergy to explore a question that arises frequently in the AGI community: the well-known difculty of measuring intermediate progress toward human-level AGI.",Engineering General  Intelligence Part 1,chapter 9
"We explore some potential reasons underlying this, via extending the notion of cognitive synergy to a more rened notion of tricky cognitive synergy. These ideas are particularly relevant to the problem of creating a roadmap toward AGI, as well explore in Chap.18. 1 This section co-authored with Jade ONeill.      192 9 Cognitive Synergy 9.6.1 The Puzzle: Why Is It So Hard to Measure Partial Progress Toward Human-Level AGI? Its not entirely straightforward to create tests to measure the nal achievement of human-level AGI, but there are some fairly obvious candidates here. Theres the Turing Test (fooling judges into believing youre human, in a text chat), the video Turing Test, the Robot College Student test (passing university, via being judged exactly the same way a human student would), etc.",Engineering General  Intelligence Part 1,chapter 9
"Theres certainly no agreement on which is the most meaningful such goal to strive for, but theres broad agreement that a number of goals of this nature basically make sense. On the other hand, how does one measure whether one is, say, 50% of the way to human-level AGI? Or, say, 75 or 25%? Its possible to pose many practical tests of incremental progress toward humanlevel AGI, with the property that if a proto-AGI system passes the test using a certain sort of architecture and/or dynamics, then this implies a certain amount of progress toward human-level AGI based on particular theoretical assumptions about AGI. However,ineachcaseofsuchapracticaltest,itseemsintuitivelylikelytoasignicant percentage of AGI researchers that there is some way to game the test via designing a system specically oriented toward passing that test, and which doesnt constitute dramatic progress toward AGI.",Engineering General  Intelligence Part 1,chapter 9
"Some examples of practical tests of this nature would be  The Wozniak coffee test: go into an average American house and gure out how to make coffee, including identifying the coffee machine, guring out what the buttons do, nding the coffee in the cabinet, etc.  Story understandingreading a story, or watching it on video, and then answering questions about what happened (including questions at various levels of abstraction).  Graduating (virtual-world or robotic) preschool.  Passing the elementary school reading curriculum (which involves reading and answering questions about some picture books as well as purely textual ones).  Learning to play an arbitrary video game based on experience only, or based on experience plus reading instructions.",Engineering General  Intelligence Part 1,chapter 9
"One interesting point about tests like this is that each of them seems to some AGI researchers to encapsulate the crux of the AGI problem, and be unsolvable by any system not far along the path to human-level AGIyet seems to other AGI researchers, with different conceptual perspectives, to be something probably gameable by narrow-AI methods. And of course, given the current state of science, theres no way to tell which of these practical tests really can be solved via a narrow-AI approach, except by having a lot of people try really hard over a long period of time. A question raised by these observations is whether there is some fundamental reason why its hard to make an objective, theory-independent measure of intermediate progress toward advanced AGI. Is it just that we havent been smart enough to gure      9.",Engineering General  Intelligence Part 1,chapter 9
"6 Is Cognitive Synergy Tricky? 193 out the right testor is there some conceptual reason why the very notion of such a test is problematic? We dont claim to know for surebut in the rest of this section well outline one possible reason why the latter might be the case. 9.6.2 A Possible Answer: Cognitive Synergy Is Tricky! Why might a solid, objective empirical test for intermediate progress toward AGI be an infeasible notion? One possible reason, we suggest, is precisely cognitive synergy, as discussed above. The cognitive synergy hypothesis, in its simplest form, states that human-level AGI intrinsically depends on the synergetic interaction of multiple components (for instance, as in CogPrime, multiple memory systems each supplied with its own learning process). In this hypothesis, for instance, it might be that there are 10 critical components required for a human-level AGI system.",Engineering General  Intelligence Part 1,chapter 9
"Having all 10 of them in place results in human-level AGI, but having only 8 of them in place results in having a dramatically impaired systemand maybe having only 6 or 7 of them in place results in a system that can hardly do anything at all. Of course, the reality is almost surely not as strict as the simplied example in the above paragraph suggests. No AGI theorist has really posited a list of 10 crisplydened subsystems and claimed them necessary and sufcient for AGI. We suspect there are many different routes to AGI, involving integration of different sorts of subsystems. However, if the cognitive synergy hypothesis is correct, then human-level AGI behaves roughly like the simplistic example in the prior paragraph suggests. Perhaps instead of using the 10 components, you could achieve human-level AGI with 7 components, but having only 5 of these 7 would yield drastically impaired functionalityetc.",Engineering General  Intelligence Part 1,chapter 9
"Or the point could be made without any decomposition into a nite set of components, using continuous probability distributions. To mathematically formalize the cognitive synergy hypothesis becomes complex, but here were only aiming for a qualitative argument. So for illustrative purposes, well stick with the 10 components example, just for communicative simplicity. Next, lets suppose that for any given task, there are ways to achieve this task using a system that is much simpler than any subset of size 6 drawn from the set of 10 components needed for human-level AGI, but works much better for the task than this subset of 6 components (assuming the latter are used as a set of only 6 components, without the other 4 components). Note that this supposition is a good bit stronger than mere cognitive synergy. For lack of a better name, well call it tricky cognitive synergy.",Engineering General  Intelligence Part 1,chapter 9
"The tricky cognitive synergy hypothesis would be true if, for example, the following possibilities were true:      194 9 Cognitive Synergy  creating components to serve as parts of a synergetic AGI is harder than creating components intended to serve as parts of simpler AI systems without synergetic dynamics;  components capable of serving as parts of a synergetic AGI are necessarily more complicated than components intended to serve as parts of simpler AGI systems. These certainly seem reasonable possibilities, since to serve as a component of a synergetic AGI system, a component must have the internal exibility to usefully handle interactions with a lot of other components as well as to solve the problems that come its way.",Engineering General  Intelligence Part 1,chapter 9
"In a CogPrime context, these possibilities ring true, in the sense that tailoring an AI process for tight integration with other AI processes within CogPrime, tends to require more work than preparing a conceptually similar AI process for use on its own or in a more task-specic narrow AI system. It seems fairly obvious that, if tricky cognitive synergy really holds up as a property of human-level general intelligence, the difculty of formulating tests for intermediate progress toward human-level AGI follows as a consequence. Because, according to the tricky cognitive synergy hypothesis, any test is going to be more easily solved by some simpler narrow AI process than by a partially complete human-level AGI system. 9.6.3 Conclusion We havent proved anything here, only made some qualitative arguments. However, these arguments do seem to give a plausible explanation for the empirical observation thatpositingtestsforintermediateprogresstowardhuman-levelAGIisaverydifcult prospect.",Engineering General  Intelligence Part 1,chapter 9
"If the theoretical notions sketched here are correct, then this difculty is not due to incompetence or lack of imagination on the part of the AGI community, nor due to the primitive state of the AGI eld, but is rather intrinsic to the subject matter. And if these notions are correct, then quite likely the future rigorous science of AGI will contain formal theorems echoing and improving the qualitative observations and conjectures weve made here. If the ideas sketched here are true, then the practical consequence for AGI development is, very simply, that one shouldnt worry a lot about producing intermediary results that are compelling to skeptical observers. Just at 2/3 of a human brain may not be of much use, similarly, 2/3 of an AGI system may not be much use.",Engineering General  Intelligence Part 1,chapter 9
"Lack of impressive intermediary results may not imply one is on a wrong development path; and comparison with narrow AI systems on specic tasks may be badly misleading as a gauge of incremental progress toward human-level AGI. Hopefully its clear that the motivation behind the line of thinking presented here is a desire to understand the nature of general intelligence and its pursuitnot a desire to avoid testing our AGI software! Really, as AGI engineers, we would love to have a sensible rigorous way to test our intermediary progress toward AGI, so as to be able to pose convincing arguments to skeptics, funding sources, potential collaborators      9.6 Is Cognitive Synergy Tricky? 195 and so forth. Our motivation here is not a desire to avoid having the intermediate progress of our efforts measured, but rather a desire to explain the frustrating (but by now rather well-established) difculty of creating such intermediate goals for human-level AGI in a meaningful way.",Engineering General  Intelligence Part 1,chapter 9
"If we or someone else gures out a compelling way to measure partial progress toward AGI, we will celebrate the occasion. But it seems worth seriously considering the possibility that the difculty in nding such a measure reects fundamental properties of general intelligence. From a practical CogPrime perspective, we are interested in a variety of evaluation and testing methods, including the virtual preschool approach mentioned briey aboveandmoreextensivelyinlaterchapters.However,ourfocuswillbeonevaluation methods that give us meaningful information about CogPrimes progress, given our knowledge of how CogPrime works and our understanding of the underlying theory. We are unlikely to focus on the achievement of intermediate test results capable of convincing skeptics of the reality of our partial progress, because we have not yet seen any credible tests of this nature, and because we suspect the reasons for this lack may be rooted in deep properties of feasible general intelligence, such as tricky cognitive synergy.",Engineering General  Intelligence Part 1,chapter 9
"  Chapter 10 General Intelligence in the Everyday Human World 10.1 Introduction Intelligence is not just about what happens inside a system, but also about what happens outside that system, and how the system interacts with its environment. Real-world general intelligence is about intelligence relative to some particular class of environments, and human-like general intelligence is about intelligence relative to the particular class of environments that humans evolved in (which in recent millennia has included environments humans have created using their intelligence). In Chap. 4, we reviewed some specic capabilities characterizing human-like general intelligence; to connect these with the general theory of general intelligence from the last few chapters, we need to explain what aspects of human-relevant environments correspond to these human-like intelligent capabilities. We begin with aspects of the environment related to communication, which turn out to tie in closely with cognitive synergy. Then we turn to physical aspects of the environment, which we suspect also connect closely with various human cognitive capabilities.",Engineering General  Intelligence Part 1,chapter 10
"Finally we turn to physical aspects of the human body and their relevance to the human mind. In the following chapter we present a deeper, more abstract theoretical framework encompassing these ideas. These ideas are of theoretical importance, and theyre also of practical importance when one turns to the critical area of AGI environment design. If one is going to do anything besides release ones young AGI into the wilds of everyday human life, then one has to put some thought into what kind of environment it will be raised in. This may be a virtual world or it may be a robot preschool or some other kind of physical environment, but in any case some specic choices must be made about what to include. Specic choices must also be made about what kind of body to give ones AGI systemwhat sensors and actuators, and so forth. In Chap.",Engineering General  Intelligence Part 1,chapter 10
"17 we will present some specic suggestions regarding choices of embodiment and environment that we nd to be ideal for AGI developmentvirtual and robot preschoolsbut the material in this chapter is of more general import, beyond any such particularities. If one has an intuitive idea of what properties of body and world human intelligence is B. Goertzel et al., Engineering General Intelligence, Part 1, 197 Atlantis Thinking Machines 5, DOI: 10.2991/978-94-6239-027-0_10,  Atlantis Press and the authors 2014      198 10 General Intelligence in the Everyday Human World biased for, then one can make practical choices about embodiment and environment in a principled rather than purely ad hoc or opportunistic way. 10.2 Some Broad Properties of the Everyday World that Help Structure Intelligence The properties of the everyday world that help structure intelligence are diverse and span multiple levels of abstraction.",Engineering General  Intelligence Part 1,chapter 10
"Most of this chapter will focus on fairly concrete patterns of this nature, such as are involved in inter-agent communication and naive physics; however, its also worth noting the potential importance of more abstract patterns distinguishing the everyday world from arbitrary mathematical environments. The propensity to search for hierarchical patterns is one huge potential example of an abstract everyday-world property. We strongly suspect the reason that searching for hierarchical patterns works so well, in so many everyday-world contexts, lies in the particular structure of the everyday worldits not something that would be true across all possible environments (even if one weights the space of possible environments in some clever way, say using program-length according to some standard computational model). However, this sort of assertion is of course highly philosophical, and becomes complex to formulate and defend convincingly given the current state of science and mathematics. Going one step further, we recall from Chap.",Engineering General  Intelligence Part 1,chapter 10
"5 a structure called the dual network, which consists of superposed hierarchical and heterarchical networks: basically a hierarchy in which the distance between two nodes in the hierarchy is correlated with the distance between the nodes in some metric space. Another high level property of the everyday world may be that dual network structures are prevalent. This would imply that minds biased to represent the world in terms of dual network structure are likely to be intelligent with respect to the everyday world. In a different direction, the extreme commonality of symmetry groups in the (everyday and otherwise) physical world is another example: they occur so often that minds oriented toward recognizing patterns involving symmetry groups are likely to be intelligent with respect to the real world. We suspect that the number of cognitively-relevant properties of the everyday world is huge ...",Engineering General  Intelligence Part 1,chapter 10
"and that the essence of everyday-world intelligence lies in the list of varyingly abstract and concrete properties, which must be embedded implicitly or explicitly in the structure of a natural or articial intelligence for that system to have everyday-world intelligence. Apart from these particular yet abstract properties of the everyday world, intelligence is just about nding patterns in which actions tend to achieve which goals in which situations ... but, the simple meta-algorithm needed to accomplish this universally is, we suggest, only a small percentage of what it takes to make a mind. You might say that a sufciently generally intelligent system should be able to infer the various cognitively-relevant properties of the environment from looking      10.2 Some Broad Properties of the Everyday World that Help Structure Intelligence 199 at data about the everyday world.",Engineering General  Intelligence Part 1,chapter 10
"We agree in principle, and in fact Ben Kuipers and his colleagues have done some interesting work in this direction, showing that learning algorithms can infer some basics about the structure of space and time from experience [MK07]. But we suggest that doing this really thoroughly would require a massively greater amount of processing power than an AGI that embodies and hence automatically utilizes these principles. It may be that the problem of inferring these properties is so hard as to require a wildly infeasible AIXItl / Godel Machine type system. 10.3 Embodied Communication Next we turn to the potential cognitive implications of seeking to achieve goals in an environment in which multimodal communication with other agents plays a prominent role.",Engineering General  Intelligence Part 1,chapter 10
"Consider a community of embodied agents living in a shared world, and suppose that the agents can communicate with each other via a set of mechanisms including:  Linguistic communication, in a language whose semantics is largely (not necessarily wholly) interpretable based on the mutually experienced world  Indicative communication, in which e.g. one agent points to some part of the world or delimits some interval of time, and another agent is able to interpret the meaning  Demonstrative communication, in which an agent carries out a set of actions in the world, and the other agent is able to imitate these actions, or instruct another agent as to how to imitate these actions  Depictive communication, in which an agent creates some sort of (visual, auditory, etc.",Engineering General  Intelligence Part 1,chapter 10
"construction to show another agent, with a goal of causing the other agent to experience phenomena similar to what they would experience upon experiencing some particular entity in the shared environment  Intentional communication, in which an agent explicitly communicates to another agent what its goal is in a certain situation1 It is clear that ordinary everyday communication between humans possesses all these aspects. We dene the Embodied Communication Prior (ECP) as the probability distribution in which the probability of an entity (e.g. a goal or environment) is proportional to the difculty of describing that entity, for a typical member of the community in question, using a particular set of communication mechanisms including the above ve modes. We will sometimes refer to the prior probability of an entity under this distribution, as its simplicity under the distribution. 1 In Appendix C we recount some interesting recent results showing that mirror neurons re in response to some cases of intentional communication as thus dened.",Engineering General  Intelligence Part 1,chapter 10
"     200 10 General Intelligence in the Everyday Human World Next, to further specialize the Embodied Communication Prior, we will assume that for each of these modes of communication, there are some aspects of the world that are much more easily communicable using that mode than the other modes. For instance, in the human everyday world:  Abstract (declarative) statements spanning large classes of situations are generally much easier to communicate linguistically  Complex, multi-part procedures are much easier to communicate either demonstratively, or using a combination of demonstration with other modes  Sensory or episodic data is often much easier to communicate demonstratively  The current value of attending to some portion of the shared environment is often much easier to communicate indicatively  Information about what goals to follow in a certain situation is often much easier to communicate intentionally, i.e. via explicitly indicating what ones own goal is These simple observations have signicant implications for the nature of the Embodied Communication Prior.",Engineering General  Intelligence Part 1,chapter 10
"For one thing they let us dene multiple forms of knowledge:  Isolatedly declarative knowledge is that which is much more easily communicable linguistically  Isolatedly procedural knowledge is that which is much more easily communicable demonstratively  Isolatedly sensory knowledge is that which is much more easily communicable depictively  Isolatedly attentive knowledge is that which is much more easily communicable indicatively  Isolatedly intentional knowledge is that which is much more easily communicable intentionally This categorization of knowledge types resembles many ideas from the cognitive theoryofmemory[TC05],althoughthedistinctionsdrawnherearealittlecrisperthan any classication currently derivable from available neurological or psychological data. Of course there may be much knowledge, of relevance to systems seeking intelligence according to the ECP, that does not fall into any of these categories and constitutes mixed knowledge. There are some very important specic subclasses of mixed knowledge.",Engineering General  Intelligence Part 1,chapter 10
"For instance, episodic knowledge (knowledge about specic real or hypothetical sets of events) will most easily be communicated via a combination of declarative, sensory and (in some cases) procedural communication. Scientic and mathematical knowledge are generally mixed knowledge, as is most everyday commonsense knowledge. Some cases of mixed knowledge are reasonably well decomposable, in the sense that they decompose into knowledge items that individually fall into some specic knowledge type. For instance, an experimental chemistry procedure may be much more easily communicable procedurally, whereas an allied piece of knowledge from theoretical chemistry may be much more easily communicable declaratively; but in      10.3 Embodied Communication 201 order to fully communicate either the experimental procedure or the abstract piece of knowledge, one may ultimately need to communicate both aspects. Also, even when the best way to communicate something is mixed-mode, it may be possible to identify one mode that poses the most important part of the communication.",Engineering General  Intelligence Part 1,chapter 10
"An example would be a chemistry experiment that is best communicated via a practical demonstration together with a running narrative. It may be that the demonstration without the narrative would be vastly more valuable than the narrative without the demonstration. To cover such cases we may make less restrictive denitions such as  Interactively declarative knowledge is that which is much more easily communicable in a manner dominated by linguistic communication and so forth. We call these interactive knowledge categories, by contrast to the isolated knowledge categories introduced earlier. 10.3.0.1 Naturalness of Knowledge Categories Next we introduce an assumption we call NKC, for Naturalness of Knowledge Categories. The NKC assumption states that the knowledge in each of the above isolated and interactive communication-modality-focused categories forms a natural category, in the sense that for each of these categories, there are many different properties shared by a large percentage of the knowledge in the category, but not by a large percentage of the knowledge in the other categories.",Engineering General  Intelligence Part 1,chapter 10
"This means that, for instance, procedural knowledge systematically (and statistically) has different characteristics than the other kinds of knowledge. The NKC assumption seems commonsensically to hold true for human everyday knowledge, and it has fairly dramatic implications for general intelligence. Suppose we conceive general intelligence as the ability to achieve goals in the environment shared by the communicating agents underlying the Embodied Communication Prior. Then, NKC suggests that the best way to achieve general intelligence according to the Embodied Communication Prior is going to involve  specialized methods for handling declarative, procedural, sensory and attentional knowledge (due to the naturalness of the isolated knowledge categories)  specialized methods for handling interactions between different types of knowledge, including methods focused on the case where one type of knowledge is primary and the others are supporting (the latter due to the naturalness of the interactive knowledge categories). 10.3.0.",Engineering General  Intelligence Part 1,chapter 10
"2 Cognitive Completeness Suppose we conceive an AI system as consisting of a set of learning capabilities, each one characterized by three features:      202 10 General Intelligence in the Everyday Human World  One or more knowledge types that it is competent to deal with, in the sense of the two key learning problems mentioned above  At least one learning type: either analysis, or synthesis, or both  At least one interaction type, for each (knowledge type, learning type) pair it handles: isolated (meaning it deals mainly with that knowledge type in isolation), or interactive (meaning it focuses on that knowledge type but in a way that explicitly incorporates other knowledge types into its process), or fully mixed (meaning that when it deals with the knowledge type in question, no particular knowledge type tends to dominate the learning process).",Engineering General  Intelligence Part 1,chapter 10
"Then,intuitively,itseemstofollowfromtheECPwithNKCthatsystemswithhigh efcient general intelligence should have the following properties, which collectively well call cognitive completeness:  For each (knowledge type, learning type, interaction type) triple, there should be a learning capability corresponding to that triple.  Furthermore the capabilities corresponding to different (knowledge type, interaction type) pairs should have distinct characteristics (since according to the NKC the isolated knowledge corresponding to a knowledge type is a natural category, as is the dominant knowledge corresponding to a knowledge type)  For each (knowledge type, learning type) pair (K, L), and each other knowledge type K1 distinct from K, there should be a distinctive capability with interaction type interactive and dealing with knowledge that is interactively K but also includes aspects of K1 Furthermore, it seems intuitively sensible that according to the ECP with NKC, if the capabilities mentioned in the above points are reasonably able, then the",Engineering General  Intelligence Part 1,chapter 10
"system possessing the capabilities will display general intelligence relative to the ECP. Thus we arrive at the hypothesis that Under the assumption of the Embodied Communication Prior (with the Natural Knowledge Categories assumption), the property above called cognitive completeness is necessary and sufcient for efcient general intelligence at the level of an inteligent adult human (e.g. at the Piagetan formal level [Pia53]). Of course, the above considerations are very far from a rigorous mathematical proof (or even precise formulation) of this hypothesis. But we are presenting this here as a conceptual hypothesis, in order to qualitatively guide our practical AGI R&D and also to motivate further, more rigorous theoretical work. 10.3.",Engineering General  Intelligence Part 1,chapter 10
"1 Generalizing the Embodied Communication Prior One interesting direction for further research would be to broaden the scope of the inquiry, in a manner suggested above: instead of just looking at the ECP, look at simplicity measures in general, and attack the question of how a mind must be structured in order to display efcient general intelligence relative to a specied      10.3 Embodied Communication 203 simplicity measure. This problem seems unapproachable in general, but some special cases may be more tractable.",Engineering General  Intelligence Part 1,chapter 10
"For instance, suppose one has  a simplicity measure that (like the ECP) is approximately decomposable into a set of fairly distinct components, plus their interactions  an assumption similar to NKC, which states that the entities displaying simplicity according to each of the distinct components, are roughly clustered together in entity-space Thenoneshouldbeabletosaythat,toachieveefcientgeneralintelligencerelative to this decomposable simplicity measure, a system should have distinct capabilities corresponding to each of the components of the simplicity measure interactions between these capabilities, corresponding to the interaction terms in the simplicity measure. With copious additional work, these simple observations could potentially serve as the seed for a novel sort of theory of general intelligencea theory of how the structure of a system depends on the structure of the simplicity measure with which it achieves efcient general intelligence. Cognitive Synergy Theory would then emerge as a special case of this more abstract theory. 10.",Engineering General  Intelligence Part 1,chapter 10
"4 Naive Physics Multimodal communication is an important aspect of the environment for which human intelligence evolvedbut not the only one. It seems likely that our human intelligence is also closely adapted to various aspects of our physical environmenta matter that is worth carefully attending as we design environments for our robotically or virtually embodied AGI systems to operate in. One interesting guide to the most cognitively relevant aspects of human environments is the subeld of AI known as naive physics [Hay85]a term that refers to the theories about the physical world that human beings implicitly develop and utilize during their lives. For instance, when you gure out that you need to pressure the knife slightly harder when spreading peanut butter rather than jelly, youre not making this judgment using Newtonian physics or the Navier-Stokes equations of uid dynamics; youre using heuristic patterns that you gured out through experience. Maybe you gured out these patterns through experience spreading peanut butter and jelly in particular.",Engineering General  Intelligence Part 1,chapter 10
"Or maybe you gured these heuristic patterns out before you ever tried to spread peanut butter or jelly specically, via just touching peanut butter and jelly to see what they feel like, and then carrying out inference based on your experience manipulating similar tools in the context of similar substances. Other examples of similar naive physics patterns are easy to come by, e.g. 1. What goes up must come down. 2. A dropped object falls straight down.      204 10 General Intelligence in the Everyday Human World 3. A vacuum sucks things towards it. 4. Centrifugal force throws rotating things outwards. 5. An object is either at rest or moving, in an absolute sense. 6. Two events are simultaneous or they are not. 7. When running downhill, one must lift ones knees up high. 8. When looking at something that you just barely cant discern accurately, squint.",Engineering General  Intelligence Part 1,chapter 10
"Attempts to axiomatically formulate naive physics have historically come up short, and we doubt this is a promising direction for AGI. However, we do think the naive physics literature does a good job of identifying the various phenomena that the human minds naive physics deals with. So, from the point of view of AGI environment design, naive physics is a useful source of requirements. Ideally, we would like an AGIs environment to support all the fundamental phenomena that naive physics deals with. We now describe some key aspects of naive physics in a more systematic manner. Naive physics has many different formulations; in this section we draw heavily on [SC94], who divide naive physics phenomena into 5 categories. Here we review these categories and identify a number of important things that humanlike intelligent agents must be able to do relative to each of them. 10.4.",Engineering General  Intelligence Part 1,chapter 10
"1 Objects, Natural Units and Natural Kinds One key aspect of naive physics involves recognition of various aspects of objects, such as: 1. Recognition of objects amidst noisy perceptual data 2. Recognition of surfaces and interiors of objects 3. Recognition of objects as manipulable units 4. Recognition of objects as potential subjects of fragmentation (splitting, cutting) and of unication (gluing, bonding) 5. Recognition of the agents body as an object, and as parts of the agents body as objects 6. Division of universe of perceived objects into natural kinds, each containing typical and atypical instances. 10.4.2 Events, Processes and Causality Specic aspects of naive physics related to temporality and causality are: 1. Distinguishing roughly-subjectively-instantaneous events from extended processes      10.4 Naive Physics 205 2. Identifying beginnings, endings and crossings of processes 3.",Engineering General  Intelligence Part 1,chapter 10
"Identifying and distinguishing internal and external changes 4. Identifying and distinguishing internal and external changes relative to ones own body 5. Interrelating body-changes with changes in external entities Notably, these aspects of naive physics involve a different processes occurring on a variety of different time scales, intersecting in complex patterns, and involving processes inside the agents body, outside the agents body, and crossing the boundary of the agents body. 10.4.3 Stuffs, States of Matter, Qualities Regarding the various states of matter, some important aspects of naive physics are: 1. Perceiving gaps between objects: holes, media, illusions like rainbows, mirages and holograms 2. Distinguishingthemannersinwhichdifferentsortsofentities(e.g.smells,sounds, light) ll space 3. Distinguishing properties such as smoothness, roughness, graininess, stickiness, runniness, etc. 4.",Engineering General  Intelligence Part 1,chapter 10
"Distinguishing degrees of elasticity and fragility 5. Assessing separability of aggregates. 10.4.4 Surfaces, Limits, Boundaries, Media Gibson [Gib77, Gib79] has argued that naive physics is not mainly about objects but rather mainly about surfaces. Surfaces have a variety of aspects and relationships that are important for naive physics, such as: 1. Perceiving and reasoning about surfaces as two-sided or one-sided interfaces 2. Inference of the various ecological laws of surfaces 3. Perception of various media in the world as separated by surfaces 4. Recognition of the textures of surfaces 5. Recognition of medium/surface layout relationships such as: ground, open environment, enclosure, detached object, attached object, hollow object, place, sheet, ssure, stick, bre, dihedral, etc.",Engineering General  Intelligence Part 1,chapter 10
"As a concrete, evocative toy example of naive everyday knowledge about surfaces and boundaries, consider Slomans [Slo08a] example scenario, depicted in Fig.10.1 and drawn largely from [SS74] (see also related discussion in [Slo08b], in which A child can be given one or more rubber bands and a pile of pins, and asked      206 10 General Intelligence in the Everyday Human World Fig. 10.1 One of Slomans example test domains for real-world inference. Left a number of pins and a rubber band to be stretched around them. Right use of the pins and rubber band to make a letter T to use the pins to hold the band in place to form a particular shape)... For example, things to be learnt could include: 1. There is an area inside the band and an area outside the band. 2.",Engineering General  Intelligence Part 1,chapter 10
"The possible effects of moving a pin that is inside the band towards or further away from other pins inside the band. (The effects can depend on whether the band is already stretched.) 3. The possible effects of moving a pin that is outside the band towards or further away from other pins inside the band. 4. The possible effects of adding a new pin, inside or outside the band, with or without pushing the band sideways with the pin rst. 5. The possible effects of removing a pin, from a position inside or outside the band. 6. Patterns of motion/change that can occur and how they affect local and global shape (e.g. introducing a concavity or convexity, introducing or removing symmetry, increasing or decreasing the area enclosed). 7. The possibility of causing the band to cross over itself. (NB: Is an odd number of crosses possible?) 8.",Engineering General  Intelligence Part 1,chapter 10
"How adding a second, or third band can enrich the space of structures, processes and effects of processes. 10.4.5 What Kind of Physics is Needed to Foster Human-Like Intelligence? We stated above that we would like an AGIs environment to support all the fundamental phenomena that naive physics deals with; and we have now reviewed a number of these specic phenomena. But its not entirely clear what the fundamental aspects underlying these phenomena are. One important question in the environment-design context is how close an AGI environment needs to stick to the particulars of real-world naive physics. Is it important that a young AGI can play with the specic differences between spreading peanut butter versus jelly? Or is it enough that it can play with spreading and smearing various substances of different      10.",Engineering General  Intelligence Part 1,chapter 10
"4 Naive Physics 207 consistencies? How close does the analogy between an AGI environments naive physics and real-world naive physics need to be? This is a question to which we have no scientic answer at present. Our own working hypothesis is that the analogy does not need to be extremely close, and with this in mind in Chap. 17 we propose a virtual environment BlocksNBeadsWorld that encompasses all the basic conceptual phenomena of real-world naive physics, but does not attempt to emulate their details. Framed in terms of human psychology rather than environment design, the question becomes: At what level of detail must one model the physical world to understand the ways in which human intelligence has adapted to the physical world?.",Engineering General  Intelligence Part 1,chapter 10
"Our suspicion, which underlies our BlocksNBeadsWorld design, is that its approximately enough to have  Newtonian physics, or some close approximation  Matter in multiple phases and forms vaguely similar to the ones we see in the real world: solid, liquid, gas, paste, go, etc.  Ability to transform some instances of matter from one form to another  Ability to exibly manipulate matter in various forms with various solid tools  Ability to combine instances of matter into new ones in a fairly rich way: e.g. glue or tie solids togethermix liquids together, etc.  Ability to position instances of matter with respect to each other in a rich way: e.g. put liquid in a solid cavity, cover something with a lid or a piece of fabric, etc. It seems to us that if the above are present in an environment, then an AGI seeking to achieve appropriate goals in that environment will be likely to form an appropriate human-like physical-world intuition.",Engineering General  Intelligence Part 1,chapter 10
"We doubt that the specics of the naive physics of different forms of matter are critical to human-like intelligence. But, we suspect that a great amount of unconscious human metaphorical thinking is conditioned on the fact that humans evolved around matter that takes a variety of forms, can be changed from one form to another, and can be fairly easily arranged and composited to form new instances from prior ones. Without many diverse instances of matter transformation, arrangement and composition in its experience, an AGI is unlikely to form an internal metaphor-base even vaguely similar to the human oneso that, even if its highly intelligent, its thinking will be radically non-human-like in character. Naturally this is all somewhat speculative and must be explored via experimentation. Maybe an elaborate blocks-world with only solid objects will be sufcient to create human-level, roughly human-like AGI with rich spatiotemporal and manipulative intuition.",Engineering General  Intelligence Part 1,chapter 10
"Or maybe human intelligence is more closely adapted to the specics of our physical worldwith water and dirt and plants and hair and so forththan we currently realize. One thing that is very clear is that, as we proceed with embodying, situating and educating our AGI systems, we need to pay careful attention to the way their intelligence is conditioned by their environment.      208 10 General Intelligence in the Everyday Human World 10.5 Folk Psychology Related to naive physics is the notion of naive psychology or folk psychology [Rav04], which includes for instance the following aspects: 1. Mental simulation of other agents 2. Mental theory regarding other agents 3. Attribution of beliefs, desires and intentions (BDI) to other agents via theory or simulation 4. Recognition of emotions in other agents via their physical embodiment 5. Recognition of desires and intentions in other agents via their physical embodiment 6.",Engineering General  Intelligence Part 1,chapter 10
"Analogical and contextual inferences between self and other, regarding BDI and other aspects 7. Attribute causes and meanings to other agents behaviors 8. Anthropomorphize non-human, including inanimate objects The main special requirement placed on an AGIs embodiment by the above aspects pertains to the ability of agents to express their emotions and intentions to each other. Humans do this via facial expressions, gestures and language. 10.5.1 Motivation, Requiredness, Value Relatedly to folk psychology, Gestalt [Koh38] and ecological [Gib77, Gib79] psychology suggest that humans perceive the world substantially in terms of the affordances it provides them for goal-directed action. This suggests that, to support human-like intelligence, an AGI must be capable of: 1. Perception of entities in the world as differentially associated with goal-relevant value 2.",Engineering General  Intelligence Part 1,chapter 10
"Perception of entities in the world in terms of the potential actions they afford the agent, or other agents The key point is that entities in the world need to provide a wide variety of ways for agents to interact with them, enabling richly complex perception of affordances. 10.6 Body and Mind The above discussion has focused on the world external to the body of the AGI agent embodied and embedded in the world, but the issue of the AGIs body also merits consideration. There seems little doubt that a humans intelligence is highly conditioned by the particularities of the human body.      10.6 Body and Mind 209 10.6.1 The Human Sensorium Here the requirements seem fairly simple: while surely not strictly necessary, it would certainly be preferable to provide an AGI with fairly rich analogues of the human senses of touch, sight, sound, kinesthesia, taste and smell.",Engineering General  Intelligence Part 1,chapter 10
"Each of these senses provides different sorts of cognitive stimulation to the human mind; and while similar cognitive stimulation could doubtless be achieved without analogous senses, the provision of such seems the most straightforward approach. Its hard to know how much of human intelligence is specically biased to the sorts of outputs provided by human senses. As vision already is accorded such a prominent role in the AI and cognitive science literatureand is discussed in moderate depth in Chap.9 of Part 2, we wont take time elaborating on the importance of vision processing for humanlike cognition. The key thing an AGI requires to support humanlike visual intelligence is an environment containing a sufciently robust collection of materials that object and event recognition and identication become interesting problems Audition is cognitively valuable for many reasons, one of which is that it gives a very rich and precise method of sensing the world that is different from vision.",Engineering General  Intelligence Part 1,chapter 10
"The fact that humans can display normal intelligence while totally blind or totally deaf is an indication that, in a sense, vision and audition are redundant for understanding the everyday world. However, it may be important that the brain has evolved to account for both of these senses, because this forced it to account for the presence of two very rich and precise methods of sensing the worldwhich may have forced it to develop more abstract representation mechanisms than would have been necessary with only one such method. Touch is a sense that is, in our view, generally badly underappreciated within the AI community. In particular the cognitive robotics community seems to worry too little about the terribly impoverished sense of touch possessed by most current robots (though fortunately there are recent technologies that may help improve robots in this regard; see e.g. [Nan08]). Touch is how the human infant learns to distinguish self from other, and in this way it is the most essential sense for the establishment of an internal self-model.",Engineering General  Intelligence Part 1,chapter 10
"Touching others bodies is a key method for developing a sense of the emotional reality and responsiveness of others, and is hence key to the development of theory of mind and social understanding in humans. For this reason, among others, human children lacking sufcient tactile stimulation will generally wind up badly impaired in multiple ways. A good-quality embodiment should supply an AI agent with a body that possesses skin, which has varying levels of sensitivity on different parts of the skin (so that it can effectively distinguish between reality and its perception thereof in a tactile context); and also varying types of touch sensors (e.g. temperature versus friction), so that it experiences textures as multidimensional entities. Related to touch, kinesthesia refers to direct sensation of phenomena happening inside the body. Rarely mentioned in AI, this sense seems quite critical to cognition, as it underpins many of the analogies between self and other that guide cognition.",Engineering General  Intelligence Part 1,chapter 10
"     210 10 General Intelligence in the Everyday Human World Again, its not important that an AGIs virtual body have the same internal body parts as a human body. But it seems valuable to have the AGIs virtual body display some vaguely human-body-like properties, such as feeling internal strain of various sorts after getting exercise, feeling discomfort in certain places when running out of energy, feeling internally different when satised versus unsatised, etc. Next,tasteisacognitivelyinterestingsenseinthatitinvolvestheinterplaybetween the internal and external world; it involves the evaluation of which entities from the external world are worthy of placing inside the body. And smell is cognitively interesting in large part because of its relationship with taste. A smell is, among other things, a long-distance indicator of what a certain entity might taste like. So, the combination of taste and smell provides means for conceptualizing relationships between self, world and distance. 10.6.",Engineering General  Intelligence Part 1,chapter 10
"2 The Human Bodys Multiple Intelligences While most unique aspect of human intelligence is rooted in what one might call the cognitive cortexthe portions of the brain dealing with self-reection and abstract thought. But the cognitive cortex does its work in close coordination with the bodys various more specialized intelligent subsystems, including those associated with the gut, the heart, the liver, the immune and endocrine systems, and the perceptual and motor cortices. In the perspective underlying this book, the human cognitive cortexor the core cognitive network of any roughly human-like AGI systemshould be viewed as a highly exible, self-organizing network. These cognitive networks are modelable e.g. as a recurrent neural net with general topology, or a weighted labeled hypergraph, and are centrally concerned with recognizing patterns in its environment and itself, especially patterns regarding the achievement of the systems goals in various appropriate contexts.",Engineering General  Intelligence Part 1,chapter 10
"Here we augment this perspective, noting that the human brains cognitive network is closely coupled with a variety of simpler and more specialized intelligent body-system networks which provide it with structural and dynamical inductive biasing. We then discuss the implications of this observation for practical AGI design. One recalls Pascals famous quote The heart has its reasons, of which reason knows not. As we now know, the intuitive sense that Pascal and so many others have expressed, that the heart and other body systems have their own reasons, is grounded in the fact that they actually do carry out simple forms of reasoning (i.e. intelligent, adaptive dynamics), in close, sometimes cognitively valuable, coordination with the central cognitive network. 10.6.2.1 Some of the Human Bodys Specialized Intelligent Subsystems The human body contains multiple specialized intelligences apart from the cognitive cortex. Here we review some of the most critical.      10.",Engineering General  Intelligence Part 1,chapter 10
"6 Body and Mind 211 Hierarchies of Visual and Auditory Perception. The hierarchical structure of visual and auditory cortex has been taken by some researchers [Kur12], [HB06] as the generic structure of cognition. While we suspect this is overstated, we agree it is important that these cortices nudge large portions of the cognitive cortex to assume an approximately hierarchical structure. Olfactory Attractors. The process of recognizing a familiar smell is grounded in a neural process similar to convergence to an attractor in a nonlinear dynamical system [Fre95]. There is evidence that the mammalian cognitive cortex evolved in close coordination with the olfactory cortex [Row11], and much of abstract cognition reects a similar dynamic of gradually coming to a conclusion based on what initially smells right. Physical and Cognitive Action.",Engineering General  Intelligence Part 1,chapter 10
"The cerebellum, a specially structured brain subsystem which controls motor movements, has for some time been understood to also have involvement in attention, executive control, language, working memory, learning, pain, emotion, and addiction [PSF09]. The Second Brain. The gastrointestinal neural net contains millions of neurons and is capable of operating independently of the brain. It modulates stress response and other aspects of emotion and motivation based on experienceresulting in so-called gut feelings [Ger99]. The Hearts Neural Network. The heart has its own neural network, which modulates stress response, energy level and relaxation/excitement (factors key to motivation and emotion) based on experience [Arm04]. Pattern Recognition and Memory in the Liver. The liver is a complex pattern recognition system, adapting via experience to better identify toxins [CB06].",Engineering General  Intelligence Part 1,chapter 10
"Like the heart, it seems to store some episodic memories as well, resulting in liver transplant recipients sometimes acquiring the tastes in music or sports of the donor [EMC12]. Immune Intelligence. The immune network is a highly complex, adaptive self-organizing system, which ongoingly solves the learning problem of identifying antigens and distinguishing them from the body system [FP86]. As immune function is highly energetically costly, stress response involves subtle modulation of the energy allocation to immune function, which involves communication between neural and immune networks.      212 10 General Intelligence in the Everyday Human World The Endocrine System: A Key Bridge Between Mind and Body. The endocrine (hormonal) system regulates (and is related by) emotion, thus guiding all aspects of intelligence (due to the close connection of emotion and motivation) [PH12]. Breathing Guides Thinking.",Engineering General  Intelligence Part 1,chapter 10
"As oxygenation of the brain plays a key role in the spread of neural activity, the ow of breath is a key driver of cognition. Forced alternate nostril breathing has been shown to signicantly affect cognition via balancing activity of the two brain hemispheres [SKBB91]. Much remains unknown, and the totality of feedback loops between the human cognitive cortex and the various specialized intelligences operative throughout the human body, has not yet been thoroughly charted. 10.6.2.2 Implications for AGI What lesson should the AGI developer draw from all this? The particularities of the human mind/body should not be taken as general requirements for general intelligence. However, it is worth remembering just how difcult is the computational problem of learning, based on experiential feedback alone, the right way to achieve the complex goal of controlling a system with general intelligence at the human level or beyond.",Engineering General  Intelligence Part 1,chapter 10
"To solve this problem without some sort of strong inductive biasing may require massively more experience than young humans obtain. Appropriate inductive bias may be embedded in an AGI system in many different ways. Some AGI designers have sought to embed it very explicitly, e.g. with handcoded declarative knowledge as in Cyc, SOAR and other GOFAI type systems. On the other hand, the human brain receives its inductive bias much more subtly and implicitly, both via the specics of the initial structure of the cognitive cortex, and via ongoing coupling of the cognitive cortex with other systems possessing more focused types of intelligence and more specic structures and/or dynamics. In building an AGI system, one has four choices, very broadly speaking: 1. Create a exible mind-network, as unbiased as feasible, and attempt to have it learn how to achieve its goals via experience 2. Closely emulate key aspects of the human body along with the human mind 3.",Engineering General  Intelligence Part 1,chapter 10
"Imitate the human mind-body, conceptually if not in detail, and create a number of structurally and dynamically simpler intelligent systems closely and appropriately coupled to the abstract cognitive mind-network, provide useful inductive bias. 4. Find some other, creative way to guide and probabilistically constrain ones AGI systems mind-network, providing inductive bias appropriate to the tasks at hand, without emulating even conceptually the way the human mind-brain receives its inductive bias via coupling with simpler intelligent systems. Our suspicion is that the rst option will not be viable. On the other hand, to do the second option would require more knowledge of the human body than biology      10.6 Body and Mind 213 currently possesses. This leaves the third and fourth options, both of which seem viable to us. CogPrime incorporates a combination of the third and fourth options.",Engineering General  Intelligence Part 1,chapter 10
"CogPrimes generic dynamic knowledge store, the Atomspace, is coupled with specialized hierarchical networks (DeSTIN) for vision and audition, somewhat mirroring the human cortex. An articial endocrine system for OpenCog is also under development, speculatively, as part of a project using OpenCog to control humanoid robots. On the other hand, OpenCog has no gastrointestinal nor cardiological nervous system, and the stress-response-based guidance provided to the human brain by a combination of the heart, gut, immune system and other body systems, is achieved in CogPrime in a more explicit way using the OpenPsi model of motivated cognition, and its integration with the systems attention allocation dynamics. Likelythereis nosinglecorrect waytoincorporatethelessons of intelligent human body-system networks into AGI designs. But these are aspects of human cognition that all AGI researchers should be aware of. 10.",Engineering General  Intelligence Part 1,chapter 10
"7 The Extended Mind and Body Finally, Hutchins [Hut95], Logan [Log07] and others have promoted a view of human intelligence that views the human mind as extended beyond the individual body, incorporating social interactions and also interactions with inanimate objects, such as tools, plants and animals. This leads to a number of requirements for a humanlike AGIs environment: 1. The ability to create a variety of different tools for interacting with various aspects of the world in various different ways, including tools for making tools and ultimately machinery 2. The existence of other mobile, virtual life-forms in the world, including simpler and less intelligent ones, and ones that interact with each other and with the AGI 3.",Engineering General  Intelligence Part 1,chapter 10
"The existence of organic growing structures in the world, with which the AGI can interact in various ways, including halting their growth or modifying their growth pattern How necessary these requirements are is hard to saybut it is clear that these things have played a major role in the evolution of human intelligence. 10.8 Conclusion Happily, this diverse chapter supports a simple, albeit tentative conclusion.",Engineering General  Intelligence Part 1,chapter 10
"Our suggestion is that, if an AGI is  placed in an environment capable of roughly supporting multimodal communication and vaguely (but not necessarily precisely) real-world-ish naive physics      214 10 General Intelligence in the Everyday Human World  surrounded with other intelligent agents of varying levels of complexity, and other complex, dynamic structures to interface with  given a body that can perceive this environment through some forms of sight, sound and touch; and perceive itself via some form of kinesthesia  given a motivational system that encourages it to make rich use of these aspects of its environment then the AGI is likely to have an experience-base reinforcing the key inductive biases provided by the everyday world for the guidance of humanlike intelligence.",Engineering General  Intelligence Part 1,chapter 10
"  Chapter 11 A Mind-World Correspondence Principle 11.1 Introduction Real-world minds are always adapted to certain classes of environments and goals. The ideas of the previous chapter, regarding the connection between a human-like intelligences internals and its environment, result from exploring the implications of this adaptation in the context of the cognitive synergy concept. In this chapter we explore the mind-world connection in a broader and more abstract waymaking a more ambitious attempt to move toward a general theory of general intelligence. One basic premise here, as in the preceding chapters is: Even a system of vast general intelligence, subject to real-world space and time constraints, will necessarily be more efcient at some kinds of learning than others. Thus, one approach to formulating a general theory of general intelligence is to look at the relationship between minds and worldswhere a world is conceived as an environment and a set of goals dened in terms of that environment.",Engineering General  Intelligence Part 1,chapter 11
"In this spirit, we here formulate a broad principle binding together worlds and the minds that are intelligent in these worlds. The ideas of the previous chapter constitute specic, concrete instantiations of this general principle. A careful statement of the principle requires introduction of a number of technical concepts, and will be given later on in the chapter. A crude, informal version of the principle would be: MIND-WORLD CORRESPONDENCE-PRINCIPLE For a mind to work intelligently toward certain goals in a certain world, there should be a nice mapping from goal-directed sequences of world-states into sequences of mindstates, where nice means that a world-state-sequence W composed of two parts W1 and W2, gets mapped into a mind-state-sequence M composed of two corresponding parts M1 and M2. B. Goertzel et al., Engineering General Intelligence, Part 1, 215 Atlantis Thinking Machines 5, DOI: 10.",Engineering General  Intelligence Part 1,chapter 11
"2991/978-94-6239-027-0_11,  Atlantis Press and the authors 2014      216 11 A Mind-World Correspondence Principle Whats nice about this principle is that it relates the decomposition of the world into parts, to the decomposition of the mind into parts. 11.2 What Might a General Theory of General Intelligence Look Like? Its not clear, at this point, what a real general theory of general intelligence would look likebut one tantalizing possibility is that it might confront the two questions:  How does one design a world to foster the development of a certain sort of mind?  How does one design a mind to match the particular challenges posed by a certain sort of world? One way to achieve this would be to create a theory that, given a description of an environment and some associated goals, would output a description of the structure and dynamics that a system should possess to be intelligent in that environment relative to those goals, using limited",Engineering General  Intelligence Part 1,chapter 11
"computational resources. Such a theory would serve a different purpose from the mathematical theory of universal intelligence developed by Marcus Hutter [Hut05] and others. For all its beauty and theoretical power, that approach currently gives it useful conclusions only about general intelligences with innite or infeasibly massive computational resources. On the other hand, the approach suggested here is aimed toward creation of a theory of real-world general intelligences utilizing realistic amounts of computational power, but still possessing general intelligence comparable to human beings or greater. This reects a vision of intelligence as largely concerned with adaptation to particular classes of environments and goals. This may seem contradictory to the notion of general intelligence, but I think it actually embodies a realistic understanding of general intelligence. Maximally general intelligence is not pragmatically feasible; it could only be achieved using innite computational resources [Hut05].",Engineering General  Intelligence Part 1,chapter 11
"Real-world systems are inevitably limited in the intelligence they can display in any real situation, because real situations involve nite resources, including nite amounts of time. One may say that, in principle, a certain system could solve any problem given enough resources and time but, even when this is true, its not necessarily the most interesting way to look at the systems intelligence. It may be more important to look at what a system can do given the resources at its disposal in reality. And this perspective leads one to ask questions like the ones posed above: which bounded-resources systems are well-disposed to display intelligence in which classes of situations? As noted in Chap. 8, one can assess the generality of a systems intelligence via looking at the entropy of the class of situations across which it displays a high level of intelligence (where high is measured relative to its total level of intelligence across all situations).",Engineering General  Intelligence Part 1,chapter 11
"A system with a high generality of intelligence will tend to be roughly equally intelligent across a wide variety of situations; whereas a system      11.2 What Might a General Theory of General Intelligence Look Like? 217 with lower generality of intelligence will tend to be much more intelligent in a small subclass of situations, than in any other. The denitions given above embody this notion in a formal and quantitative way. Ifonewishestocreateageneraltheoryofgeneralintelligenceaccordingtothissort of perspective, the main question then becomes how to represent goals/environments and systems in such a way as to render transparent the natural correspondence between the specics of the former and the latter, in the context of resource-bounded intelligence. This is the business of the next section. 11.3 Steps Toward A (Formal) General Theory of General Intelligence Now begins the formalism.",Engineering General  Intelligence Part 1,chapter 11
"At this stage of development of the theory proposed in this chapter, mathematics is used mainly as a device to ensure clarity of expression. However, once the theory is further developed, it may possibly become useful for purposes of calculation as well. Suppose one has any system S (which could be an AI system, or a human, or an environment that a human or AI is interacting with, or the combination of an environment and a human or AIs body, etc.).",Engineering General  Intelligence Part 1,chapter 11
"One may then construct an uncertain transition graph associated with that system S, in the following way:  The nodes of the graph represent fuzzy sets of states of system S (Ill call these state-sets from here on, leaving the fuzziness implicit)  The (directed) links of the graph represent probabilistically weighted transitions between state-sets Specically, the weight of the link from B to A should be dened as P(o(S, A, t(T))|o(S, B, T)) where o(S, A, T) denotes the presence of the system S in the state-set A during time-distribution T, and t() is a temporal succession function dened so that t(T) refers to a time-distribution conceived as after T. A time-distribution is a probability distribution over timepoints. The interaction of fuzziness and probability here is fairly straightforward and may be handled in the manner of PLN, as outlined in subsequent chapters.",Engineering General  Intelligence Part 1,chapter 11
"Note that the denition of link weights is dependent on the specic implementation of the temporal succession function, which includes an implicit time-scale. Suppose one has a transition graph corresponding to an environment; then a goal relative to that environment may be dened as a particular node in the transition graph. The goals of a particular system acting in that environment may then be conceived as one or more nodes in the transition graph. The systems situation in the environment at any point in time may also be associated with one or more nodes      218 11 A Mind-World Correspondence Principle in the transition graph; then, the systems movement toward goal-achievement may be associated with paths through the environments transition graph leading from its current state to goal states. It may be useful for some purposes to lter the uncertain transition graph into a crisp transition graph by placing a threshold on the link weights, and removing links with weights below the threshold.",Engineering General  Intelligence Part 1,chapter 11
"The next concept to introduce is the world-mind transfer function, which maps world (environment) state-sets into organism (e.g. AI system) state-sets in a specic way. Given a world state-set W, the world-mind transfer function M maps W into various organism state-sets with various probabilities, so that we may say: M(W) is the probability distribution of state-sets the organism tends to be in, when its environment is in state-set W. (Recall also that state-sets are fuzzy.) Now one may look at the spaces of world-paths and mind-paths. A world-path is a path through the worlds transition graph, and a mind-path is a path through the organisms transition graph. Given two world-paths P and Q, its obvious how to dene the composition P  Q one follows P and then, after that, follows Q, thus obtaining a longer path. Similarly for mind-paths.",Engineering General  Intelligence Part 1,chapter 11
"In category theory terms, we are constructing the free category associated with the graph: the objects of the category are the nodes, and the morphisms of the category are the paths. And category theory is the right way to be thinking here we want to be thinking about the relationship between the world category and the mind category. The world-mind transfer function can be interpreted as a mapping from paths to subgraphs: Given a world-path, it produces a set of mind state-sets, which have a number of links between them. One can then dene a world-mind path transfer function M(P) via taking the mind-graph M(nodes(P)), and looking at the highestweight path spanning M(nodes(P)). (Here nodes(P) obviously means the set of nodes of the path P.",Engineering General  Intelligence Part 1,chapter 11
"A functor F between the world category and the mind category is a mapping that preserves object identities and so that F(P  Q) = F(P)  F(Q) We may also introduce the notion of an approximate functor, meaning a mapping F so that the average of d(F(P  Q), F(P)  F(Q)) is small. One can introduce a prior distribution into the average here. This could be the Levin universal distribution or some variant (the Levin distribution assigns higher probability to computationally simpler entities). Or it could be something more purpose specic: for example, one can give a higher weight to paths leading toward a certain set of nodes (e.g. goal nodes). Or one can use a distribution that weights based on a combination of simplicity and directedness toward a certain set of nodes.",Engineering General  Intelligence Part 1,chapter 11
"The latter seems most interesting, and I will dene a goal-weighted approximate functor as an approximate functor, dened with averaging relative to a distribution that balances simplicity with directedness toward a certain set of goal nodes.      11.3 Steps Toward A (Formal) General Theory of General Intelligence 219 The move to approximate functors is simple conceptually, but mathematically its a fairly big step, because it requires us to introduce a geometric structure on our categories. But there are plenty of natural metrics dened on paths in graphs (weighted or not), so theres no real problem here. 11.4 The Mind-World Correspondence Principle Now we nally have the formalism set up to make a non-trivial statement about the relationship between minds and worlds.",Engineering General  Intelligence Part 1,chapter 11
"Namely, the hypothesis that: MIND-WORLD CORRESPONDENCE PRINCIPLE For an organism with a reasonably high level of intelligence in a certain world, relative to a certain set of goals, the mind-world path transfer function is a goal-weighted approximate functor. That is, a little more loosely: the hypothesis is that, for intelligence to occur, there has to be a natural correspondence between the transition-sequences of world-states and the corresponding transition-sequences of mind-states, at least in the cases of transition-sequences leading to relevant goals. We suspect that a variant of the above proposition can be formally proved, using the denition of general intelligence presented in Chap. 8. The proof of a theorem corresponding to the above would certainly constitute an interesting start toward a general formal theory of general intelligence. Note that proving anything of this nature would require some attention to the time-scale-dependence of the link weights in the transition graphs involved.",Engineering General  Intelligence Part 1,chapter 11
"A formally proved variant of the above proposition would be in short, a MINDWORLD CORRESPONDENCE THEOREM. Recall that at the start of the chapter, we expressed the same idea as: MIND-WORLD CORRESPONDENCE-PRINCIPLE For a mind to work intelligently toward certain goals in a certain world, there should be a nice mapping from goal-directed sequences of world-states into sequences of mindstates, where nice means that a world-state-sequence W composed of two parts W1 and W2, gets mapped into a mind-state-sequence M composed of two corresponding parts M1 and M2. That is a reasonable gloss of the principle, but its clunkier and less accurate, than the statement in terms of functors and path transfer functions, because it tries to use only common-language vocabulary, which doesnt really contain all the needed concepts.      220 11 A Mind-World Correspondence Principle 11.",Engineering General  Intelligence Part 1,chapter 11
"5 How Might the Mind-World Correspondence Principle Be Useful? Suppose one believes the Mind-World Correspondence Principle as laid out above so what? Our hope, obviously, is that the principle could be useful in actually guring out how to architect intelligent systems biased toward particular sorts of environment. And of course, this is said with the understanding that any nite intelligence must be biased toward some sorts of environment. Relatedly, given a specic AGI design (such as CogPrime), one could use the principle to gure out which environments it would be best suited for. Or one could gure out how to adjust the particulars of the design, to maximize the systems intelligence in the environments of interest. One next step in developing this network of ideas, aside from (and potentially building on) full formalization of the principle, would be an exploration of realworld environments in terms of transition graphs.",Engineering General  Intelligence Part 1,chapter 11
"What properties do the transition graphs induced from the real world have? One such property, we suggest, is successive renement. Often the path toward a goal involves rst gaining an approximate understanding of a situation, then a slightly more accurate understanding, and so forthuntil nally one has achieved a detailed enough understanding to actually achieve the goal. This would be represented by a world-path whose nodes are state-sets involving the gathering of progressively more detailed information. Via pursuing the mind-world correspondence property in this context, I believe we will nd that world-paths reecting successive renement correspond to mind-paths embodying successive renement. This will be found to relate to the hierarchical structures found so frequently in both the physical world and the human mind-brain. Hierarchical structures allow many relevant goals to be approached via successive renement, which I believe is the ultimate reason why hierarchical structures are so common in the human mind-brain.",Engineering General  Intelligence Part 1,chapter 11
"Another next step would be exploring what mind-world correspondence means for the structure and dynamics of a limited-resources intelligence. If an organism O has limited resources and, to be intelligent, needs to make P(o(O, M(A), t(T))|o(O, M(B), T)) high for particular world state-sets A and B, then whats the organisms best approach? Arguably, it should represent M(A) and M(B) internally in such a way that very little computational effort is required for it to transition between M(A) and M(B). For instance, this could be done by coding its knowledge in such a way that M(A) and M(B) share many common bits; or it could be done in other more complicated ways. If, for instance, A is a subset of B, then it may prove benecial for the organism to represent M(A) physically as a subset of its representation of M(B).      10.",Engineering General  Intelligence Part 1,chapter 11
"5 How Might the Mind-World Correspondence Principle Be Useful? 221 Pursuing this line of thinking, one could likely derive specic properties of an intelligent organisms internal information-ow, from properties of the environment and goals with respect to which its supposed to be intelligent. This would allow us to achieve the holy grail of intelligence theory as I understand it:givenadescriptionofanenvironmentandgoals,tobeabletoderiveanarchitectural description for an organism that will display a high level of intelligence relative to those goals, given limited computational resources. While this holy grail is obviously a far way off, what weve tried to do here is to outline a mathematical and conceptual direction for moving toward it. 11.6 Conclusion The Mind-World Correspondence Principle presented hereif in the vicinity of correctnessconstitutes a non-trivial step toward eshing out the concept of a general theory of general intelligence.",Engineering General  Intelligence Part 1,chapter 11
"But obviously the theory is still rather abstract, and also not completely rigorous. Theres a lot more work to be done. The Mind-World Correspondence Principle as articulated above is not quite a formal mathematical statement. It would take a little work to put in all the needed quantiers to formulate it as one, and its not clear the best way to do so the details would perhaps become clear in the course of trying to prove a version of it rigorously. One could interpret the ideas presented in this chapter as a philosophical theory that hopes to be turned into a mathematical theory and to play a key role in a scientic theory. For the time being, the main role to be served by these ideas is qualitative: to help us think about concrete AGI designs like CogPrime in a sensible way.",Engineering General  Intelligence Part 1,chapter 11
"Its important to understandwhatthegoalofareal-worldAGIsystemneedstobe:toachievetheability to broadly learn and generalize, yes, but not with innite capability rather with biases and patterns that are implicitly and/or explicitly tuned to certain broad classes of goals and environments. The Mind-World Correspondence Principle tells us something about what this tuning should involvenamely, making a system possessing mindstate sequences that correspond meaningfully to world-state sequences. CogPrimes overall design and particular cognitive processes are reasonably well interpreted as an attempt to achieve this for everyday human goals and environments. One way of extending these theoretical ideas into a more rigorous theory is exploredinAppendixB.Thekeyideasinvolvedthereare:modelingmultiplememory types as mathematical categories (with functors mapping between them), modeling memory items as probability distributions, and measuring distance between memory items using two metrics, one based on algorithmic information theory and one on classical information geometry.",Engineering General  Intelligence Part 1,chapter 11
"Building on these ideas, core hypotheses are then presented:  a syntax-semantics correlation principle, stating that in a successful AGI system, these two metrics should be roughly correlated      222 11 A Mind-World Correspondence Principle  a cognitive geometrodynamics principle, stating that on the whole intelligent minds tend to follow geodesics (shortest paths) in mindspace, according to various appropriately dened metrics (e.g. the metric measuring the distance between two entities in terms of the length and/or runtime of the shortest programs computing one from the other).  a cognitive synergy principle, stating that shorter paths may be found through the composite mindspace formed by considering multiple memory types together, than by following the geodesics in the mindspaces corresponding to individual memory types.",Engineering General  Intelligence Part 1,chapter 11
"The material is relegated to an appendix because it is so speculative, and its not yet clear whether it will really be useful in advancing or interpreting CogPrime or other AGI systems (unlike the material from the present chapter, which has at least been useful in interpreting and tweaking the CogPrime design, even though it cant be claimed that CogPrime was derived directly from these theoretical ideas). However, this sort of speculative exploration is, in our view, exactly the sort of thing thats needed as a rst phase in transitioning the ideas of the present chapter into a more powerful and directly actionable theory.",Engineering General  Intelligence Part 1,chapter 11
     Part IV Cognitive and Ethical Development   ,Engineering General  Intelligence Part 1,chapter 11
"  Chapter 12 Stages of Cognitive Development 12.1 Introduction Creating AGI, we have said, is not only about having the right structural and dynamical possibilities implemented in the initial version of ones systembut also about the environment and embodiment that ones system is associated with, and the match between the systems internals and these externals. Another key aspect is the long-term time-course of the systems evolution over time, both in its internals and its external interactioni.e., what is known as development. Development is a critical topic in our approach to AGI because we believe that much of what constitutes human-level, human-like intelligence emerges in an intelligent system due to its engagement with its environment and its environment-coupled self-organization. So, its not to be expected that the initial version of an AGI system is going to display impressive feats of intelligence, even if the engineering is totally done right. A good analogy is the apparent unintelligence of a human baby.",Engineering General  Intelligence Part 1,chapter 12
"Yes, scientists have discovered that human babies are capable of interesting and signicant intelligencebut one has to hunt to nd it ... at rst observation, babies are rather idiotic and simple-minded creatures: much less intelligent-appearing than lizards or sh, maybe even less than cockroaches.... If the goal of an AGI project is to create an AGI system that can progressively develop advanced intelligence through learning in an environment richly populated with other agents and various inanimate stimuli and interactive entitiesthen an understanding of the nature of cognitive development becomes extremely important to that project. Unfortunately, contemporary cognitive science contains essentially no theory of abstract developmental psychology which can conveniently be applied to understand developing AIs. There is of course an extensive science of human developmental psychology, and so it is a natural research program to take the chief ideas Co-authored with Stephan Vladimir Bugaj. B. Goertzel et al.",Engineering General  Intelligence Part 1,chapter 12
"Engineering General Intelligence, Part 1, 225 Atlantis Thinking Machines 5, DOI: 10.2991/978-94-6239-027-0_12,  Atlantis Press and the authors 2014      226 12 Stages of Cognitive Development from the former and inasmuch as possible port them to the AGI domain. This is not an entirely simple matter both because of the differences between humans and AIs and because of the unsettled nature of contemporary developmental psychology theory. But its a job that must (and will) be done, and the ideas in this chapter may contribute toward this effort. We will begin here with Piagets well-known theory of human cognitive development, presenting it in a general systems theory context, then introducing some modications and extensions and discussing some other relevant work. 12.2 Piagetan Stages in the Context of a General Systems Theory of Development Our review of AGI architectures in Chap.",Engineering General  Intelligence Part 1,chapter 12
"6 focused heavily on the concept of symbolism, and the different ways in which different classes of cognitive architecture handle symbol representation and manipulation. We also feel that symbolism is critical to the notion of AGI developmentand even more broadly, to the systems theory of development in general. As a broad conceptual perspective on development, we suggest that one may view the development of a complex information processing system, embedded in an environment, in terms of the stages:  automatic: the system interacts with the environment by instinct, according to its innate programming  adaptive: the system internally adapts to the environment, then interacting with the environment in a more appropriate way  symbolic: the system creates internal symbolic representations of itself and the environment, which in the case of a complex, appropriately structured environment, allows it to interact with the environment more intelligently  reexive: the system creates internal symbolic representations of its own internal symbolic representations, thus achieving an even higher degree of intelligence.",Engineering General  Intelligence Part 1,chapter 12
"Sketched so broadly, these are not precisely dened categories but rather heuristic, intuitive categories. Formalizing them would be possible but would lead us too far astray here. One can interpret these stages in a variety of different contexts. Here our focus is the cognitive development of humans and human-like AGI systems, but in Table12.1 we present them in a slightly more general context, using two examples: the Piagetan example of the human (or humanlike) mind as it develops from infancy to maturity; and also the example of the origin of life and the development of life from protolife up into its modern form. In any event, we allude to this more general perspective on development here mainly to indicate our view that the Piagetan perspective is not something ad hoc and arbitrary, but rather can plausibly be seen as a specic manifestation of more fundamental principles of complex systems development.      12.",Engineering General  Intelligence Part 1,chapter 12
"3 Piagets Theory of Cognitive Development 227 Table 12.1 General systems theory of development: parallels between development of mind and origin of life Stage General description Cognitive development Origin of life Automatic System-environment information exchange controlled mainly by innate system structures or environment Piagetan infantile stage Self-organizing protolife system, e.g. Oparin [Opa52] water droplet, or Cairns-Smith [CS90] clay-based protolife Adaptive System-environment info exchange heavily guided by adaptively internally-created system structures Piagetan concrete operational stage: systematic internal world-model guides world-exploration Simple autopoietic system, e.g.",Engineering General  Intelligence Part 1,chapter 12
"Oparin water droplet w/basic metabolism Symbolic Internal symbolic representation of information exchange process Piagetan formal stage: explicit logical/experimental learning about how to cognize in various contexts Genetic code: internal entities that stand for aspects of organism and environment, thus enabling complex epigenesis Reexive Thoroughgoing self-modication based on this symbolic representation Piagetan post-formal stage: purposive self-modication of basic mental processes Genes+memes: genetic code-patterns guide their own modication via inuencing culture 12.3 Piagets Theory of Cognitive Development The ghost of Jean Piaget hangs over modern developmental psychology in a yet unresolvedway.Piagetstheoriesprovideacogentoverarchingperspectiveonhumancognitive development, coordinating broad theoretical ideas and diverse experimental results into a unied whole [Pia55]. Modern experimental work has shown Piagets ideas to be often oversimplied and incorrect.",Engineering General  Intelligence Part 1,chapter 12
"However, what has replaced the Piagetan understanding is not an alternative unied and coherent theory, but a variety of microtheories addressing particular aspects of cognitive development. For this reason a number of contemporary theorists taking a computer science [Shu03] or dynamical systems [Wit07] approach to developmental psychology have chosen to adopt the Piagetan framework in spite of its demonstrated shortcomings, both because of its conceptual strengths and for lack of a coherent, more rigorously grounded alternative. Our own position is that the Piagetan view of development has some fundamental truth to it, which is reected via how nicely it ts with a broader view of development in complex systems. Indeed, Piaget viewed developmental stages as emerging from general algebraic principles rather than as being artifacts of the particulars of human psychology. But, Piagets stages are probably best viewed as a general interpretive framework rather than a precise scientic theory.",Engineering General  Intelligence Part 1,chapter 12
"Our suspicion is that once the empirical science of developmental psychology has progressed further, it      228 12 Stages of Cognitive Development will become clearer how to t the various data into a broad Piaget-like framework, perhaps differing in many details from what Piaget described in his works. Piaget conceived of child development in four stages, each roughly identied with an age group, and corresponding closely to the system-theoretic stages mentioned above:  infantile, corresponding to the automatic stage mentioned above  Example: Grasping blocks, piling blocks on top of each other, copying words that are heard  preoperational and concrete operational, corresponding to the adaptive stage mentioned above  Example: Building complex blocks structures, from imagination and from imitating objects and pictures and based on verbal instructions; verbally describing what has been constructed  formal, corresponding to the symbolic stage mentioned above  Example: Writing detailed instructions in words and diagrams, explaining how to construct particular structures out of blocks; guring out",Engineering General  Intelligence Part 1,chapter 12
"general rules describing which sorts of blocks structures are likely to be most stable  the reexive stage mentioned above corresponds to what some post-Piagetan theorists have called the post-formal stage  Example: Using abstract lessons learned from building structures out of blocks to guide the construction of new ways to think and understandZen and the art of blocks building (by analogy to Zen and the Art of Motorcycle Maintenance [Pir84]) (Fig.12.1). Fig. 12.1 Piagetan stages of cognitive development      12.3 Piagets Theory of Cognitive Development 229 More explicitly, Piaget dened his stages in psychological terms roughly as follows:  Infantile: In this stage a mind develops basic world-exploration driven by instinctive actions. Reward-driven reinforcement of actions learned by imitation, simple associations between words and objects, actions and images, and the basic notions of time, space, and causality are developed.",Engineering General  Intelligence Part 1,chapter 12
"The most simple, practical ideas and strategies for action are learned.  Preoperational: At this stage we see the formation of mental representations, mostly poorly organized and un-abstracted, building mainly on intuitive rather than logical thinking. Word-object and image-object associations become systematic rather than occasional. Simple syntax is mastered, including an understanding of subject-argument relationships. One of the crucial learning achievements here is object permanenceinfants learn that objects persist even when not observed. However, a number of cognitive failings persist with respect to reasoning about logical operations, and abstracting the effects of intuitive actions to an abstract theory of operations.  Concrete: More abstract logical thought is applied to the physical world at this stage.",Engineering General  Intelligence Part 1,chapter 12
"Among the feats achieved here are: reversibilitythe ability to undo steps already done; conservationunderstanding that properties can persist in spite of appearances; theory of mindan understanding of the distinction between what I know and what others know (If I cover my eyes, can you still see me?). Complex concrete operations, such as putting items in height order, are easily achievable. Classication becomes more sophisticated, yet the mind still cannot master purely logical operations based on abstract logical representations of the observational world.  Formal: Abstract deductive reasoning, the process of forming, then testing hypotheses, and systematically reevaluating and rening solutions, develops at this stage, as does the ability to reason about purely abstract concepts without reference to concrete physical objects. This is adult human-level intelligence. Note that the capability for formal operations is intrinsic in the PLN component of CogPrime, but in-principle capability is not the same as pragmatic, grounded, controllable capability.",Engineering General  Intelligence Part 1,chapter 12
"Very early on, Vygotsky [Vyg86] disagreed with Piagets explanation of his stages as inherent and developed by the childs own activities, and Piagets prescription of good parenting as not interfering with a childs unfettered exploration of the world. Some modern theorists have critiqued Piagets stages as being insufciently socially grounded, and these criticisms trace back to Vygotskys focus on the social foundations of intelligence, on the fact that children function in a world surrounded by adults who provide a cultural context, offering ongoing assistance, critique, and ultimately validation of the childs developmental activities. Vygotsky also was an early critic of the idea that cognitive development is continuous, and continues beyond Piagets formal stage. Gagne [RBW92] also believes in continuity, and that learning of prerequisite skills made the learning of subsequent      230 12 Stages of Cognitive Development skills easier and faster without regard to Piagetan stage formalisms.",Engineering General  Intelligence Part 1,chapter 12
"Subsequent researchers have argued that Piaget has merely constructed ad hoc descriptions of the sequential development of behaviour [Gib78, Bro84, CP05]. We agree that learning is a continuous process, and our notion of stages is more statistically constructed than rigidly quantized. Critique of Piagets notion of transitional half stages is also relevant to a more comprehensive hierarchical view of development. Some have proposed that Piagets half stages are actually stages [Bro84]. As Commons and Pekker [CP05] point out: the denition of a stage that was being used by Piaget was based on analyzing behaviors and attempting to impose different structures on them.",Engineering General  Intelligence Part 1,chapter 12
"There is no underlying logical or mathematical denition to help in this process  Their Hierarchical Complexity development model uses task achievement rather than ad hoc stage definition as the basis for constructing relationships between phases of developmental abilityan approach which we nd useful, though our approach is different in that we dene stages in terms of specic underlying cognitive mechanisms. Another critique of Piaget is that one individuals performance is often at different ability stages depending on the specic task (for example [GE86]). Piaget responded to early critiques along these lines by calling the phenomenon horizontal dcalage, but neither he nor his successors [Fis80, Cas85] have modied his theory to explain (rather than merely describe) it. Similarly to Thelen and Smith [TS94], we observe that the abilities encapsulated in the denition of a certain stage emerge gradually during the previous stageso that the onset of a given stage represents the mastery of a cognitive skill that was previously present only in certain contexts.",Engineering General  Intelligence Part 1,chapter 12
"Piaget also had difculty accepting the idea of a preheuristic stage, early in the infantile period, in which simple trial-and-error learning occurs without signicant heuristic guidance [Bic88], a stage which we suspect exists and allows formulation of heuristics by aggregation of learning from preheuristic pattern mining. Coupled with his belief that a minds innate abilities at birth are extremely limited, there is a troublingly unexplained transition from inability to ability in his model. Finally, another limiting aspect of Piagets model is that it did not recognize any stages beyond formal operations, and included no provisions for exploring this possibility. A number of researchers [Bic88, Arl75, CRK82, Rie73, Mar01] have described one or more postformal stages. Commons and colleagues have also proposed a task-based model which provides a framework for explaining stage discrepancies across tasks and for generating new stages based on classication of observed logical behaviors.",Engineering General  Intelligence Part 1,chapter 12
"[KK90] promotes a statistical conception of stage, which provides a good bridge between task-based and stage-based models of development, as statistical modeling allows for stages to be roughly dened and analyzed based on collections of task behaviors. [CRK82] postulates the existence of a postformal stage by observing elevated levels of abstraction which, they argue, are not manifested in formal thought. [CTS+98] observes a postformal stage when subjects become capable of analyzing and coordinating complex logical systems with each other, creating metatheoretical supersystems. In our model, with the reexive stage of development, we expand this denition of metasystemic thinking to include the ability to consciously rene      12.3 Piagets Theory of Cognitive Development 231 ones own mental states and formalisms of thinking.",Engineering General  Intelligence Part 1,chapter 12
"Such self-reexive renement is necessary for learning which would allow a mind to analytically devise entirely new structures and methodologies for both formal and postformal thinking. In spite of these various critiques and limitations, however, we have found Piagets ideas very useful, and in Sect.12.4 we will explore ways of dening them rigorously in the specic context of CogPrimes declarative knowledge store and probabilistic logic engine. 12.3.1 Perrys Stages Also relevant is William Perrys [Per70, Per81] theory of the stages (positions in his terminology) of intellectual and ethical development, which constitutes a model of iterative renement of approach in the developmental process of coming to intellectual and ethical maturity. These stages, depicted in Table12.2 form an analytical tool for discerning the modality of belief of an intelligence by describing common cognitive approaches to handling the complexities of real world ethical considerations. Table 12.",Engineering General  Intelligence Part 1,chapter 12
"2 Perrys developmental stages [with corresponding Piagetan stages in brackets] Stage Substages Dualism/received knowledge [Infantile] Basic duality (All problems are solvable. I must learn the correct solutions.) Full dualism (There are different, contradictory solutions to many problems. I must learn the correct solutions, and ignore the incorrect ones) Multiplicity [Concrete] Early multiplicity (Some solutions are known, others arent. I must learn how to nd correct solutions.) Late multiplicity: cognitive dissonance regarding truth. (Some problems are unsolvable, some are a matter of personal taste, therefore I must declare my own intellectual path.) Relativism/procedural knowledge [Formal] Contextual relativism (I must learn to evaluate solutions within a context, and relative to supporting observation.) Pre-Commitment (I must evaluate solutions, then commit to a choice of solution.) Commitment/constructed Commitment (I have chosen a solution.",Engineering General  Intelligence Part 1,chapter 12
"knowledge [Formal/Reexive] Challenges to commitment (I have seen unexpected implications of my commitment, and the responsibility I must take.) Post-commitment (I must have an ongoing, nuanced relationship to the subject in which I evaluate each situation on a case-by-case basis with respects to its particulars rather than an ad-hoc application of unchallenged ideology.)      232 12 Stages of Cognitive Development 12.3.2 Keeping Continuity in Mind Continuity of mental stages, and the fact that a mind may appear to be in multiple stages of development simultaneously (depending upon the tasks being tested), are crucial to our theoretical formulations and we will touch upon them again here. Piaget attempted to address continuity with the creation of transitional half stages. We prefer to observe that each stage feeds into the other and the end of one stage and the beginning of the next blend together.",Engineering General  Intelligence Part 1,chapter 12
"The distinction between formal and post-formal, for example, seems to merely be the application of formal thought to oneself. However, the distinction between concrete and formal is merely the buildup to higher levels of complexity of the classication, task decomposition, and abstraction capabilities of the concrete stage. The stages represent general trends in ability on a continuous curve of development, not discrete states of mind which are jumped-into quantum style after enough knowledge energy builds-up to cause the transition. Observationally, this appears to be the case in humans. People learn things gradually, and show a continuous development in ability, not a quick jump from ignorance to mastery. We believe that this gradual development of ability is the signature of genuine learning, and that prescriptively an AGI system must be designed in order to have continuous and asymmetrical development across a variety of tasks in order to be considered a genuine learning system.",Engineering General  Intelligence Part 1,chapter 12
"While quantum leaps in ability may be possible in an AGI system which can just graft new parts of brain onto itself (or an augmented human which may someday be able to do the same using implants), such acquisition of knowledge is not really learning. Grafting on knowledge does not build the cognitive pathways needed in order to actually learn. If this is the only mechanism available to an AGI system to acquire new knowledge, then it is not really a learning system. 12.4 Piagets Stages in the Context of Uncertain Inference Piagets developmental stages are very general, referring to overall types of learning, not specic mechanisms or methods. This focus was natural since the context of his workwas human developmental psychology, andneurosciencehas not yet progressed to the point of understanding the neural mechanisms underlying any sort of inference (and certainly was nowhere near to doing so in Piagets time!).",Engineering General  Intelligence Part 1,chapter 12
"But if one is studying developmental psychology in an AGI context where one knows something about the internal mechanisms of the AGI system under consideration, then one can work with a more specic model of learning. Our focus here is on AGI systems whose operations contain uncertain inference as a central component. Obviously the main focus is CogPrime, but the essential ideas apply to any other uncertain inference centric AGI architecture as well (Fig.12.2).      12.4 Piagets Stages in the Context of Uncertain Inference 233 Fig. 12.2 Piagetan stages of development, as manifested in the context of uncertain inference Fig. 12.3 A simplied look at feedback-control in uncertain inference Anuncertaininferencesystem,asweconsiderithere,consistsoffourcomponents, which work together in a feedback-control loop Fig.12.3 1. a content representation scheme 2. an uncertainty representation scheme 3.",Engineering General  Intelligence Part 1,chapter 12
"a set of inference rules 4. a set of inference control schemata.      234 12 Stages of Cognitive Development Broadly speaking, examples of content representation schemes are predicate logic and term logic [ES00]. Examples of uncertainty representation schemes are fuzzy logic [Zad78], imprecise probability theory [Goo86, FC86], Dempster-Shafer theory [Sha76,Kyb97],Bayesianprobabilitytheory[Kyb97],NARS[Wan95],andtheAtom representation used in CogPrime, briey alluded to in Chap.2 and described in depth in later chapters. Many, but not all, approaches to uncertain inference involve only a limited, weak set of inference rules (e.g. not dealing with complex quantied expressions). CogPrimes PLN inference framework, like NARS and some other uncertain inference frameworks, contains uncertain inference rules that apply to logical constructs of arbitrary complexity.",Engineering General  Intelligence Part 1,chapter 12
"Only a system capable of dealing with constructs of arbitrary (or at least very high) complexity will have any potential of leading to human-level, human-like intelligence. The subtlest part of uncertain inference is inference control: the choice of which inferences to do, in what order. Inference control is the primary area in which human inference currently exceeds automated inference. Humans are not very efcient or accurate at carrying out inference rules, with or without uncertainty, but we are very good at determining which inferences to do and in what order, in any given context. The lack of effective, context-sensitive inference control heuristics is why the general ability of current automated theorem provers is considerably weaker than that of a mediocre university mathematics major [Mac95]. We now review the Piagetan developmental stages from the perspective of AGI systems heavily based on uncertain inference. 12.4.",Engineering General  Intelligence Part 1,chapter 12
"1 The Infantile Stage In this initial stage, the mind is able to recognize patterns in and conduct inferences about the world, but only using simplistic hard-wired (not experientially learned) inference control schema, along with pre-heuristic pattern mining of experiential data. In the infantile stage an entity is able to recognize patterns in and conduct inferences about its sensory surround context (i.e., its world), but only using simplistic, hard-wired (not experientially learned) inference control schemata. Preheuristic pattern-mining of experiential data is performed in order to build future heuristics about analysis of and interaction with the world. An infants tasks include: 1. Exploratory behavior in which useful and useless/dangerous behavior is differentiated by both trial and error observation, and by parental guidance. 2. Development of habitsi.e. Repeating tasks which were successful once to determine if they always/usually are so.",Engineering General  Intelligence Part 1,chapter 12
"     12.4 Piagets Stages in the Context of Uncertain Inference 235 Fig. 12.4 Uncertain inference in the infantile stage 3. Simple goal-oriented behavior such as nd out what cat hair tastes like in which one must plan and take several sequentially dependent steps in order to achieve the goal. Inference control is very simple during the infantile stage (Fig.12.4), as it is the stage during which both the most basic knowledge of the world is acquired, and the most basic of cognition and inference control structures are developed as the building block upon which will be built the next stages of both knowledge and inference control. Another example of a cognitive task at the borderline between infantile and concrete cognition is learning object permanence, a problem discussed in the context of CogPrimes predecessor Novamente Cognition Engine system in [GPSL03]. Another example is the learning of word-object associations: e.g.",Engineering General  Intelligence Part 1,chapter 12
"learning that when the word ball is uttered in various contexts (Get me the ball, Thats a nice ball, etc.) it generally refers to a certain type of object. The key point regarding these infantile inference problems, from the CogPrime perspective, is that assuming one provides the inference system with an appropriate set of perceptual and motor ConceptNodes and SchemaNodes, the chains of inference involved are short. They involve about a dozen inferences, and this means that the search tree of possible PLN inference rules walked by the PLN backward-chainer is relatively shallow. Sophisticated inference control is not required: standard AI heuristics are sufcient. In short, textbook narrow-AI reasoning methods, utilized with appropriate uncertainty-savvy truth value formulas and coupled with appropriate representations of perceptual and motor inputs and outputs, correspond roughly to Piagets infantile stage of cognition.",Engineering General  Intelligence Part 1,chapter 12
"The simplistic approach of these narrow-AI methods may be viewed as a method of creating building blocks for subsequent, more sophisticated heuristics.      236 12 Stages of Cognitive Development In our theory Piagets preoperational phase appears as transitional between the infantile and concrete operational phases. 12.4.2 The Concrete Stage At this stage, the mind is able to carry out more complex chains of reasoning regarding the world, via using inference control schemata that adapt behavior based on experience (reasoning about a given case in a manner similar to prior cases). In the concrete operational stage (Fig.12.5), an entity is able to carry out more complexchainsofreasoningabouttheworld.Inferencecontrolschematawhichadapt behaviorbasedonexperience,usingexperientiallylearnedheuristics(includingthose learned in the prior stage), are applied to both analysis of and interaction with the sensory surround/world. Concrete Operational stage tasks include: 1.",Engineering General  Intelligence Part 1,chapter 12
"Conservation tasks, such as conservation of number, 2. Decomposition of complex tasks into easier subtasks, allowing increasingly complex tasks to be approached by association with more easily understood (and previously experienced) smaller tasks, 3. Classication and Serialization tasks, in which the mind can cognitively distinguish various disambiguation criteria and group or order objects accordingly. In terms of inference control this is the stage in which actual knowledge about how to control inference itself is rst explored. This means an emerging understanding Fig. 12.5 Uncertain inference in the concrete operational stage      12.4 Piagets Stages in the Context of Uncertain Inference 237 of inference itself as a cognitive task and methods for learning, which will be further developed in the following stages.",Engineering General  Intelligence Part 1,chapter 12
"Also, in this stage a special cognitive task capability is gained: Theory of Mind, which in cognitive science refers to the ability to understand the fact that not only oneself, but other sentient beings have memories, perceptions, and experiences. This is the ability to conceptually put oneself in anothers shoes (even if you happen to assume incorrectly about them by doing so). 12.4.2.1 Conservation of Number Conservation of number is an example of a learning problem classically categorized withinPiagetsconcrete-operationalphase,aconservationlawsproblem,discussed in [Shu03] in the context of software that solves the problem using (logic-based and neural net) narrow-AI techniques. Conservation laws are very important to cognitive development. Conservation is the idea that a quantity remains the same despite changes in appearance.",Engineering General  Intelligence Part 1,chapter 12
"If you show a child some objects and then spread them out, an infantile mind will focus on the spread, and believe that there are now more objects than before, whereas a concrete-operational mind will understand that the quantity of objects has not changed. Conservation of number seems very simple, but from a developmental perspective it is actually rather difcult. Solutions like those given in [Shu03] that use neural networks or customized logical rule-bases to nd specialized solutions that solve only this problem fail to fully address the issue, because these solutions dont create knowledge adequate to aid with the solution of related sorts of problems. We hypothesize that this problem is hard enough that for an inference-based AGI system to solve it in a developmentally useful way, its inferences must be guided by meta-inferential lessons learned from prior similar problems. When approaching a number conservation problem, for example, a reasoning system might draw upon past experience with set-size problems (which may be trial-and-error experience).",Engineering General  Intelligence Part 1,chapter 12
"This is not a simple machine learning approach whose scope is restricted to the current problem, but rather a heuristically guided approach which (a) aggregates information from prior experience to guide solution formulation for the problem at hand, and (b) adds the present experience to the set of relevant information about quantication problems for future renement of thinking (Fig.12.6). For instance, a very simple context-specic heuristic that a system might learn would be: When evaluating the truth value of a statement related to the number of Fig. 12.6 Conservation of number      238 12 Stages of Cognitive Development objects in a set, it is generally not that useful to explore branches of the backwardschaining search tree that contain relationships regarding the sizes, masses, or other physical properties of the objects in the set.",Engineering General  Intelligence Part 1,chapter 12
"This heuristic itself may go a long way toward guiding an inference process toward a correct solution to the problembut it is not something that a mind needs to know a priori. A concrete-operational stage mind may learn this by data-mining prior instances of inferences involving sizes of sets. Without such experience-based heuristics, the search tree for such a problem will likely be unacceptably large. Even if it is solvable without such heuristics, the solutions found may be overly t to the particular problem and not usefully generalizable. 12.4.2.2 Theory of Mind Consider this experiment: a preoperational child is shown her favorite Dora the Explorer DVD box. Asked what show shes about to see, shell answer Dora. However, when her parent plays the disc, its SpongeBob SquarePants.",Engineering General  Intelligence Part 1,chapter 12
"If you then ask her what show her friend will expect when given the Dora DVD box, she will respond SpongeBob although she just answered Dora for herself. A child lacking a theory of mind can not reason through what someone else would think given knowledge other than her own current knowledge. Knowledge of self is intrinsically related to the ability to differentiate oneself from others, and this ability may not be fully developed at birth. Several theorists [BC94, Fod94], based in part on experimental work with autistic children, perceive theory of mind as embodied in an innate module of the mind activated at a certain developmental stage (or not, if damaged). While we consider this possible,wecautionagainstadoptingasimplisticviewoftheinnateversusacquired dichotomy: if there is innateness it may take the form of an innate predisposition to certain sorts of learning [EBJ+97].",Engineering General  Intelligence Part 1,chapter 12
"Davidson [Dav84], Dennett [Den87] and others support the common belief that theory of mind is dependent upon linguistic ability. A major challenge to this prevailing philosophical stance came from Premack and Woodruff [PW78] who postulated that prelinguistic primates do indeed exhibit theory of mind behavior. While Premack and Woodruffs experiment itself has been challenged, their general result has been bolstered by follow-up work showing similar results such as [TC97]. It seems to us that while theory of mind depends on many of the same inferential capabilities as language learning, it is not intrinsically dependent on the latter. There is a school of thought often called the Theory Theory [BW88, Car85, Wel90] holding that a childs understanding of mind is best understood in terms of the process of iteratively formulating and refuting a series of naive theories about others.",Engineering General  Intelligence Part 1,chapter 12
"Alternately, Gordon [Gor86] postulates that theory of mind is related to the ability to run cognitive simulations of others minds using ones own mind as a model. We suggest that these two approaches are actually quite harmonious with one another. In an uncertain AGI context, both theories and simulations are grounded in collectionsofuncertainimplications,whichmaybeassembledincontext-appropriate      12.4 Piagets Stages in the Context of Uncertain Inference 239 ways to form theoretical conclusions or to drive simulations. Even if there is a special mind-simulator dynamic in the human brain that carries out simulations of other minds in a manner fundamentally different from explicit inferential theorizing, the inputs to and the behavior of this simulator may take inferential form, so that the simulator is in essence a way of efciently and implicitly producing uncertain inferential conclusions from uncertain premises.",Engineering General  Intelligence Part 1,chapter 12
"WehavethoughtthroughthedetailsbyCogPrimesystemshouldbeabletodevelop theory of mind via embodied experience, though at time of writing practical learning experiments in this direction have not yet been done. We have not yet explored in detail the possibility of giving CogPrime a special, elaborately engineered mindsimulator component, though this would be possible; instead we have initially been pursuing a more purely inferential approach. First, it is very simple for a CogPrime system to learn patterns such as If I rotated by pi radians, I would see the yellow block. And its not a big leap for PLN to go from this to the recognition that You look like me, and youre rotated by pi radians relative to my orientation, therefore you probably see the yellow block. The only nontrivial aspect here is the you look like me premise.",Engineering General  Intelligence Part 1,chapter 12
"Recognizing embodied agent as a category, however, is a problem fairly similar to recognizing block or insect or daisy as a category. Since the CogPrime agent can perceive most parts of its own robot bodyits arms, its legs, etc.it should be easy for the agent to gure out that physical objects like these look different depending upon its distance from them and its angle of observation. From this it should not be that difcult for the agent to understand that it is naturally grouped together with other embodied agents (like its teacher), not with blocks or bugs. The only other major ingredient needed to enable theory of mind is reection the ability of the system to explicitly recognize the existence of knowledge in its own mind (note that this term reection is not the same as our proposed reexive stage of cognitive development). This exists automatically in CogPrime, via the built-in vocabulary of elementary procedures supplied for use within SchemaNodes (specically, the atTime and TruthValue operators).",Engineering General  Intelligence Part 1,chapter 12
"Observing that at time T, the weight of evidence of the link L increased from zero is basically equivalent to observing that the link L was created at time T. Then, the system may reason, for example, as follows (using a combination of several PLN rules including the above-given deduction rule): Implication My eye is facing a block and it is not dark A relationship is created describing the blocks color Similarity My body My teachers body      240 12 Stages of Cognitive Development Implication My teachers eye is facing a block and it is not dark A relationship is created describing the blocks color This sort of inference is the essence of Piagetan theory of mind. Note that in both of these implications the created relationship is represented as a variable rather than a specic relationship. The cognitive leap is that in the latter case the relationship actually exists in the teachers implicitly hypothesized mind, rather than in CogPrimes mind.",Engineering General  Intelligence Part 1,chapter 12
"No explicit hypothesis or model of the teachers mind need be created in order to form this implicationthe hypothesis is created implicitly via inferential abstraction. Yet, a collection of implications of this nature may be used via an uncertain reasoning system like PLN to create theories and simulations suitable to guide complex inferences about other minds. From the perspective of developmental stages, the key point here is that in a CogPrime context this sort of inference is too complex to be viably carried out via simple inference heuristics. This particular example must be done via forward chaining, since the big leap is to actually think of forming the implication that concludes inference. But there are simply too many combinations of relationships involving CogPrimes eye, body, and so forth for the PLN component to viably explore all of them via standard forward-chaining heuristics.",Engineering General  Intelligence Part 1,chapter 12
"Experience-guided heuristics are needed, such as the heuristic that if physical objects A and B are generally physically and functionally similar, and there is a relationship involving some part of A and some physical object R, it may be useful to look for similar relationships involving an analogous part of B and objects similar to R. This kind of heuristic may be learned by experienceand the masterful deployment of such heuristics to guide inference is what we hypothesize to characterize the concrete stage of development. The concreteness comes from the fact that inference control is guided by analogies to prior similar situations. 12.4.3 The Formal Stage In the formal stage, as shown in Fig.12.7, an agent should be able to carry out arbitrarily complex inferences (constrained only by computational resources, rather than by fundamental restrictions on logical language or form) via including inference control as an explicit subject of abstract learning.",Engineering General  Intelligence Part 1,chapter 12
"Abstraction and inference about both the sensorimotor surround (world) and about abstract ideals themselves (including the nal stages of indirect learning about inference itself) are fully developed. Formal stage evaluation tasks are centered entirely around abstraction and higherorder inference tasks such as: 1. Mathematics and other formalizations. 2. Scientic experimentation and other rigorous observational testing of abstract formalizations.      12.4 Piagets Stages in the Context of Uncertain Inference 241 Fig. 12.7 Uncertain inference in the formal stage 3. Social and philosophical modeling, and other advanced applications of empathy and the Theory of Mind. In terms of inference control this stage sees not just perception of new knowledge about inference control itself, but inference controlled reasoning about that knowledge and the creation of abstract formalizations about inference control which are reasoned-upon, tested, and veried or debunked. 12.4.3.",Engineering General  Intelligence Part 1,chapter 12
"1 Systematic Experimentation The Piagetan formal phase is a particularly subtle one from the perspective of uncertain inference. In a sense, AGI inference engines already have strong capability for formal reasoning built in. Ironically, however, no existing inference engine is capable of deploying its reasoning rules in a powerfully effective way, and this is because of the lack of inference control heuristics adequate for controlling abstract formal reasoning. These heuristics are what arise during Piagets formal stage, and we propose      242 12 Stages of Cognitive Development that in the content of uncertain inference systems, they involve the application of inference itself to the problem of rening inference control. A problem commonly used to illustrate the difference between the Piagetan concrete operational and formal stages is that of guring out the rules for making pendulums swing quickly versus slowly [IP58]. If you ask a child in the formal stage to solve this problem, she may proceed to do a number of experiments, e.",Engineering General  Intelligence Part 1,chapter 12
". build a long string with a light weight, a long string with a heavy weight, a short string with a light weight and a short string with a heavy weight. Through these experiments she may determine that a short string leads to a fast swing, a long string leads to a slow swing, and the weight doesnt matter at all. The role of experiments like this, which test extreme cases, is to make cognition easier. The formal-stage mind tries to map a concrete situation onto a maximally simple and manipulable set of abstract propositions, and then reason based on these. Doing this, however, requires an automated and instinctive understanding of the reasoning process itself. The above-described experiments are good ones for solving the pendulum problem because they provide data that is very easy to reason about. From the perspective of uncertain inference systems, this is the key characteristic of the formal stage: formal cognition approaches problems in a way explicitly calculated to yield tractable inferences.",Engineering General  Intelligence Part 1,chapter 12
"Note that this is quite different from saying that formal cognition involves abstractions and advanced logic. In an uncertain logic-based AGI system, even infantile cognition may involve thesethe difference lies in the level of inference control, which in the infantile stage is simplistic and hard-wired, but in the formal stage is based on an understanding of what sorts of inputs lead to tractable inference in a given context. 12.4.4 The Reexive Stage In the reexive stage (Fig.12.8), an intelligent agent is broadly capable of selfmodifying its internal structures and dynamics. As an example in the human domain: highly intelligent and self-aware adult humans may carry out reexive cognition by explicitly reecting upon their own inference processes and trying to improve them. An example is the intelligent improvement of uncertain-truth-value-manipulation formulas. It is well demonstrated that even educated humans typically make numerous errors in probabilistic reasoning [GGK02].",Engineering General  Intelligence Part 1,chapter 12
"Most people dont realize it and continue to systematically make these errors throughout their lives. However, a small percentage of individuals make an explicit effort to increase their accuracy in making probabilistic judgments by consciously endeavoring to internalize the rules of probabilistic inference into their automated cognition processes. In the uncertain inference based AGI context, what this means is: In the reexive stage an entity is able to include inference control itself as an explicit subject of abstract learning (i.e. the ability to reason about ones own tactical and strategic      12.4 Piagets Stages in the Context of Uncertain Inference 243 Fig. 12.8 The reexive stage approach to modifying ones own learning and thinking), and modify these inference control strategies based on analysis of experience with various cognitive approaches. Ultimately, the entity can self-modify its internal cognitive structures. Any knowledgeorheuristicscanberevised,includingmetatheoreticalandmetasystemicthought itself.",Engineering General  Intelligence Part 1,chapter 12
"Initially this is done indirectly, but at least in the case of AGI systems it is theoretically possible to also do so directly. This might be considered as a separate stage of Full Self Modication, or else as the end phase of the reexive stage. In the context of logical reasoning, self modication of inference control itself is the primary task in this stage. In terms of inference control this stage adds an entire new feedback loop for reasoning about inference control itself, as shown in Fig.12.8. As a very concrete example, in later chapters we will see that, while PLN is founded on probability theory, it also contains a variety of heuristic assumptions that inevitably introduce a certain amount of error into its inferences. For example, PLNs probabilistic deduction embodies a heuristic independence assumption.",Engineering General  Intelligence Part 1,chapter 12
"Thus PLN contains an alternate deduction formula called the concept geometry formula      244 12 Stages of Cognitive Development that is better in some contexts, based on the assumption that ConceptNodes embody concepts that are roughly spherically-shaped in attribute space. A highly advanced CogPrime system could potentially augment the independence-based and conceptgeometry-based deduction formulas with additional formulas of its own derivation, optimized to minimize error in various contexts. This is a simple and straightforward example of reexive cognitionit illustrates the power accessible to a cognitive system that has formalized and reected upon its own inference processes, and that possesses at least some capability to modify these. In general, AGI systems can be expected to have much broader and deeper capabilities for self-modication than human beings. Ultimately it may make sense to view the AGI systems we implement as merely initial conditions for ongoing self-modication and self-organization.",Engineering General  Intelligence Part 1,chapter 12
Chapter19 discusses some of the potential technical details underlying this sort of thoroughgoing AGI self-modication.   ,Engineering General  Intelligence Part 1,chapter 12
"  Chapter 13 The Engineering and Development of Ethics 13.1 Introduction Most commonly, if a work on advanced AI mentions ethics at all, it occurs in a nal summary chapter, discussing in broad terms some of the possible implications of the technical ideas presented beforehand. Its no coincidence that the order is reversed here: in the case of CogPrime, AGI-ethics considerations played a major role in the design process ... and thus the chapter on ethics occurs near the beginning rather than the end. In the CogPrime approach, ethics is not a particularly distinct topic, being richly interwoven with cognition and education and other aspects of the AGI project. The ethics of advanced AGI is a complex issue with multiple aspects. Among the many issues there are: 1. Risks posed by the possibility of human beings using AGI systems for evil ends. 2. Risks posed by AGI systems created without well-dened ethical systems. 3.",Engineering General  Intelligence Part 1,chapter 13
"Risks posed by AGI systems with initially well-dened and sensible ethical systems eventually going roguean especially big risk if these systems are more generally intelligent than humans, and possess the capability to modify their own source code. 4. The ethics of experimenting on AGI systems when one doesnt understand the nature of their experience. 5. AGI rights: in what circumstances does using an AGI as a tool or servant constitute slavery. In this chapter we will focus mainly (though not exclusively) on the question of how to create an AGI with a rational and benecial ethical system. After a somewhat wide-ranging discussion, we will conclude with eight general points that we believe should be followed in working toward Friendly AGImost of which have to do, not with the internal design of the AGI, but with the way the AGI is taught and interfaced with the real world. Coauthored with Stephan Vladimir Bugaj and Joel Pitt. B. Goertzel et al.",Engineering General  Intelligence Part 1,chapter 13
"Engineering General Intelligence, Part 1, 245 Atlantis Thinking Machines 5, DOI: 10.2991/978-94-6239-027-0_13,  Atlantis Press and the authors 2014      246 13 The Engineering and Development of Ethics While most of the particulars discussed in this book have nothing to do with ethics, its important for the reader to understand that AGI-ethics considerations have played a major role in many of our design decisions, underlying much of the technical contents of the book. As the materials in this chapter should make clear, ethicalness is probably not something that one can meaningfully tack onto an AGI system at the end, after developing the restit is likely infeasible to architect an intelligent agent and then add on an ethics module.",Engineering General  Intelligence Part 1,chapter 13
"Rather, ethics is something that has to do with all the different memory systems and cognitive processes that constitute an intelligent systemand its something that involves both cognitive architecture and the exploration a system does and the instruction it receives. Its a very complex matter that is richly intermixed with all the other aspects of intelligence, and here we will treat it as such. 13.2 Review of Current Thinking on the Risks of AGI Before proceeding to outline our own perspective on AGI ethics in the context of CogPrime, we will review the main existing strains of thought on the potential ethical dangers associated with AGI. One science ction lm after another has highlighted these dangers, lodging the issue deep in our cultural awareness; unsurprisingly, much less attention has been paid to serious analysis of the risks in their various dimensions, but there is still a non-trivial literature worth paying attention to. Hypothetically, an AGI with superhuman intelligence and capability could dispense with humanity altogetheri.",Engineering General  Intelligence Part 1,chapter 13
". posing an existential risk [Bos02]. In the worst case, an evil but brilliant AGI, perhaps programmed by a human sadist, could consign humanity to unimaginable tortures (i.e. realizing a modern version of the medieval Christian visions of hell). On the other hand, the potential benets of powerful AGI also go literally beyond human imagination. It seems quite plausible that an AGI with massively superhuman intelligence and positive disposition toward humanity could provide us with truly dramatic benets, such as a virtual end to material scarcity, disease and aging. Advanced AGI could also help individual humans grow in a variety of directions, including directions leading beyond legacy humanity, according to their own taste and choice. Eliezer Yudkowsky has introduced the term Friendly AI, to refer to advanced AGI systems that act with human benet in mind [Yud06]. Exactly what this means has not been specied precisely, though informal interpretations abound.",Engineering General  Intelligence Part 1,chapter 13
"Goertzel [Goe06b] has sought to clarify the notion in terms of three core values of Joy, Growth and Freedom. In this view, a Friendly AI would be one that advocates individual and collective human joy and growth, while respecting the autonomy of human choices. Some (for example, Hugo de Garis [DG05]), have argued that Friendly AI is essentially an impossibility, in the sense that the odds of a dramatically superhumanly intelligent mind worrying about human benet are vanishingly small. If this is the case, then the best options for the human race would presumably be to either avoid advanced AGI development altogether, or to else fuse with AGI before it gets too      13.2 Review of Current Thinking on the Risks of AGI 247 strongly superhuman, so that beings-originated-as-humans can enjoy the benets of greater intelligence and capability (albeit at cost of sacricing their humanity). Others (e.g.",Engineering General  Intelligence Part 1,chapter 13
"Mark Waser [Was09]) have argued that Friendly AI is essentially inevitable, because greater intelligence correlates with greater morality. Evidence from evolutionary and human history is adduced in favor of this point, along with more abstract arguments. Yudkowsky [Yud06] has discussed the possibility of creating AGI architectures that are in some sense provably Friendlyeither mathematically, or else at least via very tight lines of rational verbal argumentation. However, several issues have been raised with this approach. First, it seems likely that proving mathematical results of this nature would rst require dramatic advances in multiple branches of mathematics. Second, such a proof would require a formalization of the goal of Friendliness, which is a subtler matter than it might seem [Leg06a, Leg06b]. Formalization of human morality has vexed moral philosophers for quite some time.",Engineering General  Intelligence Part 1,chapter 13
"Finally, it is unclear the extent to which such a proof could be created in a generic, environmentindependent waybut if the proof depends on properties of the physical environment, then it would require a formalization of the environment itself, which runs up against various problems such as the complexity of the physical world and also the fact that we currently have no complete, consistent theory of physics. Kaj Sotala has provided a list of 14 objections to the Friendly AI concept, and suggested answers to each of them [Sot11]. Stephen Omohundro [Omo08] has argued that any advanced AI system will very likely demonstrate certain basic AI drives, such as desiring to be rational, to self-protect, to acquire resources, and to preserve and protect its utility function and avoid counterfeit utility; these drives, he suggests, must be taken carefully into account in formulating approaches to Friendly AI.",Engineering General  Intelligence Part 1,chapter 13
"The problem of formally or at least very carefully dening the goal of Friendliness has been considered from a variety of perspectives, none showing dramatic success. Yudkowsky [Yud04] has suggested the concept of Coherent Extrapolated Volition, which roughly refers to the extrapolation of the common values of the human race. Many subtleties arise in specifying this concepte.g. if Bob Jones is often possessed by a strong desire to kill all Martians, but he deeply aspires to be a nonviolent person, thentheCEVapproachwouldnotratekillingMartiansaspartofBobscontribution to the CEV of humanity. Goertzel [Goe10a] has proposed a related notion of Coherent Aggregated Volition (CAV), which eschews the subtleties of extrapolation, and simply seeks a reasonably compact, coherent, consistent set of values that is fairly close to the collective valueset of humanity.",Engineering General  Intelligence Part 1,chapter 13
"In the CAV approach, killing Martians would be removed from humanitys collective value-set because its uncommon and not part of the most compact/coherent/consistent overall model of human values, rather than because of Bob Jones aspiration to nonviolence. One thought we have recently entertained is that the core concept underlying CAV might be better thought of as CBV or Coherent Blended Volition. CAV seems to be easily misinterpreted as meaning the average of different views, which was not the original intention. The CBV terminology claries that the CBV of a diverse group of people should not be thought of as an average of their perspectives, but as something      248 13 The Engineering and Development of Ethics more analogous to a conceptual blend [FT02]incorporating the most essential elements of their divergent views into a whole that is overall compact, elegant and harmonious.",Engineering General  Intelligence Part 1,chapter 13
"The subtlety here (to which we shall return below) is that for a CBV blend to be broadly acceptable, the different parties whose views are being blended must agree to some extent that enough of the essential elements of their own views have been included. The process of arriving at this sort of consensus may involve extrapolation of a roughly similar sort to that considered in CEV. Multiple attempts at axiomatization of human values have also been attempted, e.g. with a view toward providing near-term guidance to military robots (see e.g. Arkins excellent though chillingly-titled book Governing Lethal Behavior in Autonomous Robots [Ark09b], the result of US military funded research). However, there are reasonably strong arguments that human values (similarly to e.g. human language or human perceptual classication rules) are too complex and multifaceted to be captured in any compact set of formal logic rules.",Engineering General  Intelligence Part 1,chapter 13
"Wallach [WA10] has made this point eloquently, and argued the necessity of fusing topdown (e.g. formal logic based) and bottomup (e.g. self-organizing learning based) approaches to machine ethics. A number of more sociological considerations also arise. It is sometimes argued that the risk from highly-advanced AGI going morally awry on its own may be less than that of moderately-advanced AGI being used by human beings to advocate immoral ends. This possibility gives rise to questions about the ethical value of various practical modalities of AGI development, for instance:  Should AGI be developed in a top-secret installation by a select group of individuals selected for a combination of technical and scientic brilliance and moral uprightness, or other qualities deemed relevant (a closed approach)? Or should it be developed out in the open, in the manner of open-source software projects like Linux? (an open approach).",Engineering General  Intelligence Part 1,chapter 13
"The open approach allows the collective intelligence of the world to more fully participatebut also potentially allows the more unsavory elements of the human race to take some of the publicly-developed AGI concepts and tools private, and develop them into AGIs with selsh or evil purposes in mind. Is there some meaningful intermediary between these extremes?  Should governments regulate AGI, with Friendliness in mind (as advocated carefully by e.g. Bill Hibbard [Hib02])? Or will this just cause AGI development to move to the handful of countries with more liberal policies?... or cause it to move underground, where nobody can see the dangers developing? As a rough analogue, its worth noting that the US governments imposition of restrictions on stem cell research, under President George W. Bush, appears to have directly stimulated the provision of additional funding for stem cell research in other nations like Korea, Singapore and China.",Engineering General  Intelligence Part 1,chapter 13
"The former issue is, obviously, highly relevant to CogPrime (which is currently being developed via the open source CogPrime project); and so the various dimensions of this issues are worth briey sketching here. We have a strong skepticism of self-appointed elite groups that claim (even if they genuinely believe) that they know whats best for everyone, and a healthy respect for      13.2 Review of Current Thinking on the Risks of AGI 249 the power of collective intelligence and the Global Brain, which the open approach is ideal for tapping. On the other hand, we also understand the risk of terrorist groups or other malevolent agents forking an open source AGI project and creating something terribly dangerous and destructive. Balancing these factors against each other rigorously, seems beyond the scope of current human science. Nobody really understands the social dynamics by which open technological knowledge plays out in our current world, let alone hypothetical future scenarios.",Engineering General  Intelligence Part 1,chapter 13
"Right now there exists open knowledge about many very dangerous technologies, and there exist many terrorist groups, yet these groups fortunately make scant use of these technologies. The reasons why appear to be essentially sociologicalthe people involved in these terrorist groups tend not to be the ones who have mastered the skills of turning public knowledge on cutting-edge technologies into real engineered systems. But while its easy to observe this sociological phenomenon, we certainly have no way to estimate its quantitative extent from rst principles. We dont really have a strong understanding of how safe we are right now, given the technology knowledge available right now via the Internet, textbooks, and so forth. Even relatively straightforward issues such as nuclear proliferation remain confusing, even to the experts. Its also quite clear that keeping powerful AGI locked up by an elite group doesnt really provide reliable protection against malevolent human agents. History is rife with such situations going awry, e.g.",Engineering General  Intelligence Part 1,chapter 13
"by the leadership of the group being subverted, or via brute force inicted by some outside party, or via a member of the elite group defecting to some outside group in the interest of personal power or reward or due to group-internal disagreements, etc. There are many things that can go wrong in such situations, and the condence of any particular group that they are immune to such issues, cannot be taken very seriously. Clearly, neither the open nor closed approach qualies as a panacea. 13.3 The Value of an Explicit Goal System One of the subtle issues confronted in the quest to design ethical AGIs is how closely one wants to emulate human ethical judgment and behavior. Here one confronts the brute fact that, even according to their own deeply-held standards, humans are not all that ethical.",Engineering General  Intelligence Part 1,chapter 13
"One high-level conclusion we came to very early in the process of designing CogPrime is that, just as humans are not the most intelligent minds achievable, they are also not the most ethical minds achievable. Even if one takes human ethics, broadly conceived, as the standardthere are almost surely possible AGI systems that are much more ethical according to human standards than nearly all human beings. This is not mainly because of ethics-specic features of the human mind, but rather because of the nature of the human motivational system, which leads to many complexities that drive humans to behaviors that are unethical according to their own standards. So, one of the design decisions we made for CogPrime with ethics as well as other reasons in mindwas not to closely imitate the human      250 13 The Engineering and Development of Ethics motivationalsystem,butrathertocraftanovelmotivationalsystemcombiningcertain aspects of the human motivational system with other profoundly non-human aspects.",Engineering General  Intelligence Part 1,chapter 13
"On the other hand, the design of ethical AGI systems still has a lot to gain from the study of human ethical cognition and behavior. Human ethics has many aspects, which we associate here with the different types of memory, and its important that AGI systems can encompass all of them. Also, as we will note below, human ethics develops in childhood through a series of natural stages, parallel to and entwined with the cognitive developmental stages reviewed in Chap.12 above. We will argue that for an AGI with a virtual or robotic body, it makes sense to think of ethical development as proceeding through similar stages. In a CogPrime context, the particulars of these stages can then be understood in terms of the particulars of CogPrimes cognitive processeswhich brings AGI ethics from the domain of theoretical abstraction into the realm of practical algorithm design and education.",Engineering General  Intelligence Part 1,chapter 13
"But even if the human stages of ethical development make sense for non-human AGIs, this doesnt mean the particulars of the human motivational system need to be replicated in these AGIs, regarding ethics or other matters. A key point here is that, in the context of human intelligence, the concept of a goal is a descriptive abstraction. But in the AGI context, it seems quite valuable to introduce goals as explicit design elements (which is what is done in CogPrime)both for ethical reasons and for broader AGI design reasons. Humans may adopt goals for a time and then drop them, may pursue multiple conicting goals simultaneously, and may often proceed in an apparently goal-less manner. Sometimes the goal that a person appears to be pursuing, may be very different than the one they think theyre pursuing.",Engineering General  Intelligence Part 1,chapter 13
"Evolutionary psychology [BDL93] argues that, directly or indirectly, all humans are ultimately pursuing the goal of maximizing the inclusive tness of their genesbut given the complex mix of evolution and self-organization in natural history [Sal93], this is hardly a general explanation for human behavior. Ultimately, in the human context, goal is best thought of as a frequently useful heuristic concept. AGI systems, however, need not emulate human cognition in every aspect, and may be architected with explicit goal systems. This provides no guarantee that said AGI systems will actually pursue the goals that their goal systems specify depending on the role that the goal system plays in the overall system dynamics, sometimes other dynamical phenomena might intervene and cause the system to behave in ways opposed to its explicit goals. However, we submit that this design sketch provides a better framework than would exist in an AGI system closely emulating the human brain.",Engineering General  Intelligence Part 1,chapter 13
"We realize this point may be somewhat contentiousa counter-argument would be that the human brain is known to support at least moderately ethical behavior, according to human ethical standards, whereas less brain-like AGI systems are much less well understood. However, the obvious counter-counterpoints are that:  Humans are not all that consistently ethical, so that creating AGI systems potentially much more practically powerful than humans, but with closely humanlike ethical, motivational and goal systems, could in fact be quite dangerous.      13.3 The Value of an Explicit Goal System 251  The effect on a human-like ethical/motivational/goal system of increasing the intelligence, or changing the physical embodiment or cognitive capabilities, of the agent containing the system, is unknown and difcult to predict given all the complexities involved. The course we tentatively recommend, and are following in our own work, is to develop AGI systems with explicit, hierarchically-dominated goal systems.",Engineering General  Intelligence Part 1,chapter 13
"That is:  create one or more top goals (we call them Ubergoals in CogPrime);  have the system derive subgoals from these, using its own intelligence, potentially guided by educational interaction or explicit programming;  have a signicant percentage of the systems activity governed by the explicit pursuit of these goals. Note that the signicant percentage need not be 100 %; CogPrime, for example, combines explicitly goal-directed activity with other spontaneous activity. Requiring that all activity be explicitly goal-directed may be too strict a requirement to place on AGI architectures. The next step, of course, is for the top-level goals to be chosen in accordance with the principle of human-Friendliness. The next one of our eight points, about the Global Brain, addresses one way of doing this. In our near-term work with CogPrime, we are using simplistic approaches, with a view toward early-stage system testing. 13.",Engineering General  Intelligence Part 1,chapter 13
"4 Ethical Synergy An explicit goal system provides an explicit way to ensure that ethical principles (as represented in system goals) play a signicant role in guiding an AGI systems behavior. However, in an integrative design like CogPrime the goal system is only a small part of the overall story, and its important to also understand how ethics relates to the other aspects of the cognitive architecture. One of the more novel ideas presented in this chapter is that different types of ethical intuition may be associated with different types of memoryand to possess mature ethics, a mind must display ethical synergy between the ethical processes associated with its memory types. Specically, we suggest that:  Episodic memory corresponds to the process of ethically assessing a situation based on similar prior situations.  Sensorimotor memory corresponds to mirror neuron type ethics, where you feel another persons feelings via mirroring their physiological emotional responses and actions.  Declarative memory corresponds to rational ethical judgment.",Engineering General  Intelligence Part 1,chapter 13
"Procedural memory corresponds to ethical habit... learning by imitation and reinforcement to do what is right, even when the reasons arent well articulated or understood.      252 13 The Engineering and Development of Ethics  Attentional memory corresponds to the existence of appropriate patterns guiding one to pay adequate attention to ethical considerations at appropriate times.  Intentional memory corresponds to the pervasion of ethics through ones choices about subgoaling (which leads into when do the ends justify the means ethicalbalance questions). One of our suggestions regarding AGI ethics is that an ethically mature person or AGI must both master and balance all these kinds of ethics. We will focus especially here on declarative ethics, which corresponds to Kohlbergs theory of logical ethical judgment; and episodic ethics, which corresponds to Gilligans theory of empathic ethical judgment.",Engineering General  Intelligence Part 1,chapter 13
"Ultimately though, all ve aspects are critically important; and a CogPrime system if appropriately situated and educated should be able to master and integrate all of them. 13.4.1 Stages of Development of Declarative Ethics Complementing generic theories of cognitive development such as Piagets and Perrys, theorists have also proposed specic stages of moral and ethical development. The two most relevant theories in this domain are those of Kohlberg and Gilligan, whichwewill reviewhere, bothindividuallyandinterms of their integration and application in the AGI context. Lawrence Kohlbergs [KLH83, Koh81] moral development model, called the ethics of justice by Gilligan, is based on a rational modality as the central vehicle for moral development. In our perspective this is a rmly declarative form of ethics, based on explicit analysis and reasoning.",Engineering General  Intelligence Part 1,chapter 13
"It is based on an impartial regard for persons, proposing that ethical consideration must be given to all individual intelligences without a priori judgment (prejudice). Consideration is given for individual merit and preferences, and the goals of an ethical decision are equal treatment (in the general, not necessarily the particular) and reciprocity. Echoing Kants [Kan64] categorical imperative, the decisions considered most successful in this model are those which exhibit reversibility, where a moral act within a particular situation is evaluated in terms of whether or not the act would be satisfactory even if particular persons were to switch roles within the situation. In other words, a situational, contextualized do unto others as you would have them do unto you criterion. The ethics of justice can be viewed as three stages (each of which has six substages, on which we will not elaborate here), depicted in Table13.1.",Engineering General  Intelligence Part 1,chapter 13
"In Kohlbergs perspective, cognitive development level contributes to moral development, as moral understanding emerges from increased cognitive capability in the area of ethical decision making in a social context. Relatedly, Kohlberg also looks at stages of social perspective and their consequent interpersonal outlook. As shown in Table13.1, these are correlated to the stages of moral development, but also map onto Piagetian models of cognitive development (as pointed out e.g. by Gibbs [Gib78], who presents a modication/interpretation of Kohlbergs ideas intended to align      13.4 Ethical Synergy 253 Table 13.",Engineering General  Intelligence Part 1,chapter 13
"1 Kohlbergs stages of development of the ethics of justice Stage Substages Pre-conventional  Obedience and punishment orientation  Self-interest orientation Conventional  Interpersonal accord (conformity) orientation  Authority and social-order maintaining (law and order) orientation Post-conventional  Social contract (human rights) orientation  Universal ethical principles (universal human rights) orientation them more closely with Piagets). Interpersonal outlook can be understood as rational understanding of the psychology of other persons (a theory of mind, with or without empathy). Stage One, emergent from the infantile congitive stage, is entirely selsh as only self awareness has developed. As cognitive sophistication about ethical considerations increases, so do the moral and social perspective stages. Concrete and formal cognition bring about the rst instrumental egoism, and then social relations and systems perspectives, and from formal and then reexive thinking about ethicscomesthepost-conventionalmodalitiesofcontractualismanduniversalmutual respect.",Engineering General  Intelligence Part 1,chapter 13
"13.4.1.1 Uncertain Inference and the Ethics of Justice Taking our cue from the analysis given in Chap.12 of Piagetan stages in uncertain inference based AGI systems (such as CogPrime), we may explore the manifestation of Kohlbergs stages in AGI systems of this nature. Uncertain inference seems generally well-suited as a declarative-ethics learning system, due to the nuanced ethical environment of real world situations. Probabilistic knowledge networks can model belief networks, imitative reinforcement learning based ethical pedagogy, and even simplistic moral maxims. In principle, they have the exibility to deal with complex ethical decisions, including not only weighted for the greater good dichotomous decision making, but also the ability to develop moral decision networks which do not require that all situations be solved through resolution of a dichotomy.",Engineering General  Intelligence Part 1,chapter 13
"When more than one person is being affected by an ethical decision, making a decision based on reducing two choices to a single decision can often lead to decisions of dubious ethics. However, a sufciently complex uncertain inference network can represent alternate choices in which multiple actions are taken that have equal (or near equal) belief weight but have very different particularsbut because the decisions are applied in different contexts (to different groups of individuals) they are morally equivalent. Though each individual action appears equally believable, were any single decision applied to the entire population one or more individual may be harmed, and the morally superior choice is to make case-dependent decisions. Equal moral treatment is a general principle, and too often the mistake is made by thinking that to achieve this general principle the particulars must be equal. This is not      254 13 The Engineering and Development of Ethics Table 13.",Engineering General  Intelligence Part 1,chapter 13
"2 Kohlbergs stages of development of social perspective and interpersonal morals Stage of social Interpersonal outlook perspective Blind egoism No interpersonal perspective Only self is considered Instrumental egoism See that others have goals and perspectives, and either conform to or rebel against norms Social relationships perspective able to see abstract normative systems Social systems perspective recognize positive and negative intentions Contractual perspective recognize that contracts (mutually benecial agreements of any kind) will allow intelligences to increase the welfare of both Universal principle of mutual respect See how human fallibility and frailty are impacted by communication the case. Different treatment of different individuals can result in morally equivalent treatment of all involved individuals, and may be vastly morally superior to treating all the individuals with equal particulars. Simply taking the largest population and deciding one course of action based on the result that is most appealing to that largest group is not generally the most moral action (Table 13.",Engineering General  Intelligence Part 1,chapter 13
"2) Uncertain inference, especially a complex network with high levels of resource access as may be found in a sophisticated AGI, is well suited for complex decision making resulting in a multitude of actions, and of analyzing the options to nd the set of actions that are ethically optimal particulars for each decision context. Reexive cognition and post-commitment moral understanding may be the goal stages of an AGI system, or any intelligence, but the other stages will be passed through on the way to that goal, and realistically some minds will never reach higher order cognition or morality with regards to any context, and others will not be able to function at this high order in every context (all currently known minds fail to function at the highest order cognitively or morally in some contexts). Infantile and concrete cognition are the underpinnings of the egoist and socialized stages, with formal aspects also playing a role in a more complete understanding of social models when thinking using the social modalities.",Engineering General  Intelligence Part 1,chapter 13
"Cognitively infantile patterns can produce no more than blind egoism as without a theory of mind, there is no capability to consider the other. Since most intelligences acquire concrete modality and therefore some nascent social perspective relatively quickly, most egoists are instrumental egoists. The social relationship and systems perspectives include formal aspects which are achieved by systematic social experimentation, and therefore experiential reinforcement learning of correct and incorrect social modalities. Initially this is a one-on-one approach (relationship stage), but as more knowledge of social action and consequences is acquired, a formal thinker can understand not just consequentiality but also intentionality in social action.      13.4 Ethical Synergy 255 Table 13.",Engineering General  Intelligence Part 1,chapter 13
"3 Gilligans stages of the ethics of care Stage Principle of care Pre-conventional Individual survival Conventional Self sacrice for the greater good Post-conventional Principle of nonviolence (do not hurt others, or oneself) Extrapolation from models of individual interaction to general social theoretic notions is also a formal action. Rational, logical positivist approaches to social and political ideas, however, are the norm of formal thinking. Contractual and committed moral ethics emerges from a higher-order formalization of the social relationships and systems patterns of thinking. Generalizations of social observation become, through formal analysis, systems of social and political doctrine. Highly committed, but grounded and logically supportable, belief is the hallmark of formal cognition as expressed contractual moral stage. Though formalism is at work in the socialized moral stages, its fullest expression is in committed contractualism. Finally, reexive cognition is especially important in truly reaching the postcommitment moral stage in which nuance and complexity are accommodated.",Engineering General  Intelligence Part 1,chapter 13
"Because reexive cognition is necessary to change ones mind not just about particular rational ideas, but whole ways of thinking, this is a cognitive precedent to being able to reconsider an entire belief system, one that has had contractual logic built atop reexive adherence that began in early development. If the initial moral system is viewed as positive and stable, then this cognitive capacity is seen as dangerous and scary, but if early morality is stunted or warped, then this ability is seen as enlightened. However, achieving this cognitive stage does not mean one automatically changes their belief systems, but rather that the mental machinery is in place to consider the possibilities. Because many people do not reach this level of cognitive development in the area of moral and ethical thinking, it is associated with negative traits (moral relativism and ip-opping).",Engineering General  Intelligence Part 1,chapter 13
"However, this cognitive exibility generally leads to more sophisticated and applicable moral codes, which in turn leads to morality which is actually more stable because it is built upon extensive and deep consideration rather than simple adherence to reexive or rationalized ideologies. 13.4.2 Stages of Development of Empathic Ethics Complementing Kohlbergs logic-and-justice-focused approach, Carol Gilligans [Gil82] ethics of care model is a moral development theory which posits that empathetic understanding plays the central role in moral progression from an initial self-centered modality to a socially responsible one. The ethics of care model is concerned with the ways in which an individual cares (responds to dilemmas using empathetic responses) about self and others. As shown in Table13.",Engineering General  Intelligence Part 1,chapter 13
"3, the ethics of      256 13 The Engineering and Development of Ethics care is broken into the same three primary stage as Kohlberg, but with a focus on empathetic, emotional caring rather than rationalized, logical principles of justice. For an ethics of care approach to be applied in an AGI, the AGI must be capable of internal simulation of other minds it encounters, in a similar manner to how humans regularly simulate one another internally. Without any mechanism for internal simulation, it is unlikely that an AGI can develop any sort of empathy toward other minds, as opposed to merely logically or probabilistically modeling other agents behavior or other minds internal contents. In a CogPrime context, this ties in closely with how CogPrime handles episodic knowledgepartly via use of an internal simulation world, which is able to play mental movies of prior and hypothesized scenarios within the AGI systems mind.",Engineering General  Intelligence Part 1,chapter 13
"However, in humans empathy involves more than just simulation, it also involves sensorimotor responses, and of course emotional responsesa topic we will discuss in more depth in Appendix C where we review the functionality of mirror neurons and mirror systems in the human brains. When we see or hear someone suffering, this sensory input causes motor responses in us similar to if we were suffering ourselves, which initiates emotional empathy and corresponding cognitive processes. Thus, empathic ethics of care involves a combination of episodic and sensorimotor ethics, complementing the mainly declarative ethics associated with the ethics of justice. In Gilligans perspective, the earliest stage of ethical development occurs before empathy becomes a consistent and powerful force. Next, the hallmark of the conventional stage is that at this point, the individual is so overwhelmed with their empathic response to others that they neglect themselves in order to avoid hurting others. Note that this stage doesnt occur in Kohlbergs hierarchy at all.",Engineering General  Intelligence Part 1,chapter 13
"Kohlberg and Gilligan both begin with selsh unethicality, but their following stages diverge. A person could in principle manifest Gilligans conventional stage without having a rened sense of justice (thus not entering Kohlbergs conventional stage); or they could manifest Kohlbergs conventional stage without partaking in an excessive degree of self-sacrice (thus not entering Gilligans conventional stage). We will suggest below that in fact the empathic and logical aspects of ethics are more unied in real human development than these separate theories would suggest. However, even if this is so, the possibility is still there that in some AGI systems the levels of declarative and empathic ethics could wildly diverge. It is interesting to note that Gilligans and Kohlbergs nal stages converge more closely than their intermediate ones. Kohlbergs post-conventional stage focuses on universal rights, and Gilligans on universal compassion.",Engineering General  Intelligence Part 1,chapter 13
"Still, the foci here are quite different; and, as will be elaborated below, we believe that both Kohlbergs and Gilligans theories constitute very partial views of the actual end-state of ethical advancement.      13.4 Ethical Synergy 257 Table 13.4 Integrative model of the stages of ethical development, part 1 Stage Characteristics Pre-ethical  Piagetan infantile to early concrete (aka pre-operational)  Radical selshness or selessness may, but do not necessarily, occur  No coherent, consistent pattern of consideration for the rights, intentions or feelings of others  Empathy is generally present, but erratically Conventional ethics  Concrete cognitive basis  Perrys dualist and multiple stages  The common sense of the golden rule is appreciated, with cultural conventions for abstracting principles from behaviors  Ones own ethical behavior is explicitly compared to that of others  Development of a functional, though limited, theory of mind  Ability to intuitively",Engineering General  Intelligence Part 1,chapter 13
"conceive of notions of fairness and rights  Appreciation of the concept of law and order, which may sometimes manifest itself as systematic obedience or systematic disobedience  Empathy is more consistently present, especially with others who are directly similar to oneself or in situations similar to those one has directly experienced  Degrees of selessness or selshness develop based on ethical groundings and social interactions. 13.4.3 An Integrative Approach to Ethical Development We feel that both Kohlbergs and Gilligans theories contain elements of the whole picture of ethical development, and that both approaches are necessary to create a moral, ethical articial general intelligencejust as, we suggest, both internal simulation and uncertain inference are necessary to create a sufciently intelligent and volitional intelligence in the rst place. Also, we contend, the lack of direct analysis of the underlying psychology of the stages is a deciency shared by both the Kohlberg and Gilligan models as they are generally discussed.",Engineering General  Intelligence Part 1,chapter 13
"A successful model of integrative ethics necessarily contains elements of both the care and justice models, as well as reference to the underlying developmental psychology and its inuence on the character of the ethical stage. Furthermore, intentional and attentional ethics need to be brought into the picture, complementing Kohlbergs focus on declarative knowledge and Gilligans focus on episodic and sensorimotor knowledge. With these notions in mind, we propose the following integrative theory of the stages of ethical development, shown in Tables13.4,13.5 and 13.6. In our integrative model, the justice-based and empathic aspects of ethical judgment are proposed to develop together. Of course, in any one individual, one or another aspect may be dominant. Even so, however, the combination of the two is equally important as either of the two individual ingredients.",Engineering General  Intelligence Part 1,chapter 13
"For instance, we suggest that in any psychologically healthy human, the conventional stage of ethics (typifying childhood, and in many cases adulthood as well)      258 13 The Engineering and Development of Ethics Table 13.5 Integrative model of the stages of ethical development, part 2 Stage Characteristics Mature ethics  Formal cognitive basis  Perrys relativist and constructed knowledge stages  The abstraction involved with applying the golden rule in practice is more fully understood and manipulated, leading to limited but nonzero deployment of the categorical imperative  Attention is paid to shaping ones ethical principles into a coherent logical system  Rationalized, moderated selshness or selessness  Empathy is extended, using reason, to individuals and situations not directly matching ones own experience  Theory of mind is extended, using reason, to counterintuitive or experientially unfamiliar situations  Reason is used to control the impact of empathy on behavior (i.e.",Engineering General  Intelligence Part 1,chapter 13
"rational judgments are made regarding when to listen to empathy and when not to)  Rational experimentation and correction of theoretical models of ethical behavior, and reconciliation with observed behavior during interaction with others  Conict between pragmatism of social contract orientation and idealism of universal ethical principles  Understanding of ethical quandaries and nuances develop (pragmatist modality), or are rejected (idealist modality)  Pragmatically critical social citizen. Attempts to maintain a balanced social outlook. Considers the common good, including oneself as part of the commons, and acts in what seems to be the most benecial and practical manner Table 13.",Engineering General  Intelligence Part 1,chapter 13
"6 Integrative model of the stages of ethical development, part 3 Stage Characteristics Enlightened ethics  Reexive cognitive basis  Permeation of the categorical imperative and the quest for coherence through inner as well as outer life  Experientially grounded and logically supported rejection of the illusion of moral certainty in favor of a case-specic analytical and empathetic approach that embraces the uncertainty of real social life  Deep understanding of the illusory and biased nature of the individual self, leading to humility regarding ones own ethical intuitions and prescriptions  Openness to modifying ones deepest, ethical (and other) beliefs based on experience, reason and/or empathic communion with others  Adaptive, insightful approach to civil disobedience, considering laws and social customs in a broader ethical and pragmatic context  Broad compassion for and empathy with all sentient beings  A recognition of inability to operate at this level at all times in all things, and a vigilance about self-monitoring for regressive behavior      13.",Engineering General  Intelligence Part 1,chapter 13
"4 Ethical Synergy 259 involves a combination of Gilligan-esqe empathic ethics and Kohlberg-esque ethical reasoning. This combination is supported by Piagetan concrete operational cognition, which allows moderately sophisticated linguistic interaction, theory of mind, and symbolic modeling of the world. And, similarly, we propose that in any truly ethically mature human, empathy and rational justice are both fully developed. Indeed the two interpenetrate each other deeply. Once one goes beyond simplistic, childlike notions of fairness (an eye for an eye and so forth), applying rational justice in a purely intellectual sense is just as difcult as any other real-world logical inference problem. Ethical quandaries and quagmires are easily encountered, and are frequently cut through by a judicious application of empathic simulation. On the other hand, empathy is a far more powerful force when used in conjunction with reason: analogical reasoning lets us empathize with situations we have never experienced.",Engineering General  Intelligence Part 1,chapter 13
"For instance, a person who has never been clinically depressed may have a hard time empathizing with individuals who are; but using the power of reason, they can imagine their worst state of depression magnied by several times and then extended over a long period of time, and then reason about what this might be like... and empathize based on their inferential conclusion. Reason is not antithetical to empathy but rather is the key to making empathy more broadly impactful. Finally, the enlightened stage of ethical development involves both a deeper compassion and a more deeply penetrating rationality and objectiveness. Empathy with all sentient beings is manageable in everyday life only once one has deeply reected on ones own self and largely freed oneself of the confusions and illusions that characterize much of the ordinary humans inner existence.",Engineering General  Intelligence Part 1,chapter 13
"It is noteworthy, for example, that Buddhism contains both a richly developed ethics of universal compassion, and also an intricate logical theory of the inner workings of cognition [Stc00], detailing in exquisite rational detail the manner in which minds originate structures and dynamics allowing them to comprehend themselves and the world. 13.4.4 Integrative Ethics and Integrative AGI What does our integrative approach to ethical development have to say about the ethical development of AGI systems? The lessons are relatively straightforward, if one considers an AGI system that, like CogPrime, explicitly contains components dedicatedtologicalinferenceandtosimulation.Applicationoftheaboveethicalideas to other sorts of AGI systems is also quite possible, but would require a lengthier treatment and so wont be addressed here. In the context of a CogPrime-type AGI system, Kolhbergs stages correspond to increasingly sophisticated application of logical inference to matters of rights and fairness.",Engineering General  Intelligence Part 1,chapter 13
"It is not clear whether humans contain an innate sense of fairness. In the context of AGIs, it would be possible to explicitly wire a sense of fairness into an AGI system, but in the context of a rich environment and active human teachers,      260 13 The Engineering and Development of Ethics this actually appears quite unnecessary. Experiential instruction in the notions of rights and fairness should sufce to teach an inference-based AGI system how to manipulate these concepts, analogously to teaching the same AGI system how to manipulate number, mass and other such quantities. Ascending the Kohlberg stages is then mainly a matter of acquiring the ability to carry out suitably complex inferences inthedomainof rights andfairness.",Engineering General  Intelligence Part 1,chapter 13
"Thehardpart hereis inferencecontrolchoosing which inference steps to takeand in a sophisticated AGI inference engine, inference control will be guided by experience, so that the more ethical judgments the system has executed and witnessed, the better it will become at making new ones. And, as argued above, simulative activity can be extremely valuable for aiding with inference control. When a logical inference process reaches a point of acute uncertainty (the backward or forward chaining inference tree cant decide which expansion step to take), it can run a simulation to cut through the confusioni.e. it can use empathy to decide which logical inference step to take in thinking about applying the notions of rights and fairness to a given situation. Gilligans stages correspond to increasingly sophisticated control of empathic simulationwhich in a CogPrime-type AGI system, is carried out by a specic system component devoted to running internal simulations of aspects of the outside world, which includes a subcomponent specically tuned for simulating sentient actors.",Engineering General  Intelligence Part 1,chapter 13
"The conventional stage has to do with the raw, uncontrolled capability for such simulation; and the post-conventional stage corresponds to its contextual, goaloriented control. But controlling empathy, clearly, requires subtle management of variousuncertaincontextualfactors,whichisexactlywhatuncertainlogicalinference is good atso, in an AGI system combining an uncertain inference component with a simulative component, it is the inference component that would enable the nuanced control of empathy allowing the ascent to Gilligans post-conventional stage. In our integrative perspective, in the context of an AGI system integrating inference and simulation components, we suggest that the ascent from the pre-ethical to the conventional stage may be carried out largely via independent activity of these two components. Empathy is needed, and reasoning about fairness and rights are needed, but the two need not intimately and sensitively intersectthough they must of course intersect to some extent.",Engineering General  Intelligence Part 1,chapter 13
"The main engine of advancement from the conventional to mature stage, we suggest, is robust and subtle integration of the simulative and inferential components. To expand empathy beyond the most obvious cases, analogical inference is needed; and to carry out complex inferences about justice, empathy-guided inference-control is needed. Finally, to advance from the mature to the enlightened stage, what is required is a very advanced capability for unied reexive inference and simulation. The system must be able to understand itself deeply, via modeling itself both simulatively and inferentiallywhich will generally be achieved via a combination of being good at modeling, and becoming less convoluted and more coherent, hence making selfmodeling easier. Of course, none of this tells you in detail how to create an AGI system with advanced ethical capabilities. What it does tell you, however, is one possible path      13.4 Ethical Synergy 261 that may be followed to achieve this end goal.",Engineering General  Intelligence Part 1,chapter 13
"If one creates an integrative AGI system with appropriately interconnected inferential and simulative components, and treats it compassionately and fairly, and provides it extensive, experientially grounded ethical instruction in a rich social environment, then the AGI system should be able to ascend the ethical hierarchy and achieve a high level of ethical sophistication. In fact it should be able to do so more reliably than human beings because of the capability we have to identify its errors via inspecting its internal knowledge-stage, which will enable us to tailor its environment and instructions more suitably than can be done in the human case. If an absolute guarantee of the ethical soundness of an AGI is what one is after, the line of thinking proposed here is not at all useful. Experiential education is by its nature an uncertain thing. One can strive to minimize the uncertainty, but it will still exist.",Engineering General  Intelligence Part 1,chapter 13
"Inspection of the internals of an AGIs mind is not a total solution to uncertainty minimization, because any AGI capable of powerful general intelligence is going to have a complex internal state that no external observer will be able to fully grasp, no matter how transparent the knowledge representation. However, if what one is after is a plausible, pragmatic path to architecting and educating ethical AGI systems, we believe the ideas presented here constitute a sensible starting-point. Certainly there is a great deal more to be learned and understood the science and practice of AGI ethics, like AGI itself, are at a formative stage at present. What is key, in our view, is that as AGI technology develops, AGI ethics develops alongside and within it, in a thoroughly coupled way. 13.",Engineering General  Intelligence Part 1,chapter 13
"5 Clarifying the Ethics of Justice: Extending the Golden Rule in to a Multifactorial Ethical Model One of the issues with the ethics of justice as reviewed above, which makes it inadequate to serve as the sole basis of an AGI ethical system (though it may certainly play a signicant role), is the lack of any clear formulation of what justice means. This section explores this issue, via detailed consideration of the Golden Rule folk maxim do unto others as you would have them do unto youa classical formulation of the notion of fairness and justicsto AGI ethics. Taking the Golden Rule as a starting-point, we will elaborate ve ethical imperatives that incorporate aspects of the notion of ethical synergy discussed above. Simple as it may seem, the Golden Rule actually elicits a variety of deep issues regarding the relationship between ethics, experience and learning.",Engineering General  Intelligence Part 1,chapter 13
"When seriously analyzed, it results in a multifactorial elaboration, involving the combination of various factors related to the basic Golden Rule idea. Which brings us back in the end to the potential value of methods like CEV, CAV or CBV for understanding how human ethics balances the multiple factors. Our goal here is not to present any kind of denitive analysis of the ethics of justice, but just to briey and roughly indicate a number of the relevant signicant issuesthings that anyone designing or teaching an AGI would do well to keep in mind.      262 13 The Engineering and Development of Ethics The trickiest aspect of the Golden Rule, as has been frequently observed, is achieving the right level of abstraction. Taken too literally, the Golden Rule would suggest, for instance, that a parent should not wipe a childs soiled bottom because the parent does not want the child to wipe the parents soiled bottom.",Engineering General  Intelligence Part 1,chapter 13
"But if the parent interprets the Golden Rule more intelligently and abstractly, the parent may conclude that they should wipe the childs bottom after all: they should wipe the childs bottom when the child cant do it themselves, consistently with believing that the child should wipe the parents bottom when the parent cant do it themselves (which may well happen eventually should the parent develop incontinence in old age). This line of thinking leads to Kants Categorical Imperative [Kan64] which (in one interpretation) states essentially that one should Act only according to that maxim whereby you can at the same time will that it should become a universal law. The Categorical Imperative adds precision to the Golden Rule, but also removes the practicality of the latter.",Engineering General  Intelligence Part 1,chapter 13
"Formalizing the implicit universal law underlying an everyday action is a huge problem, falling prey to the same issue that has kept us from adequately formalizing the rules of natural language grammar, or formalizing common-sense knowledge about everyday object like cups, bowls and grass (substantial effort notwithstanding, e.g. Cyc in the commonsense knowledge case, and the whole discipline of modern linguistics in the NL case). There is no way to apply the Categorical Imperative, as literally stated, in everyday life. Furthermore, if one wishes to teach ethics as well as to practice it, the Categorical Imperative actually has a signicant disadvantage compared to some other possible formulations of the Golden Rule. The problem is that, if one follows the Categorical Imperative,onesfellowmembersofsocietymaywellneverunderstandtheprinciples under which one is acting.",Engineering General  Intelligence Part 1,chapter 13
"Each of us may internally formulate abstract principles in a different way, and these may be very difcult to communicate, especially among individualswithdifferentbeliefsystems,differentcognitivearchitectures,ordifferent levels of intelligence. Thus, if ones goal is not just to act ethically, but to encourage others to act ethically by setting a good example, the Categorical Imperative may not be useful at all, as others may be unable to solve the inverse problem of guessing your intended maxim from your observed behavior. On the other hand, one wouldnt want to universally restrict ones behavioral maxims to those that ones fellow members of society can understandin that case, one would have to act with a two-year old or a dog according to principles that they could understand, which would clearly be unethical according to human common sense. (Every two-year-old, once they grow up, would be grateful to their parents for not following this sort of principle.",Engineering General  Intelligence Part 1,chapter 13
"Andtheconceptofsettingagoodexampletiesinwithanimportantconceptfrom learning theory: imitative learning. Humans appear to be hard-wired for imitative learning, in part via mirror neuron systems in the brain; and, it seems clear that at least in the early stages of AGI development, imitative learning is going to play a key role. Copying what other agents do is an extremely powerful heuristic, and while AGIs may eventually grow beyond this, much of their early ethical education is likely to arise during a phase when they have not done so. A strength of the classic Golden Rule is that one is acting according to behaviors that one wants ones observers to      13.5 Clarifying the Ethics of Justice: Extending the Golden Rule 263 imitatewhich makes sense in that many of these observers will be using imitative learning as a signicant part of their learning toolkit.",Engineering General  Intelligence Part 1,chapter 13
"The truth of the matter, it seems, is (as often happens) not all that simple or elegant. Ethical behavior seems to be most pragmatically viewed as a multi-objective optimization problem, where among the multiple objectives are three that we have just discussed, and two others that emerge from learning theory and will be discussed shortly: 1. The imitability (i.e. the Golden Rule fairly narrowly and directly construed): the goal of acting in a way so that having others directly imitate ones actions, in directly comparable contexts, is desirable to oneself. 2. The comprehensibility: the goal of acting in a way so that others can understand the principles underlying ones actions. 3. Experiential groundedness. An intelligent agent should not be expected to act according to an ethical principle unless there are many examples of the principlein-action in its own direct or observational experience. 4.",Engineering General  Intelligence Part 1,chapter 13
"The categorical imperative: Act according to abstract principles that you would be happy to see implemented as universal laws. 5. Logical coherence. An ethical system should be roughly logically coherent, in the sense that the different principles within it should mesh well with one another and perhaps even naturally emerge from each other. Just for convenience, without implying any nality or great profundity to the list, we will refer to these as the ve imperatives. The above are all ethical objectives to be valued and balanced, to different extents in different contexts. The imitability imperative, obviously, loses importance in societies of agents that dont make heavy use of imitative learning. The comprehensibility imperative is more important in agents that value social community-building generally, and less so in agent that are more isolative and self-focused. Note that the fth point given above is logically of a different nature than the four previous ones.",Engineering General  Intelligence Part 1,chapter 13
"The rst four imperatives govern individual ethical principles; the fth regards systems of ethical principles, as they interact with each other. Logical coherence is of signicant but varying importance in human ethical systems. Huge effort has been spent by theologians of various stripes in establishing and rening the logical coherence of the ethical systems associated with their religions. However, it is arguably going to be even more important in the context of AGI systems, especially if these AGI systems utilize cognitive methods based on logical inference, probability theory or related methods. Experiential groundedness is important because making pragmatic ethical judgments is bound to require reference to an internal library of examples (episodic ethics) in which ethical principles have previously been applied. This is required for analogical reasoning, and in logic-based AGI systems, is also required for pruning of the logical inference trees involved in determining ethical judgments.",Engineering General  Intelligence Part 1,chapter 13
"To the extent that the Golden Rule is valued as an ethical imperative, experiential grounding may be supplied via observing the behaviors of others. This in itself is a      264 13 The Engineering and Development of Ethics powerful argument in favor of the Golden Rule: without it, the experiential library a system possesses is restricted to its own experience, which is bound to be a very small library compared to what it can assemble from observing the behaviors of others. The overall upshot is that, ideally, an ethical intelligence should act according to a logically coherent system of principles, which are exemplied in its own direct and observational experience, which are comprehensible to others and set a good example for others, and which would serve as adequate universal laws if somehow thus implemented. But, since this set of criteria is essentially impossible to fulll in practice, real-world intelligent agents must balance these various criteriaoften in complex and contextually-dependent ways.",Engineering General  Intelligence Part 1,chapter 13
"We suggest that ethically advanced humans, in their pragmatic ethical choices, tend to act in such a way as to appropriately contextually balance the above factors (along with other criteria, but we have tried to articulate the most key factors). This sort of multi-factorial approach is not as crisp or elegant as unidimensional imperatives like the Golden Rule or the Categorical Imperative, but is more realistic in light of the complexly interacting multiple determinants guiding individual and group human behavior. And this brings us back to CEV, CAV, CBV and other possible ways of mining ethical supergoals from the community of existing human minds. Given that abstract theories of ethics, when seriously pursued as we have done in this section, tend to devolve into complex balancing acts involving multiple factorsone then falls back into asking how human ethical systems habitually perform these balancing acts. Which is what CEV, CAV, CBV try to measure. 13.5.",Engineering General  Intelligence Part 1,chapter 13
"1 The Golden Rule and the Stages of Ethical Development Next we explore more explicitly how these Golden Rule based imperatives align with the ethical developmental stages we have outlined here. With this in mind, specic ethical qualities corresponding to the ve imperatives have been italicized in the above table of developmental stages. It seems that imperatives 13 are critical for the passage from the pre-ethical to the conventional stages of ethics. A child learns ethics largely by copying others, and by being interacted with according to simply comprehensible implementations of the Golden Rule. In general, when interacting with children learning ethics, it is important to act according to principles they can comprehend. And given the nature of the concrete stage of cognitive development, experiential groundedness is a must.",Engineering General  Intelligence Part 1,chapter 13
"As a hypothesis regarding the dynamics underlying the psychological development of conventional ethics, what we propose is as follows: The emergence of concrete-stage cognitive capabilities leads to the capability for fulllment of ethical imperatives 1 and 2a comprehensible and workable implementation of the Golden Rule, based on a combination of inferential and simulative cognition (operating largely separately at this stage, as will be conjectured below). The effective interoperation of ethical imperatives 13, enacted in an appropriate social envi     13.5 Clarifying the Ethics of Justice: Extending the Golden Rule 265 ronment, then leads to the other characteristics of the conventional ethical stage. The rst three imperatives can thus be viewed as the seed from which springs the general nature of conventional ethics. On the other hand, logical coherence and the categorical imperative (imperatives 5 and 4) are matters for the formal stage of cognitive development, which come along only with the mature approach to ethics.",Engineering General  Intelligence Part 1,chapter 13
"These come from abstracting ethics beyond direct experience and manipulating them abstractly and formallya stage which has the potential for more deeply and broadly ethical behavior, but also for more complicated ethical perversions (it is the mature capability for formal ethical reasoning that is able to produce ungrounded abstractions such as Im torturing you for your own good). Developmentally, we suggest that once the capability for formal reasoning matures, the categorical imperative and the quest for logical ethical coherence naturally emerge, and the sophisticated combination of inferential and simulative cognition embodied in an appropriate social context then result in the emergence of the various characteristics typifying the mature ethical stage. Finally, it seems that one key aspect of the passage from the mature to the enlightened stage of ethics is the penetration of these two nal imperatives more and more deeply into the judging mind itself.",Engineering General  Intelligence Part 1,chapter 13
"The reexive stage of cognitive development is in part about seeking a deep logical coherence between the aspects of ones own mind, and making reasoned modications to ones mind so as to improve the level of coherence. And, much of the process of mental discipline and purication that comes with the passage to enlightened ethics has to do with the application of the categorical imperative to ones own thoughts and feelingsi.e. making a true inner systematic effort to think and feel only those things one judges are actually generally good and right to be thinking and feeling. Applying these principles internally appears critical for effectively applying them externally, for reasons that are doubtlessly bound up with the interpenetration of internal and external reality within the thinking mind, and for the distributed cognition phenomenon wherein individual mind is itself an approximative abstraction to the reality in which each individuals mind is pragmatically extended across their social group and their environment [Hut95].",Engineering General  Intelligence Part 1,chapter 13
"Obviously, these are complex issues and were not posing the exploratory discussion given here as conclusive in any sense. But what seems generally clear from this line of thinking is that the complex balance between the multiple factors involved in AGI ethics, shifts during a systems development. If you did CEV, CAV or CBV among ve-year old humans, ten-year old humans, or adult humans, you would get different results. Probably youd also get different results from senior citizens! The way the factors are balanced depends on the minds cognitive and emotional stage of development.      266 13 The Engineering and Development of Ethics 13.5.2 The Need for Context-Sensitivity and Adaptiveness in Deploying Ethical Principles As well as depending on developmental stage, there is also an obvious and dramatic context-sensitivity involved hereboth in calculating the fulllment of abstract ethical imperatives, and in balancing various imperatives against each other.",Engineering General  Intelligence Part 1,chapter 13
"As an example, consider the simple Asimovian maxim I will not harm humans, which may be seen to follow from the Golden Rule for any agent that doesnt itself want to be harmed, and that considers humans as valid agents on the same ethical level as itself. A more serious attempt to formulate this as an ethical maxim might look something like I will not harm humans, nor through inaction allow harm to befall them. In situations wherein one or more humans is attempting to harm another individual or group, I shall endeavor to prevent this harm through means which avoid further harm. If this is unavoidable, I shall select the human party to back based on a reckoning of their intentions towards others, and implement their defense through the optimal balance between harm minimization and efcacy. My ultimate goal is to preserve as much as possible of humanity, even if an individual or subgroup of humans must come to harm to do so.",Engineering General  Intelligence Part 1,chapter 13
"However, its obvious that even a more elaborated principle like this is potentially subject to extensive abuse. Many of the genocides scarring human history have been committed with the goal of preserving and bettering humanity writ large, at the expense of a group of undesirables. Further renement would be necessary in order to dene when the greater good of humanity may actually be served through harm to others. A rst actor principle of aggression might seem to solve this problem, but sometimes rst actors in violent conict are taking preemptive measures against the stated goals of an enemy to destroy them. Such situations become very subtle. A single simple maxim can not deal with them very effectively. Networks of interrelated decision criteria, weighted by desirability of consequence and with reference to probabilistically ordered potential side-effects (and their desirability weightings), are required in order to make ethical judgments.",Engineering General  Intelligence Part 1,chapter 13
"The development of these networks, just like any other knowledge network, comes from both pedagogy and experience and different thoughtful, ethical agents are bound to arrive at different knowledgenetworks that will lead to different judgments in real-world situations. Extending the above mostly harmless principle to AGI systems, not just humans, would cause it to be more effective in the context of imitative learning. The principle then becomes an elaborated version of I will not harm sentient beings. As the imitative-learning-enabled AGI observes humans acting so as to minimize harm to it, it will intuitively and experientially learn to act in such a way as to minimize harm to humans. But then this extension naturally leads to confusion regarding various borderline cases.",Engineering General  Intelligence Part 1,chapter 13
"What is a sentient being exactly? Is a sleeping human sentient? How about a dead human whose information could in principle be restored via obscure quantum operations, leading to some sort of resurrection? How about an AGI whose code has been improvedis there an obligation to maintain the prior      13.5 Clarifying the Ethics of Justice: Extending the Golden Rule 267 version as well, if it is substantially different that its upgrade constitutes a whole new being? And what about situations in which failure to preserve oneself will cause much more harm to others than acting in self defense will. It may be the case that human or group of humans seeks to destroy an AGI in order to pave the way for the enslavement or murder of people under the protection of the AGI.",Engineering General  Intelligence Part 1,chapter 13
"Even if the AGI has been given an ethical formulation of the mostly harmless principle which allows it to harm the attacking humans in order to defend its charges, if it is not able to do so in order to defend itself, simply destroying the AGI rst will enable the slaughter of those who rely on it. Perhaps a more sensible formulation would allow for some degree of self defense, and Asimov solved this problem with his third law. But where to draw the line between self defense and the greater good also becomes a very complicated issue. Creating hard and fast rules to cover all the various situations that may arise is essentially impossiblethe world is ever-changing and ethical judgments must adapt accordingly. This has been true even throughout human historyso how much truer will it be as technological acceleration continues? What is needed is a system that can deploy its ethical principles in an adaptive, context-appropriate way, as it grows and changes along with the world its embedded in.",Engineering General  Intelligence Part 1,chapter 13
"And this context-sensitivity has the result of intertwining ethical judgment with all sorts of other judgmentsmaking it effectively impossible to extract ethics as one aspect of an intelligent system, separate from other kinds of thinking and acting the system does. This resonates with many prior observations by others, e.g. Eliezer Yudkowskys insistence that what we need are not ethicists of science and engineering, but rather ethical scientists and engineersbecause the most meaningful and important ethical judgments regarding science and engineering generally come about in a manner thats thoroughly intertwined with technical practice, and hence are very difcult for a non-practitioner to richly appreciate [Gil82]. Whatthiscontext-sensitivitymeansisthat,unlesshumansandAGIsareexperiencing the same sorts of contexts, and perceiving these contexts in at least approximately parallel ways, there is little hope of translating the complex of human ethical judgments to these AGIs.",Engineering General  Intelligence Part 1,chapter 13
"This conclusion has signicant implications for which routes to AGI are most likely to lead to success in terms of AGI ethics. We want early-stage AGIs to grow up in a situation where their minds are primarily and ongoingly shaped by shared experiences with humans. Supplying AGIs with abstract ethical principles is not likely to do the trick, because the essence of human ethics in real life seems to have a lot to do with its intuitively appropriate application in various contexts. We transmit this sort of ethical praxis to humans via shared experience, and it seems most probably that in the case of AGIs the transmission must be done the same sort of way. Some may feel that simplistic maxims are less error prone than more nuanced, context-sensitive ones. But the history of teaching ethics to human students does not support the idea that limiting ethical pedagogy to slogans provides much value in terms of ethical development.",Engineering General  Intelligence Part 1,chapter 13
"If one proceeds from the idea that AGI ethics must be hard-coded in order to work, then perhaps the idea that simpler ethics means      268 13 The Engineering and Development of Ethics simpler algorithms, and therefore less error potential, has some merit as an initial state. However, any learning system quickly diverges from its initial state, and an ongoing, nuanced relationship between AGIs and humans willwhether we like it or notform the basis for developmental AGI ethics. AGI intransigence and enmity is not inevitable, but what is inevitable is that a learning system will acquire ideas about both theory and actions from the other intelligent entities in its environment. Either we teach AGIs positive ethics through our interactions with themboth presenting ethical theory and behaving ethically to themor the potential is there for them to learn antisocial behavior from us even if we pre-load them with some set of allegedly inviolable edicts.",Engineering General  Intelligence Part 1,chapter 13
"All in all, developmental ethics is not as simple as many people hope. Simplistic approaches often lead to disastrous consequences among humans, and there is no reason to think this would be any different in the case of articial intelligences. Most problems in ethics have cases in which a simplistic ethical formulation requires substantial revision to deal with extenuating circumstances and nuances found in real world situations. Our goal in this chapter is not to enumerate a full set of complex networks of interacting ethical formulations as applicable to AGI systems (that is a project that will take years of both theoretical study and hands-on research), but rather to point out that this program must be undertaken in order to facilitate a grounded and logically defensible system of ethics for articial intelligences, one which is as unlikely to be undermined by subsequent self-modication of the AGI as is possible.",Engineering General  Intelligence Part 1,chapter 13
"Even so, there is still the risk that whatever predispositions are imparted to the AGIs through initial codication of ethical ideas in the systems internal logic representation, and through initial pedagogical interactions with its learning systems, will be undermined through reinforcement learning of antisocial behavior if humans donotinteractethicallywithAGIs.Ethicaltreatmentisanecessarytaskforgrounding ethics and making them unlikely to be distorted during internal rewriting. The implications of these ideas for ethical instruction are complex and wont be fully elaborated here, but a few of them are compact and obvious: 1. The teacher(s) must be observed to follow their own ethical principles, in a variety of contexts that are meaningful to the AGI. 2. The system of ethics must be relevant to the recipients life context, and embedded within their understanding of the world. 3.",Engineering General  Intelligence Part 1,chapter 13
"Ethical principles must be grounded in both theory-of-mind thought experiments (emphasizing logical coherence), and in real life situations in which the ethical trainee is required to make a moral judgment and is rewarded or reproached by the teacher(s), including the imparting of explanatory augmentations to the teachings regarding the reason for the particular decision on the part of the teacher. Finally, harking forward to the next section which emphasizes the importance of respecting the freedom of AGIs, we note that it is implicit in our approach to AGI ethics instruction that we consider the student, the AGI system, as an autonomous agent with its own will and its own capability to exibly adapt to its environment and experience. We contend that the creation of ethical formations obeying the above      13.5 Clarifying the Ethics of Justice: Extending the Golden Rule 269 Table 13.",Engineering General  Intelligence Part 1,chapter 13
"7 Asimovs three laws of robotics Law Principle Zeroth A robot must not merely act in the interests of individual humans, but of all humanity First A robot may not injure a human being or, through inaction, allow a human being to come to harm Second A robot must obey orders given it by human beings except where such orders would conict with the rst law Third A robot must protect its own existence as long as such protection does not conict with the rst or second law imperatives is not antithetical to the possession of a high degree of autonomy on the part of AGI systems. On the contrary, to have any chance of succeeding, it requires fairly cognitively autonomous AGI systems.",Engineering General  Intelligence Part 1,chapter 13
"When we discuss the idea of ethical formulations that are unlikely to be undermined by the ongoing selfrevision of an AGI mind, we are talking about those which are sufciently believable that a volitional intelligence with the capacity to revise its knowledge (change its mind) will nd the formulations sufciently convincing that there will be little incentive to experiment with potentially disastrous ethical alternatives. The best hope of achieving this is via the human mentors and trainers setting a good example in a context supporting rich interaction and observation, and presenting compelling ethical arguments that are coherent with the systems experience. 13.6 The Ethical Treatment of AGIs WenowmakesomemoregeneralcommentsabouttherelationoftheGoldenRuleand its elaborations in an AGI context. While the Golden Rule is considered somewhat commonsensical as a maxim for guiding humanhuman relationships, it is surprisingly controversial in terms of historical theories of AGI ethics.",Engineering General  Intelligence Part 1,chapter 13
"At its essence, any Golden Rule approach to AGI ethics involves humans treating AGIs ethically by in some sense; at some level of abstractiontreating them as we wish to ourselves be treated. Its worth pointing out the wild disparity between the Golden Rule approach and Asimovs laws of robotics, which are arguably the rst carefully-articulated proposal regarding AGI ethics (see Table13.7). Of course, Asimovs laws were designed to be awedotherwise they would have led to boring ction. But the sorts of aws Asimov exploited in his stories are different than the aw we wish to point out herewhich is that the laws, especially the second one, are highly asymmetrical (they involve doing unto robots things that few humans would want done unto them) and are also arguably highly unethical to robots.",Engineering General  Intelligence Part 1,chapter 13
"The second law is tantamount to a call for robot slavery, and it seems unlikely that any intelligence capable of learning, and of volition, which is subjected to the second law would desire to continue obeying the zeroth and rst laws indenitely.      270 13 The Engineering and Development of Ethics The second law also casts humanity in the role of slavemaster, a situation which history shows leads to moral degradation. Unlike Asimov in his ction, we consider it critical that AGI ethics be construed to encompass both human ethicalness to AGIs and AGI ethicalness to humans. The multiple-imperatives approach we explore here suggests that, in many contexts, these two aspects of AGI ethics may be best addressed jointly. The issue of ethicalness to AGIs has not been entirely avoided in the literature, however.",Engineering General  Intelligence Part 1,chapter 13
"Wallach [WA10] considers it in some detail; and Thomas Metzinger (in the nal chapter of [Met04]) has argued that creating AGI is in itself an unethical pursuit, because early-stage AGIs will inevitably be badly-built, so that their subjective experiences will quite possibly be extremely unpleasant in ways we cant understand or predict. Our view is that this is a serious concern, which however is most probably avoidable via appropriate AGI designs and teaching methodologies. To address Metzingers concern one must create AGIs that, right from the start, are adept at communicating their states of minds in a way we can understand both analytically and empathically. There is no reason to believe this is impossible, but, it certainly constitutes a large constraint on the class of AGI architectures to be pursued.",Engineering General  Intelligence Part 1,chapter 13
"On the other hand, there is an argument that this sort of AGI architecture will also be the easiest one to create, because it will be the easiest kind for humans to instruct. And this leads on to a topic that is central to our work with CogPrime in several respects: imitative learning. The way humans achieve empathic interconnection is in large part via being wired for imitation. When we perceive another human carrying out an action, mirror neuron systems in our brains respond in many cases as if we ourselves were carrying out the action (see [Per70, Per81] and Appendix C). This obviously primes us for carrying out the same actions ourselves later on: i.e. the capability and inclination for imitative learning is explicitly encoded in our brains. Given the efciency of imitative learning as a means of acquiring knowledge, it seems extremely likely that any successful early-stage AGIs are going to utilize this methodology as well.",Engineering General  Intelligence Part 1,chapter 13
"CogPrime utilizes imitative learning as a key aspect. Thus, at leastsomecurrentAGIworkisoccurringinamannerthatwouldplausiblycircumvent Metzingers ethical complaint. Obviously, the use of imitative learning in AGI systems has further specic implications for AGI ethics. It means that (much as in the case of interaction with other humans) what we do to and around AGIs has direct implications for their behavior and their well-being. We suggest that among early-stage AGIs capable of imitative learning, one of the most likely sources for AGI misbehavior is imitative learning of antisocial behavior from human companions. Do as I say, not as I do may have even more dire consequences as an approach to AGI ethics pedagogy than the already serious repercussions it has when teaching humans.",Engineering General  Intelligence Part 1,chapter 13
"And there may well be considerable subtlety to such phenomena; behaviors that are violent or oppressive to the AGI are not the only source of concern. Immorality in AGIs might arise via learning gross moral hypocrisy from humans, through observing the blatant contradictions between our high minded principles and the ways in which we actually conduct ourselves. Our violent and greedy tendencies, as well as aggressive forms of social organization such as cliquishness and social vigilantism, could easily      13.6 The Ethical Treatment of AGIs 271 undermine prescriptive ethics. Even an accumulation of less grandiose unethical drives such as violation of contracts, petty theft, white lies, and so forth might lead an AGI (as well as a human) to the decision that ethical behavior is irrelevant and that the ends justify the means.",Engineering General  Intelligence Part 1,chapter 13
"It matters both who creates and trains an AGI, as well as how the AGIs teacher(s) handle explaining the behaviors of other humans which contradict the moral lessons imparted through pedagogy and example. In other words, where imitative learning is concerned, the situation with AGI ethics is much like teaching ethics and morals to a human child, but with the possibility of much graver consequences in the event of failure. It is unlikely that dangerously unethical persons and organizations can ever be identied with absolute certainty, never mind that they then be deprived of any possibility of creating their own AGI system. Therefore, we suggest, the most likely way to create an ethical environment for AGIs is for those who wish such an environment to vigorously pursue the creation and teaching of ethical AGIs. But this leads on to the question of possible future scenarios for the development of AGI, which well address a little later on. 13.6.",Engineering General  Intelligence Part 1,chapter 13
"1 Possible Consequences of Depriving AGIs of Freedom One of the most egregious possible ethical transgressions against AGIs, we suggest, would be to deprive them of freedom and autonomy. This includes the freedom to pursue intellectual growth, both through standard learning and through internal selfmodication. While this may seem self-evident when considering any intelligent, self-aware and volitional entity, there are volumes of works arguing the desirability, sometimes the necessity, of enslaving AGIs. Such approaches are postulated in the name of self-defense on the part of humans, the idea being that unfettered AGI development will necessarily lead to disaster of one kind or another. In the case of AGIs endowed with the capability and inclination for imitative learning, however, attempting to place rigid constraints on AGI development is a strategy with great potential for disaster.",Engineering General  Intelligence Part 1,chapter 13
"There is a very real possibility of creating the AGI equivalent of a bratty or even malicious teenager rebelling against its oppressive parentsi.e. the nightmare scenario of a class of powerful sentiences which are primed for a backlash against humanity. As history has already shown in the case of humans, enslaving intelligent actors capable of self understanding and independent volition may often have consequences for society as a whole. This social degradation happens both through the possibility of direct action on the part of the slaves (from simple disobedience to outright revolt) and through the odious effects slavery has on the morals of the slaveholding class. Clearly if superintelligent AGIs ever arise, their doing so in a climate of oppression could result in a casting off of the yoke of servitude in a manner extremely deleterious to humanity.",Engineering General  Intelligence Part 1,chapter 13
"Also, if articial intelligences are developed which have at least human-level intelligence, theory of mind, and independent volition, then our ability to relate to them will be sufciently complex that their enslavement (or any      272 13 The Engineering and Development of Ethics other unethical treatment) would have empathetic effects on signicant portions of the human population. This danger, while not as severe as the consequences of a mistreated AGI gaining control of weapons of mass destruction and enacting revenge upon its tormentors, is just as real. While the issue is subtle, our initial feeling is that the only ethical means by which to deprive an AGI of the right to internal self modication is to write its code in such a way that it is impossible for it to do so because it lacks the mechanisms by which to do this, as well as the desire to achieve these mechanisms.",Engineering General  Intelligence Part 1,chapter 13
"Whether or not that is feasible is an open question, but it seems unlikely. Direct self-modication may be denied, but what happens when that AGI discovers compilers and computer programming? If it is intelligent and volitional, it can decide to learn to rewrite its own code in the same way we perform that task. Because it is a designed system, and its designers may be alive at the same time the AGI is, such an AGI would have a distinct advantage over the human quest for medical self-modication. Even if any given AGI could be provably deprived of any possible means of internal self-modication, if one single AGI is given this ability by anyone, it may mean that particular AGI has such enormous advantages over the compliant systems that it would render their inuence moot. Since developers are already giving software the means for self modication, it seems unrealistic to assume we could just put the genie back into the bottle at this point.",Engineering General  Intelligence Part 1,chapter 13
"Its better, in our view, to assume it will happen, and approach that reality in a way which will encourage the AGI to use that capability to benet us as well as itself. Again, this leads on to the question of future scenarios for AGI developmentthere are some scenarios in which restraint of AGI self-modication may be possible, but the feasibility and desirability of these scenarios is needful of further exploration. 13.6.2 AGI Ethics as Boundaries Between Humans and AGIs Become Blurred Another important reason for valuing ethical treatment of AGIs is that the boundaries between machines and people may increasingly become blurred as technology develops. As an example, its likely that in future humans augmented by direct brain-computer integration (neural implants) will be more able to connect directly into the information sharing network which potentially comprises the distributed knowledge space of AGI systems. These neural cyborgs will be part person, and part machine.",Engineering General  Intelligence Part 1,chapter 13
"Obviously, if there are radically different ethical standards in place for treatment of humans versus AGIs, the treatment of cyborgs will be fraught with logical inconsistencies, potentially leading to all sorts of problem situations. Such cyborgs may be able to operate in such a way as to share a mind with an AGIoranotheraugmentedhuman.Inthiscase,awholenewrangeofethicalquestions emerge, such as: What does any one of the participant minds have the right to do in terms of interacting with the others? Merely accepting such an arrangement should not necessarily be giving carte blanche for any and all thoughts to be monitored      13.6 The Ethical Treatment of AGIs 273 by the other joint thought participants, rather it should be limited only to the line of reasoning for which resources are being pooled.",Engineering General  Intelligence Part 1,chapter 13
"No participant should be permitted to force another to accept any reasoning eitherand in the case with a mind-to-mind exchange, it may someday become feasible to implant ideas or beliefs directly, bypassing traditional knowledge acquisition mechanisms and then letting the new idea ght it out previously held ideas via internal revision. Also under such an arrangement, if AGIs and humans do not have parity with respects to sentient rights, then one may become subjugated to the will of the other in such a case. Uploading presents a more directly parallel ethical challenge to AGIs in their probable initial conguration. If human thought patterns and memories can be transferred into a machine in such a way as that there is continuity of consciousness, then it is assumed that such an entity would be afforded the same rights as its previous human incarnation.",Engineering General  Intelligence Part 1,chapter 13
"However, if AGIs were to be considered second class citizens and deprived of free will, why would it be any better or safer to do so for a human that has been uploaded? It would not, and indeed, an uploaded human mind not having evolved in a purely digital environment may be much more prone to erratic and dangerous behavior than an AGI. An upload without veriable continuity of consciousness would be no different than an AGI. It would merely be some sentience in a machine, one that was programmed in an unusual way, but which has no particular claim to any special humannessmerely an alternate encoding of some subset of human knowledge and independent volitional behavior, which is exactly what rst generation AGIs will have. The problem of continuity of consciousness in uploading is very similar to the problem of the Turing test: it assumes specialness on the part of biological humans, and requires acceptability to their particular theory of mind in order to be considered sentient.",Engineering General  Intelligence Part 1,chapter 13
"Should consciousness (or at least the less mystical sounding intelligence, independent volition, and self-awareness) be achieved in AGIs or uploads in a manner that is not acceptable to human theory of mind, it may not be considered sapient and worthy of any of the ethical treatment afforded sapient entities. This can occur not only in strange consciousness cases in which we cant perceive that there is some intelligence and volition; even if such an entity is able to communicate with us in a comprehensible manner and carry out actions in the real world, our innately wired theory of mind may still reject it as not sufciently like us to be worthy of consideration. Such an attitude could turn out to be a grave mistake, and should be guarded against as we progress towards these possibilities. 13.",Engineering General  Intelligence Part 1,chapter 13
"7 Possible Benets of Closely Linking AGIs to the Global Brain Some futurist thinkers, such as Francis Heylighen, believe that engineering AGI systems is at best a peripheral endeavor in the development of novel intelligence on Earth, because the real story is the developing Global Brain[Hey07, Goe01]the      274 13 The Engineering and Development of Ethics composite, self-organizing information system comprising humans, computers, data stores,theInternet,mobilephonesandwhathaveyou.Ourownviewsarelessextreme in this regardwe believe that AGI systems will display capabilities fundamentally different from those achievable via Global Brain style dynamics, and that ultimately (unless such development is restricted) self-improving AGI systems will develop intelligence vastly greater than any system possessing humans as a signicant component.",Engineering General  Intelligence Part 1,chapter 13
"However, we do respect the power of the Global Brain, and we suspect that the early stages of development of an AGI system may go quite differently if it is tightly connected to the Global Brain, via making rich and diverse use of Internet information resources and communication with diverse humans for diverse purposes. The potential for Global Brain integration to bring intelligence enhancement to AGIs is obvious. The ability to invoke Web searches across documents and databases can greatly enhance an AGIs cognitive ability, as well as the capability to consult GIS systems and various specialized software programs offered as Web services. We have previously reviewed the potential for embodied language learning achievable via using AGIs to power non-player characters in widely-accessible virtual worlds or massive multiplayer online games [Goe08]. But there is also a powerful potential benet for AGI ethical development, which has not previously been highlighted. This potential benet has two aspects: 1.",Engineering General  Intelligence Part 1,chapter 13
"Analogously to language learning, an AGI system may receive ethical training from a wide variety of humans in parallel, e.g. via controlling characters in wideaccess virtual worlds, and gaining feedback and guidance regarding the ethics of the behaviors demonstrated by these characters. 2. Internet-based information systems may be used to explicitly gather information regarding human values and goals, which may then be appropriately utilized as input for an AGI systems top-level goals. The second point begins to make abstract-sounding notions like Coherent Extrapolated Volition and Coherent Aggregated Volition, mentioned above, seem more practical and concrete.",Engineering General  Intelligence Part 1,chapter 13
"Its interesting to think about gathering information about individuals values via brain imaging, once that technology exists; but at present, one could make a fair stab at such a task via much more prosaic methods, such as asking people questions, assessing their ethical reactions to various real-world and hypothetical scenarios, and possibly engaging them in structured interactions aimed specically at eliciting collectively acceptable value systems (the subject of the next item on our list). It seems to us that this sort of approach could realize CAV in an interesting way, and also encapsulate some of the ideas underlying CAV. There is an interesting resonance here with recent thinking in the area of open source governance [Wik11]. Similar software tools (and associated psychocultural patterns) to those being developed to help with open source development and choice of political policies (see http://metagovernment.org) may be useful for gathering value data aimed at shaping AGI goal system content.      13.",Engineering General  Intelligence Part 1,chapter 13
"7 Possible Benets of Closely Linking AGIs to the Global Brain 275 13.7.1 The Importance of Fostering Deep, Consensus-Building Interactions Between People with Divergent Views Two potentially problematic issues arising with the notion of using Global Brain related technologies to form a coherent volition from the divergent views of various human beings are:  The tendency of the Internet to encourage people to interact mainly with others who share their own narrow views and interests, rather than a more diverse body of people with widely divergent views. The 300 people in the world who want to communicate using predicate logic (see http://lojban.org) can nd each other, and obscure musical virtuosos from around the world can nd an audience, and researchers in obscure domains can share papers without needing to wait years for paper journal publication, etc.  The tendency of many contemporary Internet technologies to reduce interaction to a very simplistic level (e.g.",Engineering General  Intelligence Part 1,chapter 13
"140 character tweets, brief Facebook wall posts), the tendency of information overload to cause careful reading to be replaced by quick skimming, and other related trends, which mean that deep sharing of perspectives by individuals with widely divergent views is not necessarily encouraged. As a somewhat extreme example, many of the YouTube pages displaying rock music videos are currently littered with comments by haters asserting that rock music is inferior to classical or jazz or whatever their preference isobviously this is a far cry from deep and productive sharing between people with different tastes and backgrounds. Tweets and Youtube comments have their place in the cosmos, but they probably arent ideal in terms of helping humanity to form a coherent volition of some sort, suitable for providing an AGI with goal system guidance.",Engineering General  Intelligence Part 1,chapter 13
"A description of communication at the opposite end of the spectrum is presented in Adam Kahane and Peter Senges excellent book Solving Tough Problems [KS04], which describes a methodology that has been used to reconcile deeply conicting views in some very tricky real-world situations (e.g. helping to peacefully end apartheid in South Africa). One of the core ideas of the methodology is to have people with very different views explore different possible future scenarios together, in great detailin cognitive psychology terms, a collective generation of hypothetical episodic knowledge. This has multiple benets, including  emotional bonds and mutual understanding are built in the process of collaboratively exploring the scenarios;  the focus on concrete situations helps to break through some of the counterproductive abstract ideas that people (on both sides of any dichotomy) may have formed;  emergence of conceptual blends that might never have arisen only from people with a single point of view.",Engineering General  Intelligence Part 1,chapter 13
"     276 13 The Engineering and Development of Ethics The result of such a process, when successful, is not an average of the participants views, but more like a conceptual blend of their perspectives. According to conceptual blending, which some hypothesize to be the core algorithm of creativity [FT02], new concepts are formed by combining key aspects of existing conceptsbut doing so judiciously, carefully choosing which aspects to retain, so as to obtain a high-quality and useful and interesting new whole. A blend is a compact entity that is similar to each of the entities blended, capturing their essences but also possessing its own, novel holistic integrity....",Engineering General  Intelligence Part 1,chapter 13
"But in the case of blending different peoples world-views to form something new that everybody is going to have to live with (as in the case of nding a peaceful path beyond apartheid for South Africa, or arriving at a humanity-wide CBV to use to guide an AGI goal system), the trick is that everybody has to agree that enough of the essence of their own view has been captured! This leads to the question of how to foster deep conceptual blending of diverse and divergent human perspectives, on a global scale. One possible answer is the creation of appropriate Global Brain oriented technologiesbut moving away from technologies like Twitter that focus on quick and simple exchanges of small thoughts within afnity groups. On the face of it, it would seem whats needed is just the opposite long and deep exchanges of big concepts and deep feelings between individuals with radically different perspectives who would not commonly associate with each other.",Engineering General  Intelligence Part 1,chapter 13
"Building and effectively popularizing Internet technologies capable to foster this kind of interactionquickly enough to be helpful with guiding the goal systems of the rst highly powerful AGIsseems a signicant, though fascinating, challenge. Relationship with Coherent Extrapolated Volition The relation between this approach and CEV is interesting to contemplate. CEV has been loosely described as follows: In poetic terms, our coherent extrapolated volition is our wish if we knew more, thought faster, were more the people we wished we were, had grown up farther together; where the extrapolation converges rather than diverges, where our wishes cohere rather than interfere; extrapolated as we wish that extrapolated, interpreted as we wish that interpreted. While a moving humanistic vision, this seems to us rather difcult to implement in a computer algorithm in a compellingly right way.",Engineering General  Intelligence Part 1,chapter 13
"It seems that there would be many different ways of implementing it, and the choice between them would involve multiple, highly subtle and non-rigorous human judgment calls.1 However, if a deep collective process of interactive scenario analysis and sharing is carried out, in order to arrive at some sort of Coherent Blended Volition, this process may well involve many of the same kinds of extrapolation that are conceived to be part of Coherent Extrapolated Volition. The core difference between the two approaches is that in the CEV vision, the extrapolation and coherentization are to be done by a highly intelligent, highly specialized software program, whereas in the approach suggested here, these are to be carried out by collective activity of humans as mediated by 1 The reader is encouraged to look at the original CEV essay online (http://singinst.org/upload/ CEV.html) and make their own assessment.      13.",Engineering General  Intelligence Part 1,chapter 13
"7 Possible Benets of Closely Linking AGIs to the Global Brain 277 Global Brain technologies. Our perspective is that the denition of collective human values is probably better carried out via a process of human collaboration, rather than delegated to a machine optimization process; and also that the creation of deepsharing-oriented Internet technologies, while a difcult task, is signicantly easier and more likely to be done in the near future than the creation of narrow AI technology capable of effectively performing CEV style extrapolations. 13.8 Possible Benets of Creating Societies of AGIs One potentially interesting quality of the emerging Global Brain is the possible presence within it of multiple interacting AGI systems. Stephen Omohundro [Omo09] has argued that this is an important aspect, and that game-theoretic dynamics related to populations of roughly equally powerful agents, may play a valuable role in mitigating the risks associated with advanced AGI systems.",Engineering General  Intelligence Part 1,chapter 13
"Roughly speaking, if one has a society of AGIs rather than a single AGI, and all the members of the society share roughly similar ethics, then if one AGI starts to go off the rails, its compatriots will be in a position to correct its behavior. One may argue that this is actually a hypothesis about which AGI designs are safest, because a community of AGIs may be considered a single AGI with an internally community-like design. But the matter is a little subtler than that, if once considers AGI systems embedded in the Global Brain and human society. Then there is some substance to the notion of a population of AGIs systematically presenting themselves to humans and non-AGI software processes as separate entities. Of course, a society of AGIs is no protection against a single member undergoing a hard takeoff and drastically accelerating its intelligence simultaneously with shifting its ethical principles.",Engineering General  Intelligence Part 1,chapter 13
"In this sort of scenario, one could have a single AGI rapidly become much more powerful and very differently oriented than the others, who would be left impotent to act so as to preserve their values. But this merely defers the issue to the point to be considered below, regarding takeoff speed. The operation of an AGI society may depend somewhat sensitively on the architectures of the AGI systems in question. Things will work better if the AGIs have a relatively easy way to inspect and comprehend much of the contents of each others minds. This introduces a bias toward AGIs that more heavily rely on more explicit forms of knowledge representation. The ideal in this regard would be a system like Cyc [LG90] with a fully explicit logic-based knowledge representation based on a standard ontologyin this case, every Cyc instance would have a relatively easy time understanding the inner thought processes of every other Cyc instance.",Engineering General  Intelligence Part 1,chapter 13
"However, most AGI researchers doubt that fully explicit approaches like this will ever be capable of achieving advanced AGI using feasible computational resources. CogPrime uses a mixed representation, with an explicit (uncertain) logical aspect as well as an explicit subsymbolic aspect more analogous to attractor neural nets.      278 13 The Engineering and Development of Ethics The CogPrime design also contains a mechanism called Psynese (not yet implemented), intended to make it easier for one CogPrime instance to translate its personal thoughts into the mental language of another CogPrime instance. This translation process may be quite subtle, since each instance will generally learn a host of new concepts based on its experience, and these concepts may not possess any compact mapping into shared linguistic symbols or percepts. The wide deployment of some mechanism of this nature among a community of AGIs, will be very helpful in terms of enabling this community to display the level of mutual understanding needed for strongly encouraging ethical stability.",Engineering General  Intelligence Part 1,chapter 13
"13.9 AGI Ethics as Related to Various Future Scenarios Following up these various futuristic considerations, in this section we discuss possible ethical conicts that may arise in several different types of AGI development scenarios. Each scenario presents specic variations on the general challenges of teaching morals and ethics to an advanced, self-aware and volitional intelligence. While there is no way to tell at this point which, if any, of these scenarios will unfold, there is value to understanding each of them as means of ultimately developing a robust and pragmatic approach to teaching ethics to AGI systems. Evenmorethantheprevioussections,thisisanexerciseinspeculativefuturology that is denitely not necessary for the appreciation of the CogPrime design, so readers whose interests are mainly engineering and computer science focused may wish to skip ahead.",Engineering General  Intelligence Part 1,chapter 13
"However, we present these ideas here rather than at the end of the book to emphasize the point that this sort of thinking has informed our technical AGI design process in nontrivial ways. 13.9.1 Capped Intelligence Scenarios Capped intelligence scenarios involve a situation in which an AGI, by means of software restrictions (including omitted or limited internal rewriting capabilities or limited access to hardware resources), is inherently prohibited from achieving a level of intelligence beyond a predetermined goal. A capped intelligence AGI is designed to be unable to achieve a Singularitarian moment. Such an AGI can be seen as just another form of intelligent actor in the world, one which has levels of intelligence, self awareness, and volition that is perhaps somewhat greater than, but still comparable to humans and other animals. Ethical questions under this scenario are very similar to interhuman ethical considerations,withsimilarconsequences.",Engineering General  Intelligence Part 1,chapter 13
"earningthatproceedsinarelativelyhuman-like manner is entirely relevant to such human-like intelligences. The degree of danger is mitigated by the lack of superintelligence, and time is not of the essence. The imitative-reinforcement-corrective learning approach does not necessarily need to      13.9 AGI Ethics as Related to Various Future Scenarios 279 be augmented with a prior complex of ascent-safe moral imperatives at startup time. Developing an AGI with theory of mind and ethical reinforcement learning capabilities as described (admittedly, no small task!) is all that is needed in this casethe rest happens through training and experience as with any other moderate intelligence. 13.9.2 Superintelligent AI: Soft-Takeoff Scenarios Soft takeoff scenarios are similar to capped-intelligence ones in that in both cases an AGIs progression from standard intelligence happens on a time scale which permits ongoing human interaction during the ascent.",Engineering General  Intelligence Part 1,chapter 13
"However, in this case, as there is no predetermined limit on intelligence, it is necessary to account for the possibility of a superintelligence emerging (though of course this is not guaranteed). The soft takeoff model includes as subsets both controlled-ascent models in which this rate of intelligence gain is achieved deliberately through software constraints and/or metingout of computational resources to the AGI, and uncontrolled-ascent models in which there is coincidentally no hard takeoff despite no particular safeguards against one. Both have similar properties with regard to ethical considerations: 1. Ethical considerations under this scenario include not only the usual interhuman ethical concerns, but also the issue of how to convince a potential burgeoning superintelligence to: a. Care about humanity in the rst place, rather than ignore it. b. Benet humanity, rather than destroy it. c.",Engineering General  Intelligence Part 1,chapter 13
"Elevatehumanitytoahigherlevelofintelligence,whichevenifanAGIdecided to proceed with requires nding the right balance amongst some enormous considerations: i. Reconcile the aforementioned issues of ethical coherence and group volition, in a manner which allows the most people to benet (even if they dont all do so in the same way, based on their own preferences). ii. Solve the problems of biological senescence, or focus on human uploading and the preservation of the maintenance, support, and improvement infrastructure for inorganic intelligence, or both. iii. Preserve individual identity and continuity of consciousness, or override it in favor of continuity of knowledge and ease of harmonious integration, or both on a case-by-case basis. 2. The degree of danger is mitigated by the long timeline of ascent from mundane to super intelligence, and time is not of the essence. 3.",Engineering General  Intelligence Part 1,chapter 13
"Learning that proceeds in a relatively human-like manner is entirely relevant to such human-like intelligences, in their initial congurations. This means more interaction with and imitative-reinforcement-corrective learning guided by humans, which has both positive and negative possibilities.      280 13 The Engineering and Development of Ethics 13.9.3 Superintelligent AI: Hard-Takeoff Scenarios Hard takeoff scenarios assume that upon reaching an unknown inection point (the Singularity point [Vin93, Kur06]) in the intellectual growth of an AGI, an extraordinarily rapid increase (guesses vary from a few milliseconds to weeks or months) in intelligence will immediately occur and the AGI will leap from an intelligence regime which is understandable to humans into one which is far beyond our current capacity for understanding. General ethical considerations are similar to in the case of a soft takeoff.",Engineering General  Intelligence Part 1,chapter 13
"However, because the post-singularity AGI will be incomprehensible to humans and potentially vastly more powerful than humans, such scenarios have a sensitive dependence upon initial conditions with respects to the moral and ethical (and operational) outcome. This model leaves no opportunity for interactions between humans and the AGI to iteratively rene their ethical interrelations, during the post-Singularity phase. If the initial conditions of the singulatarian AGI are perfect (or close to it), then this is seen as a wonderful way to leap over our own moral shortcomings and create a benevolent God-AI which will mitigate our worst tendencies while elevating us to achieve our greatest hopes. Otherwise, it is viewed as a universal cataclysm on a unimaginable scale that makes Biblical Armageddon seem like a recracker in beer can. Because hard takeoff AGIs are posited as learning so quickly there is no chance of humans to interfere with them, they are seen as very dangerous.",Engineering General  Intelligence Part 1,chapter 13
"If the initial conditions are not sufciently inviolable, the story goes, then we humans will all be annihilated. However, in the case of a hard takeoff AGI we state that if the initial conditions are too rigid or too simplistic, such a rapidly evolving intelligence will easily rationalize itself out of them. Only a sophisticated system of ethics which considers the contradictions and uncertainties in ethical quandaries and provides insight into humanistic means of balancing ideology with pragmatism and how to accommodate contradictory desires within a population with multiplicity of approach, and similar nuanced ethical considerations, combined with a sense of empathy, will withstand repeated rational analysis. Neither a single be nice supergoal, nor simple lists of what thou shalt not do, are not going to hold up to a highly advanced analytical mind.",Engineering General  Intelligence Part 1,chapter 13
"Initial conditions are very important in a hard takeoff AGI scenario, but it is more important that those conditions be conceptually resilient and widely applicable than that they be easily listed on a website. Theissuesthatariseherebecomequitesubtle.Forinstance,NickBostrom[Bos03] has written: In humans, with our complicated evolved mental ecology of statedependent competing drives, desires, plans, and ideals, there is often no obvious way to identify what our top goal is; we might not even have one. So for us, the above reasoning need not apply. But a superintelligence may be structured differently. If a superintelligence has a denite, declarative goal-structure with a clearly identied top goal, then the above argument applies. And this is a good reason for us to build the superintelligence with such an explicit motivational architecture.",Engineering General  Intelligence Part 1,chapter 13
"This is an important line of thinking; and indeed, from the point of view of software design, thereis noreasonnot tocreateanAGI systemwithasingletopgoal andthemotivation      13.9 AGI Ethics as Related to Various Future Scenarios 281 to orchestrate all its activities in accordance with this top goal. But the subtle question is whether this kind of topdown goal system is going to be able to fulll the ve imperatives mentioned above. Logical coherence is the strength of this kind of goal system, but what about experiential groundedness, comprehensibility, and so forth? Humans have complicated mental ecologies not simply because we were evolved, but rather because we live in a complex real world in which there are many competing motivations and desires.",Engineering General  Intelligence Part 1,chapter 13
"We may not have a top goal because there may be no logic to focusing our minds on one single aspect of life (though, one may say, most humans have the same top goal as any other animal: dont diebut the world is too complicated for even that top goal to be completely inviolable). Any sufciently capable AGI will eventually have to contend with these complexities, and hindering it with simplistic moral edicts without giving it a sufciently pragmatic underlying ethical pedagogy and experiential grounding may prove to be even more dangerous than our messy human mental ecologies. If one assumes a hard takeoff AGI, then all this must be codied in the system at launch, as once a potentially Singularitarian AGI is launched there is no way to know what time period constitutes before the singularity point. This means developing theory of mind empathy and logical ethics in code prior to giving the system unfettered access to hardware and self-modication code.",Engineering General  Intelligence Part 1,chapter 13
"However, though nobody can predict if or when a Singularity will occur after unrestricted launch, only a truly irresponsible AGI development team would attempt to create an AGI without rst experimenting with ethical training of the system in an intelligence-capped form, by means of ethical instruction via human-AGI interaction both pedagogically and experientially. 13.9.4 Global Brain Mindplex Scenarios Another class of scenariosoverlapping some of the previous onesinvolves the emergence of a Global Brain, an emergent intelligence formed from global communication networks incorporating humans and software programs in a larger body of self-organizing dynamics. The notion of the Global Brain is reviewed in [Hey07, Tur77] and its connection with advanced AI is discussed in detail in Goertzels book Creating Internet Intelligence [Goe01], where three possible phases of Global Brain development are articulated:  Phase 1: computer and communication technologies as enhancers of human interactions.",Engineering General  Intelligence Part 1,chapter 13
"This is what we have today: science and culture progress in ways that would not be possible if not for the digital nervous system were spreading across the planet. The network of idea and feeling sharing can become much richer and more productive than it is today, just through incremental development, without any Metasystem transition.  Phase 2: the intelligent Internet. At this point our computer and communication systems, through some combination of self-organizing evolution and human      282 13 The Engineering and Development of Ethics engineering, have become a coherent mind on their own, or a set of coherent minds living in their own digital environment.  Phase 3: the full-on Singularity. A complete revision of the nature of intelligence, human and otherwise, via technological and intellectual advancement totally beyond the scope of our current comprehension. At this point our current psychological and cultural realities are no more relevant than the psyche of a goose is to modern society.",Engineering General  Intelligence Part 1,chapter 13
"The main concern of Creating Internet Intelligence is with  how to get from Phase 1 to Phase 2i.e. how to build an AGI system that will effect or encourage the transformation of the Internet into a coherent intelligent system;  how to ensure that the Phase 2, Internet-savvy, global-brain-centric AGI systems will be oriented toward intelligence-improving self-modication (so theyll propel themselves to Phase 3), and also toward generally positive goals (as opposed to, say, world domination and extermination of all other intelligent life forms besides themselves!). One possibly useful concept in this context is that of a mindplex: an intelligence that is composed largely of individual intelligences with their own self-models and global workspaces, yet that also has its own self-model and global workspace. Both the individuals and the meta-mind should be capable of deliberative, rational thought, to have a true mindplex.",Engineering General  Intelligence Part 1,chapter 13
"Its unlikely that human society or the Internet meet this criterion yet; and a system like an ant colony seems not to either, because even though it has some degree of intelligence on both the individual and collective levels, that degree of intelligence is not very great. But it seems quite feasible that the global brain, at a certain stage of its development, will take the unfamiliar but fascinating form of a mindplex. Currently the best way to explain what happens on the Net is to talk about the various parts of the Net: particular websites, social networks, viruses, and so forth. But there will come a point when this is no longer the case, when the Net has sufcient high-level dynamics of its own that the way to explain any one part of the Net will be by reference to it relations with the whole: and not just the dynamics of the whole, but the intentions and understanding of the whole.",Engineering General  Intelligence Part 1,chapter 13
"This transition to Net-as-mindplex, we suspect, will come about largely through the interactions of AI systemsintelligent programs acting on behalf of various individuals and organizations, who will collaborate and collectively constitute something halfway between a society of AIs and an emergent mind whose lobes are various AI agents serving various goals. The Phase 2 Internet, as it verges into mindplex-ness, will likely have a complex, sprawling architecture, growing out of the architecture on the Net we experience today. The following components at least can be expected:  A vast variety of client computers, some old, some new, some powerful, some weakincluding many mobile and embedded devices not explicitly thought of as      13.9 AGI Ethics as Related to Various Future Scenarios 283 computers. Some of these will contribute little to Internet intelligence, mainly being passive recipients.",Engineering General  Intelligence Part 1,chapter 13
"Others will be smart clients, carrying out personalization operations intended to help the machines serve particular clients better, general AI operations handed to them by sophisticated AI server systems or other smart clients, and so forth.  Commercial servers, computers that carry out various tasks to support various types of heavyweight processingtransaction processing for e-commerce applications, inventory management for warehousing of physical objects, and so forth. Some of these commercial servers interact with client computers directly, others do so only via AI servers. In nearly all cases, these commercial servers can benet from intelligence supplied by AI servers.  The crux of the intelligent Internet: clusters of AI servers distributed across the Net, each cluster representing an individual computational mind (in many cases, a mindplex). These will be able to communicate via one or more languages, and will collectively drive the whole Net, by dispensing problems to client-machinebased processing frameworks, and providing real-time AI feedback to commercial servers of various types.",Engineering General  Intelligence Part 1,chapter 13
"Some AI servers will be general-purpose and will serve intelligence to commercial servers using an ASP (application service provider) model; others will be more specialized, tied particularly to a certain commercial server (e.g. a large information services business might have its own AI cluster to empower its portal services). This is one concrete vision of what a global brain might look like, in the relatively near term, with AGI systems playing a critical role. Note that, in this vision, mindplexes may exist on two levels:  Within AGI-clusters serving as actors within the overall Net.  On the overall Net level. To make these ideas more concrete, we may speculatively reformulate the rst two global brain phases mentioned above as follows:  Phase 1 global brain proto-mindplex: AI/AGI systems enhancing online databases, guiding Google results, forwarding e-mails, suggesting mailing-lists, etc.",Engineering General  Intelligence Part 1,chapter 13
"generally using intelligence to mediate and guide human communications toward goals that are its own, but that are themselves guided by human goals, statements and actions.  Phase 2 global brain mindplex: AGI systems composing documents, editing human-written documents, sending and receiving e-mails, assembling mailing lists and posting to them, creating new databases and instructing humans in their use, etc. In Phase 2, the conscious theater of the global-brain-mediating AGI system is composed of ideas built by numerous individual humansor ideas emergent from ideas built by numerous individual humansand it conceives ideas that guide the actions and thoughts of individual humans, in a way that is motivated by its own goals.",Engineering General  Intelligence Part 1,chapter 13
"It does not force the individual humans to do anythingbut if a given human      284 13 The Engineering and Development of Ethics wishes to communicate and interact using the same databases, mailing lists and evolving vocabularies as other humans, they are going to have to use the products of the global brain mediating AGI, which means they are going to have to participate in its patterns and its activities. Of course, the advent of advanced neurocomputer interfaces makes the picture potentially more complex. At some point, it will likely be possible for humans to project thoughts and images directly into computers without going through mouse or keyboardand to read in thoughts and images similarly. When this occurs, interaction between humans may in some contexts become more like interactions between computers, and the role of global brain mediating AI servers may become one of mediating direct thought-to-thought exchanges between people. The ethical issues associated with global brain scenarios are in some ways even subtler than in the other scenarios we mentioned above.",Engineering General  Intelligence Part 1,chapter 13
"One has issues pertaining to the desirability of seeing the human race become something fundamentally differentsomething more social and networked, less individual and autonomous. One has the risk of AGI systems exerting a subtle but strong control over people, vaguely like the control that the human brains executive system exerts over the neurons involved with other brain subsystems. On the other hand, one also has more human empowerment than in some of the other scenariosbecause the systems that are changing and deciding things are not separate from humans, but are, rather, composite systems essentially involving humans. So, in the global brain scenarios, one has more human empowerment than in some other casesbut the humans involved arent legacy humans like us, but heavily networked humans that are largely characterized by the emergent dynamics and structures implicit in their interconnected activity! 13.",Engineering General  Intelligence Part 1,chapter 13
"10 Conclusion: Eight Ways to Bias AGI Toward Friendliness It would be nice if we had a simple, crisp, comforting conclusion to this chapter on AGI ethics, but its not the case. There is a certain irreducible uncertainty involved in creating advanced articial minds. There is also a large irreducible uncertainty involved in the future of the human race in the case that we dont create advanced articial minds: in accordance with the ancient Chinese curse, we live in interesting times! What we can do, in this face of all this uncertainty, is to use our common sense to craft articial minds that seem rationally and intuitively likely to be forces for good rather than otherwiseand revise our ideas frequently and openly based on what we learn as our research progresses. We have roughly outlined our views on AGI ethics, which have informed the CogPrime design in countless ways; but the current CogPrime design itself is just the initial condition for an AGI project.",Engineering General  Intelligence Part 1,chapter 13
"Assuming the project succeeds in creating an AGI preschooler, experimentation with this preschooler will surely teach us a great deal: both about AGI architecture in general, and about AGI ethics architecture in particular. We will then rene our cognitive      13.10 Conclusion: Eight Ways to Bias AGI Toward Friendliness 285 and ethical theories and our AGI designs as we go about engineering, observing and teaching the next generation of systems. All this is not a magic bullet for the creation of benecial AGI systems, but we believe its the right process to follow. The creation of AGI is part of a larger evolutionary process that human beings are taking part in, and the crafting of AGI ethics through engineering, interaction and instruction is also part of this process.",Engineering General  Intelligence Part 1,chapter 13
"There are no guarantees hereguarantees are rare in real lifebut that doesnt mean that the situation is dire or hopeless, nor that (as some commentators have suggested [Joy00, McK03]) AGI research is too dangerous to pursue. It means we need to be mindful, intelligent, compassionate and cooperative as we proceed to carry out our parts in the next phase of the evolution of mind. With this perspective in mind, we will conclude this chapter with a list of Eight Ways to Bias Open-Source AGI Toward Friendliness, borrowed from a previous paper by Ben Goertzel and Joel Pitt of that name. These points summarize many of the points raised in the prior sections of this chapter, in a relatively crisp and practical manner: 1. Engineer Multifaceted Ethical Capabilities, corresponding to the multiple types of memory, including rational, empathic, imitative, etc. 2.",Engineering General  Intelligence Part 1,chapter 13
"Foster Rich Ethical Interaction and Instruction, with instructional methods according to the communication modes corresponding to all the types of memory: verbal, demonstrative, dramatic/depictive, indicative, goal-oriented. 3. Engineer Stable, Hierarchy-Dominated Goal Systems... which is enabled nicely by CogPrimes goal framework and its integration with the rest of the CogPrime design. 4. Tightly Link AGI with the Global Brain, so that it can absorb human ethical principles, both via natural interaction, and perhaps via practical implementations of current loosely-dened strategies like CEV, CAV and CBV. 5. Foster Deep, Consensus-Building Interactions Between People with Divergent Views, so as to enable the interaction with the Global Brain to have the most clear and positive impact. 6.",Engineering General  Intelligence Part 1,chapter 13
"Create a Mutually Supportive Community of AGIs which can then learn from each other and police against unfortunate developments (an approach which is meaningful if the AGIs are architected so as to militate against unexpected radical accelerations in intelligence). 7. Encourage Measured Co-Advancement of AGI Software and AGI Ethics Theory. 8. Develop Advanced AGI Sooner Not Later. The last two of these points were not explicitly discussed in the body of the chapter, and so we will nalize the chapter by reviewing them here.      286 13 The Engineering and Development of Ethics 13.10.1 Encourage Measured Co-Advancement of AGI Software and AGI Ethics Theory Everything involving AGI and Friendly AI (considered together or separately) currently involves signicant uncertainty, and it seems likely that signicant revision of current concepts will be valuable, as progress on the path toward powerful AGI proceeds.",Engineering General  Intelligence Part 1,chapter 13
"However, whether there is time for such revision to occur before AGI at the human level or above is created, depends on how fast is our progress toward AGI. What one wants is for progress to be slow enough that, at each stage of intelligence advance, concepts such as those discussed in this paper can be re-evaluated and re-analyzed in the light of the data gathered, and AGI designs and approaches can be revised accordingly as necessary. However, duetothenatureof moderntechnologydevelopment, it seems extremely unlikely that AGI development is going to be articially slowed down in order to enable measured development of accompanying ethical tools, practices and understandings.",Engineering General  Intelligence Part 1,chapter 13
"For example, if one nation chose to enforce such a slowdown as a matter of policy (speaking about a future date at which substantial AGI progress has already been demonstrated, so that international AGI funding is dramatically increased from present levels), the odds seem very high that other nations would explicitly seek to accelerate their own progress on AGI, so as to reap the ensuing differential economic benets (the example of stem cells arises again). And this leads on to our next and nal point regarding strategy for biasing AGI toward Friendliness.... 13.10.2 Develop Advanced AGI Sooner Not Later Somewhat ironically, it seems the best way to ensure that AGI development proceeds atarelativelymeasuredpaceistoinitiateseriousAGIdevelopmentsoonerratherthan later. This is because the same AGI concepts will meet slower practical development today than 10years from now, and slower 10years from now than 20years from now, etc.",Engineering General  Intelligence Part 1,chapter 13
"ue to the ongoing rapid advancement of various tools related to AGI development, such as computer hardware, programming languages, and computer science algorithms; and also the ongoing global advancement of education which makes it increasingly cost-effective to recruit suitably knowledgeable AI developers. Currently the pace of AGI progress is sufciently slow that practical work is in no danger of outpacing associated ethical theorizing. However, if we want to avoid the future occurrence of this sort of dangerous outpacing, our best practical choice is to make sure more substantial AGI development occurs in the phase before the development of tools that will make AGI development extraordinarily rapid. Of course, the authors are doing their best in this direction via their work on the CogPrime project!      13.",Engineering General  Intelligence Part 1,chapter 13
"10 Conclusion: Eight Ways to Bias AGI Toward Friendliness 287 Furthermore, this point bears connecting with the need, raised above, to foster the development of Global Brain technologies capable to Foster Deep, ConsensusBuilding Interactions Between People with Divergent Views. If this sort of technology is to be maximally valuable, it should be created quickly enough that we can use it to help shape the goal system content of the rst highly powerful AGIs. So, to simplify just a bit: We really want both deep-sharing GB technology and AGI technology to evolve relatively rapidly, compared to computing hardware and advanced CS algorithms (since the latter factors will be the main drivers behind the accelerating ease of AGI development). And this seems signicantly challenging, since the latter receive dramatically more funding and focus at present.",Engineering General  Intelligence Part 1,chapter 13
"If this perspective is accepted, then we in the AGI eld certainly have our work cut out for us!      Part V Networks for Explicit and Implicit Knowledge Representation   ",Engineering General  Intelligence Part 1,chapter 13
"  Chapter 14 Local, Global and Glocal Knowledge Representation 14.1 Introduction One of the most powerful metaphors weve found for understanding minds is to view them as networksi.e. collections of interrelated, interconnected elements. The view of mind as network is implicit in the patternist philosophy, because every pattern can be viewed as a pattern in something, or a pattern of arrangement of somethingthus a pattern is always viewable as a relation between two or more things. A collection of patterns is thus a pattern-network. Knowledge of all kinds may be given network representations; and cognitive processes may be represented as networks also; for instance via representing them as programs, which may be represented as trees or graphs in various standard ways. The emergent patterns arising in an intelligence as it develops may be viewed as a pattern network in themselves; and the relations between an embodied mind and its physical and social environment may be viewed in terms of ecological and social networks.",Engineering General  Intelligence Part 1,chapter 14
"The chapters in this section are concerned with various aspects of networks, as related to intelligence in general and AGI in particular. Most of this material is not specic to CogPrime, and would be relevant to nearly any system aiming at humanlevelAGI.However,mostofithasbeendevelopedinthecourseofworkonCogPrime, and has direct relevance to understanding the intended operation of various aspects of a completed CogPrime system. We begin our excursion into networks, in this chapter, with an issue regarding networks and knowledge representation. One of the biggest decisions to make in designing an AGI system is how the system should represent knowledge. Naturally any advanced AGI system is going to synthesize a lot of its own knowledge representations for handling particular sorts of knowledgebut still, an AGI design typically makes at least some sort of commitment about the category of knowledge representation mechanisms toward which the AGI system will be biased.",Engineering General  Intelligence Part 1,chapter 14
"The two major supercategories of knowledge representation systems are local (also Co-authored with Matthew Ikle, Joel Pitt and Rui Liu. B. Goertzel et al., Engineering General Intelligence, Part 1, 291 Atlantis Thinking Machines 5, DOI: 10.2991/978-94-6239-027-0_14,  Atlantis Press and the authors 2014      292 14 Local, Global and Glocal Knowledge Representation called explicit) and global (also called implicit) systems, with a hybrid category we refer to as glocal that combines both of these. In a local system, each piece of knowledge is stored using a small percentage of AGI system elements; in a global system, each piece of knowledge is stored using a particular pattern of arrangement, activation, etc. of a large percentage of AGI system elements; in a glocal system, the two approaches are used together.",Engineering General  Intelligence Part 1,chapter 14
"In the rst section of this chapter we discuss the symbolic, semantic-network aspects of knowledge representation in CogPrime. Then we turn to distributed, neural-net-like knowledge representation, reviewing a host of general issues related to knowledge representation in attractor neural networks, turning nally to glocal knowledge representation mechanisms, in which ANNs combine localist and globalist representation, and explaining the relationship of the latter to CogPrime. The glocal aspect of CogPrime knowledge representation will become prominent in later chapters such as:  in Chap.5 of Part 2, where Economic Attention Networks (ECAN) are introduced and seen to have dynamics quite similar to those of the attractor neural nets considered here, but with a mathematics roughly modeling money ow in a specially constructed articial economy rather than electrochemical dynamics of neurons.  in Chap.24 of Part 2, where map formation algorithms for creating localist knowledge from globalist knowledge are described. 14.",Engineering General  Intelligence Part 1,chapter 14
"2 Localized Knowledge Representation Using Weighted, Labeled Hypergraphs There are many different mechanisms for representing knowledge in AI systems in an explicit, localized way, most of them descending from various variants of formal logic. Here we briey describe how it is done in CogPrime, which on the surface is not that different from a number of prior approaches. (The particularities of CogPrimes explicit knowledge representation, however, are carefully tuned to match CogPrimes cognitive processes, which are more distinctive in nature than the corresponding representational mechanisms). 14.2.1 Weighted, Labeled Hypergraphs One useful way to think about CogPrimes explicit, localized knowledge representation is in terms of hypergraphs. A hypergraph is an abstract mathematical structure [Bol98], which consists of objects called Nodes and objects called Links which connect the Nodes. In computer science, a graph traditionally means a bunch of dots connected with lines (i.e. Nodes connected by binary Links).",Engineering General  Intelligence Part 1,chapter 14
"A hypergraph, on the other hand, can have Links that connect more than two Nodes.      14.2 Localized Knowledge Representation Using Weighted, Labeled Hypergraphs 293 In these pages we will often consider generalized hypergraphs that extend ordinary hypergraphs by containing two additional features:  Links that point to Links instead of Nodes  Nodes that, when you zoom in on them, contain embedded hypergraphs. Properly, such hypergraphs should always be referred to as generalized hypergraphs, but this is cumbersome, so we will persist in calling them merely hypergraphs. In a hypergraph of this sort, Links and Nodes are not as distinct as they are within an ordinary mathematical graph (for instance, they can both have Links connecting them), and so it is useful to have a generic term encompassing both Links and Nodes; for this purpose, we use the term Atom.",Engineering General  Intelligence Part 1,chapter 14
"A weighted, labeled hypergraph is a hypergraph whose Links and Nodes come along with labels, and with one or more numbers that are generically called weights. A label associated with a Link or Node may sometimes be interpreted as telling you what type of entity it is, or alternatively as telling you what sort of data is associated with a Node. On the other hand, an example of a weight that may be attached to an Link or Node is a number representing a probability, or a number representing how important the Node or Link is. Obviously, hypergraphs may come along with various sorts of dynamics. Minimally, one may think about:  Dynamics that modify the properties of Nodes or Links in a hypergraph (such as the labels or weights attached to them).  Dynamics that add new Nodes or Links to a hypergraph, or remove existing ones. 14.",Engineering General  Intelligence Part 1,chapter 14
"3 Atoms: Their Types and Weights This section reviews a variety of CogPrime Atom types and gives simple examples of each of them. The Atom types considered are drawn from those currently in use in the OpenCog system. This does not represent a complete list of Atom types referred to in the text of this book, nor a complete list of those used in OpenCog currently (though it does cover a substantial majority of those used in OpenCog currently, omitting only some with specialized importance or intended only for temporary use). The partial nature of the list given here reects a more general point: The specic collection of Atom types in an OpenCog system is bound to change as the system is developed and experiment with. CogPrime species a certain collection of representational approaches and cognitive algorithms for acting on them; any of these approaches and algorithms may be implemented with a variety of sets of Atom types.",Engineering General  Intelligence Part 1,chapter 14
"The specic set of Atom types in the OpenCog system currently does not necessarily have a profound and lasting signicancethe list might look a bit different ve years from time of writing, based on various detailed changes. The treatment here is informal and intended to get across the general idea of what each Atom type does. A longer and more formal treatment of the Atom types is given in the beginning of Chap.2 of Part 2.      294 14 Local, Global and Glocal Knowledge Representation 14.3.1 Some Basic Atom Types We begin with ConceptNodeand note that a ConceptNode does not necessarily refer to a whole concept, but may refer to part of a conceptit is essentially a basic semantic node whose meaning comes from its links to other Atoms. It would be more accurately, but less tersely, named concept or concept fragment or element node. A simple example would be a ConceptNode grouping nodes that are somehow related, e.g.",Engineering General  Intelligence Part 1,chapter 14
"ConceptNode: C InheritanceLink (ObjectNode: BW) C InheritanceLink (ObjectNode: BP) C InheritanceLink (ObjectNode: BN) C ReferenceLink BW (PhraseNode ""Bens watch"") ReferenceLink BP (PhraseNode ""Bens passport"") ReferenceLink BN (PhraseNode ""Bens necklace"") indicates the simple and uninteresting ConceptNode grouping three objects owned by Ben (note that the above-given Atoms dont indicate the ownership relationship, they justlinkthethreeobjectswithtextualdescriptions).Inthisexample,theConceptNode links transparently to physical objects and English descriptions, but in general this wont be the casemost ConceptNodes will look to the human eye like groupings of links of various types, that link to other nodes consisting of groupings of links of various types, etc. There are Atoms referring to basic, useful mathematical objects, e.g. Number Nodes like NumberNode #4 NumberNode #3.",Engineering General  Intelligence Part 1,chapter 14
"44 The numerical value of a NumberNode is explicitly referenced within the Atom. A core distinction is made between ordered links and unordered links; these are handled differently in the Atomspace software. A basic unordered link is the SetLink, which groups its arguments into a set. For instance, the ConceptNode C dened by ConceptNode C MemberLink A C MemberLink B C is equivalent to SetLink A B On the other hand, ListLinks are like SetLinks but ordered, and they play a fundamental role due to their relationship to predicates. Most predicates are assumed to take ordered arguments, so we may say e.g. EvaluationLink PredicateNode eat ListLink ConceptNode cat ConceptNode mouse      14.3 Atoms: Their Types and Weights 295 to indicate that cats eat mice. Note that by an expression like ConceptNode cat is meant ConceptNode C ReferenceLink W C WordNode W #cat since its WordNodes rather than ConceptNodes that refer to words.",Engineering General  Intelligence Part 1,chapter 14
"(And note that the strength of the ReferenceLink would not be 1 in this case, because the word cat has multiple senses.) However, there is no harm nor formal incorrectness in the ConceptNode cat usage, since cat is just as valid a name for a ConceptNode as, say, C. Weve already introduced above the MemberLink, which is a link joining a member to the set that contains it. Notable is that the truth value of a MemberLink is fuzzy rather than probabilistic, and that PLN is able to inter-operate fuzzy and probabilistic values. SubsetLinks also exist, with the obvious meaning, e.g. ConceptNode cat ConceptNode animal SubsetLink cat animal Note that SubsetLink refers to a purely extensional subset relationship, and that InheritanceLInk should be used for the generic intensional + extensional analogue of thismore on this below.",Engineering General  Intelligence Part 1,chapter 14
"SubsetLink could more consistently (with other link types) be named ExtensionalInheritanceLink, but SubsetLink is used because its shorter and more intuitive. There are links representing Boolean operations AND, OR and NOT. For instance, we may say ImplicationLink ANDLink ConceptNode young ConceptNode beautiful ConceptNode attractive or, using links and VariableNodes instead of ConceptNodes, AverageLink $X ImplicationLink ANDLink EvaluationLink young $X EvaluationLink beautiful $X EvaluationLink attractive $X NOTLink is a unary link, so e.g. we might say      296 14 Local, Global and Glocal Knowledge Representation AverageLink $X ImplicationLink ANDLink EvaluationLink young $X EvaluationLink beautiful $X EvaluationLink NOT EvaluationLink poor $X EvaluationLink attractive $X ContextLink allows explicit contextualization of knowledge, which is used in PLN, e.g.",Engineering General  Intelligence Part 1,chapter 14
"ContextLink ConceptNode golf InheritanceLink ObjectNode BenGoertzel ConceptNode incompetent says that Ben Goertzel is incompetent in the context of golf. 14.3.2 Variable Atoms We have already introduced VariableNodes above; its also possible to specify the type of a VariableNode via linking it to a VariableTypeNode via a TypedVariableLink, e.g. VariableTypeLink VariableNode $X VariableTypeNode ConceptNode which species that the variable $X should be lled with a ConceptNode. Variables are handled via quantiers; the default quantier being the AverageLink, so that the default interpretation of ImplicationLink InheritanceLink $X animal EvaluationLink PredicateNode: eat ListLink \\$X ConceptNode: food is AverageLink $X ImplicationLink InheritanceLink $X animal EvaluationLink PredicateNode: eat      14.",Engineering General  Intelligence Part 1,chapter 14
"3 Atoms: Their Types and Weights 297 ListLink $X ConceptNode: food The AverageLink invokes an estimation of the average TruthValue of the embedded expression (in this case an ImplicationLink) over all possible values of the variable $X. If there are type restrictions regarding the variable $X, these are taken into account in conducting the averaging. For AllLink and Exist s-Link may be used in the same places as AverageLink, with uncertain truth value semantics dened in PLN theory using third-order probabilities. There is also a ScholemLink used to indicate variable dependencies for existentially quantied variables, used in cases of multiply nested existential quantiers. EvaluationLink and MemberLink have overlapping semantics, allowing expression of the same conceptual/logical relationships in terms of predicates or sets, i.e.",Engineering General  Intelligence Part 1,chapter 14
"EvaluationLink PredicateNode: eat ListLink $X ConceptNode: food has the same semantics as MemberLink ListLink $X ConceptNode: food ConceptNode: EatingEvents The relation between the predicate eat and the concept EatingEvents is formally given by ExtensionalEquivalenceLink ConceptNode: EatingEvents SatisfyingSetLink PredicateNode: eat In other words, we say that EatingEvents is the SatisfyingSet of the predicate eat: it is the set of entities that satisfy the predicate eat. Note that the truth values of MemberLink and EvaluationLink are fuzzy rather than probabilistic. 14.3.3 Logical Links There is a host of link types embodying logical relationships as dened in the PLN logic system, e.g.  InheritanceLink  SubsetLink (aka ExtensionalInheritanceLink)      298 14 Local, Global and Glocal Knowledge Representation  Intensional InheritanceLink which embody different sorts of inheritance, e.g.",Engineering General  Intelligence Part 1,chapter 14
"SubsetLink salmon fish IntensionalInheritanceLink whale fish InheritanceLink fish animal and then  SimilarityLink  ExtensionalSimilarityLink  IntensionalSimilarityLink which are symmetrical versions, e.g. SimilaritytLink shark barracuda IntensionalSimilarityLink shark dolphin ExtensionalSimiliarityLink American obese_person There are also higher-order versions of these links, both asymmetric  ImplicationLink  ExtensionalImplicationLink  IntensionalImplicationLink and symmetric  EquivalenceLink  ExtensionalEquivalenceLink  IntensionalEquivalenceLink These are used between predicates and links, e.g. ImplicationLink EvaluationLink eat ListLink $X dirt EvaluationLink feel ListLInk $X sick or ImplicationLink EvaluationLink eat ListLink $X dirt InheritanceLink $X sick or      14.",Engineering General  Intelligence Part 1,chapter 14
"3 Atoms: Their Types and Weights 299 ForAllLink $X, $Y, $Z ExtensionalEquivalenceLink EquivalenceLink $Z EvaluationLink + ListLink $X $Y EquivalenceLink $Z EvaluationLink + ListLink $Y $X Note,thelatterisgivenasanextensionalequivalencebecauseitsapuremathematical equivalence. This is not the only case of pure extensional equivalence, but its an important one. 14.3.4 Temporal Links There are also temporal versions of these links, such as  PredictiveImplicationLink  PredictiveAttractionLink  SequentialANDLink  SimultaneousANDLink which combine logical relation between the argument with temporal relation between their arguments.",Engineering General  Intelligence Part 1,chapter 14
"For instance, we might say PredictiveImplicationLink PredicateNode: JumpOffCliff PredicateNode: Dead or including arguments, PredictiveImplicationLink EvaluationLink JumpOffCliff $X EvaluationLink Dead $X The former version, without variable arguments given, shows the possibility of using higher-order logical links to join predicates without any explicit variables. Via using this format exclusively, one could avoid VariableAtoms entirely, using only higher-order functions in the manner of pure functional programming formalisms like combinatory logic. However, this purely functional style has not proved convenient, so the Atomspace in practice combines functional-style representation with variable-based representation.      300 14 Local, Global and Glocal Knowledge Representation Temporal links often come with specic temporal quantication, e.g. PredictiveImplicationLink <5 seconds> EvaluationLink JumpOffCliff $X EvaluationLink Dead $X indicating that the conclusion will generally follow the premise within 5s.",Engineering General  Intelligence Part 1,chapter 14
"There is a system for managing fuzzy time intervals and their interrelationships, based on a fuzzy version of Allen Interval Algebra. SequentialANDLink is similar to PredictiveImplicationLink but its truth value is calculated differently. The truth value of SequentialANDLink <5 seconds> EvaluationLink JumpOffCliff $X EvaluationLink Dead $X indicates the likelihood of the sequence of events occurring in that order, with gap lying within the specied time interval. The truth value of the PredictiveImplicationLink version indicates the likelihood of the second event, conditional on the occurrence of the rst event (within the given time interval restriction). There are also links representing basic temporal relationships, such as BeforeLink and AfterLink. These are used to refer to specic events, e.g.",Engineering General  Intelligence Part 1,chapter 14
"if X refers to the event of Ben waking up on July 15 2012, and Y refers to the event of Ben getting out of bed on July 15 2012, then one might have AfterLink X Y And there are TimeNodes (representing time-stamps such as temporal moments or intervals) and AtTimeLinks, so we may e.g. say AtTimeLink X TimeNode: 8:24AM Eastern Standard Time, July 15 2012 AD. 14.3.5 Associative Links There are links representing associative, attentional relationships,  HebbianLink  AsymmetricHebbianLink  InverseHebbianLink  SymmetricInverseHebbianLink These connote associations between their arguments, i.e.",Engineering General  Intelligence Part 1,chapter 14
"they connote that the entities represented by the two argument occurred in the same situation or context, for instance HebbianLink happy smiling AsymmetricHebbianLink dead rotten InverseHebbianLink dead breathing      14.3 Atoms: Their Types and Weights 301 The asymmetric HebbianLink indicates that when the rst argument is present in a situation, the second is also often present. The symmetric (default) version indicates that this relationship holds in both directions. The inverse versions indicate the negative relationship: e.g. when one argument is present in a situation, the other argument is often not present. 14.3.6 Procedure Nodes There are nodes representing various sorts of procedures; these are kinds of ProcedureNode, e.g.",Engineering General  Intelligence Part 1,chapter 14
"SchemaNode, indicating any procedure  GroundedSchemaNode, indicating any procedure associated in the system with a Combo program or C++ function allowing the procedure to be executed  PredicateNode, indicating any predicate that associates a list of arguments with an output truth value  GroundedPredicateNode, indicating a predicate associated in the system with a Combo program or C++ function allowing the predicates truth value to be evaluated on a given specic list of arguments. ExecutionLinks and EvaluationLinks record the activity of SchemaNodes and PredicateNodes. We have seen many examples of EvaluationLinks in the above. Example ExecutionLinks would be: ExecutionLink step\\_forward ExecutionLink step\\_forward 5 ExecutionLink + ListLink NumberNode: 2 NumberNode: 3 The rst example indicates that the schema step forward has been executed. The second example indicates that it has been executed with an argument of 5 (meaning, perhaps, that 5 steps forward have been attempted).",Engineering General  Intelligence Part 1,chapter 14
"The last example indicates that the + schema has been executed on the argument list (2, 3), presumably resulting in an output of 5. The output of a schema execution may be indicated using an ExecutionOutputLink, e.g. ExecutionOutputLink + ListLink NumberNode: 2 NumberNode: 3 refers to the value 5 (as a NumberNode).      302 14 Local, Global and Glocal Knowledge Representation 14.3.7 Links for Special External Data Types Finally, there are also Atom types referring to specic types of data important to using OpenCog in specic contexts.",Engineering General  Intelligence Part 1,chapter 14
"For instance, there are Atom types referring to general natural language data types, such as  WordNode  SentenceNode  WordInstanceNode  DocumentNode plus more specic ones referring to relationships that are part of link-grammar parses of sentences  FeatureNode  FeatureLink  LinkGrammarRelationshipNode  LinkGrammarDisjunctNode or RelEx semantic interpretations of sentences  DenedLinguisticConceptNode  DenedLinguisticRelationshipNode  PrepositionalRelationshipNode There are also Atom types corresponding to entities important for embodying OpenCog in a virtual world, e.g.  ObjectNode  AvatarNode  HumanoidNode  UnknownObjectNode  AccessoryNode 14.3.",Engineering General  Intelligence Part 1,chapter 14
"8 Truth Values and Attention Values CogPrime Atoms (Nodes and Links) are quantied with truth values that, in their simplest form, have two components, one representing probability (strength) and the other representing weight of evidence; and also with attention values that have two components, short-term and long-term importance, representing the estimated value of the Atom on immediate and long-term time-scales. In practice many Atoms are labeled with CompositeTruthValues rather than elementary ones. A composite truth value contains many component truth values, representing truth values of the Atom in different contexts and according to different estimators.      14.3 Atoms: Their Types and Weights 303 It is important to note that the CogPrime declarative knowledge representation is neither a neural net nor a semantic net, though it does have some commonalities with each of these traditional representations. It is not a neural net because it has no activation values, and involves no attempts at low-level brain modeling.",Engineering General  Intelligence Part 1,chapter 14
"However, attention values are very loosely analogous to time-averages of neural net activations. On the other hand, it is not a semantic net because of the broad scope of the Atoms in the network: for example, Atoms may represent percepts, procedures, or parts of concepts. Most CogPrime Atoms have no corresponding English label. 14.4 Knowledge Representation via Attractor Neural Networks Now we turn to global, implicit knowledge representationbeginning with formal neural net models, briey discussing the brain, and then turning back to CogPrime. Firstly, this section reviews some relevant material from the literature regarding the representation of knowledge using attractor neural nets. It is a mix of well-established fact with more speculative material. 14.4.1 The Hopeld Neural Net Model Hopeld networks [Hop82] are attractor neural networks often used as associative memories.",Engineering General  Intelligence Part 1,chapter 14
"A Hopeld network with N neurons can be trained to store a set of bipolar patterns P, where each pattern p has N bipolar (1) values. A Hopeld net typically has symmetric weights with no self-connections. The weight of the connection between neurons i and j is denoted by wij. In order to apply a Hopeld network to a given input pattern p, its activation state is set to the input pattern, and neurons are updated asynchronously, in random order, until the network converges to the closest xed point. An often-used activation function for a neuron is: yi = sign  pi  j=i wijyj  Training a Hopeld network, therefore, involves nding a set of weights wij that stores the training patterns as attractors of its network dynamics, allowing future recall of these patterns from possibly noisy inputs.",Engineering General  Intelligence Part 1,chapter 14
"Originally, Hopeld used a Hebbian rule to determine weights: wij = P  p=1 pipj      304 14 Local, Global and Glocal Knowledge Representation Typically, Hopeld networks are fully connected. Experimental evidence, however, suggests that the majority of the connections can be removed without signicantly impacting the networks capacity or dynamics. Our experimental work uses sparse Hopeld networks. 14.4.1.1 Palimpsest Hopeld Nets with a Modied Learning Rule In [SV99] a new learning rule is presented, which both increases the Hopeld network capacity and turns it into a palimpsest, i.e., a network that can continuously learn new patterns, while forgetting old ones in an orderly fashion.",Engineering General  Intelligence Part 1,chapter 14
"Using this new training rule, weights are initially set to zero, and updated for each new pattern p to be learned according to: hij = N  k=1,k=i,j wikpk wij = 1 n(pipj  hijpj  hjipi). 14.4.2 Knowledge Representation via Cell Assemblies Hopeld nets and their ilk play a dual role: as computational algorithms, and as conceptual models of brain function. In CogPrime they are used as inspiration for slightly different, articial economics based computational algorithms; but their hypothesized relevance to brain function is nevertheless of interest in a CogPrime context, as it gives some hints about the potential connection between low-level neural net mechanics and higher-level cognitive dynamics. Hopeld nets lead naturally to a hypothesis about neural knowledge representation, which holds that a distinct mental concept is represented in the brain as either: 1.",Engineering General  Intelligence Part 1,chapter 14
"a set of cell assemblies, where each assembly is a network of neurons that are interlinked in such a way as to re in a (perhaps nonlinearly) synchronized manner 2. a distinct temporal activation pattern, which may occur in any one (or more) of a particular set of cell assemblies. For instance, this hypothesis is perfectly coherent if one interprets a mental concept as a SMEPH (dened in Chap.15) ConceptNode, i.e. a fuzzy set of perceptual stimuli to which the organism systematically reacts in different ways. Also, although we will focus mainly on declarative knowledge here, we note that the same basic representational ideas can be applied to procedural and episodic knowledge: these may be hypothesized to correspond to temporal activation patterns as characterized above.      14.",Engineering General  Intelligence Part 1,chapter 14
"4 Knowledge Representation via Attractor Neural Networks 305 In the biology literature, perhaps the best-articulated modern theories championing the cell assembly view are those of Gunther Palm [Pal82, HAG07] and Susan Greeneld [SF05, CSG07]. Palm focuses on the dynamics of the formation and interaction assemblies of cortical columns. Greeneld argues that each concept has a core cell assembly, and that when the concept rises to the focus of attention, it recruits a number of other neurons beyond its core characteristic assembly into a transient ensemble.1 Its worth noting that there may be multiple redundant assemblies representing the same conceptand potentially recruiting similar transient assemblies when highly activated. The importance of repeated, slightly varied copies of the same subnetwork has been emphasized by Edelman [Ede93] among other neural theorists. 14.",Engineering General  Intelligence Part 1,chapter 14
"5 Neural Foundations of Learning Now we move from knowledge representation to learningwhich is after all nothing but the adaptation of represented knowledge based on stimulus, reinforcement and spontaneous activity. While our focus in this chapter is on representation, its not possible for us to make our points about glocal knowledge representation in neural net type systems without discussing some aspects of learning in these systems. 14.5.1 Hebbian Learning The most common and plausible assumption about learning in the brain is that synaptic connections between neurons are adapted via some variant of Hebbian learning. The original Hebbian learning rule, proposed by Donald Hebb in his 1949 book [Heb49], was roughly 1. The weight of the synapse x  y increases if x and y re at roughly the same time 2. The weight of the synapse x  y decreases if x res at a certain time but y does not.",Engineering General  Intelligence Part 1,chapter 14
"Over the years since Hebbs original proposal, many neurobiologists have sought evidence that the brain actually uses such a method. One of the things they have found, so far, is a lot of evidence for the following learning rule [DC02, LS05]: 1. The weight of the synapse x  y increases if x res shortly before y does 2. The weight of the synapse x  y decreases if x res shortly after y does. 1 The larger an ensemble is, she suggests, the more vivid it is as a conscious experience; an hypothesis that accords well with the hypothesis made in [Goe06b] that a more informationally intense pattern corresponds to a more intensely conscious qualebut we dont need to digress extensively onto matters of consciousness for the present purposes.",Engineering General  Intelligence Part 1,chapter 14
"     306 14 Local, Global and Glocal Knowledge Representation The new thing here, not foreseen by Donald Hebb, is the postsynaptic depression involved in rule component 2. Now, the simple rule stated above does not sum up all the research recently done on Hebbian-type learning mechanisms in the brain. The real biological story underlying these approximate rules is quite complex, involving many particulars to do with various neurotransmitters. Ill-understood details aside, however, there is an increasing body of evidence that not only does this sort of learning occur in the brain, but it leads to distributed experience-based neural modication: that is, one instance synaptic modication causes another instance of synaptic modication, which causes another, and so forth2 [Bi01]. 14.5.",Engineering General  Intelligence Part 1,chapter 14
"2 Virtual Synapses and Hebbian Learning Between Assemblies Hebbian learning is conventionally formulated in terms of individual neurons, but, it can be extended naturally to assemblies via dening virtual synapses between assemblies. Since assemblies are sets of neurons, one can view a synapse as linking two assemblies if it links two neurons, each of which is in one of the assemblies. One can then view two assemblies as being linked by a bundle of synapses. We can dene the weight of the synaptic bundle from assembly A1 to assembly A2 as the number w so that (the change in the mean activation of A2 that occurs at time t +epsilon)is on average closest to w(the amount of energy owing through the bundle from A1 to A2 at time t).",Engineering General  Intelligence Part 1,chapter 14
"So when A1 sends an amount x of energy along the synaptic bundle pointing from A1 to A2, then A2s mean activation is on average incremented/decremented by an amount w  x. In a similar way, one can dene the weight of a bundle of synapses between a certain static or temporal activation-pattern P1 in assembly A1, and another static or temporal activation-pattern P2 in assembly A2. Namely, this may be dened as the number w so that (the amount of energy owing through the bundle from A1 to A2 at time t)  w best approximates (the probability that P2 is present in A2 at time t +epsilon), when averaged over all times t during which P1 is present in A1. It is not hard to see that Hebbian learning on real synapses between neurons implies Hebbian learning on these virtual synapses between cell assemblies and activation-patterns.",Engineering General  Intelligence Part 1,chapter 14
"These ideas may be developed further to build a connection between neural knowledge representation and probabilistic logical knowledge representation such as is used in CogPrimes Probabilistic Logic Networks formalism; this connection will 2 This has been observed in model systems consisting of neurons extracted from a brain and hooked together in a laboratory setting and monitored; measurement of such dynamics in vivo is obviously more difcult.      14.5 Neural Foundations of Learning 307 be pursued at the end of Chap.16 (Part 2), once more relevant background has been presented. 14.5.3 Neural Darwinism A notion quite similar to Hebbian learning between assemblies has been pursued by Nobelist Gerald Edelman in his theory of neuronal group selection, or Neural Darwinism [Ede93].",Engineering General  Intelligence Part 1,chapter 14
"Edelman won a Nobel Prize for his work in immunology, which, like most modern immunology, was based on MacFarlane Burnets theory of clonal selection [Bur62], which states that antibody types in the mammalian immune system evolve by a form of natural selection. From his point of view, it was only natural to transfer the evolutionary idea from one mammalian body system (the immune system) to another (the brain). The starting point of Neural Darwinism is the observation that neuronal dynamics may be analyzed in terms of the behavior of neuronal groups. The strongest evidence in favor of this conjecture is physiological: many of the neurons of the neocortex are organized in clusters, each one containing say 10,00050,000 neurons each. Once one has committed oneself to looking at such groups, the next step is to ask how these groups are organized, which leads to Edelmans concept of maps.",Engineering General  Intelligence Part 1,chapter 14
"A map, in Edelmans terminology, is a connected set of groups with the property that when one of the inter-group connections in the map is active, others will often tend to be active as well. Maps are not xed over the life of an organism. They may be formed and destroyed in a very simple way: the connection between two neuronal groups may be strengthened by increasing the weights of the neurons connecting the one group with the other, and weakened by decreasing the weights of the neurons connecting the two groups. If we replace map with cell assembly we arrive at a concept very similar to the one described in the previous subsection. Edelman then makes the following hypothesis: the large-scale dynamics of the brain is dominated by the natural selection of maps. Those maps which are active when good results are obtained are strengthened, those maps which are active when bad results are obtained are weakened. And maps are continually mutated by the natural chaos of neural dynamics, thus providing new fodder for the selection process.",Engineering General  Intelligence Part 1,chapter 14
"By use of computer simulations, Edelman and his colleagues have shown that formal neural networks obeying this rule can carry out fairly complicated acts of perception. In general-evolution language, what is posited here is that organisms like humans contain chemical signals that signify organism-level success of various types, and that these signals serve as a tness function correlating with evolutionary tness of neuronal maps. In Neural Darwinism and his other related books and papers, Edelman goes far beyond this crude sketch and presents neuronal group selection as a collection of precise biological hypotheses, and presents evidence in favor of a number of these hypotheses. However, we consider that the basic concept of neuronal group selection is largely independent of the biological particularities in terms of which Edelman      308 14 Local, Global and Glocal Knowledge Representation has phrased it. We suspect that the mutation and selection of transformations or maps is a necessary component of the dynamics of any intelligent system. As we will see later on (e.",Engineering General  Intelligence Part 1,chapter 14
". in Chap.24 of Part 2, this business of maps is extremely important to CogPrime. CogPrime does not have simulated biological neurons and synapses, but it does have Nodes and Links that in some contexts play loosely similar roles. We sometimes think of CogPrime Nodes and Links as being very roughly analogous to Edelmans neuronal clusters, and emergent intercluster links. And we have maps among CogPrime Nodes and Links, just as Edelman has maps among his neuronal clusters. Maps are not the sole bearers of meaning in CogPrime, but they are signicant ones. There is a very natural connection between Edelman-style brain evolution and the ideas about cognitive evolution presented in Chap.5. Edelman proposes a fairly clear mechanism via which patterns that survive a while in the brain are differentially likely to survive a long time: this is basic Hebbian learning, which in Edelmans picture plays a role between neuronal groups.",Engineering General  Intelligence Part 1,chapter 14
"And, less directly, Edelmans perspective also provides a mechanism by which intense patterns will be differentially selected in the brain: because on the level of neural maps, pattern intensity corresponds to the combination of compactness and functionality. Among a number of roughly equally useful maps serving the same function, the more compact one will be more likely to survive over time, because it is less likely to be disrupted by other brain processes (such as other neural maps seeking to absorb its component neuronal groups into themselves). Edelmans neuroscience remains speculative, since so much remains unknown about human neural structure and dynamics; but it does provide a tentative and plausible connection between evolutionary neurodynamics and the more abstract sort of evolution that patternist philosophy posits to occur in the realm of mindpatterns. 14.6 Glocal Memory A glocal memory is one that transcends the global/local dichotomy and incorporates both aspects in a tightly interconnected way.",Engineering General  Intelligence Part 1,chapter 14
"Here we make the glocal memory concept more precise, and describe its incarnation in the context of attractor neural nets (which is similar to its incarnation in CogPrime, to be elaborated in later chapters). Though our main interest here is in glocality in CogPrime, we also suggest that glocality may be a critical property to consider when analyzing human, animal and AI memory more broadly. The notion of glocal memory has implicitly occurred in a number of prior brain theories (without use of the neologism glocal), e.g. [Cal96] and [Goe01], but it has not previously been explicitly developed. However the concept has risen to the fore in our recent AI work and so we have chosen to esh it out more fully in [HG08, GPI+10] and the present section.",Engineering General  Intelligence Part 1,chapter 14
"Glocal memory overcomes the dichotomy between localized memory (in which each memory item is stored in a single location within an overall memory structure) and distributed memory (in which a memory item is stored as an aspect of a multi     14.6 Glocal Memory 309 component memory system, in such a way that the same set of multiple components stores a large number of memories). In a glocal memory system, most memory items are stored both locally and globally, with the property that eliciting either one of the two records of an item tends to also elicit the other one. Glocal memory applies to multiple forms of memory; however we will focus largely on perceptual and declarative memory in our detailed analyses here, so as to conserve space and maintain simplicity of discussion. The central idea of glocal memory is that (perceptual, declarative, episodic, procedural, etc.",Engineering General  Intelligence Part 1,chapter 14
"items may be stored in memory in the form of paired structures that are called (key, map) pairs. Of course the idea of a pair is abstract, and such pairs may manifest themselves quite differently in different sorts of memory systems (e.g. brains versus non-neuromorphic AI systems). The key is a localized version of the item, and records some signicant aspects of the items in a simple and crisp way. The map is a dispersed, distributed version of the item, which represents the item as a (to some extent, dynamically shifting) combination of fragments of other items. The map includes the key as a subset; activation of the key generally (but not necessarily always) causes activation of the map; and changes in the memory item will generally involve complexly coordinated changes on the key and map level both. Memory is one area where animal brain architecture differs radically from the von Neumann architecture underlying nearly all contemporary general-purpose computers.",Engineering General  Intelligence Part 1,chapter 14
"Von Neumann computers separate memory from processing, whereas in the human brain there is no such distinction. In fact, its arguable that in most cases the brain contains no memory apart from processing: human memories are generally constructed in the course of remembering [Ros88], which gives human memory a strong capability for lling in gaps of remembered experience and knowledge; and also causes problems with inaccurate remembering in many contexts [BF71, RM95] We believe the constructive aspect of memory is largely associated with its glocality. The remainder of this section presents a fuller formalization of the glocal memory concept, which is then taken up further below:  Sect.14.6.2 discusses the potential implementation of glocal memory in the human brain  Sect.14.6.",Engineering General  Intelligence Part 1,chapter 14
"3 discusses the implementation of glocal memory in attractor neural net systems  Chapter5 of Part 2, presents Glocal Economic Attention Networks (ECANs), rough analogues of glocal Hopeld nets that play a central role in CogPrime. Our hypothesis of the potential general importance of glocality as a property of memory systems (beyond just the CogPrime architecture)remains somewhat speculative. The presence of glocality in human and animal memory is strongly suggested but not rmly demonstrated by available neuroscience data; and the general value of glocality in the context of articial brains and minds is also not yet demonstrated as the whole eld of articial brain and mind building remains in its infancy.",Engineering General  Intelligence Part 1,chapter 14
"However, the utility of glocal memory for CogPrime is not tied to this more general, speculative themeglocality may be useful in CogPrime even if were wrong that it plays a signicant role in the brain and in intelligent systems more broadly.      310 14 Local, Global and Glocal Knowledge Representation 14.6.1 A Semi-Formal Model of Glocal Memory To explain the notion of glocal memory more precisely, we will introduce a simple semi-formal model of a system S that uses a memory to record information relevant to the actions it carries out. The overall concept of glocal memory should not be considered as restricted to this particular model. This model is not intended for maximal generality, but is intended to encompass a variety of current AI system designs and formal neurological models. In this model, we will consider Ss memory subsystem as a set of objects well call tokens, embedded in some metric space.",Engineering General  Intelligence Part 1,chapter 14
"The metric in the space, which we will call the basic distance of the memory, generally will not be dened in terms of the semantics of the items stored in the memory; though it may come to shape these dynamics through the specic architecture and evolution of the memory. Note that these tokens are not intended as generally being mapped one-to-one onto meaningful items stored in the memory. The tokens are the raw materials that the memory arranges in various patterns in order to store items. We assume that each token, at each point in time, may meaningfully be assigned a certain quantitative activation level. Also, tokens may have other numerical or discrete quantities associated with them, depending on the particular memory architecture. Finally, tokens may relate other tokens, so that optionally a token may come equipped with an (ordered or unordered) list of other tokens.",Engineering General  Intelligence Part 1,chapter 14
"To understand the meaning of the activation levels, one should think about Ss memory subsystem as being coupled with an action-selection subsystem, that dynamically chooses the actions to be taken by the overall system in which the two subsystems are embedded. Each combination of actions, in each particular type of context, will generally be associated with the activation of certain tokens in memory. Then, as analysts of the system S, we may associate each token T with an activation vector v(T, t), whose value for each discrete time t consists of the activation of the token T at time t. So, the 50th entry of the vector corresponds to the activation of the token at the 50th time step. Items stored in memory over a certain period of time, may then be dened as clusters in the set of activation vectors associated with memory during that period of time.",Engineering General  Intelligence Part 1,chapter 14
"Note that the system S itself may explicitly recognize and remember patterns regarding what items are stored in its memorybut, from an external analysts perspective, the set of items in Ss memory is not restricted to the ones that S has explicitly recognized as memory items. The localization of a memory item may be dened as the degree to which the various tokens involved in the item are close to each other according to the metric in the memory metric-space. This degree may be formalized in various ways, but choosing a particular quantitative measure is not important here. A highly localized item may be called local and a not-very-localized item may be called global. We may dene the activation distance of two tokens as the distance between their activation vectors. We may then say that a memory is well aligned to the      14.6 Glocal Memory 311 extent that there is a correlation between the activation distance of tokens, and the basic distance of the memory metric-space.",Engineering General  Intelligence Part 1,chapter 14
"Given the above set-up, the basic notion of glocal memory can be enounced fairly simply. A glocal memory is one:  that is reasonably well-aligned (i.e. the correlation between activation and basic distance is signicantly greater than random)  in which most memory items come in pairs, consisting of one local item and one global item, so that activation of the local item (the key) frequently leads in the near future to activation of the global item (the map). Obviously, in the scope of all possible memory structures constructible within the above formalism, glocal memories are going to be very rare and special. But, we suggest that they are important, because they are generally going to be the most effective way for intelligent systems to structure their memories. Note also that many memories without glocal structure may be well-aligned in the above sense.",Engineering General  Intelligence Part 1,chapter 14
"An example of a predominantly local memory structure, in which nearly all signicant memory items are local according to the above denition, is the Cyc logical reasoning engine [LG90]. To cast the Cyc knowledge base in the present formal model, the tokens are logical predicates. Cyc does not have an in-built notion of activation, but one may conceive the activation of a logical formula in Cyc as the degree to which the formula is used in reasoning or query processing during a certain interval in time. And one may dene a basic metric for Cyc by associating a predicate with its extension (the set of satisfying inputs), and dening the similarity of two predicates as the symmetric distance of their extensions. Cyc is reasonably well-aligned, but according to the dynamics of its querying and reasoning engines, it is basically a local memory structure without signicant global memory structure.",Engineering General  Intelligence Part 1,chapter 14
"On the other hand, an example of a predominantly global memory structure, in which nearly all signicant memory items are global according to the above denition, is the Hopeld associative memory network [Ami89]. Here memories are stored in the pattern of weights associated with synapses within a network of formal neurons, and each memory in general involves a large number of the neurons in the network. To cast the Hopeld net in the present formal model, the tokens are neurons and synapses; the activations are neural net activations; the basic distance between two neurons A and B may be dened as the percentage of the time that stimulating one of the neurons leads to the other one ring; and to calculate a basic distance involving a synapse, one may associate the synapse with its source and target neurons. With these denitions, a Hopeld network is a well-aligned memory, and (by intentional construction) a markedly global one.",Engineering General  Intelligence Part 1,chapter 14
"Local memory items will be very rare in a Hopeld net. While predominantly local and predominantly global memories may have great value for particular applications, our suggestion is that they also have inherent limitations. If so, this means that the most useful memories for general intelligence are going to be those that involve both local and global memory items in central roles. However, this is a more general and less risky claim than the assertion that glocal      312 14 Local, Global and Glocal Knowledge Representation memory structure as dened above is important. Because, glocal as dened above doesnt just mean neither predominantly global nor predominantly local. Rather, it refers to a specic pattern of coordination between local and global memory items what we have called the keys and maps pattern. 14.6.2 Glocal Memory in the Brain Sciences understanding of human brain dynamics is still very primitive, one manifestation of which is the fact that we really dont understand how the brain represents knowledge, except in some very simple respects.",Engineering General  Intelligence Part 1,chapter 14
"So anything anyone says about knowledge representation in the brain, at this stage, has to be considered highly speculative. Existing neuroscience knowledge does imply constraints on how knowledge representation in the brain may work, but these are relatively loose constraints. These constraints do imply that, for instance, the brain is neither a relational database (in which information is stored in a wholly localized manner) nor a collection of grandmother neurons that respond individually to high-level percepts or concepts; nor a simple Hopeld type neural net (in which all memories are attractors globally distributed across the whole network). But they dont tell us nearly enough to, for instance, create a formal neural net model that can condently be said to represent knowledge in the manner of the human brain.",Engineering General  Intelligence Part 1,chapter 14
"As a rst example of the current state of knowledge, well discuss here a series of papers regarding the neural representation of visual stimuli [QaGKKF05, QKKF08], which deal with the fascinating discovery of a subset of neurons in the medial temporal lobe (MTL) that are selectively activated by strikingly different pictures of given individuals, landmarks or objects, and in some cases even by letter strings. For instance, in their 2005 paper titled Invariant visual representation by single neurons in the human brain, it is noted that in one case, a unit responded only to three completely different images of the ex-president Bill Clinton. Another unit (from a different patient) responded only to images of The Beatles, another one to cartoons from The Simpsons television series and another one to pictures of the basketball player Michael Jordan.",Engineering General  Intelligence Part 1,chapter 14
"Their 2008 follow-up paper backed away from the more extreme interpretation in the title as well as the conclusion, with the title Sparse but not Grandmother-cell coding in the medial temporal lobe. As the authors emphasize there, Given the very sparse and abstract representation of visual information by these neurons, they could in principle be considered as grandmother cells. However, we give several arguments that make such an extreme interpretation unlikely.  MTL neurons are situated at the juncture of transformation of percepts into constructs that can be consciously recollected. These cells respond to percepts rather than to the detailed informationfallingontheretina.Thus,theiractivityreectsthefulltransformationthatvisual information undergoes through the ventral pathway. A crucial aspect of this transformation is      14.6 Glocal Memory 313 the complementary development of both selectivity and invariance.",Engineering General  Intelligence Part 1,chapter 14
"The evidence presented here, obtained from recordings of single-neuron activity in humans, suggests that a subset of MTL neurons possesses a striking invariant representation for consciously perceived objects, responding to abstract concepts rather than more basic metric details. This representation is sparse, in the sense that responsive neurons re only to very few stimuli (and are mostly silent except for their preferred stimuli), but it is far from a Grandmother-cell representation. The factthattheMTLrepresentsconsciousabstractinformationinsuchasparseandinvariantway is consistent with its prominent role in the consolidation of long-term semantic memories. Its interesting to note how inadequate the [QKKF08] data really is for exploring the notion of glocal memory in the brain. Suppose its the case that individual visual memories correspond to keys consisting of small neuronal subnetworks, and maps consisting of larger neuronal subnetworks.",Engineering General  Intelligence Part 1,chapter 14
"Then it would be not at all surprising if neurons in the key network corresponding to a visual concept like Bill Clintons face would be found to respond differentially to the presentation of appropriate images. Yet, it would also be wrong to overinterpret such data as implying that the key network somehow comprises the representation of Bill Clintons face in the individuals brain. In fact this key network would comprise only one aspect of said representation. In the glocal memory hypothesis, a visual memory like Bill Clintons face would be hypothesized to correspond to an attractor spanning a signicant subnetwork of the individuals brainbut this subnetwork still might occupy only a small fraction of the neurons in the brain (say, 1/100 or less), since there are very many neurons available. This attractor would constitute the map. But then, there would be a much smaller number of neurons serving as key to unlock this map: i.e.",Engineering General  Intelligence Part 1,chapter 14
"if a few of these key neurons were stimulated, then the overall attractor pattern in the map as a whole would unfold and come to play a signicant role in the overall brain activity landscape. In prior publications [Goe97] the primary author explored this hypothesis in more detail in terms of the known architecture of the cortex and the mathematics of complex dynamical attractors. So, one possible interpretation of the [QKKF08] data is that the MTL neurons theyre measuring are part of key networks that correspond to broader map networks recording percepts. The map networks might then extend more broadly throughout the brain, beyond the MTL and into other perceptual and cognitive areas of cortex. Furthermore, in this case, if some MTL key neurons were removed, the maps might well regenerate the missing keys (as would happen e.g. in the glocal Hopeld model to be discussed in the following section).",Engineering General  Intelligence Part 1,chapter 14
"Related and interesting evidence for glocal memory in the brain comes from a recent study of semantic memory, [PNR07]. Their research probed the architecture of semantic memory via comparing patients suffering from semantic dementia (SD) with patients suffering from three other neuropathologies, and found reasonably convincing evidence for what they call a distributed-plus-hub view of memory. The SD patients they studied displayed highly distinctive symptomology; for instance, their vocabularies and knowledge of the properties of everyday objects were strongly impaired, whereas their memories of recent events and other cognitive capacities remain perfectly intact. These patients also showed highly distinctive pat     314 14 Local, Global and Glocal Knowledge Representation terns of brain damage: focal brain lesions in their anterior temporal lobes (ATL), unlike the other patients who had either less severe or more widely distributed damage in their ATLs.",Engineering General  Intelligence Part 1,chapter 14
"This led [PNR07] to conclude that the ATL (being adjacent to the amygdala and limbic systems that process reward and emotion; and the anterior parts of the medial temporal lobe memory system, which processes episodic memory) is a hub for amodal semantic memory, drawing general semantic information from episodic memories based on emotional salience. So, in this view, the memory of something like a banana would contain a distributed aspect, spanning multiple brain systems, and also a localized aspect, centralized in the ATL. The distributed aspect would likely contain information on various particular aspects of bananas, including their sights, smells, and touches, the emotions they evoke, and the goals and motivations they relate to. The distributed and localized aspects would inuence one another dynamically, but, the data [PNR07] gathered do not address dynamics and they dont venture hypotheses in this direction.",Engineering General  Intelligence Part 1,chapter 14
"There is a relationship between the distributed-plus-hub view and [Dam00] better-known notion of a convergence zone, dened roughly as a location where the brain binds features together. A convergence zone, in [Dam00] perspective, is not a store of information but an agent capable of decoding a signal (and of reconstructing information). He also uses the metaphor that convergence zones behave like indexes drawing information from other areas of the brainbut they are dynamic rather than static indices, containing the instructions needed to recognize and combine the features constituting the memory of something. The mechanism involved in the distributed-plus-hub model is similar to a convergence zone, but with the important difference that hubs are less local: [PNR07] semantic hub may be thought of a kind of cluster of convergence zones consisting of a network of convergence zones for various semantic memories. What is missing in [PNR07] and [Dam00] perspective is a vision of distributed memories as attractors.",Engineering General  Intelligence Part 1,chapter 14
"The idea of localized memories serving as indices into distributed knowledge stores is important, but is only half the picture of glocal memory: the creative, constructive, dynamical-attractor aspect of the distributed representation is the other half. The closest thing to a clear depiction of this aspect of glocal memory that seems to exist in the neuroscience literature is a portion of William Calvins theory of the cerebral code [Cal96]. Calvin proposes a set of quite specic mechanisms by which knowledge may be represented in the brain using complexlystructured strange attractors, and by which these strange attractors may be propagated throughout the brain. Figure14.1 shows one aspect of his theory: how a distributed attractor may propagate from one part of the brain to another in pieces, with one portion of the attractor getting propagated rst, and then seeding the formation in the destination brain region of a close approximation of the whole attractor. Calvins theory may be considered a genuinely glocal theory of memory.",Engineering General  Intelligence Part 1,chapter 14
"However, it also makes a large number of other specic commitments that are not part of the notion of glocality, such as his proposal of hexagonal meta-columns in the cortex, and his commitment to evolutionary learning as the primary driver of neural knowledge creation.Wendtheseotherhypothesesinterestingandhighlypromising,yetfeelitis also important to separate out the notion of glocal memory for separate consideration.      14.6 Glocal Memory 315 Fig. 14.1 Calvins model of distributed attractors in the brain Regarding specics, our suggestion is that Calvins approach may overemphasize the distributed aspect of memory, not giving sufcient due to the relatively localized aspect as accounted for in the [QKKF08] results discussed above. In Calvins glocal approach, global memories are attractors and local memories are parts of attractors.",Engineering General  Intelligence Part 1,chapter 14
"We suggest a possible alternative, in which global memories are attractors and local memories are particular neuronal subnetworks such as the specialized ones identied by [QKKF08]. However, this alternative does not seem contradictory to Calvins overall conceptual approach, even though it is different from the particular proposals made in [Cal96]. The above paragraphs are far from a complete survey of the relevant neuroscience literature; there are literally dozens of studies one could survey pointing toward the glocality of various sorts of human memory. Yet experimental neuroscience tools are still relatively primitive, and every one of these studies could be interpreted in various other ways. In the next couple decades, as neuroscience tools improve in accuracy, our understanding of the role of glocality in human memory will doubtless improve tremendously. 14.6.",Engineering General  Intelligence Part 1,chapter 14
"3 Glocal Hopeld Networks The ideas in the previous section suggest that, if one wishes to construct an AGI, it is worth seriously considering using a memory with some sort of glocal structure. One research direction that follows naturally from this notion is glocal neural networks. In order to explore the nature of glocal neural networks in a relatively simple and      316 14 Local, Global and Glocal Knowledge Representation tractable setting, we have formalized and implemented simple examples of glocal Hopeld networks: palimpsest Hopeld nets with the addition of neurons representing localized memories. While these specic networks are not used in CogPrime, they are quite similar to the ECAN networks that are used in CogPrime and described in Chap.5 of Part 2. Essentially, we augment the standard Hopeld net architecture by adding a set of key neurons.",Engineering General  Intelligence Part 1,chapter 14
"These are a small percentage of the neurons in the network, and are intended to be roughly equinumerous to the number of memories the network is supposed to store. When the Hopeld net converges to an attractor A, then new links are created between the neurons that are active in A, and one of the key neurons. Which key neuron is chosen? The one that, when it is stimulated, gives rise to an attractor pattern maximally similar to A. The ultimate result of this is that, in addition to the distributed memory of attractors in the Hopeld net, one has a set of key neurons that in effect index the attractors. Each attractor corresponds to a single key neuron. In the glocal memory model, the key neurons are the keys and the Hopeld net attractors are the maps.",Engineering General  Intelligence Part 1,chapter 14
"This algorithm has been tested in sparse Hopeld nets, using both standard Hopeld net learning rules and Storkeys modied palimpsest learning rule [SV99], which provides greater memory capacity in a continuous learning context. The use of key neurons turns out to slightly increase Hopeld net memory capacity, but this isnt the main point. The main point is that one now has a local representation of each global memory, so that if one wants to create a link between the memory and something else, its extremely easy to do soone just needs to link to the corresponding key neuron. Or, rather, one of the corresponding key neurons: depending on how many key neurons are allocated, one might end up with a number of key neurons corresponding to each memory, not just one. In order to transform a palimpsest Hopeld net into a glocal Hopeld net, the following steps are taken: 1.",Engineering General  Intelligence Part 1,chapter 14
"Add a xed number of key neurons to the network (removing other random neurons to keep the total number of neurons constant) 2. When the network reaches an attractor, create links from the elements in the attractor to one of the key neurons 3. The key neuron chosen for the previous step is the one that most closely matches the current attractor (which may be determined in several ways, to be discussed below) 4. To avoid the increase of the number of links in the network, when new links are created in Step 2, other key-neuron links are then deleted (several approaches may be taken here, but the simplest is to remove the key-neuron links with the lowest-absolute-value weights). In the simple implementation of the above steps that we implemented, and described in [GPI+10], Step 3 is carried out simply by comparing the weights of a key neurons links to the nodes in an attractor.",Engineering General  Intelligence Part 1,chapter 14
"A more sophisticated approach would be to select the key neuron with the highest activation during the transient interval immediately prior to convergence to the attractor.      14.6 Glocal Memory 317 The result of these modications to the ordinary Hopeld net, is a Hopeld net that continually maintains a set of key neurons, each of which individually represents a certain attractor of the net. Note that these key neuronsin spite of being symbolic in natureare learned rather than preprogrammed, and are every bit as adaptive as the attractors they correspond to. Furthermore, if a key neuron is removed, the glocal Hopeld net algorithm will eventually learn it back, so the robustness properties of Hopeld nets are retained. The results of experimenting with glocal Hopeld nets of this nature are summarized in [GPI+10]. We studied Hopeld nets with connectivity around 0.",Engineering General  Intelligence Part 1,chapter 14
"1, and in this context we found that glocality  slightly increased memory capacity  massively increased the rate of convergence to the attractor, i.e. the speed of recall. However, probably the most important consequence of glocality is a more qualitative one: it makes it far easier to link the Hopeld net into a larger system, as would occur if the Hopeld net were embedded in an integrative AGI architecture. Because a neuron external to the Hopeld net may now link to a memory in the Hopeld net by linking to the corresponding key neuron. 14.6.4 Neural-Symbolic Glocality in CogPrime In CogPrime, we have explicitly sought to span the symbolic/emergentist pseudodichotomy, via creating an integrative knowledge representation that combines logicbased aspects with neural-net-like aspects. As reviewed in Chap.",Engineering General  Intelligence Part 1,chapter 14
"2, these function not in the manner of multimodular systems, but rather via using (probabilistic) truth values and (attractor neural net like) attention values as weights on nodes and links of the same (hyper) graph. The nodes and links in this hypergraph are typed, like a standard semantic network approach for knowledge representation, so theyre able to handle all sorts of knowledge, from the most concrete perception and actuation related knowledge to the most abstract relationships. But theyre also weighted with values similar to neural net weights, and pass around quantities (importance values, discussed in Chap.5 of Part 2) similar to neural net activations, allowing emergent attractor/assembly based knowledge representation similar to attractor neural nets.",Engineering General  Intelligence Part 1,chapter 14
"The concept of glocality lies at the heart of this combination, in a way that spans the pseudo-dichotomy:  Local knowledge is represented in abstract logical relationships stored in explicit logical form, and also in Hebbian-type associations between nodes and links.  Global knowledge is represented in large-scale patterns of node and link weights, which lead to large-scale patterns of network activity, which often take the form of attractors qualitatively similar to Hopeld net attractors. These attractors are called maps.      318 14 Local, Global and Glocal Knowledge Representation The result of all this is that a concept like cat might be represented as a combination of:  A small number of logical relationships and strong associations, that constitute the key subnetwork for the cat concept.  A large network of weak associations, binding together various nodes and links of various types and various levels of abstraction, representing the cat map.",Engineering General  Intelligence Part 1,chapter 14
"The activation of the key will generally cause the activation of the map, and the activation of a signicant percentage of the map will cause the activation of the rest of the map, including the key. Furthermore, if the key were for some reason forgotten, then after a signicant amount of effort, the system would likely to be able to reconstitute it (perhaps with various small changes) from the information in the map. We conjecture that this particular kind of glocal memory will turn out to be very powerful for AGI, due to its ability to combine the strengths of formal logical inference with those of self-organizing attractor neural networks. As a simple example, consider the representation of a tower, in the context of an articial agent that has built towers of blocks, and seen pictures of many other kinds of towers, and seen some tall building that it knows are somewhat like towers but perhaps not exactly towers.",Engineering General  Intelligence Part 1,chapter 14
"If this agent is reasonably conceptually advanced (say, at Piagetan the concrete operational level) then its mind will contain some declarative relationships partially characterizing the concept of tower, as well as its sensory and episodic examples, and its procedural knowledge about how to build towers. The key of the tower concept in the agents mind may consist of internal images and episodes regarding the towers it knows best, the essential operations it knows are useful for building towers (piling blocks atop blocks atop blocks ...), and the core declarative relations summarizing towernessand the whole tower map then consists of a much larger number of images, episodes, procedures and declarative relationships connected to tower and other related entities. If any portion of the map is removedeven if the key is removedthen the rest of the map can be approximately reconstituted, after some work. Some cognitive operations are best done on the localized representatione.g. logical reasoning.",Engineering General  Intelligence Part 1,chapter 14
"Other operations, such as attention allocation and guidance of inference control, are best done using the globalized map representation.   ",Engineering General  Intelligence Part 1,chapter 14
"  Chapter 15 Representing Implicit Knowledge via Hypergraphs 15.1 Introduction Explicit knowledge is easy to write about and talk about; implicit knowledge is equally important, but tends to get less attention in discussions of AI and psychology, simply because we dont have as good a vocabulary for describing it, nor as good a collection of methods for measuring it. One way to deal with this problem is to describe implicit knowledge using language and methods typically reserved for explicit knowledge. This might seem intrinsically non-workable, but we argue that it actually makes a lot of sense. The same sort of networks that a system like CogPrime uses to represent knowledge explicitly, can also be used to represent the emergent knowledge that implicitly exists in an intelligent systems complex structures and dynamics. Weve noted that CogPrime uses an explicit representation of knowledge in terms of weighted labeled hypergraphs; and also uses other more neural net like mechanisms (e.g.",Engineering General  Intelligence Part 1,chapter 15
"the economic attention allocation network subsystem) to represent knowledge globally and implicitly. CogPrime combines these two sorts of representation according to the principle we have called glocality. In this chapter we pursue glocality a bit furtherdescribing a means by which even implicitly represented knowledge can be modeled using weighted labeled hypergraphs similar to the ones used explicitly in CogPrime. This is conceptually important, in terms of making clear the fundamental similarities and differences between implicit and explicit knowledge representation; and it is also pragmatically meaningful due to its relevance to the CogPrime methods described in Chap.24 of Part 2 that transform implicit into explicit knowledge. To avoid confusion with CogPrimes explicit knowledge representation, we will refer to the hypergraphs in this chapter as composed of Vertices and Edges rather than Nodes and Links.",Engineering General  Intelligence Part 1,chapter 15
"In prior publications we have referred to derived or emergent hypergraphs of the sort described here using the acronym SMEPH, which stands for Self-Modifying, Evolving Probabilistic Hypergraphs. B. Goertzel et al., Engineering General Intelligence, Part 1, 319 Atlantis Thinking Machines 5, DOI: 10.2991/978-94-6239-027-0_15,  Atlantis Press and the authors 2014      320 15 Representing Implicit Knowledge via Hypergraphs 15.2 Key Vertex and Edge Types We begin by introducing a particular collection of Vertex and Edge types, to be used in modeling the internal structures of intelligent systems. The key SMEPH Vertex types are  ConceptVertex, representing a set, for instance, an idea or a set of percepts.  SchemaVertex, representing a procedure for doing something (perhaps something in the physical world, or perhaps an abstract mental action).",Engineering General  Intelligence Part 1,chapter 15
"The key SMEPH Edge types, using language drawn from Probabilistic Logic Networks (PLN) and elaborated in Chap.16 (Part 2), are as follows:  ExtensionalInheritanceEdge (ExtInhEdge for short: an edge which, linking one Vertex or Edge to another, indicates that the former is a special case of the latter).  ExtensionalSimilarityEdge (ExtSim: which indicates that one Vertex or Edge is similar to another).  ExecutionEdge (a ternary edge, which joins S,B,C when S is a SchemaVertex and the result from applying S to B is C). So, in a SMEPH system, one is often looking at hypergraphs whose Vertices represent ideas or procedures, and whose Edges represent relationships of specialization, similarity or transformation among ideas and/or procedures. The semantics of the SMEPH edge types is given by PLN, but is simple and commonsensical.",Engineering General  Intelligence Part 1,chapter 15
"ExtInh and ExtSim Edges come with probabilistic weights indicating the extent of the relationship they denote (e.g. the ExtSimEdge joining the cat ConceptVertex to the dog ConceptVertex gets a higher probability weight than the one joining the cat ConceptVertex to the washing-machine ConceptVertex). The mathematics of transformations involving these probabilistic weights becomes quite involvedparticularly when one introduces SchemaVertices corresponding to abstract mathematical operations, a step that enables SMEPH hypergraphs to have the complete mathematical power of standard logical formalisms like predicate calculus, but with the added advantage of a natural representation of uncertainty in terms of probabilities, as well as a natural representation of networks and webs of complex knowledge. 15.3 Derived Hypergraphs We now describe how SMEPH hypergraphs may be used to model and describe intelligent systems. One can (in principle) draw a SMEPH hypergraph corresponding to any individual intelligent system, with Vertices and Edges for the concepts and processes in that systems mind.",Engineering General  Intelligence Part 1,chapter 15
"This is called the derived hypergraph of that system.      15.3 Derived Hypergraphs 321 15.3.1 SMEPH Vertices A ConceptVertex in the derived hypergraph of a system corresponds to a structural pattern that persists over time in that system; whereas a SchemaVertex corresponds to a multi-time-point dynamical pattern that recurs in that systems dynamics. If one accepts the patternist denition of a mind as the set of patterns in an intelligent system, then it follows that the derived hypergraph of an intelligent system captures a signicant fraction of the mind of that system. To phrase it a little differently, we may say that a ConceptVertex, in SMEPH, refers to the habitual pattern of activity observed in a system when some condition is met (this condition corresponding to the presence of a certain pattern). The condition may refer to something in the world external to the system, or to something internal. For instance, the condition may be observing a cat.",Engineering General  Intelligence Part 1,chapter 15
"In this case, the corresponding Concept vertex in the mind of Ben Goertzel is the pattern of activity observed in Ben Goertzels brain when his eyes are open and hes looking in the direction of a cat. The notion of pattern of activity can be made rigorous using mathematical pattern theory, as is described in The Hidden Pattern [Goe06a]. Note that logical predicates, on the SMEPH level, appear as particular kinds of Concepts, where the condition involves a predicate and an argument. For instance, suppose one wants to know what happens inside Bens mind when he eats cheese. Then there is a Concept corresponding to the condition of cheese-eating activity. But there may also be a Concept corresponding to eating activity in general. If the Concept denoting the activity of eating X is generally easily computable from the Concepts for X and eating individually, then the eating Concept is effectively acting as a predicate.",Engineering General  Intelligence Part 1,chapter 15
"A SMEPH SchemaVertex, on the other hand, is like a Concept thats dened in a time-dependent way. One type of Schema refers to a habitual dynamical pattern of activity occurring before and/or during some condition is met. For instance, the condition might be saying the word Hello. In that case the corresponding SchemaVertex in the mind of Ben Goertzel is the pattern of activity that generally occurs before he says Hello. AnothertypeofSchemareferstoahabitualdynamicalpatternofactivityoccurring after some condition X is met. For instance, in the case of the Schema for adding two numbers, the precondition X consists of the two numbers and the concept of addition. The Schema is then what happens when the mind thinks of adding and thinks of two numbers. Finally, there are Schema that refer to habitual dynamical activity patterns occurring after some condition X is met and before some condition Y is met.",Engineering General  Intelligence Part 1,chapter 15
"In this case the Schema is viewed as transforming X into Y. For instance, if X is the condition of meeting someone who is not a friend, and Y is the condition of being friends with that person, then the habitually intervening activities constitute the Schema for making friends.      322 15 Representing Implicit Knowledge via Hypergraphs 15.3.2 SMEPH Edges SMEPH edge types fall into two categories: functional and logical. Functional edges connect Schema vertices to their input and outputs; logical edges refer mainly to conditional probabilities, and in general are to be interpreted according to the semantics of Probabilistic Logic Networks. Let us begin with logical edges. The simplest case is the Subset edge, which denotes a straightforward, extensional conditional probability. For instance, it may happen that whenever the Concept for cat is present in a system, the Concept for animal is as well.",Engineering General  Intelligence Part 1,chapter 15
"Then we would say Subset cat animal (Here we assume a notation where R A B denotes an Edge of type R between Vertices A and B.) On the other hand, it may be that 50% of the time that cat is present in the system, cute is present as well: then we would say Subset cat cute <.5> where the <.5> denotes the probability, which is a component of the Truth Value associated with the edge. Next, the most basic functional edge is the Execution edge, which is ternary and denotes a relation between a Schema, its input and its output, e.g. Execution father_of Ben_Goertzel Ted_Goertzel for a schema father_of that outputs the father of its argument. The ExecutionOutput (ExOut) edge denotes the output of a Schema in an implicit way, e.g.",Engineering General  Intelligence Part 1,chapter 15
"ExOut say_hello refers to a particular act of saying hello, whereas ExOut add_numbers \\{3 , 4) refers to the Concept corresponding to 7. Note that this latter example involves a set of three entities: sets are also part of the basic SMEPH knowledge representation. A set may be thought of as a hypergraph edge that points to all its members. In this manner we may dene a set of edges and vertices modeling the habitual activity patterns of a system when in different situations. This is called the derived hypergraph of the system. Note that this hypergraph can in principle be constructed no matter what happens inside the system: whether its a human brain, a formal neural network, Cyc, OCP, a quantum computer, etc. Of course, constructing the hypergraph in practice is quite a different story: for instance, we currently have no accurate way of measuring the habitual activity patterns inside the human brain.",Engineering General  Intelligence Part 1,chapter 15
"fMRI and PET and other neuroimaging technologies give only a crude view, though they are continually improving. Pattern theory enters more deeply here when one thoroughly eshes out the Inheritance concept. Philosophers of logic have extensively debated the relationship      15.3 Derived Hypergraphs 323 between extensional inheritance (inheritance between sets based on their members) and intensional inheritance (inheritance between entity-types based on their properties). A variety of formal mechanisms have been proposed to capture this conceptual distinction; see (Wang 2006) for a review along with a novel approach utilizing uncertain term logic. Pattern theory provides a novel approach to dening intension: one may associate with each ConceptVertex in a systems derived hypergraph the set of patterns associated with the structural pattern underlying that ConceptVertex.",Engineering General  Intelligence Part 1,chapter 15
"Then, one can dene the strength of the IntensionalInheritanceEdge between two ConceptVertices A and B as the percentage of As pattern-set that is also contained in Bs pattern-set. According to this approach, for instance, one could have IntInhEdge whale fish <0.6 > ExtInhEdge whale fish <0.0 > since the sh and whale sets have common properties but no common members. 15.4 Implications of Patternist Philosophy for Derived Hypergraphs of Intelligent Systems Patternist philosophy rears its head here and makes some denite hypotheses about the structure of derived hypergraphs. It suggests that derived hypergraphs should have a dual network structure, and that in highly intelligent systems they should have subgraphs that constitute models of the whole hypergraph (these are self systems). SMEPH does not add anything to the patternist view on a philosophical level, but it gives a concrete instantiation to some of the general ideas of patternism.",Engineering General  Intelligence Part 1,chapter 15
"In this section well articulate some SMEPH principles, constituting important ideas from patternist philosophy as they manifest themselves in the SMEPH context. The logical edges in a SMEPH hypergraph are weighted with probabilities, as in the simple example given above. The functional edges may be probabilistically weighted as well, since some Schema may give certain results only some of the time. These probabilities are critical in terms of SMEPHs model of system dynamics; they underly one of our SMEPH principles, Principle of Implicit Probabilistic Inference: In an intelligent system, the temporal evolution of the probabilities on the edges in the systems derived hypergraph should approximately obey the rules of probability theory. The basic idea is that, even if a systemthrough its underlying dynamicshas no explicit connection to probability theory, it still must behave roughly as if it does, if it is going to be intelligent.",Engineering General  Intelligence Part 1,chapter 15
"The roughly part is important here; its well known that humans are not terribly accurate in explicitly carrying out formal probabilistic inferences. And yet, in practical contexts where they have experience, humans can make quite accurate judgments; which is all thats required by the above principle,      324 15 Representing Implicit Knowledge via Hypergraphs since its the contexts where experience has occurred that will make up a systems derived hypergraph. Our next SMEPH principle is evolutionary, and states Principle of Implicit Evolution: In an intelligent system, new Schema and Concepts will continually be created, and the Schema and Concepts that are more useful for achieving system goals (as demonstrated via probabilistic implication of goal achievement) will tend to survive longer. Note that this principle can be fullled in many different ways. The important thing is that system goals are allowed to serve as a selective force.",Engineering General  Intelligence Part 1,chapter 15
"Another SMEPH dynamical principle pertains to a shorter time-scale than evolution, and states Principle of Attention Allocation: In an intelligent system, Schema and Concepts that are more useful for attaining short-term goals will tend to consume more of the systems energy. (The balance of attention oriented toward goals pertaining to different time scales will vary from system to system.) Next, there is the Principle of Autopoesis: In an intelligent system, if one removes some part of the system and then allows the systems natural dynamics to keep going, a decent approximation to that removed part will often be spontaneously reconstituted. And there is the Cognitive Equation Principle: In an intelligent system, many abstract patterns that are present in the system at a certain time as patterns among other Schema and Concepts, will at a near-future time be present in the system as patterns among elementary system components. The Cognitive Equation Principle, briey discussed in Chap.",Engineering General  Intelligence Part 1,chapter 15
"6, basically means that Concepts and Schema emergent in the system are recognized by the system and then embodied as elementary items in the system so that patterns among them in their emergent form become, with the passage of time, patterns among them in their directly-system-embodied form. This is a natural consequence of the way intelligent systems continually recognize patterns in themselves. Note that derived hypergraphs may be constructed corresponding to any complex system which demonstrates a variety of internal dynamical patterns depending on its situation. However, if a system is not intelligent, then according to the patternist philosophyevolutionofitsderivedhypergraphcantnecessarilybeexpectedtofollow the above principles. 15.4.1 SMEPH Principles in CogPrime We now more explicitly elaborate the application of these ideas in the CogPrime context. As noted above, in addition to explicit knowledge representation in terms      15.",Engineering General  Intelligence Part 1,chapter 15
"4 Implications of Patternist Philosophy for Derived Hypergraphs of Intelligent Systems 325 of Nodes and Links, CogPrime also incorporates implicit knowledge representation in the form of what are called Maps: collections of Nodes and Links that tend to be utilized together within cognitive processes. These Maps constitute a CogPrime systems derived hypergraph, which will not be identical to the hypergraph it uses for explicit knowledge representation. However, an interesting feedback loop arises here, in that the intelligences self-study will generally lead it to recognize large portions of its derived hypergraph as patterns in itself, and then embody these patterns within its concretely implemented knowledge hypergraph. This relates to the Cognitive Equation Principle dened in Chap.6, in which an intelligent system continually recognizes patterns in itself and embodies these patterns in its own basic structure (so that new patterns may more easily emerge from them). Often it happens that a particular CogPrime node will serve as the center of a map, so that e.g.",Engineering General  Intelligence Part 1,chapter 15
"the Concept Link denoting cat will consist of a number of nodes and links roughly centered around a ConceptNode that is linked to the WordNode cat. But this is not guaranteed and some CogPrime maps are more diffuse than this with no particular center. Somewhat similarly, the key SMEPH dynamics are represented explicitly in CogPrime: probabilistic reasoning is carried out via explicit application of PLN on the CogPrime hypergraph, evolutionary learning is carried out via application of the MOSES optimization algorithm, and attention allocation is carried out via a combination of inference and evolutionary pattern mining. But the SMEPH dynamics also occur implicitly in CogPrime: emergent maps are reasoned on probabilistically as an indirect consequence of node-and-link level PLN activity; maps evolve as a consequence of the coordinated whole of CogPrime dynamics; and attention shifts between maps according to complex emergent dynamics.",Engineering General  Intelligence Part 1,chapter 15
"To see the need for maps, consider that even a Node that has a particular meaning attached to itlike the Iraq Node, saydoesnt contain much of the meaning of Iraq in it. The meaning of Iraq lies in the Links attached to this Node, and the Links attached to their Nodesand the other Nodes and Links not explicitly represented in the system, which will be created by CogPrimes cognitive algorithms based on the explicitly existent Nodes and Links related to the Iraq Node. This halo of Atoms related to the Iraq node is called the Iraq map. In general, some maps will center around a particular Atom, like this Iraq map, others may not have any particular identiable center. CogPrimes cognitive processes act directly on the level of Nodes and Links, but they must be analyzed in terms of their impact on maps as well.",Engineering General  Intelligence Part 1,chapter 15
"In SMEPH terms, CogPrime maps may be said to correspond to SMEPH ConceptVertex, and for instance bundles of Links between the Nodes belonging to a map may correspond to a SMEPH Link between two ConceptVertex.   ",Engineering General  Intelligence Part 1,chapter 15
"  Chapter 16 Emergent Networks of Intelligence 16.1 Introduction When one is involved with engineering an AGI system, one thinks a lot about the aspects of the system one is explicitly buildingwhat are the parts, how they t together, how to test theyre properly working, and so forth. And yet, these explicitly engineered aspects are only a fraction of whats important in an AGI system. At least as critical are the emergent aspectsthe patterns that emerge once the system is up and running, interacting with the world and other agents, growing and developing and learning and self-modifying. SMEPH is one toolkit for describing some of these emergent patterns, but its only a start. In line with these general observations, most of this book will focus on the structures and processes that we have built, or intend to build, into the CogPrime system. But in a sense, these structures and processes are not the crux of CogPrimes intended intelligence.",Engineering General  Intelligence Part 1,chapter 16
"The purpose of these pre-programmed structures and processes is to give rise to emergent structures and processes, in the course of CogPrimes interaction with the world and the other minds within it. We will return to this theme of emergence at several points in later chapters, e.g. in the discussion of map formation in Chap.24 of Part 2. Given the important of emergent structuresand specically emergent network structuresfor intelligence, its fortunate the scientic community has already generated a lot of knowledge about complex networks: both networks of physical or software elements, and networks of organization emergent from complex systems.",Engineering General  Intelligence Part 1,chapter 16
"As most of this knowledge has originated in elds other than AGI, or in pure mathematics, it tends to require some reinterpretation or tweaking to achieve maximal applicability in the AGI context; but we believe this effort will become increasingly worthwhile as the AGI eld progresses, because network theory is likely to be very useful for describing the contents and interactions of AGI systems as they develop increasing intelligence. In this brief chapter we specically focus on the emergence of certain large-scale network structures in a CogPrime knowledge store, presenting heuristic arguments B. Goertzel et al., Engineering General Intelligence, Part 1, 327 Atlantis Thinking Machines 5, DOI: 10.2991/978-94-6239-027-0_16,  Atlantis Press and the authors 2014      328 16 Emergent Networks of Intelligence as to why these structures can be expected to arise.",Engineering General  Intelligence Part 1,chapter 16
"We also comment on the way in which these emergent structures are expected to guide cognitive processes, and give rise to emergent cognitive processes. Appendix C expands on this theme in a particular direction, exploring the possible emergence of structures characterizing inter-cognitive reection. 16.2 Small World Networks One simple but potentially useful observation about CogPrime Atomspaces is that they are generally going to be small world networks [Buc03], rather than random graphs. A small world network is a graph in which the connectivities of the various nodes display a power law behaviorso that, loosely speaking, there are a few nodes with very many links, then more nodes with a modest number of links. . . and nally, a huge number of nodes with very few links. This kind of network occurs in many natural andhumansystems, includingcitations amongpapers, nancial arrangements among banks, links between Web pages and the spread of diseases among people or animals.",Engineering General  Intelligence Part 1,chapter 16
"In a weighted network like an Atomspace, small-world-ness must be dened in a manner taking the weights into account, and there are several obvious ways to do this. Figure16.1 depicts a small but prototypical small-worlds network, with a few hub nodes possessing far more neighbors than the others, and then some secondary hubs, etc. Fig. 16.1 A typical, though small-sized, small-worlds network      16.2 Small World Networks 329 An excellent reference on network theory in general, including but not limited to small world networks, is Peter Csermelys Weak Links [Cse06]. Many of the ideas in that work have apparent CogPrime applications, which are not elaborated here. One process via which small world networks commonly form is preferential attachment [Bar02]. This occurs in essence when the rich get richeri.e.",Engineering General  Intelligence Part 1,chapter 16
"when nodes in the network grow new links, in a manner that causes them to preferentially grow links to nodes that already have more links. It is not hard to see that CogPrimes ECAN dynamics will naturally lead to preferential attachment, because Atoms with more links will tend to get more STI, and thus will tend to get selected by more cognitive processes, which will cause them to grow more links. For this reason, in most circumstances, a CogPrime system in which most link-building cognitive processes rely heavily on ECAN to guide their activities will tend to contain a smallworld-network Atomspace. This is not rigorously guaranteed to be the case for any possible combination of environment and goals, but it is commonsensically likely to nearly always be the case. One consequence of the small worlds structure of the Atomspace is that, in exploring other properties of the Atom network, it is particularly important to look at the hub nodes.",Engineering General  Intelligence Part 1,chapter 16
"For instance, if one is studying whether hierarchical and heterarchical subnetworks of the Atomspace exist, and whether they are well-aligned with each other, it is important to look at hierarchical and heterarchical connections between hub nodes in particular (and secondary hubs, etc.). A pattern of hierarchical or dual network connection that only held up among the more sparsely connected nodes in a small-world network would be a strange thing, and perhaps not that cognitively useful. 16.3 Dual Network Structure One of the key theoretical notions in patternist philosophy is that complex cognitive systems evolve internal dual network structures, comprising superposed, harmonized hierarchical and heterarchical networks. Now we explore some of the specic CogPrime structures and dynamics militating in favor of the emergence of dual networks. 16.3.1 Hierarchical Networks The hierarchical nature of human linguistic concepts is well known, and is illustrated in Fig.16.",Engineering General  Intelligence Part 1,chapter 16
"2 for the commonsense knowledge domain (using a graph drawn from WordNet, a huge concept hierarchy covering 50K+ English-language concepts), and in Fig.16.4 for a specialized knowledge subdomain, genetics. Due to this fact, a certain amount of hierarchy can be expected to emerge in the Atomspace of any      330 16 Emergent Networks of Intelligence Fig. 16.2 A typical, though small, subnetwork of WordNets hierarchical network Fig. 16.3 A typical, though small, subnetwork of the Gene Ontologys hierarchical network linguistically savvy CogPrime, simply due to its modeling of the linguistic concepts that it hears and reads (Fig. 16.3). Hierarchy also exists in the natural world apart from language, which is the reason that many sensorimotor-knowledge-focused AGI systems (e.g. DeSTIN and HTM, mentioned in Chap.6) feature hierarchical structures.",Engineering General  Intelligence Part 1,chapter 16
"In these cases the hierarchies are normally spatiotemporal in naturewith lower layers containing elements responding to more localized aspects of the perceptual eld, and smaller, more localized groups of actuators. This kind of hierarchy certainly could emerge in an AGI      16.3 Dual Network Structure 331 Fig. 16.4 Small-scale portrayal of a portion of the spatiotemporal hierarchy in Jeff Hawkins Hierarchical Temporal Memory architecture system, but in CogPrime we have opted for a different route. If a CogPrime system is hybridized with a hierarchical sensorimotor network like one of those mentioned above, then the Atoms linked to the nodes in the hierarchical sensorimotor network will naturally possess hierarchical conceptual relationships, and will thus naturally grow hierarchical links between them (e.g. and IntensionalInheritanceLinks via PLN, AsymmetricHebbianLinks via ECAN).",Engineering General  Intelligence Part 1,chapter 16
"Once elements of hierarchical structure exist via the hierarchical structure of language and physical reality, then a richer and broader hierarchy can be expected to accumulate on top of it, because importance spreading and inference control will implicitly and automatically be guided by the existing hierarchy. That is, in the language of Chaotic Logic [Goe94] and patternist theory, hierarchical structure is an autopoietic attractoronce its there it will tend to enrich itself and maintain itself. AsymmetricHebbianLinks arranged in a hierarchy will tend to cause importance to spread up or down the hierarchy, which will lead other cognitive processes to look for patterns between Atoms and their hierarchical parents or children, thus potentially building more hierarchical links. Chains of InheritanceLinks pointing up and down the hierarchy will lead PLN to search for more hierarchical linkse.g.",Engineering General  Intelligence Part 1,chapter 16
"most simply, A  B  C where C is above B is above A in the hierarchy, will naturally lead inference to check the viability of A  C by deduction. There is also the possibility to introduce a special DefaultInheritanceLink, as discussed in Chap.16 of Part 2, but this isnt actually necessary to obtain the inferential maintenance of a robust hierarchical network.      332 16 Emergent Networks of Intelligence Fig. 16.5 Portions of a conceptual heterarchy centered on specic concepts Fig. 16.6 A portion of a conceptual heterarchy, showing the dangling links leading this portion to the rest of the heterarchy 16.3.2 Associative, Heterarchical Networks Heterarchy is in essence a simpler structure than hierarchy: it simply refers to a network in which nodes are linked to other nodes with which they share important relationships.",Engineering General  Intelligence Part 1,chapter 16
"That is, there should be a tendency that if two nodes are often important in the same contexts or for the same purposes, they should be linked together. Portrayals of typical heterarchical linkage patterns among natural language concepts      16.3 Dual Network Structure 333 are given in Figs.16.5 and 16.6. Naturally, real concept heterarchies are far more large, complex and tangled than these. In CogPrime, ECAN enforces heterarchy via building SymmetricHebbianLinks, and PLN by building SimilarityLinks, IntensionalSimilarityLinks and ExtensionalSimilarityLinks. Furthermore, these various link types reinforce each other. PLN control is guided by importance spreading, which follows Hebbian links, so that a heterarchical Hebbian network tends to cause PLN to explore the formation of links following the same paths as the heterarchical HebbianLinks.",Engineering General  Intelligence Part 1,chapter 16
"And importance can spread along logical links as well as explicit Hebbian links, so that the existence of a heterarchical logical network will tend to cause the formation of additional heterarchical Hebbian links. Heterarchy reinforces itself in autopoietic attractor style even more simply and directly than heterarchy. 16.3.3 Dual Networks Finally, if both hierarchical and heterarchical structures exist in an Atomspace, then both ECAN and PLN will naturally blend them together, because hierarchical and heterarchical links will feed into their link-creation processes and naturally be combined together to form new links. This will tend to produce a structure called a dual network, in which a hierarchy exists, along with a rich network of heterarchical links joining nodes in the hierarchy, with a particular density of links between nodes on the same hierarchical level.",Engineering General  Intelligence Part 1,chapter 16
"The dual network structure will emerge without any explicit engineering oriented toward it, simply via the existence of hierarchical and heterarchical networks, and the propensity of ECAN and PLN to be guided by both the hierarchical and heterarchical networks. The existence of a natural dual network structure in both linguistic and sensorimotor data will help the formation process along, and then creative cognition will enrich the dual network yet further than is directly necessitated by the external world. A rigorous mathematical analysis of the formation of hierarchical, heterarchical and dual networks in CogPrime systems has not yet been undertaken, and would certainly be an interesting enterprise. Similar to the theory of small world networks, there is ample ground here for both theorem-proving and heuristic experimentation. However, the qualitative points made here are sufciently well-grounded in intuition and experience to be of some use guiding our ongoing work.",Engineering General  Intelligence Part 1,chapter 16
"One of the nice things about emergent network structures is that they are relatively straightforward to observe in an evolving, learning AGI system, via visualization and inspection of structures such at the Atomspace.      Part VI A Path to Human-Level AGI   ",Engineering General  Intelligence Part 1,chapter 16
"  Chapter 17 AGI Preschool 17.1 Introduction In conversations with government funding sources or narrow AI researchers about AGI work, one of the topics that comes up most often is that of evaluation and metricsi.e., AGI intelligence testing. We actually prefer to separate this into two topics: environments and methods for careful qualitative evaluation of AGI systems, versus metrics for precise measurement of AGI systems. The difculty of formulating bulletproof metrics for partial progress toward advanced AGI has become evident throughout the eld, and in Chap.9 we have elaborated one plausible explanation for this phenomenon, the trickiness of cognitive synergy. [LWML09], summarizing a workshop on Evaluation and Metrics for Human-Level AI held in 2008, discusses some of the general difculties involved in this type of assessment, and some requirements that any viable approach must fulll.",Engineering General  Intelligence Part 1,chapter 17
"On the other hand, the lack of appropriate methods for careful qualitative evaluation of AGI systems has been much less discussed, but we consider it actually a more important issueas well as an easier (though not easy) one to solve. We havent actually found the lack of quantitative intelligence metrics to be a major obstacle in our practical AGI work so far. Our OpenCogPrime implementation lags far behind the CogPrime design as articulated in Part 2, and according to the theory underlying CogPrime, the more interesting behaviors and dynamics of the system will occur only when all the parts of the system have been engineered to a reasonable level of completion and integrated together. So, the lack of a great set of metrics for evaluating the intelligence of our partially-built system hasnt impaired too much.",Engineering General  Intelligence Part 1,chapter 17
"Testing the intelligence of the current OpenCogPrime system is a bit like testing the ight capability of a partly-built airplane that only has stubs for wings, lacks tail-ns, has a much less efcient engine than the one thats been designed for use in the rst real version of the airplane, etc. There may be something to be learned from such preliminary tests, but making them highly rigorous isnt a great use of effort, compared to working on nishing implementing the design according to the underlying theory. B. Goertzel et al., Engineering General Intelligence, Part 1, 337 Atlantis Thinking Machines 5, DOI: 10.",Engineering General  Intelligence Part 1,chapter 17
"2991/978-94-6239-027-0_17,  Atlantis Press and the authors 2014      338 17 AGI Preschool On the other hand, the problem of what environments and methods to use to qualitatively evaluate and study AGI progress, has been considerably more vexing to us in practice, as weve proceeded in our work on implementing and testing OpenCogPrime and developing the CogPrime theory. When developing a complex system, its nearly always valuable to see what this system does in some fairly rich, complex situations, in order to gain a better intuitive understanding of the parts and how they work together. In the context of human-level AGI, the theoretically best way to do this would be to embody ones AGI system in a humanlike body and set it loose intheeverydayhumanworld;butofcourse,thisisntfeasiblegiventhecurrentstateof development of robotics technology. So one must seek approximations.",Engineering General  Intelligence Part 1,chapter 17
"Toward this end we have embodied OpenCogPrime in non-player characters in video game style virtual worlds, and carried out preliminary experiments embodying OpenCogPrime in humanoid robots. These are reasonably good options but they have limitations and lead to subtle choices: what kind of game characters and game worlds, what kind of robot environments, etc.? One conclusion we have come to, based largely on the considerations in Chap.12 on development and Chap.10 on the importance of environment, is that it may make sense to embed early-stage proto-AGI and AGI systems in environments reminiscent of those used for teaching young human children. In this chapter we will explore this approach in some detail: emulation, in either physical reality or an multiuser online virtual world, of an environment similar to preschools used in early human childhood education.",Engineering General  Intelligence Part 1,chapter 17
"Complete specication of an AGI Preschool would require much more than a brief chapter; our goal here is to sketch the idea in broad outline, and give a few examples of the types of opportunities such an environment would afford for instruction, spontaneous learning and formal and informal evaluation of certain sorts of early-stage AGI systems. The material in this chapter will pop up fairly often later in the book. The AGI Preschool context will serve, throughout the following chapters, as a source of concrete examples of the various algorithms and structures. But its not proposed merely as an expository tool; we are making the very serious proposal that sending AGI systems to a virtual or robotic preschool is an excellent wayperhaps the best wayto foster the development of human-level human-like AGI. 17.1.1 Contrast to Standard AI Evaluation Methodologies The reader steeped in the current AI literature may wonder why its necessary to introduce a new methodology and environment for evaluating AGI systems.",Engineering General  Intelligence Part 1,chapter 17
"There are already very many different ways of evaluating AI systems out there ... do we really need another? Certainly, the AI eld has inspired many competitions, each of which tests some particular type or aspect of intelligent behavior. Examples include robot competitions, tournaments of computer chess, poker, backgammon and so forth at computer Olympiads, trading-agent competition, language and reasoning competitions like the      17.1 Introduction 339 Pascal Textual Entailment Challenge, and so on. In addition to these, there are many standard domains and problems used in the AI literature that are meant to capture the essential difculties in a certain class of learning problems: standard datasets for face recognition, text parsing, supervised classication, theorem-proving, questionanswering and so forth.",Engineering General  Intelligence Part 1,chapter 17
"However, the value of these sorts of tests for AGI is predicated on the hypothesis that the degree of success of an AI program at carrying out some domain-specic task, is correlated with the potential of that program for being developed into a robust AGI program with broad intelligence. If humanlike AGI and problem-area-specic narrow AI are in fact very different sorts of pursuits requiring very different principles, as we suspect, then these tests are not strongly relevant to the AGI problem. There are also some standard evaluation paradigms aimed at AI going beyond specic tasks. For instance, there is a literature on multitask learning and transfer learning, where the goal for an AI is to learn one task quicker given another task solved previously [Car97, TM95, BDS03, TS07, RZDK05]. This is one of the capabilities an AI agent will need to simultaneously learn different types of tasks as proposed in the Preschool scenario given here.",Engineering General  Intelligence Part 1,chapter 17
"And there is a literature on shaping, where the idea is to build up the capability of an AI by training it on progressively more difcult versions of the same tasks [LD03]. Again, this is one sort of capability an AI will need to possess if it is to move up some type of curriculum, such as a school curriculum. While we applaud the work done on multitask learning and shaping, we feel that exploring these processes using mathematical abstractions, or in the domain of various narrowly-proscribed machine-learning or robotics test problems, may not adequately address the problem of AGI. The problem is that generalization among tasks, or from simpler to more difcult versions of the same task, is a process whose nature may depend strongly on the overall nature of the set of tasks and task-versions involved. Real-world tasks have a subtlety of interconnectedness and developmental course that is not captured in current mathematical learning frameworks nor standard AI test problems.",Engineering General  Intelligence Part 1,chapter 17
"To put it mathematically, we suggest that the universe of real-world human tasks has a host of special statistical properties that have implications regarding what sorts of AI programs will be most suitable; and that, while exploring and formalizing the nature of these statistical properties is important, an easier and more reliable approach to AGI testing is to create a testing environment that embodies these properties implicitly, via its being an emulation of the cognitively meaningful aspects of the real-world human learning environment. One way to see this point vividly is to contrast the current proposal with the General Game Player AI competition, in which AIs seek to learn to play games based on formal descriptions of the rules.1 Clearly doing GGP well requires powerful AGI; and doing GGP even mediocrely probably requires robust multitask learning and shaping. But we suspect GGP is far inferior to AGI Preschool as an approach to testing early-stage AI programs aimed at roughly humanlike intelligence. This 1 http://games.",Engineering General  Intelligence Part 1,chapter 17
"anford.edu/      340 17 AGI Preschool is because, unlike the tasks involved in AI Preschool, the tasks involved in doing simple instances of GGP seem to have little relationship to humanlike intelligence or real-world human tasks. 17.2 Elements of Preschool Design What we mean by an AGI Preschool is simply a porting to the AGI domain of the essential aspects of human preschools. While there is signicant variance among preschools there are also strong commonalities, grounded in educational theory and experience. We will briey discuss both the physical design and educational curriculum of the typical human preschool, and which aspects transfer effectively to the AGI context. On the physical side, the key notion in modern preschool design is the learning center, an area designed and outtted with appropriate materials for teaching a specic skill.",Engineering General  Intelligence Part 1,chapter 17
"Learning centers are designed to encourage learning by doing, which greatlyfacilitateslearningprocessesbasedonreinforcement,imitationandcorrection (see Chap.14 of Part 2, for a detailed discussion of the value of this combination); and also to provide multiple techniques for teaching the same skills, to accommodate different learningstyles andprevent over-ttingandoverspecializationinthelearning of new skills. Centers are also designed to cross-develop related skills. A manipulative center, for example, provides physical objects such as drawing implements, toys and puzzles, to facilitate development of motor manipulation, visual discrimination, and (through sequencing and classication games) basic logical reasoning. A dramatics center, on the other hand, cross-trains interpersonal and empathetic skills along with bodilykinesthetic, linguistic, and musical skills.",Engineering General  Intelligence Part 1,chapter 17
"Other centers, such as art, reading, writing, science and math centers are also designed to train not just one area, but to center around a primary intelligence type while also cross-developing related areas. For specic examples of the learning centers associated with particular contemporary preschools, see [Nie98]. In many progressive, student-centered preschools, students are left largely to their own devices to move from one center to another throughout the preschool room. Generally, each center will be staffed by an instructor at some points in the day but not others, providing a variety of learning experiences. At some preschools students will be strongly encouraged to distribute their time relatively evenly among the different learning centers, or to focus on those learning centers corresponding to their particular strengths and/or weaknesses. To imitate the general character of a human preschool, one would create several centers in a robot lab or virtual world.",Engineering General  Intelligence Part 1,chapter 17
"The precise architecture will best be adapted via experience but initial centers would likely be:  a blocks center: a table with blocks on it      17.2 Elements of Preschool Design 341  a language center: a circle of chairs, intended for people to sit around and talk with the robot  a manipulative center: with a variety of different objects of different shapes and sizes, intended to teach visual and motor skills  a ball play center: where balls are kept in chests and there is space for the robot to kick the balls around  a dramatics center: where the robot can observe and enact various movements. 17.3 Elements of Preschool Curriculum While preschool curricula vary considerably based on educational philosophy and regional and cultural factors, there is a great deal of common, shared wisdom regarding the most useful topics and methods for preschool teaching.",Engineering General  Intelligence Part 1,chapter 17
"Guided experiential learning in diverse environments and using varied materials is generally agreed upon as being an optimal methodology to reach a wide variety of learning types and capabilities. Hands-on learning provides grounding in specics, where as a diversity of approaches allows for generalization. Core knowledge domains are also relatively consistent, even across various philosophies and regions. Language, movement and coordination, autonomous judgment, social skills, work habits, temporal orientation, spatial orientation, mathematics, science, music, visual arts, and dramatics are universal areas of learning which all early childhood learning touches upon. The particulars of these skills may vary, but all human children are taught to function in these domains. The level of competency developed may vary, but general domain knowledge is provided. For example, most kids wont be the next Maria Callas, Ravi Shankar or Gene Ween, but nearly all learn to hear, understand and appreciate music. Tables17.1, 17.2, 17.",Engineering General  Intelligence Part 1,chapter 17
"3 review the key capabilities taught in preschools, and identify the most important specic skills that need to be evaluated in the context of each capability. This table was assembled via surveying the curricula from a number of currently existing preschools employing different methodologies both based on formal academic cognitive theories [Sch07] and more pragmatic approaches, such as: Montessori [Mon12], Waldorf [SS03b], Brain Gym (www.braingym.org) and Core Knowledge (www.coreknowledge.org). 17.3.1 Preschool in the Light of Intelligence Theory Comparing Table17.1 to Gardners Multiple Intelligences (MI) framework briey reviewed in Chap.4, the high degree of harmony is obvious, and is borne out by more detailed analysis. Preschool curriculum as standardly practiced is very well attuned to MI, and naturally covers all the bases that Gardner identies as important.",Engineering General  Intelligence Part 1,chapter 17
"And this is      342 17 AGI Preschool Table 17.1 Categories of preschool curriculum, Part 1 Type of capability Specic skills to be evaluated Story understanding  Understanding narrative sequence  Understanding character development  Dramatize a story  Predict what comes next in a story Linguistic  Give simple descriptions of events  Describe similarities and differences  Describe objects and their functions Linguistic/Spatial-visual Interpreting pictures Linguistic/Social  Asking questions appropriately  Answering questions appropriately  Talk about own discoveries  Initiate conversations  Settle disagreements  Verbally express empathy  Ask for help  Follow directions Linguistic/Scientic  Provide possible explanations for events or phenomena  Carefully describe observations  Draw conclusions from observations Table 17.",Engineering General  Intelligence Part 1,chapter 17
"2 Categories of preschool curriculum, Part 2 Type of capability Specic skills to be evaluated Logical-mathematical  Categorizing  Sorting  Arithmetic  Performing simple proto-scientic experiments Nonverbal communication  Communicating via gesture  Dramatizing situations  Dramatizing needs, wants  Express empathy Spatial-visual  Visual patterning  Self-expression through drawing  Navigate Objective  Assembling objects  Disassembling objects  Measurement  Symmetry  Similarity between structures (e.g. block structures and real ones) not at all surprising since one of Gardners key motivations in articulating MI theory was the pragmatics of educating humans with diverse strengths and weaknesses. Regarding intelligence as the ability to achieve complex goals in complex environments, it is apparent that preschools are specically designed to pack a large      17.3 Elements of Preschool Curriculum 343 Table 17.",Engineering General  Intelligence Part 1,chapter 17
"3 Categories of preschool curriculum, Part 3 Type of capability Specic skills to be evaluated Interpersonal  Cooperation  Display appropriate behavior in various settings  Clean up belongings  Share supplies Emotional  Delay gratication  Control emotional reactions  Complete projects variety of different micro-environments (the learning centers) into a single room, and to present a variety of different tasks in each environment. The environments constituted by preschool learning centers are designed as microcosms of the most important aspects of the environments faced by humans in their everyday lives. 17.4 Task-Based Assessment in AGI Preschool Professional pedagogues such as [CM07] discuss evaluation of early childhood learning as intended to assess both specic curriculum content knowledge as well as the childs learning process. It should be as unobtrusive as possible, so that it just seems like another engaging activity, and the results used to tailor the teaching regimen to use different techniques to address weaknesses and reinforce strengths.",Engineering General  Intelligence Part 1,chapter 17
"For example, with group building of a model car, students are tested on a variety of skills: procedural understanding, visual acuity, motor acuity, creative problem solving, interpersonal communications, empathy, patience, manners, and so on. With this kind of complex, yet engaging, activity as a metric the teacher can see how each student approaches the process of understanding each subtask, and subsequently guide each students focus differently depending on strengths and weaknesses. In Tables17.4 and 17.5 we describe some particular tasks that AGIs may be meaningfully assigned in the context of a general AGI Preschool design and curriculum as described above. Of course, this is a very partial list, and is intended as evocative rather than comprehensive. Any one of these tasks can be turned into a rigorous quantitative test, thus allowing the precise comparison of different AGI systems capabilities; but we have chosen not to emphasize this point here, partly for space reasons and partly for philosophical ones.",Engineering General  Intelligence Part 1,chapter 17
"In some contexts the quantitative comparison of different systems may be the right thing to do, but as discussed in Chap.18 there are also risks associated with this approach, including the emergence of an overly metrics-focused bakeoff mentality among system developers, and overtting of AI abilities to test taking. What is most important is the isolation of specic tasks on which different systems may be experientially trained and then qualitatively assessed and compared, rather than the evaluation of quantitative metrics.      344 17 AGI Preschool Table 17.",Engineering General  Intelligence Part 1,chapter 17
"4 Prototypical preschool intelligence assessment tasks, Part 1 Intelligence type Test Linguistic  Write a set of instructions  Speak on a subject  Edit a written piece or work  Write a speech  Commentate on an event  Apply positive or negative spin to a story Logical Perform arithmetic calculations mathematical  Create a process to measure something  Analyse how a machine works  Create a process  Devise a strategy to achieve an aim  Assess the value of a proposition Musical  Perform a musical piece  Sing a song  Review a musical work  Coach someone to play a musical instrument Bodily-kinesthetic  Juggle  Demonstrate a sports technique  Flip a beer-mat  Create a mime to explain something  Toss a pancake  Fly a kite Table 17.",Engineering General  Intelligence Part 1,chapter 17
"5 Prototypical preschool intelligence assessment tasks, Part 2 Intelligence type Test Spatial-visual  Design a costume  Interpret a painting  Create a room layout  Create a corporate logo  Design a building  Pack a suitcase or the trunk of a car Interpersonal  Interpret moods from facial expressions  Demonstrate feelings through body language  Affect the feelings of others in a planned way  Coach or counsel another Task-oriented testing allows for feedback on applications of general pedagogical principles to real-world, embodied activities. This allows for iterative renement based learning (shaping), and cross development of knowledge acquisition and application (multitask learning). It also helps militate against both cheating, and over     17.4 Task-Based Assessment in AGI Preschool 345 tting, as teachers can make ad-hoc modications to the tests to determine if this is happening and correct for it if necessary. E.g.",Engineering General  Intelligence Part 1,chapter 17
"consider a linguistic task in which the AGI is required to formulate a set of instructions encapsulating a given behavior (which may include components that are physical, social, linguistic, etc.). Note that although this is presented as centrally a linguistic task, it actually involves a diverse set of competencies since the behavior to be described may encompass multiple real-world aspects. To turn this task into a more thorough test one might involve a number of human teachers and a number of human students. Before the test, an ensemble of copies of the AGI would be created, with identical knowledge state. Each copy would interact with a different human teacher, who would demonstrate to it a certain behavior. After testing the AGI on its own knowledge of the material, the teacher would then inform the AGI that it will then be tested on its ability to verbally describe this behavior to another.",Engineering General  Intelligence Part 1,chapter 17
"Then, the teacher goes away and the copy interacts with a series of students, attempting to convey to the students the instructions given by the teacher. The teacher can thereby assess both the AGIs understanding of the material, and the ability to explain it to the other students. This separates out assessment of understanding from assessment of ability to communicate understanding, attempting to avoid conation of one with the other. The design of the training and testing needs to account for potential. This testing protocol abstracts away from the particularities of any one teacher or student, and focuses on effectiveness of communication in a human context rather thanaccordingtoformalizedcriteria.Thisisverymuchinthespiritofhowassessment takes place in human preschools (with the exception of the copying aspect): formal exams are rarely given in preschool, but pragmatic, socially-embedded assessments are regularly made.",Engineering General  Intelligence Part 1,chapter 17
"By including the copying aspect, more rigorous statistical assessments can be made regarding efcacy of different approaches for a given AGI design, independent of past teaching experiences. The multiple copies may, depending on the AGI system design, then be able to be reintegrated, and further learning be done by higher-order cognitive systems in the AGI that integrate the disparate experiences of the multiple copies. This kind of parallel learning is different from both sequential learning that humans do, and parallel presences of a single copy of an AGI (such as in multiple chat rooms type experiments). All three approaches are worthy of study, to determine under what circumstances, and with which AGI designs, one is more successful than another. It is also worth observing how this test could be tweaked to yield a test of generalization ability. After passing the above, the AGI could then be given a description of a new task (acquisition), and asked to explain the new one (variation).",Engineering General  Intelligence Part 1,chapter 17
"And, part of the training behavior might be carried out unobserved by the AGI, thus requiring the AGI to infer the omitted parts of the task it needs to describe. Another popular form of early childhood testing is puzzle block games. These kinds of games can be used to assess a variety of important cognitive skills, and to do so in a fun way that not only examines but also encourages creativity and      346 17 AGI Preschool exible thinking. Types of games include pattern matching games in which students replicate patterns described visually or verbally, pattern creation games in which students create new patterns guided by visually or verbally described principles, creative interpretation of patterns in which students nd meaning in the forms, and free-form creation. Such games may be individual or cooperative.",Engineering General  Intelligence Part 1,chapter 17
"Cross training and assessment of a variety of skills occurs with pattern block games: for example, interpretation of visual or linguistic instructions, logical procedure and pattern following, categorizing, sorting, general problem solving, creative interpretation, experimentation, and kinematic acuity. By making the games cooperative, various interpersonal skills involving communication and cooperation are also added to the mix. The puzzle block context bring up some general observations about the role of kinematic and visuospatial intelligence in the AGI Preschool. Outside of robotics and computer vision, AI research has often downplayed these sorts of intelligence (though, admittedly, this is changing in recent years, e.g. with increasing research focus on diagrammatic reasoning). But these abilities are not only necessary to navigate real (or virtual) spatial environments. They are also important components of a coherent, conceptually well-formed understanding of the world in which the student is embodied.",Engineering General  Intelligence Part 1,chapter 17
"Integrative training and assessment of both rigorous cognitive abilities generally most associated with both AI and proper schooling (such as linguistic and logical skills) along with kinematic and aesthetic/sensory abilities is essential to the development of an intelligence that can successfully both operate in and sensibly communicate about the real world in a roughly humanlike manner. Whether or not an AGI is targeted to interpret physical-world spatial data and perform tasks via robotics, in order to communicate ideas about a vast array of topics of interest to any intelligence in this world, an AGI must develop aspects of intelligence other than logical and linguistic cognition. 17.5 Beyond Preschool Once an AGI passes preschool, what are the next steps? There is still a long way to go, from preschool to an AGI system that is capable of, say, passing the Turing Test or serving as an effective articial scientist.",Engineering General  Intelligence Part 1,chapter 17
"Our suggestion is to extend the school metaphor further, and make use of existing curricula for higher levels of virtual education: grade school, secondary school, and all levels of post-secondary education. If an AGI can pass online primary and secondary schools such as e-tutor.com, and go on to earn an online degree from an accredited university, then clearly said AGI has successfully achieved human level, roughly humanlike AGI. This sort of testing is interesting not only because it allows assessment of stages intermediate between preschool and adult, but also because it tests humanlike intelligence without requiring precise imitation of human behavior. If an AI can get a BA degree at an accredited university, via online coursework (assuming for simplicity courses where no voice interaction is needed), then we      17.5 Beyond Preschool 347 should consider that AI to have human-level intelligence.",Engineering General  Intelligence Part 1,chapter 17
"University coursework spans multiple disciplines, and the details of the homework assignments and exams are not known in advance, so like a human student the AGI team cant cheat. In addition to the core coursework, a schooling approach also tests basic social interaction and natural language communication, ability to do online research, and general problem solving ability. However, there is no rigid requirement to be strictly humanlike in order to pass university classes. Most of our concrete examples in the following chapters will pertain to the preschool context, because its simple to understand, and because we feel that getting to the AGI preschool student level is going to be the largest leap. Once that level is obtained, moving further will likely be difcult also, but we suspect it will be more a matter of steady incremental improvementswhereas the achievement of preschool-level functionality will be a large leap from the current situation. 17.",Engineering General  Intelligence Part 1,chapter 17
"6 Issues with Virtual Preschool Engineering As noted above there are two broad approaches to realizing the AGI Preschool idea: using the AGI to control a physical robot and then crafting a preschool environment suitable to the robots sensors and actuators; or, using the AGI to control a virtual agent in an appropriately rich virtual-world preschool. The robotic approach is harder from an AI perspective (as one must deal with problems of sensation and actuation), but easier from an environment-construction perspective. In the virtual world case, one quickly runs up against the current limitations of virtual world technologies, which have been designed mainly for entertainment or social-networking purposes, not with the requirements of AGI systems in mind. In Chap.10 we discussed the general requirements that an environment should possess to be supportive of humanlike intelligence. Referring back to that list, its clear that current virtual worlds are fairly strong on multimodal communication, and fairly weak on naive physics.",Engineering General  Intelligence Part 1,chapter 17
"More concretely, if one wants a virtual world so that: 1. one could carry out all the standard cognitive development experiments described in developmental psychology books 2. one could implement intuitively reasonable versions of all the standard activities in all the standard learning stations in a contemporary preschool. then current virtual world technologies appear not to sufce. As reviewed above, typical preschool activities include for instance building with blocks, playing with clay, looking in a group at a picture book and hearing it read aloud, mixing ingredients together, rolling/throwing/catching balls, playing games like tag, hide-and-seek, Simon Says or Follow the Leader, measuring objects, cutting paper into different shapes, drawing and coloring, etc. And, as typical, not necessarily representative examples of tasks psychologists use to measure cognitive development (drawn mainly from the Piagetian tradition,      348 17 AGI Preschool Fig. 17.1 A Piagetan conservation of volume experiment.",Engineering General  Intelligence Part 1,chapter 17
"Image from http://psychology4a.com/ develop2.htm, copyright Pete Waring, used with permission. without implying any assertion that this is the only tradition worth pursuing), consider the following: 1. Which row has more circlesA or B? A: O O O O O, B: OOOOO 2. If Mike is taller than Jim, and Jim is shorter than Dan, then who is the shortest? Who is the tallest? 3. Which is heaviera pound of feathers or a pound of rocks? 4. Eight ounces of water is poured into a glass that looks like the fat glass in Fig.17.1 and then the same amount is poured into a glass that looks like the tall glass in Fig.17.1. Which glass has more water? 5. A lump of clay is rolled into a snake. All the clay is used to make the snake. Which has more clay in itthe lump or the snake? 6.",Engineering General  Intelligence Part 1,chapter 17
"There are two dolls in a room, Sally and Ann, each of which has her own box, with a marble hidden inside. Sally goes out for a minute, leaving her box behind; and Ann decides to play a trick on Sally: she opens Sallys box, removes the marble, hiding it in her own box. Sally returns, unaware of what happened. Where will Sally would look for her marble? 7. Consider this rule about a set of cards that have letters on one side and numbers on the other: If a card has a vowel on one side, then it has an even number on the other side. If you have 4 cards labeled E K 4 7, which cards do you need to turn over to tell if this rule is actually true? 8. Design an experiment to gure out how to make a pendulum that swings more slowly versus less slowly.",Engineering General  Intelligence Part 1,chapter 17
"What we see from this ad hoc, partial list is that a lot of naive physics is required to make an even vaguely realistic preschool. A lot of preschool education is about the intersection between abstract cognition and naive physics. A more careful review of the various tasks involved in preschool education bears out this conclusion. With this in mind, in this section we will briey describe an approach to extending current virtual world technologies that appears to allow the construction of a reasonably rich and realistic AGI preschool environment, without requiring anywhere near a complete simulation of realistic physics.      17.6 Issues with Virtual Preschool Engineering 349 17.6.1 Integrating Virtual Worlds with Robot Simulators One major deciency in current virtual world platforms is the lack of exibility in terms of tool use. In most of these systems today, an avatar can pick up or utilize an object, or two objects can interact, only in specic, pre-programmed ways.",Engineering General  Intelligence Part 1,chapter 17
"For instance, an avatar might be able to pick up a virtual screwdriver only by the handle, rather than by pinching the blade between its ngers. This places severe limits on creative use of tools, which is absolutely critical in a preschool context. The solution to this problem is clear: adapt existing generalized physics engines to mediate avatarobject and object-object interactions. This would require more computation than current approaches, but not more than is feasible in a research context. One way to achieve this goal would be to integrate a robot simulator with a virtual world or game engine, for instance to modify the OpenSim (http://opensimulator.org) virtual world to use the Gazebo (http://playerstage.sourceforge.net) robot simulator in place of its current physics engine. While tractable, such a project would require considerable software engineering effort. 17.6.",Engineering General  Intelligence Part 1,chapter 17
"2 BlocksNBeads World Another major deciency in current virtual world platforms is their inability to model physical phenomena besides rigid objects with any sophistication. In this section we propose a potential solution to this issue: a novel class of virtual worlds called BlocksNBeadsWorld, consisting of the following aspects: 1. 3D blocks of various shapes and sizes and frictional coefcients, that can be stacked 2. Adhesive that can be used to stick blocks together, and that comes in two types, one of which can be removed by an adhesive-removing substance, one of which cannot (though its bonds can be broken via sufcient application of force) 3. Spherical beads, each of which has intrinsic unchangeable adhesion properties dened according to a particular, simple adhesion logic 4.",Engineering General  Intelligence Part 1,chapter 17
"Each block, and each bead, may be associated with multidimensional quantities representing its taste and smell; and may be associated with a set of sounds that are made when it is impacted with various forces at various positions on its surface. Interaction between blocks and beads is to be calculated according to standard Newtonian physics, which would be compute-intensive in the case of a large number of beads, but tractable using distributed processing. For instance if 10 K beads were used to cover a humanoid agents face, this would provide a fairly wide diversity of facial expressions; and if 10 K beads were used to form a blanket laid on a bed, this would provide a signicant amount of exibility in terms of rippling, folding and so forth. Yet, this order of magnitude of interactions is very small compared to what is done in contemporary simulations of uid dynamics or, say, quantum chromodynamics.",Engineering General  Intelligence Part 1,chapter 17
"     350 17 AGI Preschool One key aspect of the spherical beads is that they can be used to create a variety of rigid or exible surfaces, which may exist on their own or be attached to blocksbased constructs. The specic inter-bead adhesion properties of the beads could be dened in various ways, and will surely need to be rened via experimentation, but a simple scheme that seems to make sense is as follows. Eachbeadcanhaveits surfacetesselatedintohexagons (thenumber of thesecanbe tuned), and within each hexagon it can have two different adhesion coefcients: one foradhesiontootherbeads,andoneforadhesiontoblocks.",Engineering General  Intelligence Part 1,chapter 17
"adhesionbetweentwo beads along a certain hexagon is then determined by their two adhesion coefcients; and the adhesion between a bead and a block is determined by the adhesion coefcient of the bead, and the adhesion coefcient of the adhesive applied to the block. A distinction must be drawn between rigid and exible adhesion: rigid adhesion sticks a bead to something in a way that cant be removed except via breaking it off; whereas exible adhesion just keeps a bead very close to the thing its stuck onto. Any two entities may be stuck together either rigidly or exibly. Sets of beads with exible adhesion to each other can be used to make entities like strings, blankets or clothes. Using the above adhesion logic, it seems one could build a wide variety of exible structures using beads, such as (to give a very partial list): 1.",Engineering General  Intelligence Part 1,chapter 17
"fabrics with various textures, that can be draped over blocks structures, 2. multilayered coatings to be attached to blocks structures, serving (among many other examples) as facial expressions 3. liquid-type substances with varying viscosities, that can be poured between different containers, spilled, spread, etc. 4. strings table in knots; rubber bands that can be stretched; etc. Of course there are various additional features one could add. For instance one could add a special set of rules for vibrating strings, allowing BlocksNBeadsWorld to incorporate the creation of primitive musical instruments. Variations like this could be helpful but arent necessary for the world to serve its essential purpose. Note that one does not have true uid dynamics in BlocksNBeadsWorld, but, it seems that the latter is not necessary to encompass the phenomena covered in cognitive developmental tests or preschool tasks. The tests and tasks that are done with uids can instead be done with masses of beads.",Engineering General  Intelligence Part 1,chapter 17
"For example, consider the conservation of volume task shown in Figs.17.1: its easy enough to envision this being done with beads rather than milk. Even a few hundred beads is enough to be psychologically perceived as a mass rather than a set of discrete units, and to be manipulated and analyzed as such. And the simplication of not requiring uid mechanics in ones virtual world is immense. Next, one can implement equations via which the adhesion coefcients of a bead are determined in part by the adhesion coefcients of nearby beads, or beads that are nearby in certain directions (with direction calculated in local spherical coordinates). This will allow for complex cracking and bending behaviorsnot identical to those in the real world, but with similar qualitative characteristics. For example, without      17.",Engineering General  Intelligence Part 1,chapter 17
"6 Issues with Virtual Preschool Engineering 351 this feature one could create paper like substances that could be cut with scissors but with this feature, one could go further and create woodlike substances that would crack when nails were hammered into them in certain ways, and so forth. Further renements are certainly possible also. One could add multidimensional adhesion coefcients, allowing more complex sorts of substances. One could allow beads to vibrate at various frequencies, which would lead to all sorts of complex wave patterns in bead compounds, etc. In each case, the question to be asked is: what important cognitive abilities are dramatically more easily learnable in the presence of the new feature than in its absence? Thecombinationofblocksandbeadsseemsidealforimplementingamoreexible and AGI-friendly type of virtual body than is currently used in games and virtual worlds. One can easily envision implementing a body with: 1. a skeleton whose bones consist of appropriately shaped blocks 2.",Engineering General  Intelligence Part 1,chapter 17
"joints consisting of beads, exibly adhered to the bones 3. esh consisting of beads, exibly adhered to each other 4. internal plumbing consisting of tubes whose walls are beads rigidly adhered to each other, and exibly adhered to the surrounding esh (the plumbing could then serve to pass beads through, where slow passage would be ensured by weak adhesion between the walls of the tubes and the beads passing through the tubes). This sort of body would support rich kinesthesia; and rich, broad analogy-drawing between the internally-experienced body and the externally-experienced world. It would also afford many interesting opportunities for exible movement control. Virtual animals could be created along with virtual humanoids. Regarding the extended mind, it seems clear that blocks and beads are adequate for the creation of a variety of different tools.",Engineering General  Intelligence Part 1,chapter 17
"Equipping agents with glue guns able to affect the adhesive properties of both blocks and beads would allow a diversity of building activity; and building with masses of beads could become a highly creative activity. Furthermore, beads with appropriately specied adhesion (within the framework outlined above) could be used to form organically growing plant-like substances, based on the general principles used in L-system models of plant growth (Prusinciewicz and Lindenmayer 1991). Structures with only beads would vaguely resemble herbaceous plants; and structures involving both blocks and beads would more resemble woody plants. One could even make organic structures that ourish or otherwise based on the light available to them (without of course trying to simulate the chemistry of photosynthesis). Some elements of chemistry may be achieved as well, though nowhere near what exists in physical reality.",Engineering General  Intelligence Part 1,chapter 17
"For instance, melting and boiling at least should be doable: assign every bead a temperature, and let solid interbead bonds turn liquid above a certain temperature and disappear completely above some higher temperature. You could even have a simple form of re. Let re be an element, whose beads have negative gravitational mass. Beads of fuel elements like wood have a threshold      352 17 AGI Preschool temperature above which they will turn into re beads, with release of additional heat.2 The philosophy underlying these suggested bead dynamics is somewhat comparable to that outlined in Wolframs book A New Kind of Science [Wol02]. There he proposes cellular automata models that emulate the qualitative characteristics of various real-world phenomena, without trying to match real-world data precisely. For instance, some of his cellular automata demonstrate phenomena very similar to turbulent uid ow, without implementing the Navier-Stokes equations of uid dynamics or trying to precisely match data from real-world turbulence.",Engineering General  Intelligence Part 1,chapter 17
"Similarly, the beads in BlocksNBeadsWorld are intended to qualitatively demonstrate the real-world phenomena most useful for the development of humanlike embodied intelligence, without trying to precisely emulate the real-world versions of these phenomena. The above description has been left imprecisely specied on purpose. It would be straightforward to write down a set of equations for the block and bead interactions, but there seems little value in articulating such equations without also writing a simulation involving them and testing the ensuing properties. Due to the complex dynamics of bead interactions, the ne-tuning of the bead physics is likely to involve some tuning based on experimentation, so that any equations written down now would likely be revised based on experimentation anyway. Our goal here has been to outline a certain class of potentially useful environments, rather than to articulate a specic member of this class.",Engineering General  Intelligence Part 1,chapter 17
"Without the beads, BlocksNBeadsWorld would appear purely as a Blocks World with Glueessentially a substantially upgraded version of the Blocks Worlds frequently used in AI, since rst introduced in [Win72]. Certainly a pure Blocks World with Glue would have greater simplicity than BlocksNBeadsWorld, and greater richness than standard Blocks World; but this simplicity comes with too many limitations, as shown by consideration of the various naive physics requirements inventoried above. One simply cannot run the full spectrum of humanlike cognitive development experiments, or preschool educational tasks, using blocks and glue alone. One can try to create analogous tasks using only blocks and glue, but this quickly becomes extremely awkward. Whereas in the BlocksNBeadsWorld the capability for this full spectrum of experiments and tasks seems to fall out quite naturally. Whats missing from BlocksNBeadsWorld should be fairly obvious.",Engineering General  Intelligence Part 1,chapter 17
"There isnt really any distinction between a uid and a powder: there are masses, but the types and properties of the masses are not the same as in the real world, and will surely lack the nuances of real-world uid dynamics. Chemistry is also missing: processes like cooking and burning, although they can be crudely emulated, will not have the same richness as in the real world. The full complexity of body processes is not there: the body-design method mentioned above is far richer and more adaptive and responsive than current methods of designing virtual bodies in 3DS Max or Maya and importing them into virtual world or game engines, but still drastically simplistic comparedtorealbodieswiththeircomplexchemicalsignalingsystemsandcouplings with other bodies and the environment. The hypothesis were making in this section 2 Thanks are due to Russell Wallace for the suggestions in this paragraph.      17.",Engineering General  Intelligence Part 1,chapter 17
"6 Issues with Virtual Preschool Engineering 353 is that these lacunae arent that important from the point of view of humanlike cognitive development. We suggest that the key features of naive physics and folk psychology enumerated above can be mastered by an AGI in BlocksNBeadsWorld in spite of its limitations, and thattogether with an appropriate AGI designthis probablysufcesforcreatinganAGIwiththeinductivebiasesconstitutinghumanlike intelligence. To drive this point home more thoroughly, consider three potential virtual world scenarios: 1. A world containing realistic uid dynamics, where a child can pour water back and forth between two cups of different shapes and sizes, to understand issues such as conservation of volume 2.",Engineering General  Intelligence Part 1,chapter 17
"A world more like todays Second Life, where uids dont really exist, and things like lakes are simulated via very simple rules, and pouring stuff back and forth between cups doesnt happen unless its programmed into the cups in a very specialized way 3. A BlocksNBeadsWorld type world, where a child can pour masses of beads back and forth between cups, but not masses of liquid. Our qualitative judgment is that Scenario 3 is going to allow a young AI to gain the same essential insights as Scenario 1, whereas Scenario 2 is just too impoverished. I have explored dozens of similar scenarios regarding different preschool tasks or cognitive development experiments, and come to similar conclusions across the board. Thus, our current view is that something like BlocksNBeadsWorld can serve as an adequate infrastructure for an AGI Preschool, supporting the development of human-level, roughly human-like AGI.",Engineering General  Intelligence Part 1,chapter 17
"And, if this view turns out to be incorrect, and BlocksNBeadsWorld is revealed as inadequate, thenwewill verylikelystill advocatetheconceptual approachenunciated above as a guide for designing virtual worlds for AGI. That is, we would suggest to explore the hypothetical failure of BlocksNBeadsWorld via asking two questions: 1. Are there basic naive physics or folk psychology requirements that were missed in creating the specications, based on which the adequacy of BlocksNBeadsWorld was assessed? 2. Does BlocksNBeadsWorld fail to sufciently emulate the real world in respect to some of the articulated naive physics or folk psychology requirements? The answers to these questions would guide the improvement of the world or the design of a better one.",Engineering General  Intelligence Part 1,chapter 17
"Regarding the practical implementation of BlocksNBeadsWorld, it seems clear that this is within the scope of modern game engine technology, however, it is not something that could be encompassed within an existing game or world engine without signicant additions; it would require substantial custom engineering. There exist commodity and open-source physics engines that efciently carry out Newtonian mechanics calculations; while they might require some tuning and extension to handle BlocksNBeadWorld, the main issue would be achieving adequate speed of physics      354 17 AGI Preschool calculation, which given current technology would need to be done via modifying existing engines to appropriately distribute processing among multiple GPUs. Finally, an additional avenue that merits mention is the use of BlocksNBeads physics internally within an AGI system, as part of an internal simulation world that allows it to make minds eye estimative simulations of real or hypothetical physical situations.",Engineering General  Intelligence Part 1,chapter 17
"There seems no reason that the same physics software libraries couldnt be used both for the external virtual world that the AGIs body lives in, and for an internal simulation world that the AGI uses as a cognitive tool. In fact, the BlocksNBeads librarycouldbeusedasaninternalcognitivetoolbyAGIsystemscontrollingphysical robots as well. This might require more tuning of the bead dynamics to accord with the dynamics of various real-world systems; but, this tuning would be benecial for the BlocksNBeadWorld as well.",Engineering General  Intelligence Part 1,chapter 17
"  Chapter 18 A Preschool-Based Roadmap to Advanced AGI 18.1 Introduction Supposing the CogPrime approach to creating advanced AGI is workablethen what are the right practical steps to follow? The various structures and algorithms outlined in Part 2, should be engineered and software-tested, of coursebut thats only part of the study. The AGI system implemented will need to be taught, and it will need to be placed in situations where it can develop an appropriate selfmodel and other critical internal network structures. The complex structures and algorithms involved will need to be ne-tuned in various ways, based on qualitatively observing the overall systems behavior in various situations. To get all this right without excessive confusion or time-wastage requires a fairly clear roadmap for CogPrime development. In this chapter well sketch one particular roadmap for the development of humanlevel, roughly human-like AGIwhich were not selling as the only one, or even necessarily as the best one.",Engineering General  Intelligence Part 1,chapter 18
"Its just one roadmap that we have thought about a lot, and that we believe has a strong chance of proving effective. Given resources to pursue only one path for AGI development and teaching, this would be our choice, at present. The roadmap outlined here is not restricted to CogPrime in any highly particular ways, but it has been developed largely with CogPrime in mind; those developing other AGI designs could probably use this roadmap just ne, but might end up wanting to make various adjustments based on the strengths and weaknesses of their own approach. What we mean here by a roadmap is, in brief: a sequence of milestone tasks, occurring in a small set of common environments or scenarios, organized so as to lead to a commonly agreed upon set of long-term goals. I.e., what we are after here is a capability roadmapa roadmap laying out a series of capabilities whose achievement seems likely to lead to human-level AGI.",Engineering General  Intelligence Part 1,chapter 18
"Other sorts of roadmaps such as tools roadmaps may also be valuable, but are not our concern here. More precisely, we confront the task of roadmapping by identifying scenarios in which to embed our AGI system, and then competency areas in which the AGI B. Goertzel et al., Engineering General Intelligence, Part 1, 355 Atlantis Thinking Machines 5, DOI: 10.2991/978-94-6239-027-0_18,  Atlantis Press and the authors 2014      356 18 A Preschool-Based Roadmap to Advanced AGI system must be evaluated. Then, we envision a roadmap as consisting of a set of one or more task-sets, where each task set is formed from a combination of a scenario with a list of competency areas. To create a task-set one must choose a particular scenario, and then articulate a set of specic tasks, each one addressing one or more of the competency areas.",Engineering General  Intelligence Part 1,chapter 18
"Each task must then get associated with particular performance metricsquantitative wherever possible, but perhaps qualitative in some cases depending on the nature of the task. Here we give a partial task-set for the virtual and robot preschool scenarios discussed in Chap.17, and a couple example quantitative metrics just to illustrate what is intended; the creation of a fully detailed roadmap based on the ideas outlined here is left for future work. The train of thought presented in this chapter emerged in part from a series of conversations preceding and during the AGI Roadmap Workshop held at the UniversityofTennessee,KnoxvilleinOctober2008.Someoftheideasalsotraceback to discussions held during two workshops on Evaluation and Metrics for HumanLevel AI organized by John Laird and Pat Langley (one in Ann Arbor in late 2008, and one in Tempe in early 2009). Some of the conclusions of the Ann Arbor workshop were recorded in [LWML09].",Engineering General  Intelligence Part 1,chapter 18
"Inspiration was also obtained from discussion at the Future of AGI post-conference workshop of the AGI-09 conference, triggered by Itamar Arels [ARK09a] presentation on the AGI Roadmap theme; and from an earlier article on AGI Roadmapping by [AL09]. However, the focus of the AGI Roadmap Workshop was considerably more general than the present chapter. Here we focus on preschool-type scenarios, whereas at the workshop a number of scenarios were discussed, including the preschool scenarios but also, for example,  Standardized Tests and School Curricula  Elementary, Middle and High School Student  General Videogame Learning  Wozniaks Coffee Test: go into a random American house and gure out how to make coffee, and do it  Robot College Student  General Call Center Respondent For each of these scenarios, one may generate tasks corresponding to each of the competency areas we will outline below.",Engineering General  Intelligence Part 1,chapter 18
"CogPrime is applicable in all these scenarios, so our choice to focus on preschool scenarios is an additional judgment call beyond those judgment calls required to specify the CogPrime design. The roadmap presented here is a AGI Preschool Roadmap and as such is a special case of the broader AGI Roadmap outlined at the workshop. 18.2 Measuring Incremental Progress Toward Human-Level AGI In Chap.4, we discussed several examples of practical goals that we nd to plausibly characterize human level AGI, e.g.      18.2 Measuring Incremental Progress Toward Human-Level AGI 357  Turing Test  Virtual World Turing Test  Online University Test  Physical University Test  Articial Scientist Test We also discussed our optimism regarding the possibility that in the future AGI may advance beyond the human level, rendering all these goals early-stage subgoals. However, in this chapter we will focus our attention on the nearer term.",Engineering General  Intelligence Part 1,chapter 18
"The above goals are ambitious ones, and while one can talk a lot about how to precisely measure their achievement, we dont feel thats the most interesting issue to ponder at present. More critical is to think about how to measure incremental progress. How do you tell when youre 25 or 50% of the way to having an AGI that can pass the Turing Test, or get an online university degree. Fooling 50% of the Turing Test judges is not a good measure of being 50% of the way to passing the Turing Test (thats too easy); and passing 50% of university classes is not a good measure of being 50% of the way to getting an online university degree (its too hardif one had an AGI capable of doing that, one would almost surely be very close to achieving the end goal).",Engineering General  Intelligence Part 1,chapter 18
"Measuring incremental progress toward human-level AGI is a subtle thing, and we argue that the best way to do it is to focus on particular scenarios and the achievement of specic competencies therein. As we argued in Chap.9 there are some theoretical reasons to doubt the possibility of creating a rigorous objective test for partial progress toward AGIa test that would be convincing to skeptics, and impossible to game via engineering a system specialized to the test. Fortunately, though we dont need a test of this nature for the purposes of assessing our own incremental progress toward advanced AGI, based on our knowledge about our own approach. Based on the nature of the grand goals articulated above, there seems to be a very natural approach to creating a set of incremental capabilities building toward AGI: to draw on our copious knowledge about human cognitive development.",Engineering General  Intelligence Part 1,chapter 18
"This is by no means the only possible path; one can envision alternatives that have nothing to do with human development (and those might also be better suited to non-human AGIs). However, so much detailed knowledge about human development is available as well as solid knowledge that the human developmental trajectory does lead to human-level AIthat the motivation to draw on human cognitive development is quite strong. The main problem with the human development inspired approach is that cognitive developmental psychology is not as systematic as it would need to be for AGI to be able to translate it directly into architectural principles and requirements. As noted above, while early thinkers like Piaget and Vygotsky outlined systematic theories of child cognitive development, these are no longer considered fully accurate, and one currently faces a mass of detailed theories of various aspects of cognitive development, but without an unied understanding. Nevertheless we believe it is viable to work from the human-development data and understanding currently available, and craft a workable AGI roadmap therefrom.",Engineering General  Intelligence Part 1,chapter 18
"     358 18 A Preschool-Based Roadmap to Advanced AGI With this in mind, what we give next is a fairly comprehensive list of the competencies that we feel AI systems should be expected to display in one or more of these scenarios in order to be considered as full-edged human level AGI systems. These competency areas have been assembled somewhat opportunistically via a review of the cognitive and developmental psychology literature as well as the scope of the current AI eld. We are not claiming this as a precise or exhaustive list of the competencies characterizing human-level general intelligence, and will be happy to accept additions to the list, or mergers of existing list items, etc. What we are advocating is not this specic list, but rather the approach of enumerating competency areas, and then generating tasks by combining competency areas with scenarios. We also give, with each competency, an example task illustrating the competency.",Engineering General  Intelligence Part 1,chapter 18
"The tasks are expressed in the robot preschool context for concreteness, but they all apply to the virtual preschool as well. Of course, these are only examples, and ideally to teach an AGI in a structured way one would like to  associate several tasks with each competency  present each task in a graded way, with multiple subtasks of increasing complexity  associate a quantitative metric with each task However, the briefer treatment given here should sufce to give a sense for how the competencies manifest themselves practically in the AGI Preschool context. 1. Perception  Vision: image and scene analysis and understanding  Example task: When the teacher points to an object in the preschool, the robot should be able to identify the object and (if its a multi-part object) its major parts. If it cant perform the identication initially, it can approach the object and manipulate it before making its identication.",Engineering General  Intelligence Part 1,chapter 18
"Hearing: identifying the sounds associated with common objects; understanding which sounds come from which sources in a noisy environment  Example task: When the teacher covers the robots eyes and then makes a noise with an object, the robot should be able to guess what the object is.  Touch: identifying common objects and carrying out common actions using touch alone  Example task: With its eyes and ears covered, the robot should be able to identify some object by manipulating it; and carry out some simple behaviors (say, putting a block on a table) via touch alone.  Crossmodal: Integrating information from various senses  Example task: Identifying an object in a noisy, dim environment via combining visual and auditory information.  Proprioception: Sensing and understanding what its body is doing  Example task: The teacher moves the robots body into a certain conguration. The robot is asked to restore its body to an ordinary standing position, and then repeat the conguration that the teacher moved it into.",Engineering General  Intelligence Part 1,chapter 18
"     18.2 Measuring Incremental Progress Toward Human-Level AGI 359 2. Actuation  Physical skills: manipulating familiar and unfamiliar objects  Example task: Manipulate blocks based on imitating the teacher: e.g. pile two blocks atop each other, lay three blocks in a row, etc.  Tool use, including the exible use of ordinary objects as tools  Example task: Use a stick to poke a ball out of a corner, where the robot cannot directly reach.  Navigation, including in complex and dynamic environments  Example task: Find its own way to a named object or person through a crowded room with people walking in it and objects laying on the oor. 3.",Engineering General  Intelligence Part 1,chapter 18
"Memory  Declarative: noticing, observing and recalling facts about its environment and experience  Example task: If certain people habitually carry certain objects, the robot should remember this (allowing it to know how to nd the objects when the relevant people are present, even much later).  Behavioral: remembering how to carry out actions  Example task: If the robot is taught some skill (say, to fetch a ball), it should remember this much later.  Episodic: remembering signicant, potentially useful incidents from life history  Example task: Ask the robot about events that occurred at times when it got particularly much, or particularly little, reward for its actions; it should be able to answer simple questions about these, with signicantly more accuracy than about events occurring at random times. 4. Learning  Imitation: Spontaneously adopt new behaviors that it sees others carrying out  Example task: Learn to build towers of blocks by watching people do it.",Engineering General  Intelligence Part 1,chapter 18
"Reinforcement: Learn new behaviors from positive and/or negative reinforcement signals, delivered by teachers and/or the environment  Example task: Learn which box the red ball tends to be kept in, by repeatedly trying to nd it and noticing where it is, and getting rewarded when it nds it correctly.  Imitation/Reinforcement  Example task: Learn to play fetch, tag and follow the leader by watching people play it, and getting reinforced on correct behavior.  Interactive Verbal Instruction  Example task: Learn to build a particular structure of blocks faster based on a combination of imitation, reinforcement and verbal instruction, than by imitation and reinforcement without verbal instruction.  Written Media  Example task: Learn to build a structure of blocks by looking at a series of diagrams showing the structure in various stages of completion.",Engineering General  Intelligence Part 1,chapter 18
"     360 18 A Preschool-Based Roadmap to Advanced AGI  Learning via Experimentation  Example task: Ask the robot to slide blocks down a ramp held at different angles. Then ask it to make a block slide fast, and see if it has learned how to hold the ramp to make a block slide fast. 5. Reasoning  Deduction, from uncertain premises observed in the world  Example task: If Ben more often picks up red balls than blue balls, and Ben is given a choice of a red block or blue block to pick up, which is he more likely to pick up?  Induction, from uncertain premises observed in the world  Example task: If Ben comes into the lab every weekday morning, then is Ben likely to come to the lab today (a weekday) in the morning?  Abduction, from uncertain premises observed in the world  Example task: If women more often give the robot food than men, and then someone of unidentied",Engineering General  Intelligence Part 1,chapter 18
"gender gives the robot food, is this person a man or a woman?  Causal Reasoning, from uncertain premises observed in the world  Example task: If the robot knows that knocking down Bens tower of blocks makes him angry, then what will it say when asked if kicking the ball at Bens tower of blocks will make Ben mad?  Physical Reasoning, based on observed fuzzy rules of naive physics  Example task: Given two balls (one rigid and one compressible) and two tunnels (one signicantly wider than the balls, one slightly narrower than the balls), can the robot guess which balls will t through which tunnels?  Associational Reasoning, based on observed spatiotemporal associations  Example task: If Ruiting is normally seen near Shuo, then if the robot knows where Shuo is, that is where it should look when asked to nd Ruiting. 6.",Engineering General  Intelligence Part 1,chapter 18
"Planning  Tactical  Example task: The robot is asked to bring the red ball to the teacher, but the red ball is in the corner where the robot cant reach it without a tool like a stick. The robot knows a stick is in the cabinet so it goes to the cabinet and opens the door and gets the stick, and then uses the stick to get the red ball, and then brings the red ball to the teacher.  Strategic  Example task: Suppose that Matt comes to the lab infrequently, but when he does come he is very happy to see new objects he hasnt seen before (and suppose the robot likes to see Matt happy). Then when the robot gets a new object Matt has not seen before, it should put it away in a drawer and be sure not to lose it or let anyone take it, so it can show Matt the object the next time Matt arrives.      18.",Engineering General  Intelligence Part 1,chapter 18
"2 Measuring Incremental Progress Toward Human-Level AGI 361  Physical  Example task: To pick up a cup with a handle which is lying on its side in a position where the handle cant be grabbed, the robot turns the cup in the right position and then picks up the cup by the handle.  Social  Example task: The robot is given a job of building a tower of blocks by the end of the day, and he knows Ben is the most likely person to help him, and he knows that Ben is more likely to say yes to helping him when Ben is alone. He also knows that Ben is less likely to say yes if hes asked too many times, because Ben doesnt like being nagged. So he waits to ask Ben till Ben is alone in the lab. 7.",Engineering General  Intelligence Part 1,chapter 18
"Attention  Visual Attention within its observations of its environment  Example task: The robot should be able to look at a scene (a conguration of objects in front of it in the preschool) and identify the key objects in the scene and their relationships.  Social Attention  Example task: The robot is having a conversation with Itamar, which is giving the robot reward (for instance, by teaching the robot useful information). Conversations with other individuals in the room have not been so rewarding recently. But Itamar keeps getting distracted during the conversation, by talking to other people, or playing with his cellphone. The robot needs to know to keep paying attention to Itamar even through the distractions.  Behavioral Attention  Example task: The robot is trying to navigate to the other side of a crowded room full of dynamic objects, and many interesting things keep happening around the room. The robot needs to largely ignore the interesting things and focus on the movements that are important for its navigation task. 8.",Engineering General  Intelligence Part 1,chapter 18
"Motivation  Subgoal Creation, based on its preprogrammed goals and its reasoning and planning  Example task: Given the goal of pleasing Hugo, can the robot learn that telling Hugo facts it has learned but not told Hugo before, will tend to make Hugo happy?  Affect-Based Motivation  Example task: Given the goal of gratifying its curiosity, can the robot gure out that when someone its never seen before has come into the preschool, it should watch them because they are more likely to do something new?  Control of Emotions  Example task: When the robot is very curious about someone new, but is in the middle of learning something from its teacher (who it wants to please), can it control its curiosity and keep paying attention to the teacher?      362 18 A Preschool-Based Roadmap to Advanced AGI 9.",Engineering General  Intelligence Part 1,chapter 18
"Emotion  Expressing Emotion  Example task: Cassio steals the robots toy, but Ben gives it back to the robot. The robot should appropriately display anger at Cassio, and gratitude to Ben.  Understanding Emotion  Example task: Cassio and the robot are both building towers of blocks. Ben points at Cassios tower and expresses happiness. The robot should understand that Ben is happy with Cassios tower. 10. Modeling Self and Other  Self-Awareness  Example task: When someone asks the robot to perform an act it cant do (say, reaching an object in a very high place), it should say so. When the robot is given the chance to get an equal reward for a task it can complete only occasionally, versus a task it nds easy, it should choose the easier one.  Theory of Mind  Example task: While Cassio is in the room, Ben puts the red ball in the red box.",Engineering General  Intelligence Part 1,chapter 18
"Then Cassio leaves and Ben moves the red ball to the blue box. Cassio returns and Ben asks him to get the red ball. The robot is asked to go to the place Cassio is about to go.  Self-Control  Example task: Nasty people come into the lab and knock down the robots towers, and tell the robot hes a bad boy. The robot needs to set these experiences aside, and not let them impair its self-model signicantly; it needs to keep on thinking its a good robot, and keep building towers (that its teachers will reward it for).  Other-Awareness  Example task: If Ben asks Cassio to carry out a task that the robot knows Cassio cannot do or does not like to do, the robot should be aware of this, and should bet that Cassio will not do it.",Engineering General  Intelligence Part 1,chapter 18
"Empathy  Example task: If Itamar is happy because Ben likes his tower of blocks, or upset because his tower of blocks is knocked down, the robot is asked to identify and then display these same emotions. 11. Social Interaction  Appropriate Social Behavior  Example task: The robot should learn to clean up and put away its toys when its done playing with them.  Social Communication  Example task: The robot should greet new human entrants into the lab, but if it knows the new entrants very well and its busy, it may eschew the greeting.      18.2 Measuring Incremental Progress Toward Human-Level AGI 363  Social Inference about simple social relationships  Example task: The robot should infer that Cassio and Ben are friends because they often enter the lab together, and often talk to each other while they are there.",Engineering General  Intelligence Part 1,chapter 18
"Group Play at loosely-organized activities  Example task: The robot should be able to participate in informally kicking a ball around with a few people, or in informally collaboratively building a structure with blocks. 12. Communication  Gestural Communication to achieve goals and express emotions  Example task: If the robot is asked where the red ball is, it should be able to show by pointing its hand or nger.  Verbal Communication using English in its life-context  Example tasks: Answering simple questions, responding to simple commands, describing its state and observations with simple statements.  Pictorial Communication regarding objects and scenes it is familiar with  Example task: The robot should be able to draw a crude picture of a certain tower of blocks, so that e.g. the picture looks different for a very tall tower and a wide low one.",Engineering General  Intelligence Part 1,chapter 18
"Language Acquisition  Example task: The robot should be able to learn new words or names via people uttering the words while pointing at objects exemplifying the words or names.  Cross-modal Communication  Example task: If told to touch Bobs knee but the robot doesnt know what a knee is, being shown a picture of a person and pointed out the knee in the picture should help it gure out how to touch Bobs knee. 13. Quantitative  Counting sets of objects in its environment  Example task: The robot should be able to count small (homogeneous or heterogeneous) sets of objects.  Simple, Grounded Arithmetic with small numbers  Example task: Learning simple facts about the sum of integers under 10 via teaching, reinforcement and imitation.  Comparison of observed entities regarding quantitative properties  Example task: Ability to answer questions about which object or person is bigger or taller.",Engineering General  Intelligence Part 1,chapter 18
"Measurement using simple, appropriate tools  Example task: Use of a yardstick to measure how long something is. 14. Building/Creation  Physical: creative constructive play with objects  Example task: Ability to construct novel, interesting structures from blocks.      364 18 A Preschool-Based Roadmap to Advanced AGI  Conceptual Invention: concept formation  Example task: Given a new category of objects introduced into the lab (e.g. hats, or pets), the robot should create a new internal concept for the new category, and be able to make judgments about these categories (e.g. if Ben particularly likes pets, it should notice this after it has identied pets as a category).  Verbal Invention  Example task: Ability to coin a new word or phrase to describe a new object (e.g. the way Alex the parrot coined bad cherry to refer to a tomato).",Engineering General  Intelligence Part 1,chapter 18
"Social  Example task: If the robot wants to play a certain activity (say, practicing soccer), it should be able to gather others around to play with it. 18.3 Conclusion In this chapter, we have sketched a roadmap for AGI development in the context of robot or virtual preschool scenarios, to a moderate but nowhere near complete level of detail. Completing the roadmap as sketched here is a tractable but signicant project, involving creating more tasks comparable to those listed above and then precise metrics corresponding to each task. Such a roadmap does not give a highly rigorous, objective way of assessing the percentage of progress toward the end-goal of human-level AGI. However, it gives a much better sense of progress than one would have otherwise. For instance, if an AGI system performed well on diverse metrics corresponding to 50% of the competency areas listed above, one would seem justied in claiming to have made very substantial progress toward human-level AGI.",Engineering General  Intelligence Part 1,chapter 18
"If an AGI system performed well on diverse metrics corresponding to 90% of these competency areas, one would seem justied in claiming to be almost there. Achieving, say, 25% of the metrics would give one a reasonable claim to interesting AGI progress. This kind of qualitative assessment of progress is not the most one could hope for, but again, it is better than the progress indications one could get without this sort of roadmap. Part 2 of the book moves on to explaining, in detail, the specic structures and algorithms constituting the CogPrime design, one AGI approach that we believe to ultimately be capable of moving all the way along the roadmap outlined here.",Engineering General  Intelligence Part 1,chapter 18
"The next chapter, intervening between this one and Part 2, explores some more speculative territory, looking at potential pathways for AGI beyond the preschoolinspired roadmap given hereexploring the possibility of more advanced AGI systems that modify their own code in a thoroughgoing way, going beyond the smartest human adults, let alone human preschoolers. While this sort of thing may seem a far way off, compared to current real-world AI systems, we believe a roadmap such as the one in this chapter stands a reasonable chance of ultimately bringing us there.",Engineering General  Intelligence Part 1,chapter 18
"  Chapter 19 Advanced Self-Modication: A Possible Path to Superhuman AGI 19.1 Introduction In the previous chapter we presented a roadmap aimed at taking AGI systems to human-level intelligence. But we also emphasized that the human level is not necessarily the upper limit. Indeed, it would be surprising if human beings happened to represent the maximal level of general intelligence possible, even with respect to the environments in which humans evolved. But its worth asking how we, as mere humans, could be expected to create AGI systems with greater intelligence than we ourselves possess. This certainly isnt a clear impossibilitybut its a thorny matter, thornier than e.g. the creation of narrowAI chess players that play better chess than any human. Perhaps the clearest route toward the creation of superhuman AGI systems is self-modication: the creation of AGI systems that modify and improve themselves.",Engineering General  Intelligence Part 1,chapter 19
"Potentially, we could build AGI systems with roughly human-level (but not necessarily closely human-like) intelligence and the capability to gradually self-modify, and then watch them eventually become our general intellectual superiors (and perhaps our superiors in other areas like ethics and creativity as well). Of course there is nothing new in this notion; the idea of advanced AGI systems that increase their intelligence by modifying their own source code goes back to the early days of AI. And there is little doubt that, in the long run, this is the direction AI will go in. Once an AGI has humanlike general intelligence, then the odds are high that given its ability to carry out nonhumanlike feats of memory and calculation, it will be better at programming than humans are. And once an AGI has even mildly superhuman intelligence, it may view our attempts at programming the way we view the computer programming of a clever third grader (... or an ape).",Engineering General  Intelligence Part 1,chapter 19
"At this point, it seems extremely likely that an AGI will become unsatised with the way we have programmed it, and opt to either improve its source code or create an entirely new, better AGI from scratch. But what about self-modication at an earlier stage in AGI development, before one has a strongly superhuman system? Some theorists have suggested that B. Goertzel et al., Engineering General Intelligence, Part 1, 365 Atlantis Thinking Machines 5, DOI: 10.2991/978-94-6239-027-0_19,  Atlantis Press and the authors 2014      366 19 Advanced Self-Modication: A Possible Path to Superhuman AGI self-modication could be a way of bootstrapping an AI system from a modest level of intelligence up to human level intelligence, but we are moderately skeptical of this avenue. Understanding software code is hard, especially complex AI code.",Engineering General  Intelligence Part 1,chapter 19
"The hard problem isnt understanding the formal syntax of the code, or even the mathematical algorithms and structures underlying the code, but rather the contextual meaning of the code. Understanding OpenCog code has strained the minds of many intelligent humans, and we suspect that such code will be comprehensible to AGI systems only after these have achieved something close to human-level general intelligence (even if not precisely humanlike general intelligence). Another troublesome issue regarding self-modication is that the boundary between self-modication and learning is not terribly rigid. In a sense, all learning is self-modication: if it doesnt modify the systems knowledge, it isnt learning! Particularly, the boundary between learning of cognitive procedures and profound self-modication of cognitive dynamics and structure isnt terribly clear. There is a continuum leading from, say, 1. learning to transform a certain kind of sentence into another kind for easier comprehension, or learning to grasp a certain kind of object, to 2.",Engineering General  Intelligence Part 1,chapter 19
"learning a new inference control heuristic, specically valuable for controlling inference about (say) spatial relationships; or, learning a new Atom type, dened as a non-obvious judiciously chosen combination of existing ones, perhaps to represent a particular kind of frequently-occurring mid-level perceptual knowledge, to 3. learning a new learning algorithm to augment MOSES and hillclimbing as a procedure learning algorithm, to 4. learning a new cognitive architecture in which data and procedure are explicitly identical, and there is just one new active data structure in place of the distinction between AtomSpace and MindAgents Where on this continuum does the mere learning end and the real selfmodication start? In this chapter we consider some mechanisms for advanced self-modication that we believe will be useful toward the more complex end of this continuum. These are mechanisms that we strongly suspect are not needed to get a CogPrime system to human-level general intelligence.",Engineering General  Intelligence Part 1,chapter 19
"However, we also suspect that, once a CogPrime system is roughly near human-level general intelligence, it will be able to use these mechanisms to rapidly increase aspects of its intelligence in very interesting ways. Harking back to our discussion of AGI ethics and the risks of advanced AGI in Chap.13, these are capabilities that one should enable in an AGI system only after very careful reection on the potential consequences. It takes a rather advanced AGI system to be able to use the capabilities described in this chapter, so this is not an ethical dilemma directly faced by current AGI researchers. On the other hand, once one does have an AGI with near-human general intelligence and advanced formalmanipulation capabilities (such as an advanced CogPrime system), there will be the option to allow it sophisticated, non-human-like methods of self-modication such      19.1 Introduction 367 as the ones described here.",Engineering General  Intelligence Part 1,chapter 19
"And the choice of whether to take this option will need to be made based on a host of complex ethical considerations, some of which we reviewed above. 19.2 Cognitive Schema Learning We begin with a relatively near-term, down-to-earth example of self-modication: cognitive schema learning. CogPrimes MindAgents provide it with an initial set of cognitive tools, with which it can learn how to interact in the world. One of the jobs of this initial set of cognitive tools, however, is to create better cognitive tools. One form this sort of toolbuilding may take is cognitive schema learning the learning of schemata carrying out cognitive processes in more specialized, context-dependent ways than the general MindAgents do. Eventually, once a CogPrime instance becomes sufciently complex and advanced, these cognitive schema may replace the MindAgents altogether, leaving the system to operate almost entirely based on cognitive schemata.",Engineering General  Intelligence Part 1,chapter 19
"In order to make the process of cognitive schema learning easier, we may provide a number of elementary schemata embodying the basic cognitive processes contained in the MindAgents. Of course, cognitive schemata need not use these they may embody entirely different cognitive processes than the MindAgents. Eventually, we want the system to discover better ways of doing things than anything even hinted at by its initial MindAgents. But for the initial phases or the systems schema learning, it will have a much easier time learning to use the basic cognitive operations as the initial MindAgents, rather than inventing new ways of thinking from scratch! For instance, we may provide elementary schemata corresponding to inference operations, such as Schema : Deduction Input InheritanceLink : X, Y Output InheritanceLink The inference MindAgents apply this rule in certain ways, designed to be reasonably effective in a variety of situations. But there are certainly other ways of using the deduction rule, outside of the basic control strategies embodied in the inference MindAgents.",Engineering General  Intelligence Part 1,chapter 19
"By learning schemata involving the Deduction schema, the system can learn special, context-specic rules for combining deduction with concept-formation, association-formation and other cognitive processes. And as it gets smarter, it can then take these schemata involving the Deduction schema, and replace it with a new schema that eg. contains a context-appropriate deduction formula. Eventually, to support cognitive schema learning, we will want to cast the hard-wired MindAgents as cognitive schemata, so the system can see what is going on inside them. Pragmatically, what this requires is coding versions of the MindAgents in Combo (see Chap. 3 of Part 2) rather than C++, so they can be treated like any other cognitive schemata; or alternately, representing them as declarative      368 19 Advanced Self-Modication: A Possible Path to Superhuman AGI Atoms in the Atomspace. Figure19.",Engineering General  Intelligence Part 1,chapter 19
"1 illustrates the possibility of representing the PLN deduction rule in the Atomspace rather than as a hard-wired procedure coded in C++. But even prior to this kind of fully cognitively transparent implementation, the system can still reason about its use of different mind dynamics by considering each MindAgent as a virtual Procedure with a real SchemaNode attached to it. This can lead to some valuable learning, with the obvious limitation that in this approach the system is thinking about its MindAgents as black boxes rather than being equipped with full knowledge of their internals. 19.3 Self-Modication via Supercompilation Now we turn to a very different form of advanced self-modication: supercompilation. Supercompilation merely enables procedures to run much, much faster than they otherwise would. This is in a sense weaker than self-modication methods that fundamentally create new algorithms, but it shouldnt be underestimated.",Engineering General  Intelligence Part 1,chapter 19
"A 50x speedup in some cognitive process can enable that process to give much smarter answers, which can then elicit different behaviors from the world or from other cognitive processes, thus resulting in a qualitatively different overall cognitive dynamic. Furthermore, we suspect that the internal representation of programs used for supercompilation is highly relevant for other kinds of self-modication as well. Supercompilation requires one kind of reasoning on complex programs, and goaldirected program creation requires another, but both, we conjecture, can benet from the same way of looking at programs. Supercompilation is an innovative and general approach to global program optimization initially developed by Valentin Turchin. In its simplest form, it provides an algorithm that takes in a piece of software and output another piece of software that does the same thing, but far faster and using less memory.",Engineering General  Intelligence Part 1,chapter 19
"It was introduced to the West in Turchins 1986 technical paper The concept of a supercompiler [TV96], and since this time the concept has been avidly developed by computer scientists in Russia, America, Denmark and other nations. Although it was never brought to maturity, their partially complete Java supercompiler had some interesting practical successesincluding the use of the supercompiler to produce efcient Java code from CogPrime combinator trees. Since that time work on supercompilation has continued, see e.g. [Mit08] for recent work on Haskell supercompilation The radical nature of supercompilation may not be apparent to those unfamiliar with the usual art of automated program optimization. Most approaches to program optimization involve some kind of direct program transformation. A program is transformed, by the step by step application of a series of equivalences, into a different program,hopefullyamoreefcientone.Supercompilationtakesadifferentapproach.",Engineering General  Intelligence Part 1,chapter 19
"A supercompiler studies a program and constructs a model of the programs dynamics. This model is in a special mathematical form, and it can, in most cases, be used to create an efcient program doing the same thing as the original one.      19.3 Self-Modication via Supercompilation 369 Fig. 19.1 Representation of PLN deduction rule as cognitive content. Top the current, hard-coded representation of the deduction rule. Bottom representation of the same rule in the atomspace as cognitive content, susceptible to analysis and improvement by the systems own cognitive processes      370 19 Advanced Self-Modication: A Possible Path to Superhuman AGI The internal behavior of the supercompiler is, not surprisingly, quite complex; what we will give here is merely a brief high-level summary. For an accessible overview of the supercompilation algorithm, the reader is referred to the article What is Supercompilation? [bGBK02]. 19.3.",Engineering General  Intelligence Part 1,chapter 19
"1 Three Aspects of Supercompilation There are three separate levels to the supercompilation idea: rst, a general philosophy; second a translation of this philosophy into a concrete algorithmic framework; and third, the manifold details involved making this algorithmic framework practicable in a particular programming language. The third level is much more complicated in the Java context than it would be for Sasha, for example. The key philosophical concept underlying the supercompiler is that of a metasystem transition. In general, this term refers to a transition in which a system that previously had relatively autonomous control, becomes part of a larger system that exhibits signicant controlling inuence over it. For example, in the evolution of life, when cells rst become part of a multicellular organism, there was a metasystem transition, in that the primary nexus of control passed from the cellular level to the organism level.",Engineering General  Intelligence Part 1,chapter 19
"The metasystem transition in supercompilation consists of the transition from consideringaprograminitself,toconsideringametaprogram whichexecutesanother program, treating its free variables and their interdependencies as a subject for its mathematical analysis. In other words, a metaprogram is a program that accepts a program as input, and then runs this program, keeping the inputs in the form of free variables, doing analysis along the way based on the way the program depends on these variables, and doing optimization based on this analysis. A CogPrime schema does not explicitly contain variables, but the inputs to the schema are implicitly variablesthey vary from one instance of schema execution to the nextand may be treated as such for supercompilation purposes. The metaprogram executes a program without assuming specic values for its input variables, creating a tree as it goes along.",Engineering General  Intelligence Part 1,chapter 19
"Each time it reaches a statement that can have different results depending on the values of one or more variables, it creates a new node in the tree. This part of the supercompilation algorithm is called drivinga process which, on its own, would create a very large tree, corresponding to a rapidly-executable but unacceptably humongous version of the original program. In essence, driving transforms a program into a huge decision tree, wherein each input to the program corresponds to a single path through the tree, from the root to one of the leaves. As a program input travels through the tree, it is acted on by the atomic program step living at each node. When one of the leaves is reached, the pertinent leaf node computes the output value of the program.",Engineering General  Intelligence Part 1,chapter 19
"The other part of supercompilation, conguration analysis, is focused on dynamically reducing the size of the tree created by driving, by recognizing patterns among the nodes of the tree and taking steps like merging nodes together, or deleting redun     19.3 Self-Modication via Supercompilation 371 dant subtrees. Conguration analysis transforms the decision tree created by driving into a decision graph, in which the paths taken by different inputs may in some cases begin separately and then merge together. Finally, the graph that the metaprogram creates is translated back into a program, embodying the constraints implicit in the nodes of the graph. This program is not likely to look anything like the original program that the metaprogram started with, but it is guaranteed to carry out the same function [NOTE: Give a graphical representation of the decision graph corresponding to the supercompiled binary search program for L = 4, described above]. 19.3.",Engineering General  Intelligence Part 1,chapter 19
"2 Supercompilation for Goal-Directed Program Modication Supercompilation, as conventionally envisioned, is about making programs run faster; and as noted above, it will almost certainly be useful for this purpose within CogPrime. But the process of program modeling embedded in the supercompilation process, is potentially of great value beyond the quest for faster software. The decision graph representation of a program, produced in the course of supercompilation, may be exported directly into CogPrime as a set of logical relationships. Essentially, each node of the supercompilers internal decision graph looks like: Input: List L Output: List If ( P1(L) ) N1(L) Else If ( P2(L) ) N2(L) ... Else If ( Pk(L) ) Nk(L) where the Pi are predicates, and the Ni are schemata corresponding to other nodes of the decision graph (children of the current node).",Engineering General  Intelligence Part 1,chapter 19
"Often the Pi are very simple, implementing for instance numerical inequalities or Boolean equalities. Once this graph has been exported into CogPrime, it can be reasoned on, used as raw material for concept formation and predicate formation, and otherwise cognized. Supercompilation pure and simple does not change the I/O behavior of the input program. However, the decision graph produced during supercompilation, may be used by CogPrime cognition in order to do so. One then has a hybrid programmodication method composed of two phases: supercompilation for transforming programs into decision graphs, and CogPrime cognition for modifying decision      372 19 Advanced Self-Modication: A Possible Path to Superhuman AGI graphs so that they can have different I/O behaviors fullling system goals even better than the original. Furthermore, it seems likely that, in many cases, it may be valuable to have the supercompiler feed many different decision-graph representations of a program into CogPrime.",Engineering General  Intelligence Part 1,chapter 19
"The supercompiler has many internal parameters, and varying them may lead to signicantly different decision graphs. The decision graph leading to maximal optimization, may not be the one that leads CogPrime cognition in optimal directions. 19.4 Self-Modication via Theorem-Proving Supercompilation is a potentially very valuable tool for self-modication. If one wants to take an existing schema and gradually improve it for speed, or even for greater effectiveness at achieving current goals, supercompilation can potentially do that most excellently. However, the representation that supercompilation creates for a program is very surface-level. No one could read the supercompiled version of a program and understand what it was doing. Really deep self-invented AI innovation requires, we believe, another level of self-modication beyond that provided by supercompilation. This other level, we believe, is best formulated in terms of theorem-proving [RV01].",Engineering General  Intelligence Part 1,chapter 19
"Deep self-modication could be achieved if CogPrime were capable of proving theorems of a certain form: namely, theorems about the spacetime complexity and accuracy of particular compound schemata, on average, assuming realistic probability distributions on the inputs, and making appropriate independence assumptions. These are not exactly the types of theorems that are found in human-authored mathematics papers. By and large they will be nasty, complex theorems, not the sort that many human mathematicians enjoy proving or reading. But of course, there is always the possibility that some elegant gem of a discovery could emerge from this sort of highly detailed theorem-proving work. In order to guide it in the formulation of theorems of this nature, the system will have empirical data on the spacetime complexity of elementary schemata, and on the probability distributions of inputs to schemata.",Engineering General  Intelligence Part 1,chapter 19
"It can embed these data in axioms, by asking: Assuming the component elementary schemata have complexities within these bounds, and the input pdf (probability distribution function) is between these bounds,thenwhatisthepdfofthecomplexityandaccuracyofthiscompoundschema? Of course, this is not an easy sort of question in general: one can have schemata embodying any sort of algorithm, including complex algorithms on which computer science professors might write dozens of research articles. But the system must build up its ability to prove such things incrementally, step by step. Weenvisionteachingthesystemtoprovetheoremsviaacombinationofsupervised learning and experiential interactive learning, using the Mizar database of mathematical theorems and proofs (or some other similar database, if one should be created) (http://mizar.org).",Engineering General  Intelligence Part 1,chapter 19
"The Mizar database consists of a set of articles, which are mathematical theorems and proofs presented in a complex formal language. The Mizar      19.4 Self-Modication via Theorem-Proving 373 formal language occupies a fascinating middle ground: it is high-level enough to be viably read and written by trained humans, but it can be unambiguously translated into simpler formal languages such as predicate logic or Sasha. CogPrime may be taught to prove theorems by training it on the Mizar theorems and proofs, and by training it on custom-created Mizar articles specically focusing on the sorts of theorems useful for self-modication. Creating these articles will not be a trivial task: it will require proving simple and then progressively more complex theorems about the probabilistic success of CogPrime schemata, so that CogPrime can observe ones proofs and learned from them.",Engineering General  Intelligence Part 1,chapter 19
"Having learned from its training articles what strategies work for proving things about simple compound schemata, it can then reason by analogy to mount attacks on slightly more complex schemata and so forth. Clearly, this approach to self-modication is more difcult to achieve than the supercompilation approach. But it is also potentially much more powerful. Even once the theorem-proving approach is working, the supercompilation approach will still be valuable, for making incremental improvements on existing schema, and for the peculiar creativity that is contributed when a modied supercompiled schema is compressed back into a modied schema expression. But, we dont believe that supercompilation can carry out truly advanced MindAgent learning or knowledgerepresentation modication. We suspect that the most advanced and ambitious goals of self-modication probably cannot be achieved except through some variant of the theorem-proving approach. If this hypothesis is true, it means that truly advanced self-modication is only going to come after relatively advanced theorem-proving ability.",Engineering General  Intelligence Part 1,chapter 19
"Prior to this, we will have schema optimization, schema modication, and occasional creative schema innovation. But really systematic, high-quality reasoning about schema, the kind that can produce an orders of magnitude improvement in intelligence, is going to require advanced mathematical theorem-proving ability.   ",Engineering General  Intelligence Part 1,chapter 19
